{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, download_url\n",
        "import networkx as nx\n",
        "from networkx.algorithms import community\n",
        "from torch_geometric.utils import to_networkx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from pyvis.network import Network\n",
        "import string\n",
        "from colorama import Fore\n",
        "import termtables as tt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./Data/Net-content.json\",\"r\", encoding='utf-8') as f:\n",
        "    re_num = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Demo_Data(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "    #file\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "   \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "    # download from website\n",
        "    # def download(self):\n",
        "    #     # Download to `self.raw_dir`.\n",
        "    #     download_url(url, self.raw_dir)\n",
        "        ...\n",
        "    #data generate\n",
        "    def process(self):\n",
        "        # Read data into huge `Data` list.\n",
        "        # Read data into huge `Data` list.\n",
        "        # building\n",
        "        \n",
        "        start=0\n",
        "        end=len(list_source)\n",
        "        \n",
        "        Edge_index = torch.tensor([list_source,\n",
        "                                   list_target], dtype=torch.long)\n",
        "        \n",
        "        # s=list_source[start:end].copy()\n",
        "        # s.extend(list_target[start:end])\n",
        "        print(len(re_num))\n",
        "        \n",
        "        # node_num=len(set(s))\n",
        "\n",
        "        # node feature\n",
        "        sample_feature=[0]*len(re_num)\n",
        "        X = torch.tensor(np.array(sample_feature).reshape(-1, 1).tolist(), dtype=torch.float)\n",
        "        # node label\n",
        "        # Y = torch.tensor([0,1,0],dtype=torch.float)\n",
        "        \n",
        "        #building\n",
        "        # data = Data(x=X, edge_index=Edge_index, y=Y)\n",
        "        data = Data(x=X,edge_index=Edge_index)\n",
        "        \n",
        "        # put in datalist\n",
        "        data_list = [data]\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        data.num_nodes = len(re_num)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Demo_Data(\"Whole_Network\")\n",
        "data=dataset[0]\n",
        "dataset.get_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display \n",
        "G = to_networkx(data, to_undirected=False)# Directed\n",
        "degrees = [val for (node, val) in G.degree()]\n",
        "display(pd.DataFrame(pd.Series(degrees).describe()).transpose().round(2))\n",
        "print(len(degrees))\n",
        "print(sum(degrees))\n",
        "plt.figure(figsize=(10, 6))\n",
        "# plt.hist(degrees, bins=100)\n",
        "# plt.hist(degrees, range(0,10))\n",
        "plt.hist(degrees, range(0,50),rwidth=0.5,align=\"left\")\n",
        "\n",
        "plt.xlabel(\"node degree\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#nltk_word\n",
        "def word_div(strings):\n",
        "    strings=strings.lower()\n",
        "    strings=strings.translate(str.maketrans('', '', string.punctuation))\n",
        "    token=nltk.word_tokenize(strings)\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens=[]\n",
        "    for i_2 in range(len(token)):\n",
        "        if token[i_2] not in stop_words:\n",
        "            tokens.append(token[i_2])   \n",
        "    filter_word=['from', 'subject', 're', 'edu', 'use','introduction','conclusion','evaluation','figure','table','survey','et','al','research','we','use','work','issue','also','future','however','finally','task','tasks','paper','nlp','recent','natural','language','processing','nevertheless']\n",
        "    clean_token=[]\n",
        "    for item in tokens:\n",
        "        item=lemmatizer.lemmatize(item, pos=\"n\")\n",
        "        if item not in filter_word and re.search(r'^\\d+$',item) is None:\n",
        "            # clean_token.append(st.stem(item))\n",
        "            clean_token.append(item)\n",
        "    return clean_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "st = LancasterStemmer()\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('scibert_scivocab_uncased')\n",
        "# model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
        "sentences=[]\n",
        "strings_total=''\n",
        "branch=2\n",
        "pair_list=[]\n",
        "pair_list_total=[]\n",
        "for item in re_num:\n",
        "    if 'conclusion' not in re_num[item].keys():\n",
        "        # print(re_num[item]['num'])\n",
        "        continue\n",
        "    \n",
        "    sentences.append(re_num[item]['conclusion'])\n",
        "    strings_total=strings_total+re_num[item]['conclusion']\n",
        "    \n",
        "\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i_1 in range(len(cosine_scores)-1):\n",
        "    for j_1 in range(i_1+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i_1, j_1], 'score': cosine_scores[i_1][j_1]})\n",
        "\n",
        "#Sort scores in decreasing order\n",
        "pairs_dec = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "#Simliarity of each article (n-branch)\n",
        "start=0\n",
        "for i in range(1,len(pairs)):\n",
        "    strings_pair=''\n",
        "    flag=pairs[i-1]['index'][0]\n",
        "    i_1, j_1 = pairs[i]['index']\n",
        "    if i_1!=flag:\n",
        "        pair=pairs[start:i]\n",
        "        pair = sorted(pair, key=lambda x: x['score'], reverse=True)\n",
        "        # for building tree\n",
        "        for j in range(branch):\n",
        "            pair[j]['s1']=sentences[pair[j]['index'][0]]\n",
        "            pair[j]['s2']=sentences[pair[j]['index'][1]]\n",
        "            strings_pair=sentences[pair[j]['index'][0]]+sentences[pair[j]['index'][1]]\n",
        "            # print(Fore.BLACK + \"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[pair[j]['index'][0]], sentences[pair[j]['index'][1]], pair[j]['score']))\n",
        "\n",
        "            #nltk->top:10\n",
        "            clean_token=word_div(strings_pair)\n",
        "            freq_dist = nltk.FreqDist(clean_token)\n",
        "            # print(Fore.BLUE + 'hypernym:',freq_dist.most_common(10))\n",
        "            pair[j]['hypernym']=freq_dist.most_common(10)\n",
        "            \n",
        "            #nltk ->common_word\n",
        "            S_0=word_div(sentences[pair[j]['index'][0]])\n",
        "            S_1=word_div(sentences[pair[j]['index'][1]])\n",
        "            and_list=set(S_0)&set(S_1)\n",
        "            # print(Fore.MAGENTA + 'common_word:',and_list)\n",
        "            pair[j]['common_word']=and_list\n",
        "\n",
        "            #add data to list\n",
        "            pair_list.append(pair[j])\n",
        "\n",
        "        start=i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for j in range(len(pairs)):\n",
        "    pairs[j]['s1']=sentences[pairs[j]['index'][0]]\n",
        "    pairs[j]['s2']=sentences[pairs[j]['index'][1]]\n",
        "    strings_pair=sentences[pairs[j]['index'][0]]+sentences[pairs[j]['index'][1]]\n",
        "    # print(Fore.BLACK + \"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[pair[j]['index'][0]], sentences[pair[j]['index'][1]], pair[j]['score']))\n",
        "\n",
        "    #nltk->top:10\n",
        "    clean_token=word_div(strings_pair)\n",
        "    freq_dist = nltk.FreqDist(clean_token)\n",
        "    # print(Fore.BLUE + 'hypernym:',freq_dist.most_common(10))\n",
        "    pairs[j]['hypernym']=freq_dist.most_common(10)\n",
        "\n",
        "    #nltk -> common word \n",
        "    S_0=word_div(sentences[pairs[j]['index'][0]])\n",
        "    S_1=word_div(sentences[pairs[j]['index'][1]])\n",
        "    and_list=set(S_0)&set(S_1)\n",
        "    # print(Fore.MAGENTA + 'common_word:',and_list)\n",
        "    pairs[j]['common_word']=and_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pair_list"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gnn_citations",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
