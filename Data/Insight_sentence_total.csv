,corpusid,paper-title,section-title,Target,Sentence,Label,pdf-url
0,221971009,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,conclusion,Insight-tree,our findings also suggest promising future research directions.,Unsolved,https://www.aclweb.org/anthology/2021.naacl-main.47.pdf
1,221971009,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,conclusion,Insight-tree,"the proposed method does not support multi-hop reasoning, an important attribute that enables qa systems to answer more complex questions that require collecting multiple evidence passages.",Unsolved,https://www.aclweb.org/anthology/2021.naacl-main.47.pdf
2,211572557,TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing,conclusion and future work,Insight-tree,textbrewer also has its limitations.,Unsolved,https://www.aclweb.org/anthology/2020.acl-demos.2.pdf
3,211572557,TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing,conclusion and future work,Insight-tree,"for example, its usability in generation tasks such as machine translation has not been tested.",Unsolved,https://www.aclweb.org/anthology/2020.acl-demos.2.pdf
4,211572557,TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing,conclusion and future work,Insight-tree,we will keep adding more examples and tests to expand textbrewer's scope of application.,Unsolved,https://www.aclweb.org/anthology/2020.acl-demos.2.pdf
5,256461186,Graph-Induced Transformers for Efficient Multi-Hop Question Answering,conclusions,Insight-tree,future directions of our work may include using git in downstream nlp applications where the graph inductive bias is necessary and dataset is scarce.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.702.pdf
6,225067629,AQUAMUSE: Automatically Generating Datasets for Query-Based Multi-Document Summarization,conclusion,Insight-tree,our experiments demonstrates that there is still much headroom for existing state-of-the-art models and we hope aquamuse will spur further advancements query focused multi-document summarization algorithms.,Unsolved,https://arxiv.org/pdf/2010.12694v1.pdf
7,245218995,Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants,discussion and conclusion,Insight-tree,"we find that while gaas are effective for improving standard data collection, we still do not approach the performance obtained when using adversarial data collection.",Unsolved,https://www.aclanthology.org/2022.naacl-main.275.pdf
8,244954717,Does Structure Matter? Leveraging Data-to-Text Generation for Answering Complex Information Needs,conclusion,Insight-tree,experimental evaluation of a planning-based dtt model using the trec car dataset shows the potential of our intuition.,Unsolved,https://arxiv.org/pdf/2112.04344v1.pdf
9,244954717,Does Structure Matter? Leveraging Data-to-Text Generation for Answering Complex Information Needs,conclusion,Insight-tree,we believe that our work opens up novel areas of investigation including answer generation and explanation in conversational systems for ir.,Unsolved,https://arxiv.org/pdf/2112.04344v1.pdf
10,237737294,Dual-Channel Reasoning Model for Complex Question Answering,conclusion and future work,Insight-tree,"in the future, we believe that the following issue will be worth studying.",Unsolved,
11,237737294,Dual-Channel Reasoning Model for Complex Question Answering,conclusion and future work,Insight-tree,"for the dual-channel reasoning architecture, the interaction strategy between the two channels, such as the soft parameter sharing of the homogeneous neural network components of the two channels, is worthy of further study.",Unsolved,
12,57721315,Multi-Style Generative Reading Comprehension,conclusion,Insight-tree,our future work will involve exploring the potential of our multi-style learning towards natural language understanding.,Unsolved,https://arxiv.org/pdf/1901.02262v2.pdf
13,235399966,FINQA: A Dataset of Numerical Reasoning over Financial Data,conclusion and future work,Insight-tree,"the questions in finqa pose great challenge for existing models to resolve domain-specific knowledge, as well as to acquire complex numerical reasoning abilities.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.300.pdf
14,235399966,FINQA: A Dataset of Numerical Reasoning over Financial Data,conclusion and future work,Insight-tree,"this encourages potential future work on developing pre-training tasks for such realistic, complex application domains.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.300.pdf
15,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,limitation,Insight-tree,a limitation of this approach is the trade-off between completeness and noise in the training data.,Unsolved,https://export.arxiv.org/pdf/2212.09282v2.pdf
16,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,limitation,Insight-tree,"while our method using keywords to extract text from wikipedia is effective, implication likely contains redundant sentences that cannot improve the model's logical reasoning capability.",Unsolved,https://export.arxiv.org/pdf/2212.09282v2.pdf
17,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,limitation,Insight-tree,a better rule-based or neural model might be able to extract a better corpus with potentially higher computational costs.,Unsolved,https://export.arxiv.org/pdf/2212.09282v2.pdf
18,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,limitation,Insight-tree,"additionally, using pos tagging limits the application of this approach to languages with well-defined pos taggers.",Unsolved,https://export.arxiv.org/pdf/2212.09282v2.pdf
19,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,limitation,Insight-tree,"switching to a more universal semantic tagging system (abzianidze and bos, 2017) can potentially alleviate this.",Unsolved,https://export.arxiv.org/pdf/2212.09282v2.pdf
20,173188058,MULTIQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension,conclusions,Insight-tree,performance over an rc dataset can be improved by retrieving web snippets for all questions and adding them as examples (context augmentation).,Unsolved,https://arxiv.org/pdf/1905.13453v1.pdf
21,245334850,MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding,conclusions and future work,Insight-tree,"the multimedia baselines are still considerably behind human performance, suggesting ample room for improvement.",Unsolved,https://arxiv.org/pdf/2112.10728v2.pdf
22,245334850,MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding,conclusions and future work,Insight-tree,"future work will incorporate other forms of media in news, such as video and audio, to facilitate information seeking from more comprehensive data sources.",Unsolved,https://arxiv.org/pdf/2112.10728v2.pdf
23,245334850,MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding,conclusions and future work,Insight-tree,another direction is to infuse the endto-end multimedia qa system with additional input from the grounding and visual attribute extraction systems.,Unsolved,https://arxiv.org/pdf/2112.10728v2.pdf
24,254853987,Source-Free Domain Adaptation for Question Answering with Masked Self-training,limitation,Insight-tree,"however, since available medical qa datasets are extremely limited and hard to access, currently we still conduct experiments on commonly used general-purpose qa datasets.",Unsolved,https://export.arxiv.org/pdf/2212.09563v1.pdf
25,256846551,STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK,conclusion,Insight-tree,future avenues of research include exploring the reasoning capabilities and knowledge retrieval and using supervised models trained on multi-step reasoning data to bootstrap unsupervised learning for multi-step reasoning.,Unsolved,https://export.arxiv.org/pdf/2302.06729v1.pdf
26,258865893,Mixture of Prompt Experts for Generalizable and Interpretable Question Answering,conclusion,Insight-tree,"while we focused exclusively on prompt experts, the idea of combining the strengths of diverse specialized models can extend to any type of specialized qa models, even non-neural models such as traditional information retrieval models.",Unsolved,https://export.arxiv.org/pdf/2305.14628v1.pdf
27,258865893,Mixture of Prompt Experts for Generalizable and Interpretable Question Answering,conclusion,Insight-tree,we leave such extensions to future exploration.,Unsolved,https://export.arxiv.org/pdf/2305.14628v1.pdf
28,257901155,UKP-SQuARE v3: A Platform for Multi-Agent QA Research,conclusions and discussions,Insight-tree,"however, when simple deployment is needed or the model is not expected to be updated, made and unifiedqa might be more appropriate.",Unsolved,https://export.arxiv.org/pdf/2303.18120v2.pdf
29,215737171,Longformer: The Long-Document Transformer,conclusion and future work,Insight-tree,"for future work, we would like to explore other attention patterns that are more efficient by dynamically adapting to the input.",Unsolved,https://arxiv.org/pdf/2004.05150v1.pdf
30,215737171,Longformer: The Long-Document Transformer,conclusion and future work,Insight-tree,we also would like to apply our model to other relevant long document tasks such as summarization.,Unsolved,https://arxiv.org/pdf/2004.05150v1.pdf
31,211258652,Training Question Answering Models From Synthetic Data,conclusion,Insight-tree,we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.,Unsolved,https://arxiv.org/pdf/2002.09599v1.pdf
32,248085193,Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering,conclusion,Insight-tree,"in the future, we hope to further close the gap with state-of-the-art documentbased retrieve-and-read models and extend this approach to a broader set of tasks.",Unsolved,https://export.arxiv.org/pdf/2204.04581v3.pdf
33,201646361,Movie Plot Analysis via Turning Point Identification,conclusions,Insight-tree,"in future work, we will investigate the usefulness of tps for summarization and question answering.",Unsolved,https://www.aclweb.org/anthology/D19-1180.pdf
34,201646361,Movie Plot Analysis via Turning Point Identification,conclusions,Insight-tree,we will also scale the tripod dataset and move to a multi-modal setting where tps are identified directly in video data.,Unsolved,https://www.aclweb.org/anthology/D19-1180.pdf
35,221970190,What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams,conclusion,Insight-tree,we anticipate more research efforts from the community can be devoted to this dataset so that future openqa models can be strong enough to solve such real-world complex problems.,Unsolved,https://arxiv.org/pdf/2009.13081v1.pdf
36,248366293,Association for Computational Linguistics,conclusion,Insight-tree,"while our results show that seal could already compete with more established retrieval systems, we believe there is potential in exploring the use of existing (or yet to come) larger autoregressive models.   ",Unsolved,https://arxiv.org/pdf/2204.10628v1.pdf
37,232075995,Published as a conference paper at ICLR 2021 LEARNING REASONING PATHS OVER SEMANTIC GRAPHS FOR VIDEO-GROUNDED DIALOGUES,conclusion,Insight-tree,"we hope our approach can motivate further study to investigate reasoning over multiple turns, especially in complex settings with interconnected dialogue flows (sun et al., 2019).",Unsolved,https://export.arxiv.org/pdf/2103.00820v2.pdf
38,234334701,REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training,conclusion and future work,Insight-tree,"in the future, we plan to extend the proposed pre-training approach to the more challenging open-domain settings.",Unsolved,https://arxiv.org/pdf/2105.04201v2.pdf
39,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,conclusion,Insight-tree,our in-depth analysis suggests several concrete directions for future work: improving the in-distribution performance of icl methods and understanding why different few-shot fine-tuning methods yield the squadshifts wiki dataset is derived from the same data source (wikipedia) as squad.,Unsolved,https://export.arxiv.org/pdf/2210.12517v1.pdf
40,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,one limitation of squad is that the training set is mainly collected from wikipedia articles which may not be optimal for building a qa model that generalizes to many domains.,Unsolved,https://export.arxiv.org/pdf/2210.12517v1.pdf
41,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,future work could explore the robustness of models trained on datasets from other domains for increased coverage.specialized modeling methods.,Unsolved,https://export.arxiv.org/pdf/2210.12517v1.pdf
42,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,our work does not evaluate models with task or data specific components.,Unsolved,https://export.arxiv.org/pdf/2210.12517v1.pdf
43,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,evaluating the robustness of methods like these are an exciting area for future investigations.few-shot gpt evaluations.,Unsolved,https://export.arxiv.org/pdf/2210.12517v1.pdf
44,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,future work could examine the impact of architecture on this trend by evaluating other models with more than a billion parameters like t5.multiple fine-tuning runs.,Unsolved,https://export.arxiv.org/pdf/2210.12517v1.pdf
45,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,further investigation into the effect of random seeds on robustness would improve our understanding of the robustness of individual data points.,Unsolved,https://export.arxiv.org/pdf/2210.12517v1.pdf
46,237347226,"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning",conclusion,Insight-tree,future works include extending our work to task agnostic scenarios where the distribution of data may shift continuously and studying algorithms for continual refinement of large-scale pre-trained models with emerging unlabeled data.,Unsolved,https://export.arxiv.org/pdf/2104.08808v4.pdf
47,222140958,Context Modeling with Evidence Filter for Multiple Choice Question Answering,conclusions,Insight-tree,our future work is to enhance the evidence filter by more complex components.,Unsolved,https://arxiv.org/pdf/2010.02649v1.pdf
48,259096138,Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation,conclusion and future work,Insight-tree,"looking forward, it is of interest to explore qa-dedicated pre-training and ways of pipelining pretraining and prompt-tuning for downstream few-shot qa needs.",Unsolved,https://export.arxiv.org/pdf/2306.04101v1.pdf
49,246638887,E M E R G I N G T R E N D S Emerging Trends: SOTA-Chasing,conclusions/recommendations,Insight-tree,"many papers are sota-chasing, and more will do so in the future.",Unsolved,
50,237420912,FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models,conclusion,Insight-tree,"for future, as our framework doesn't explicitly enforce the answer to be a span in the input text, it'd be interesting to consider its applications to generative qa tasks.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.491.pdf
51,258959022,Plug-and-Play Document Modules for Pre-trained Models 5 Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,"in the future, we will explore more effective plugin learning tasks and further attempt to represent knowledge graphs, and figures as plugins to provide knowledge for ptms.",Unsolved,https://export.arxiv.org/pdf/2305.17660v1.pdf
52,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,limitations,Insight-tree,a more efficient parameter-free routing approach can be explored in the future.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.356.pdf
53,253244513,Natural Language Deduction with Incomplete Information,conclusion,Insight-tree,"future work can improve our system by scaling up the models used, plus using additional notions of validation as discussed in the error analysis.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
54,253244513,Natural Language Deduction with Incomplete Information,conclusion,Insight-tree,we believe our overall framework can be a promising foundation for future reasoning systems.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
55,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"with better heuristics and validators it may be possible to reduce the time complexity further, but that is left for future work.both the entailmentbank and enwn dataset were written in english and capture relatively limited domains of textual reasoning.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
56,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,different languages might introduce easier lexical patterns for abstraction though and could be a promising path forward.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
57,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"we believe adgv and its variants should work on non-english languages, but testing this was left to future work.enwn draws on everyday ethical scenarios because this was a domain we found fruitful to exhibit the kind of reasoning our system can do",Unsolved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
58,253157925,DyREx: Dynamic Query Representation for Extractive Question Answering,conclusion,Insight-tree,"in future work, it would be interesting to adapt dyrex for multi-span extraction tasks such as named entity recognition and keyphrase extraction.",Unsolved,https://export.arxiv.org/pdf/2210.15048v1.pdf
59,250729995,ScienceQA: a novel resource for question answering on scholarly articles,conclusion and future work,Insight-tree,our future works would include:,Unsolved,
60,222272049,Case Study: Deontological Ethics in NLP,conclusion,Insight-tree,two principles of deontological ethics-namely the generalization principle and respect for autonomy via informed consent-can be used to decide if an action is ethical.,Unsolved,https://www.aclweb.org/anthology/2021.naacl-main.297.pdf
61,222272049,Case Study: Deontological Ethics in NLP,conclusion,Insight-tree,"despite the limitations of these principles, they can provide useful insights into making nlp systems more ethical.",Unsolved,https://www.aclweb.org/anthology/2021.naacl-main.297.pdf
62,222272049,Case Study: Deontological Ethics in NLP,conclusion,Insight-tree,"for each of the case studies, we also present potential directions for nlp research to move forward and make the system more ethical.",Unsolved,https://www.aclweb.org/anthology/2021.naacl-main.297.pdf
63,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"in addition, the authors argue that memorisation is an important part of linguistic competence, and as such probes should not be artificially punished (via control tasks) for doing this.",Unsolved,https://export.arxiv.org/pdf/2103.11072v2.pdf
64,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"furthermore, the significance of well-performing probes is not clear: models may encode linguistic information not actually used by the end-task [138], showing that the presence of linguistic information does not imply it is being used for prediction.",Unsolved,https://export.arxiv.org/pdf/2103.11072v2.pdf
65,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"more causal approaches such as amnesiac probing [50], which directly intervene in the underlying model's representations, may better distinguish between these cases.",Unsolved,https://export.arxiv.org/pdf/2103.11072v2.pdf
66,238856994,Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,conclusion and future work,Insight-tree,"both of the two main contributions are aimed to build the pareto frontier for nlu tasks, such that the position of existing work can be clearly recognized, and future work can be easily and fairly measured.",Unsolved,https://www.aclanthology.org/2022.naacl-main.240.pdf
67,238856994,Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,conclusion and future work,Insight-tree,"our future work is mainly in four aspects: (1) including more baselines in elue, (2) supporting the evaluation for more frameworks such as tensor-flow (abadi et al., 2016)",Unsolved,https://www.aclanthology.org/2022.naacl-main.240.pdf
68,214802134,Graph Sequential Network for Reasoning over Sequences,conclusion,Insight-tree,for future work we would like to apply gsn to other applications in nlp that require complex reasoning.,Unsolved,https://arxiv.org/pdf/2004.02001v1.pdf
69,236477844,PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval,conclusion and future work,Insight-tree,"in future work, we will design more principle ranking functions and apply current retrieval approach to downstream tasks such as question answering and passage re-ranking.",Unsolved,https://export.arxiv.org/pdf/2108.06027v2.pdf
70,254246737,What is Not in the Context? Evaluation of Few-shot Learners with Informative Demonstrations,conclusion,Insight-tree,"we find that current incontext few-shot learners are largely insensitive to the concepts presented in demonstrations, suggesting that their ability to learn in context might be conditioned by other factors, such as their memo-rization capacity, rather than their level of task comprehension.",Unsolved,https://export.arxiv.org/pdf/2212.01692v1.pdf
71,254246737,What is Not in the Context? Evaluation of Few-shot Learners with Informative Demonstrations,conclusion,Insight-tree,conceptual few-shot learning poses a challenge of more controllable and scalable fewshot learning to future few-shot learners.,Unsolved,https://export.arxiv.org/pdf/2212.01692v1.pdf
72,215238846,Multi-Step Inference for Reasoning Over Paragraphs,conclusion,Insight-tree,we have additionally presented some analysis of ropes that should inform future work on this dataset.,Unsolved,https://arxiv.org/pdf/2004.02995v2.pdf
73,215238846,Multi-Step Inference for Reasoning Over Paragraphs,conclusion,Insight-tree,"while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets.",Unsolved,https://arxiv.org/pdf/2004.02995v2.pdf
74,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,"overall, ca-mtl offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. extending such ideas will be an objective for future work.",Unsolved,https://arxiv.org/pdf/2009.09139v3.pdf
75,252368291,Learning to Answer Semantic Queries over Code,conclusions and future work,Insight-tree,"despite a diverse set of 52 queries requiring varied program analyses, the proposed models perform reasonably well if relevant code is given.",Unsolved,https://export.arxiv.org/pdf/2209.08372v1.pdf
76,252368291,Learning to Answer Semantic Queries over Code,conclusions and future work,Insight-tree,"at the same time, our evaluation under pragmatic considerations indicates that scalability to entire files and learning from a limited number of examples have much room for improvement.",Unsolved,https://export.arxiv.org/pdf/2209.08372v1.pdf
77,252368291,Learning to Answer Semantic Queries over Code,conclusions and future work,Insight-tree,"we plan to explore models with the ability to handle larger contexts (e.g., [dai et al., 2019]), better training of relevance classifier and span-prediction inspired pre-training objectives (e.g., [joshi et al., 2020, ram et al., 2021) in the future.",Unsolved,https://export.arxiv.org/pdf/2209.08372v1.pdf
78,252368291,Learning to Answer Semantic Queries over Code,conclusions and future work,Insight-tree,we could also add many more semantic queries and programming languages to our dataset.,Unsolved,https://export.arxiv.org/pdf/2209.08372v1.pdf
79,236771976,MuSiQue: Multihop Questions via Single-hop Question Composition,conclusion,Insight-tree,"the dataset also comes with unanswerable questions, and question decompositions which we hope spurs further work in developing models that get right answers for the right reasons.",Unsolved,https://www.aclanthology.org/2022.tacl-1.31.pdf
80,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,limitations,Insight-tree,at the implementation and operations level one of the the key limitations our proposed system is the number of inference steps to solve the problem.,Unsolved,https://export.arxiv.org/pdf/2211.08466v1.pdf
81,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,limitations,Insight-tree,the number of times model inference may be needed to solve a single example is equal the length of the longest task sequence chain in the reasoning circuit.,Unsolved,https://export.arxiv.org/pdf/2211.08466v1.pdf
82,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,limitations,Insight-tree,one possible solution for this could be by training the model to solve the entire problem by generating all the steps of reasoning and the target string in a single inference step and could massively reduce inference time and costs.,Unsolved,https://export.arxiv.org/pdf/2211.08466v1.pdf
83,253510370,World Knowledge in Multiple Choice Reading Comprehension,limitations,Insight-tree,"however, the remaining questions are not necessarily high-quality questions, since many other aspects make up question quality.",Unsolved,https://export.arxiv.org/pdf/2211.07040v2.pdf
84,253510370,World Knowledge in Multiple Choice Reading Comprehension,limitations,Insight-tree,"second, the experiments are conducted using only the electra model, though it is expected similar trends will be picked up by alternative transformer-based language models.",Unsolved,https://export.arxiv.org/pdf/2211.07040v2.pdf
85,253510370,World Knowledge in Multiple Choice Reading Comprehension,limitations,Insight-tree,"further, exams might be aimed at a level where a lack of specific knowledge may be assumed.",Unsolved,https://export.arxiv.org/pdf/2211.07040v2.pdf
86,253510370,World Knowledge in Multiple Choice Reading Comprehension,limitations,Insight-tree,"our work does not consider variable candidate knowledge levels, and our evaluation was only done by highly educated (we'd like to think) graduate students.",Unsolved,https://export.arxiv.org/pdf/2211.07040v2.pdf
87,253510370,World Knowledge in Multiple Choice Reading Comprehension,limitations,Insight-tree,"finally, we acknowledge that our human evaluation was limited in size and questions, however it is clearly demonstrated that for low 'shortcut entropy' questions, comprehension is not necessarily required.",Unsolved,https://export.arxiv.org/pdf/2211.07040v2.pdf
88,243756815,Extracting a Knowledge Base of COVID-19 Events from Social Media,conclusion,Insight-tree,"for example, our methodology could be applied to develop knowledge bases for natural disasters (spiliopoulou et al., 2020) or future disease outbreaks.",Unsolved,https://export.arxiv.org/pdf/2006.02567v4.pdf
89,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"despite this evidence, we did not establish that qlora can match full 16-bit finetuning performance at 33b and 65b scales.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
90,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"due to the immense resource costs, we leave this study to future work.another limitation is the evaluation of instruction finetuning models.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
91,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"while we provide evaluations on mmlu, the vicuna benchmark, and the oa benchmark, we did not evaluate on other benchmarks such as bigbench, raft, and helm, and it is not ensured that our evaluations generalize to these benchmarks.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
92,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"on the other hand, we perform a very broad study on mmlu and develop new methods for evaluating chatbots.from the evidence presented, it appears that the performance of these benchmarks likely depends how similar the finetuning data is to the benchmark dataset.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
93,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"this highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
94,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? maybe something else?,Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
95,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"we should ensure as a community that the benchmarks measure what we care about.while we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible ai evaluation of guanaco.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
96,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"while these results are encouraging, it is unclear if guanaco does also well when assessed on other types of biases.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
97,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"we leave further evaluation of analyzing biases in guanaco and similar chatbots to future work.an additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base models, or different adapter methods.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
98,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"besides lora, there is also a wide variety parameter efficient finetuning (peft) methods that have been shown to work well. however, it is unclear if these methods scale to large models.",Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
99,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,we used lora as many results established its robustness but other adapters might yield better performance.,Unsolved,https://export.arxiv.org/pdf/2305.14314v1.pdf
100,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,"however, there are no other discernible differences in ratings in the background condition with or without highlight.",Unsolved,https://export.arxiv.org/pdf/2305.14331v1.pdf
101,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,one of the future directions of this research is assessing domain-specific ptms in other areas of app reviews and exploring ways to increase performance in zero-shot setting.,Unsolved,https://arxiv.org/pdf/2104.05861v3.pdf
102,165163607,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,conclusion,Insight-tree,"future work could include building a document-level version of this task, which would increase its difficulty and its correspondence to an end-user application.",Unsolved,https://www.aclweb.org/anthology/N19-1300.pdf
103,212644640,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,conclusion,Insight-tree,"this framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach.",Unsolved,https://www.aclweb.org/anthology/2020.lrec-1.660.pdf
104,212644640,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,conclusion,Insight-tree,"studying how to introduce those modifiers into gold standards and observing whether state-of-the-art mrc models are capable of performing reading comprehension on text containing them, is a future research goal.",Unsolved,https://www.aclweb.org/anthology/2020.lrec-1.660.pdf
105,212644640,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,conclusion,Insight-tree,a future line of research is to extend the framework to be able to identify the different types of exploitable cues such as question or entity typing and concrete overlap patterns. this will allow the framework to serve as an interpretable estimate of reading comprehension complexity of gold standards.,Unsolved,https://www.aclweb.org/anthology/2020.lrec-1.660.pdf
106,212644640,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,conclusion,Insight-tree,"finally, investigating gold standards under this framework where mrc models outperform the human baseline (e.g. squad) will contribute to a deeper understanding of the seemingly superb performance of deep learning approaches on them.",Unsolved,https://www.aclweb.org/anthology/2020.lrec-1.660.pdf
107,233296243,Learning with Instance Bundles for Reading Comprehension,conclusion,Insight-tree,we believe our results give strong motivation for further work in techniques to both create and use instance bundles in nlp datasets.,Unsolved,https://www.aclanthology.org/2021.emnlp-main.584.pdf
108,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,"future works include systematically studying the effect of background knowledge in mrc, and developing more effective methods to improve the robustness of mrc models.",Unsolved,https://www.aclanthology.org/2022.naacl-main.37.pdf
109,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"however, following the prompt pre-training proposed in gu et al. (2021), a possible future work is to pretrain the prompt contextualizer in a task-agnostic way",Unsolved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
110,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"more extensive experimental analysis and better techniques for compressing the codebook, we leave as future work",Unsolved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
111,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"also, though we use t5 as plm in this work, our vip architecture can be used in bert or gpt style prediction or generation as well.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
112,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"however, a formal analysis of this is left as future work. ",Unsolved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
113,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"the language understanding tasks and datasets were predominantly in the english language, and thus limit our claims to the english language.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
114,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"gender, age, race, and other socioeconomic biases may exist in these datasets, and models trained on these datasets may propagate these biases.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
115,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,"under statistical analysis and downstream evaluation on code generation task, we demonstrate that lang2code is a large-scale and high-quality nl-pl pair corpus and can greatly help developing data-hungry models in future research.",Unsolved,https://arxiv.org/pdf/2203.10744v1.pdf
116,214233456,Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension,conclusion,Insight-tree,experimental results demonstrate the there is still a significant performance gap between the best-performing model (68.5%) and human readers (96.0%) and a need for better ways for exploiting rich resources in other languages.,Unsolved,https://www.aclweb.org/anthology/2020.tacl-1.10.pdf
117,238744204,Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?,conclusion,Insight-tree,for future work we plan to explore if a dense retriever can be trained to learn lexical matching directly without relying on a teacher model.,Unsolved,https://export.arxiv.org/pdf/2110.06918v3.pdf
118,238744204,Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?,conclusion,Insight-tree,"moreover, there are several intriguing findings in this work that may warrant further study, such as why spar's acc@k improves relatively to the hybrid model as k increases, and why joint training is less effective than post-hoc vector concatenation.",Unsolved,https://export.arxiv.org/pdf/2110.06918v3.pdf
119,252873674,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,discussion & conclusion,Insight-tree,"more concerning, the explanations might not be factually grounded in the provided prompt.",Unsolved,https://export.arxiv.org/pdf/2205.03401v2.pdf
120,252873674,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,discussion & conclusion,Insight-tree,"this shortcoming should caution against any deployment of this technology in practice: because the explanations are grammatical english and look very convincing, they may deceive users into believing the system's responses even when those responses are incorrect.",Unsolved,https://export.arxiv.org/pdf/2205.03401v2.pdf
121,248426967,HYBRIDIALOGUE: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data,conclusion,Insight-tree,"in addition to the baseline tasks, future research can utilize hybridialogue to explore automatic multihop question decomposition.",Unsolved,https://www.aclanthology.org/2022.findings-acl.41.pdf
122,254926391,Multi-hop Evidence Retrieval for Cross-document Relation Extraction,conclusion,Insight-tree,"future works include extending mr.cod to more retrieval methods, such as generative dense retrievers.",Unsolved,https://export.arxiv.org/pdf/2212.10786v2.pdf
123,257985191,CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval,conclusion,Insight-tree,"in the future, we will further explore incorporating new pre-training paradigms to get more effective and robust retrievers.",Unsolved,https://export.arxiv.org/pdf/2304.03158v1.pdf
124,207852656,Meta Answering for Machine Reading,conclusion,Insight-tree,mma cannot use the contextual information that is effortlessly exploited by humans.,Unsolved,https://arxiv.org/pdf/1911.04156v2.pdf
125,207852656,Meta Answering for Machine Reading,conclusion,Insight-tree,"thus, it might prove a suitable framework to advance on these challenges.",Unsolved,https://arxiv.org/pdf/1911.04156v2.pdf
126,257532206,Semantic matching based legal information retrieval system for COVID-19 pandemic,conclusion,Insight-tree,"meanwhile, the study could benefit developing a more comprehensive legal ir system and similar systems.",Unsolved,
127,257532206,Semantic matching based legal information retrieval system for COVID-19 pandemic,conclusion,Insight-tree,"in the future, it is worth doing more experiments on more data set to analyse the effect of various neural models for such tasks and accordingly improving the system.",Unsolved,
128,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"despite the positive evidence, it remains to be seen: (i) if task decomposition can be fully automated, and (ii) if different decompositions-in the case of tasks that allow for multiple decompositions-yield similar results.",Unsolved,https://export.arxiv.org/pdf/2210.12607v1.pdf
129,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,we hope that our results will add to theirs in attracting more attention to these questions in the future.another limitation of this work is that cft is not applicable to several decomposition datasets that have been proposed.,Unsolved,https://export.arxiv.org/pdf/2210.12607v1.pdf
130,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,cft is not designed for intermediate steps that carry out very heterogeneous logic.,Unsolved,https://export.arxiv.org/pdf/2210.12607v1.pdf
131,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"nonetheless, as shown in the recommendation tasks, cft is still relevant for a substantial family of tasks with real-world applicability.lastly, this work is limited by its focus on the english language, and by the use of gpt-3 for its unique range of model sizes.",Unsolved,https://export.arxiv.org/pdf/2210.12607v1.pdf
132,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"for example, when we discuss that cft on a 13b parameter model (curie) is a much cheaper alternative to chain of thought prompting on a 175b parameter model (davinci), the finding is limited to this setting.",Unsolved,https://export.arxiv.org/pdf/2210.12607v1.pdf
133,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"it is important to replicate this work on other languages and models, which we plan to do as these become available.comprises two attributes.",Unsolved,https://export.arxiv.org/pdf/2210.12607v1.pdf
134,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"we have a = {price, distance} where restaurant prices are obtained from yelp and distances to a hypothetical location are randomly generated, thus limiting the lm's access to prior knowledge in this scenario.",Unsolved,https://export.arxiv.org/pdf/2210.12607v1.pdf
135,254069686,Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering,conclusion,Insight-tree,the failure of entropy maximization to improve out-of-distribution generalization may be caused by the unnaturalness of the perturbed inputs.,Unsolved,https://export.arxiv.org/pdf/2211.16093v1.pdf
136,254069686,Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering,conclusion,Insight-tree,modifying the perturbation functions to effectively improve outof-distribution generalization is future work.,Unsolved,https://export.arxiv.org/pdf/2211.16093v1.pdf
137,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,conclusion,Insight-tree,"in future, we'd like to improve oreolm by training to conduct more reasoning steps, supporting locial reasoning, and apply oreolm to a broader range of knowledge-intensive nlp tasks.",Unsolved,https://export.arxiv.org/pdf/2211.08380v1.pdf
138,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"limited reasoning steps in our experiments, we show that using reasoning step t = 2 has better performance to t = 1 on one-hop and multi-hop (mostly two) qa datasets. thus, it's a natural question about whether we could extending reasoning steps more?",Unsolved,https://export.arxiv.org/pdf/2211.08380v1.pdf
139,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"as previous kg reasoning mostly could support very long path (with lstm design)though we didn't spend much time exploring before the paper submission, we indeed try using t = 3, but currently it didn't get better results.",Unsolved,https://export.arxiv.org/pdf/2211.08380v1.pdf
140,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"thus, it's harder to get accurate reasoning path ground-truth for high-order t .",Unsolved,https://export.arxiv.org/pdf/2211.08380v1.pdf
141,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"another potential reason is that within transformer model, the representation space in lower and upper layer might be very different, say, encode more syntax and surface knowledge at lower layers, while more semantic knowledge at upper layers.",Unsolved,https://export.arxiv.org/pdf/2211.08380v1.pdf
142,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"currently we adopt a mlp projection head, wishing to map integrated knowledge into the same space, but it might have many flaws and need further improvement.",Unsolved,https://export.arxiv.org/pdf/2211.08380v1.pdf
143,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"table requires pre-training and gpu resources our current design has a huge entity embedding table, which should be learned through additional supervision and could not directly fine-tune to downstream tasks. this is restricts our approach's usage.",Unsolved,https://export.arxiv.org/pdf/2211.08380v1.pdf
144,256662717,Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories,conclusion,Insight-tree,"we hope our findings and illustrations can inspire more future research in better augmenting language models, to provide other alternatives to achieve generalization ability beyond solely relying on model scale.",Unsolved,https://export.arxiv.org/pdf/2302.03754v1.pdf
145,2945602,Proteome Science Detection and identification of NAP-2 as a biomarker in hepatitis B-related hepatocellular carcinoma by proteomic approach,conclusion,Insight-tree,"further studies with larger sample sizes will be needed to verify this specific protein marker and to address its efficacy, especially with regard to discriminating histologic types of hcc and disease stages.",Unsolved,
146,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,conclusion,Insight-tree,"however, current multimodal models still have a significant gap to human performance, showing the potential of mr. right as a challenge in multimodal retrieval.",Unsolved,https://export.arxiv.org/pdf/2209.13764v1.pdf
147,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,conclusion,Insight-tree,we believe mr. right can breathe new insights into information retrieval for more robust retrieval systems.,Unsolved,https://export.arxiv.org/pdf/2209.13764v1.pdf
148,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,limitations and future work,Insight-tree,"in mr. right, we only consider text-based queries, which may limit the search modalities from users.",Unsolved,https://export.arxiv.org/pdf/2209.13764v1.pdf
149,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,limitations and future work,Insight-tree,"we can expand our dataset with additional domain queries and documents such as images, audio, and video.",Unsolved,https://export.arxiv.org/pdf/2209.13764v1.pdf
150,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,limitations and future work,Insight-tree,"we can explore other sources such as news, blogs, or commercial websites.",Unsolved,https://export.arxiv.org/pdf/2209.13764v1.pdf
151,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,limitations and future work,Insight-tree,"mr. right is a preliminary attempt to explore multimodal retrieval, and there are still challenges we need to analyze and study in future work.",Unsolved,https://export.arxiv.org/pdf/2209.13764v1.pdf
152,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,limitations and future work,Insight-tree,"our work has a few limitations which we briefly mention them below and for future work: different compression algorithms: in our work, we considered jpq and bpr due to its popularity and effectiveness shown in our preliminary results.",Unsolved,https://arxiv.org/pdf/2205.11498v1.pdf
153,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,limitations and future work,Insight-tree,"in future, we can work on extending our methods to more recent memory compression algorithms such as repconc (zhan et al., 2022).better backbone models: we suspect the performances on the beir can further improved with stronger backbone models in comparison with tas-b. due to the model agnostic nature of our method, we can easily extend our work to different state-ofthe-art dense retrievers in the upcoming future.requires separate models: bpr and jpq both require training separate models for each domain or task with our technique.",Unsolved,https://arxiv.org/pdf/2205.11498v1.pdf
154,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,limitations and future work,Insight-tree,"this can be quite cumbersome for practical use-cases involving several hundreds of domains or retrieval tasks, for which one would need to train multiple models.compute intensive: our method gpl is compute intensive: (1) for bpr+gpl, every dense retriever requires to separately compute embeddings for the whole corpus for hard-negative mining.(2) cross-encoder teacher model although very effective, slows down the training significantly as required to label during training.",Unsolved,https://arxiv.org/pdf/2205.11498v1.pdf
155,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,limitations and future work,Insight-tree,"in future, we can explore efficient and faster teachers instead of cross-encoders for gpl such as colbert (khattab and zaharia, 2020) or tilde (zhuang and zuccon, 2021)",Unsolved,https://arxiv.org/pdf/2205.11498v1.pdf
156,225039884,On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries,conclusion,Insight-tree,our oracle studies also show that there is large unrealized further potential for our annotations.,Unsolved,https://www.aclweb.org/anthology/2020.findings-emnlp.167.pdf
157,225039884,On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries,conclusion,Insight-tree,"thus, it remains an exciting challenge for future research to use our lexical alignment annotations more effectively.",Unsolved,https://www.aclweb.org/anthology/2020.findings-emnlp.167.pdf
158,225039884,On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries,conclusion,Insight-tree,one potential future direction is to further investigate the utility of lexical alignments in a cross-dataset/domain evaluation setting,Unsolved,https://www.aclweb.org/anthology/2020.findings-emnlp.167.pdf
159,235125947,Fact-driven Logical Reasoning,conclusion,Insight-tree,"in the future, we intend to design more elaborate mechanisms to cope with different question types and logical types as well as combine the symbolic and neural approaches.",Unsolved,https://arxiv.org/pdf/2105.10334v1.pdf
160,249890380,KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP,conclusion and future work,Insight-tree,"moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration.",Unsolved,https://export.arxiv.org/pdf/2206.10265v1.pdf
161,232147859,Semantic Models for the First-stage Retrieval: A Comprehensive Review,conclusion,Insight-tree,"in addition, we discuss several challenges and promising directions that are important for future researches.",Unsolved,https://arxiv.org/pdf/2103.04831v4.pdf
162,232147859,Semantic Models for the First-stage Retrieval: A Comprehensive Review,conclusion,Insight-tree,we look forward to working with the community on these issues.,Unsolved,https://arxiv.org/pdf/2103.04831v4.pdf
163,258615731,Active Retrieval Augmented Generation,conclusion,Insight-tree,future directions include better alternatives for active retrieval and developing lm architectures for efficient active retrieval augmentation.,Unsolved,https://export.arxiv.org/pdf/2305.06983v1.pdf
164,258615731,Active Retrieval Augmented Generation,limitation,Insight-tree,this issue can be potentially alleviated with special architectural designs that encode the retrieved documents d qt and the input/generation (x/y <t ) independently.,Unsolved,https://export.arxiv.org/pdf/2305.06983v1.pdf
165,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,limitations of reflexion,Insight-tree,"however, after only 4 trials, we terminated the baseline and reflexion runs as the agent did not show improvement in accuracy  and was not generating helpful, intuitive self-reflections.",Unsolved,https://export.arxiv.org/pdf/2303.11366v1.pdf
166,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,limitations of reflexion,Insight-tree,"this observation may not be a direct limitation of the reflexion approach, but it highlights the ability of a reflexion agent to optimize reasoning trace and action execution but not complete awareness of the quality of the tools that it may be using.",Unsolved,https://export.arxiv.org/pdf/2303.11366v1.pdf
167,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,conclusion,Insight-tree,"in addition, we include an inconclusive attempt to improve performance on the webshop benchmark and provide a discussion that highlights a few limitations of this approach.",Unsolved,https://export.arxiv.org/pdf/2303.11366v1.pdf
168,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,conclusion,Insight-tree,"we encourage others to apply reflexion to more complex tasks in which the agent must learn to develop new ideas, explore larger unseen state spaces, and form more accurate plans of action through its experiences in past environments.",Unsolved,https://export.arxiv.org/pdf/2303.11366v1.pdf
169,243865235,Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI,conclusion,Insight-tree,"results on three lms show that although some lms (roberta) own a certain interpretable fol reasoning ability, they still cannot make sensible fol reasoning like humans.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.303.pdf
170,243865235,Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI,conclusion,Insight-tree,detailed analysis motivates us to enhance specific reasoning abilities or explore new methods to make neural models understand more refined logic.,Unsolved,https://www.aclanthology.org/2021.emnlp-main.303.pdf
171,248780469,MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,limitations and future work,Insight-tree,"although the proposed mt2net model outperforms other baseline models, it still performs significantly worse than human experts, which reflects the challenge of multihiertt.",Unsolved,https://www.aclanthology.org/2022.acl-long.454.pdf
172,248780469,MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,limitations and future work,Insight-tree,"primarily, we find that models do not perform well on certain types of questions: 1) questions requiring reasoning across multiple tables; 2) questions requiring multi-step reasoning; 3) questions requiring reasoning over tables with complex hierarchical structures; and 4) questions requiring external financial knowledge.to deal with these challenges, we believe that four main directions of work may be workable: 1) designing a specialized module to handle multitable reasoning; 2) decomposing a complex question requiring multi-step reasoning into several simpler sub-questions that qa models can handle (perez et al., 2020;chen et al., 2020); 3) applying a more advanced table-encoding method.",Unsolved,https://www.aclanthology.org/2022.acl-long.454.pdf
173,248780469,MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,conclusion,Insight-tree,the results of comprehensive experiments showed that current qa models (best f 1 : 38.43%) still lag far behind the human expert performance (f 1 : 87.03%).,Unsolved,https://www.aclanthology.org/2022.acl-long.454.pdf
174,248780469,MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,conclusion,Insight-tree,this motivates further research on developing qa models for such complex hybrid data with multiple hierarchical tables.,Unsolved,https://www.aclanthology.org/2022.acl-long.454.pdf
175,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,conclusion,Insight-tree,"overall, we find that fine-tuning lms such as roberta and t5 on deductive reasoning datasets is not sufficient to learn the semantics of the logical operators conjunction, disjunction, and negation.",Unsolved,https://export.arxiv.org/pdf/2205.12598v2.pdf
176,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,conclusion,Insight-tree,"although well-aligned training dataset improves model performance, the models still find it challenging to understand negations, both in logical contrast and logical equivalence sets.",Unsolved,https://export.arxiv.org/pdf/2205.12598v2.pdf
177,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,a key limitation of the work is the synthetic nature of the dataset.,Unsolved,https://export.arxiv.org/pdf/2205.12598v2.pdf
178,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,another limitation is the complexity of the datasets we explore.,Unsolved,https://export.arxiv.org/pdf/2205.12598v2.pdf
179,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,some more complex forms of logical reasoning-based theories can potentially reveal even more limitations of deductive reasoning models.,Unsolved,https://export.arxiv.org/pdf/2205.12598v2.pdf
180,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,another interesting aspect we do not explore in this scope is potential techniques to improve these models on deductive reasoning tasks.,Unsolved,https://export.arxiv.org/pdf/2205.12598v2.pdf
181,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,"this might involve trying different inductive biases in the form of architectural designs, more specialized datasets, etc.",Unsolved,https://export.arxiv.org/pdf/2205.12598v2.pdf
182,186346769,The Connection Commonalities in the Mathematical Content of Lesson Sequences,conclusion,Insight-tree,"then again, why is the least mathematically equipped teacher doing the most innovative teaching?",Unsolved,
183,186346769,The Connection Commonalities in the Mathematical Content of Lesson Sequences,conclusion,Insight-tree,"a possible explanation that can be suggested is that, perhaps, since he did not have the relevant mathematical education background, he may have not seen the connection himself.",Unsolved,
184,186346769,The Connection Commonalities in the Mathematical Content of Lesson Sequences,conclusion,Insight-tree,"we need to encourage teachers who are mathematical experts, not take for granted the connections that are obvious to them (and possibly, will be obvious to their students), and to include what x-t2 was doing, and to actually, very deliberately try to create the connections within and across their own lessons.",Unsolved,
185,248377036,Evaluating Extrapolation Performance of Dense Retrieval,conclusions,Insight-tree,"although this paper focuses on evaluating how dr models extrapolate, the methodologies can also be used for other models in the future.",Unsolved,https://arxiv.org/pdf/2204.11447v1.pdf
186,211258645,Unsupervised Question Decomposition for Question Answering,conclusion,Insight-tree,"our unsupervised decompositions are largely extractive, which is effective for compositional, multi-hop questions but not all complex questions, showing room for future work.",Unsolved,https://arxiv.org/pdf/2002.09758v1.pdf
187,258556908,VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens,conclusions,Insight-tree,"for future work, we believe that extending our method to the decoder of the encoder-decoder models will further boost the efficiency of transformers while maintaining similar model performance.    ",Unsolved,https://export.arxiv.org/pdf/2305.04241v2.pdf
188,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,limitations,Insight-tree,"our dataset is currently limited to multi-word, english noun-phrases.",Unsolved,https://www.aclanthology.org/2023.eacl-main.1.pdf
189,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,limitations,Insight-tree,"furthermore, it is expected to contain around a 5% error on pr-pass (i.e. the best human performance is 95% em).",Unsolved,https://www.aclanthology.org/2023.eacl-main.1.pdf
190,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,limitations,Insight-tree,"on pr-page, there may be more than one correct target phrase; however, we only label one phrase as the correct answer per document.",Unsolved,https://www.aclanthology.org/2023.eacl-main.1.pdf
191,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,limitations,Insight-tree,we use only phrases that contain at least one wic word.,Unsolved,https://www.aclanthology.org/2023.eacl-main.1.pdf
192,258841172,IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,limitations,Insight-tree,"the main limitation of ifqa dataset is that it only covers event-based questions, due to the nature of creating counterfactual presuppositions.",Unsolved,https://export.arxiv.org/pdf/2305.14010v1.pdf
193,258841172,IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,limitations,Insight-tree,"therefore, our dataset is not intended for training general opendomain qa models or evaluate their capabilities.for data collection, we relied heavily on human annotators, both for question annotation and verification.",Unsolved,https://export.arxiv.org/pdf/2305.14010v1.pdf
194,258841172,IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,limitations,Insight-tree,"despite our efforts to mitigate annotator bias by providing explicit instructions and examples and by sampling annotators from diverse populations, it is not possible to completely remove this bias.",Unsolved,https://export.arxiv.org/pdf/2305.14010v1.pdf
195,258841172,IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,limitations,Insight-tree,"in addition, we use heuristic rules to select only a small portion of wikipedia passages and then present them to human annotators (as mentioned in section 3.1.1), which might lead to pattern-oriented bias in the annotated data.for evaluated models, large language models performance on our dataset may preserve biases learned from the web text during pre-training or and make biased judgments as a result.",Unsolved,https://export.arxiv.org/pdf/2305.14010v1.pdf
196,248157463,ASQA: Factoid Questions Meet Long-Form Answers,conclusion,Insight-tree,our analysis suggests that strong performance on asqa is contingent upon both high-quality retrieval and summarization. these aspects constitute important directions for future work on asqa.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.566.pdf
197,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"we note, however, that this unnecessary extra information may still be penalized by the rouge-l metric.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.566.pdf
198,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"moreover, in the presence of distractors, we also expect the accuracy of the roberta model used for reading comprehension to degrade, thereby effectively penalizing a low precision.on a separate note, the disambig-f1 metric requires a high-accuracy qa system.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.566.pdf
199,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"hence, for domains that are significantly different from wikipedia, fine-tuning the roberta squadv2 model on the task might be important to ensure the effectiveness of the disambig-f1 metric.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.566.pdf
200,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,"experiments with state-of-art language models indicated that they struggle to solve commaqa, even when provided with agents' internal knowledge.",Unsolved,https://arxiv.org/pdf/2110.08542v1.pdf
201,248299683,STANDING ON THE SHOULDERS OF GIANT FROZEN LANGUAGE MODELS,conclusions,Insight-tree,"while fine-tuning huge lms can often yield excellent performance, this approach is expensive at training time, requires serving a plethora of models at runtime, and provides poor adaptability in the face of variations in the targeted task.",Unsolved,https://arxiv.org/pdf/2204.10019v1.pdf
202,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,"this suggests that finetuning on these datasets cause domain-agnostic fsl gains, though we were unable to find clear patterns to explain why this happens for some data and not others.",Unsolved,https://export.arxiv.org/pdf/2208.01009v2.pdf
203,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"future work can investigate the types of questions that arise when users interact with other file types such as pdf, excel, and powerpoint.",Unsolved,https://export.arxiv.org/pdf/2203.15073v2.pdf
204,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,another property that could have conceivably impacted the types of questions users asked is the quality of the answers that they received.,Unsolved,https://export.arxiv.org/pdf/2203.15073v2.pdf
205,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"with this diverse set of document types, investigating the relationship between types of questions and types of documents would require collecting far more questions by running the study for a long time.the setup of our q&a system was such that users submitted one-shot questions as there were no affordances for following up on previously asked questions.",Unsolved,https://export.arxiv.org/pdf/2203.15073v2.pdf
206,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"future work should investigate whether the types of questions that users ask a document q&a system differ if the system provides the users with the means for following up on their previous questions or multi-round conversations with the assistant.another area for future work would be to explore the context of user needs including when users need different types of assistance with their documents, what they do before the seek help, and what they do after they receive answers.",Unsolved,https://export.arxiv.org/pdf/2203.15073v2.pdf
207,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"to minimize concerns about the confidentiality of the business data, the contextual information that we collected in our study concerned the metadata of the document (e.g., file size, last modified date, etc.) and not the content.",Unsolved,https://export.arxiv.org/pdf/2203.15073v2.pdf
208,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"therefore, given the data we collected, we cannot examine the context of user needs.",Unsolved,https://export.arxiv.org/pdf/2203.15073v2.pdf
209,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,future work can study this question through interviews and user-produced logs.,Unsolved,https://export.arxiv.org/pdf/2203.15073v2.pdf
210,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"however, the computational overhead of sequence-to-sequence models places a hard limit on the tree size and premise count that can be handled in the end-to-end setting; moreover, recent results call into question how well end-to-end transformers can generalize this type of reasoning (zhang et al., 2022).",Unsolved,https://export.arxiv.org/pdf/2201.06028v2.pdf
211,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"however, by discretizing the reasoning in the scsearch procedure, we do face a runtime theoretically exponential in proof size to do exhaustive search.",Unsolved,https://export.arxiv.org/pdf/2201.06028v2.pdf
212,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"future work on higher tree depths may have to reckon with the theoretical limitations of this procedure, possibly through the use of better heuristics.our experiments are conducted exclusively on english datasets.",Unsolved,https://export.arxiv.org/pdf/2201.06028v2.pdf
213,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"while we hypothesize that our approach would work equally well for another language given a pretrained sequence-to-sequence model for that language with equivalent capacity, such models are not available universally across languages, representing an obstacle for transferring our results to languages beyond english.furthermore, the entailmentbank dataset on which we train and evaluate targets the elementary science domain, raising a question of domain specificity.",Unsolved,https://export.arxiv.org/pdf/2201.06028v2.pdf
214,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"in future work, we plan to evaluate deduction models on additional datasets with different style, conceptual content, and types of reasoning in order to verify that the factored approach is equally applicable across diverse settings.",Unsolved,https://export.arxiv.org/pdf/2201.06028v2.pdf
215,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,"we first empirically find that the it performance is largely restricted by the number of distinct tasks, instructions, and training samples in data-scarce tasks.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.105.pdf
216,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,there are three directions for future work: (1) designing automatic and generalizable methods to construct pseudo-labeled data for instruction tuning.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.105.pdf
217,247593935,R E L I C : Retrieving Evidence for Literary Claims,conclusion,Insight-tree,"important future directions include (1) building better models of primary sources that integrate narrative and discourse structure into the candidate representations instead of computing them out-of-context, and (2) integrating relic models into real tools that can benefit humanities researchers.",Unsolved,https://www.aclanthology.org/2022.acl-long.517.pdf
218,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,limitations,Insight-tree,"however, we did not examine if our technique would be effective on other types of qa datasets, such as truthfulqa (lin et al., 2022b), which exploits imitative falsehoods and contains longer desired responses, or boolqa (clark et al., 2019), which has ""yes"" or ""no"" as the only possible answers.further, due to financial constrains, we test abcd and fine-grained self-evaluation through preliminary experiments on a subset of data from our three datasets.",Unsolved,https://export.arxiv.org/pdf/2305.14750v1.pdf
219,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,limitations,Insight-tree,"in addition, since we only examined a subset of questions from our newly-collected dataset obscureqa, this opens up future research directions leveraging our dataset.",Unsolved,https://export.arxiv.org/pdf/2305.14750v1.pdf
220,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,limitations,Insight-tree,"we believe that obscureqa could be used to evaluate llms on a variety of facets, including benchmarking the academic knowledge of state-of-the-art llms, and given that this dataset frequently elicits untruthful responses, studying confidence and uncertainty calibration techniques.",Unsolved,https://export.arxiv.org/pdf/2305.14750v1.pdf
221,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"environment complexity is limited, and it's currently difficult to author complex worlds.",Unsolved,https://www.aclanthology.org/2022.wordplay-1.1.pdf
222,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"two competing needs are currently at odds: the desire for complex environments to learn complex skills, and the desire for environment variation to encourage robustness in models.",Unsolved,https://www.aclanthology.org/2022.wordplay-1.1.pdf
223,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"current tooling emphasizes creating varied procedural environments, but those environments have limited complexity, and require agents to complete straightforward tasks.",Unsolved,https://www.aclanthology.org/2022.wordplay-1.1.pdf
224,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"currently, few analogs exist for text worlds.",Unsolved,https://www.aclanthology.org/2022.wordplay-1.1.pdf
225,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"in the near-term, it is likely that a domain-specific language specific to complex text world modeling would be required to address these needs while simultaneously reducing the time investment and barrier-to-entry for end users.analyses of environment complexity can inform agent design and evaluation.",Unsolved,https://www.aclanthology.org/2022.wordplay-1.1.pdf
226,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"agent performance is typically reported as a distribution over a large number of environments, and the methodological groundwork required to understand when different models exceed others in time or performance over these environment distributions is critical to making forward progress.",Unsolved,https://www.aclanthology.org/2022.wordplay-1.1.pdf
227,215238360,Query Focused Multi-Document Summarization with Distant Supervision,conclusions,Insight-tree,"in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019;chu and liu, 2019) and investigate how recent advances in open domain qa qi et al., 2019) can be adapted for query focused summarization.",Unsolved,https://arxiv.org/pdf/2004.03027v1.pdf
228,234741852,TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,conclusion,Insight-tree,experiments show tat-qa dataset is very challenging and more effort is demanded for tackling qa tasks over hybrid data.,Unsolved,https://www.aclanthology.org/2021.acl-long.254.pdf
229,234741852,TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,conclusion,Insight-tree,"we expect our tat-qa dataset and tagop model would serve as a benchmark and baseline respectively to help build more advanced qa models, facilitating the development of qa technologies to address more complex and realistic hybrid data, especially those requiring numerical reasoning.",Unsolved,https://www.aclanthology.org/2021.acl-long.254.pdf
230,235097535,MULTIPROVER: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,conclusion,Insight-tree,multiprover's modeling is fairly generic and similar methods can be used in generating a set of structured explanations for other nlp tasks like multi-hop qa.,Unsolved,https://www.aclweb.org/anthology/2021.naacl-main.287.pdf
231,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,"there are a number of limitations in our work: first, we only study sparse retrieval (bm25) which is where query expansion is important.",Unsolved,https://export.arxiv.org/pdf/2305.03653v1.pdf
232,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,"dense retrieval systems (e.g. dual encoders) are less prone to the vocabulary gap and, as a result, are less likely to benefit from a query expansion.",Unsolved,https://export.arxiv.org/pdf/2305.03653v1.pdf
233,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,"finally, the computational cost of llms may be prohibitive to deploy llm-based query expansions in practice.",Unsolved,https://export.arxiv.org/pdf/2305.03653v1.pdf
234,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,it may be possible to distill the output of the large model into a smaller servable model.,Unsolved,https://export.arxiv.org/pdf/2305.03653v1.pdf
235,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,how to productionize llm-based query expansions is left as an open problem.,Unsolved,https://export.arxiv.org/pdf/2305.03653v1.pdf
236,245769925,Does Entity Abstraction Help Generative Transformers Rea- son?,conclusion,Insight-tree,"this last result is due to the noisy entity tagging from ""off-the-shelf"" taggers, and due to the nature of the task at hand.",Unsolved,https://export.arxiv.org/pdf/2201.01787v2.pdf
237,256662612,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",conclusion and discussion,Insight-tree,"although chatgpt performs well in most of the tasks, there are still some failure cases on each task ( 3.1).",Unsolved,https://export.arxiv.org/pdf/2302.04023v2.pdf
238,256662612,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",conclusion and discussion,Insight-tree,"in the summarization task, chatgpt sometimes generates a summary that is even longer than the input document.",Unsolved,https://export.arxiv.org/pdf/2302.04023v2.pdf
239,256662612,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",conclusion and discussion,Insight-tree,"in the machine translation task, chatgpt sometimes produces an incorrect translation for some words, making the meaning slightly shifted.",Unsolved,https://export.arxiv.org/pdf/2302.04023v2.pdf
240,256662612,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",conclusion and discussion,Insight-tree,"therefore, dealing with these special cases is a complex but important task.",Unsolved,https://export.arxiv.org/pdf/2302.04023v2.pdf
241,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,"in future work, we will consider how to improve the efficiency of the framework and how to introduce more tools to interact with llm for more tasks.",Unsolved,https://export.arxiv.org/pdf/2304.14732v4.pdf
242,247476296,Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach,conclusion & future work,Insight-tree,"in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities.",Unsolved,https://export.arxiv.org/pdf/2203.08383v2.pdf
243,247476296,Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach,conclusion & future work,Insight-tree,"the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely a human can track along the plm's train of thought and make edits and corrections at different steps, which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process. we leave these investigations as future work.",Unsolved,https://export.arxiv.org/pdf/2203.08383v2.pdf
244,233189566,Globalizing BERT-based Transformer Architectures for Long Document Summarization,conclusion,Insight-tree,"in the future, we plan to adapt our model to other tasks that require understanding long documents, as question-answering and document-scale machine translation.",Unsolved,https://www.aclweb.org/anthology/2021.eacl-main.154.pdf
245,248476204,QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance,conclusion,Insight-tree,"existing evaluation metrics for question generation are still reference-based and ignore the crucial input context of generation, lacking a deep understanding of the relevance between the generated questions and context.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.37.pdf
246,237502990,Adaptive Information Seeking for Open-Domain Question Answering,conclusion and future work,Insight-tree,"in the future, we will explore other adaptive retrieval strategies, like directly optimizing various informationseeking metrics by using reinforcement learning techniques.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.293.pdf
247,254125744,NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,conclusion,Insight-tree,"in this paper, we point out that although there are some differences among the various information retrieval tasks, there are still essential matching signals shared by the various tasks, including exact matching, relevance matching and inference matching. if the model can capture and exploit these signals, the generalization ability of the model across tasks and domains will be improved.",Unsolved,https://export.arxiv.org/pdf/2212.00229v2.pdf
248,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,"as section 5 reveals, there is a need for higher-quality question decompositions.",Unsolved,https://export.arxiv.org/pdf/2204.07693v2.pdf
249,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,"while we have demonstrated the potential for decomposition probes to help users build mental models of system behavior, these results are not fully realizable in real applications until decompo-sition systems improve.the probing strategy explored in this paper is particular to the qa setting and datasets that don't have predefined categories of answers.",Unsolved,https://export.arxiv.org/pdf/2204.07693v2.pdf
250,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,other probing strategies may exist that are not explored in this paper.it is noted that multi-hop questions do not always require multi-hop reasoning to solve.,Unsolved,https://export.arxiv.org/pdf/2204.07693v2.pdf
251,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,indeed we intentionally use a non-multi-hop questionanswering model to answer the original question to disadvantage the system so that explanations are required.,Unsolved,https://export.arxiv.org/pdf/2204.07693v2.pdf
252,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,"our study did not look at non-multi-hop questions, which may require other probing strategies yet to be invented.",Unsolved,https://export.arxiv.org/pdf/2204.07693v2.pdf
253,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,"specifically, we show that large pretrained multi-lingual lms are not enough for this task.",Unsolved,https://arxiv.org/pdf/2012.05958v1.pdf
254,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,we hope these techniques spur further research in the field such as exploring other multilingual lms and invoking additional networks on top of large lms for multilingual nlp.,Unsolved,https://arxiv.org/pdf/2012.05958v1.pdf
255,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,"however, the error analysis showed the inadequacy under complicated scenarios.",Unsolved,https://arxiv.org/pdf/2106.02399v1.pdf
256,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,"therefore, our future work will focus on applying interpretable reasoning on complex reasoning tasks.",Unsolved,https://arxiv.org/pdf/2106.02399v1.pdf
257,202572622,PubMedQA: A Dataset for Biomedical Research Question Answering,conclusion,Insight-tree,"we provide a strong baseline using multi-phase fine-tuning of biobert with long answer as additional supervision, but it's still much worse than just single human performance.",Unsolved,https://www.aclweb.org/anthology/D19-1259.pdf
258,202572622,PubMedQA: A Dataset for Biomedical Research Question Answering,conclusion,Insight-tree,"there are several interesting future directions to explore on pubmedqa, e.g.: (1) about 21% of pubmedqa contexts contain no natural language descriptions of numbers, so how to properly handle these numbers is worth studying; (2) we use binary bow statistics prediction as a simple demonstration for additional supervision of long answers.",Unsolved,https://www.aclweb.org/anthology/D19-1259.pdf
259,202572622,PubMedQA: A Dataset for Biomedical Research Question Answering,conclusion,Insight-tree,learning a harder but more informative auxiliary task of long answer generation might lead to further improvements.,Unsolved,https://www.aclweb.org/anthology/D19-1259.pdf
260,258887816,Linguistic Properties of Truthful Response,limitation,Insight-tree,our main limitation comes from dataset size.,Unsolved,https://export.arxiv.org/pdf/2305.15875v2.pdf
261,258887816,Linguistic Properties of Truthful Response,limitation,Insight-tree,this was limited because we used human evaluation to label model responses as truthful or untruthful.,Unsolved,https://export.arxiv.org/pdf/2305.15875v2.pdf
262,258887816,Linguistic Properties of Truthful Response,limitation,Insight-tree,"frankly, the limitations caused by the small size of the dataset were quite evident because the truthfulness detector was often biased towards producing one label (either 1 or 0).",Unsolved,https://export.arxiv.org/pdf/2305.15875v2.pdf
263,258887816,Linguistic Properties of Truthful Response,limitation,Insight-tree,"we attempted to solve this problem using lower regularization parameters, but this often produced models with lower performances.",Unsolved,https://export.arxiv.org/pdf/2305.15875v2.pdf
264,258887816,Linguistic Properties of Truthful Response,limitation,Insight-tree,"an ideal solution to this problem would be training the truthfulness detector on a large set of training instances, which is also our future direction.",Unsolved,https://export.arxiv.org/pdf/2305.15875v2.pdf
265,237940861,Paradigm Shift in Natural Language Processing,conclusion,Insight-tree,"in contrast, other potential unified paradigms, i.e. matching, mrc, and seq2seq, are underexplored in the context of pre-training.",Unsolved,https://arxiv.org/pdf/2109.12575v2.pdf
266,237940861,Paradigm Shift in Natural Language Processing,conclusion,Insight-tree,"one of the main reasons is that these paradigms require large-scale annotated data to conduct pre-training, especially seq2seq is notorious for data hungry.",Unsolved,https://arxiv.org/pdf/2109.12575v2.pdf
267,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,limitations,Insight-tree,"in this study, we limit long-term memory to a sliding window with maximum capacity, but we encourage future work to extend the memory component of reflexion with more advanced structures such as vector embedding databases or traditional sql databases.",Unsolved,https://export.arxiv.org/pdf/2303.11366v2.pdf
268,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,limitations,Insight-tree,"specific to code generation, there are many practical limitations to testdriven development in specifying accurate input-output mappings such as non-deterministic generator functions, impure functions that interact with apis, functions that vary output according to hardware specifications, or functions that invoke parallel or concurrent behavior that may be difficult to predict.",Unsolved,https://export.arxiv.org/pdf/2303.11366v2.pdf
269,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,conclusion,Insight-tree,"in future work, reflexion could be used to employ more advanced techniques that have been thoroughly studied in traditional rl settings, such as value learning in natural language or off-policy exploration techniques.",Unsolved,https://export.arxiv.org/pdf/2303.11366v2.pdf
270,237194607,MeDiaQA: A Question Answering Dataset on Medical Dialogues,conclusion,Insight-tree,"due to the unique characteristics, it is challenging for existing qa models compared with human performance.",Unsolved,https://arxiv.org/pdf/2108.08074v1.pdf
271,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,limitation,Insight-tree,"we highlight three limitations on our work: (1) our approach depends on synthesizing large amounts of data, which are expensive even if we used llama 65b which are much smaller than palm 540b and gpt-3.5;(2)",Unsolved,https://export.arxiv.org/pdf/2305.13691v1.pdf
272,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,limitation,Insight-tree,"our approach finetunes language models and thus is not applicable to the closedsource language models, e.g., gpt-3 and palm; and (3) our approach depends on the availability of powerful llms for synthesizing finetuning data.",Unsolved,https://export.arxiv.org/pdf/2305.13691v1.pdf
273,256827065,Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering,conclusion,Insight-tree,we encourage future studies to investigate the effectiveness of the entity-level reasoning task in the form of sub-questions.,Unsolved,https://export.arxiv.org/pdf/2302.05963v1.pdf
274,253237103,RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees,conclusion,Insight-tree,future directions include applying rl framework on other stepwise methods with more stable and sophisticated rl algorithms.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.483.pdf
275,14182996,Integrating miRNA and mRNA Expression Profiling Uncovers miRNAs Underlying Fat Deposition in Sheep,conclusion,Insight-tree,further studies are needed to verify the correlation between key mirnas and their target genes by in vitro approach and elucidate the functional impacts that mirnas serve during adiposeness.,Unsolved,
276,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,"however, this becomes infeasible as the degree of uncertainty increases.",Unsolved,https://arxiv.org/pdf/cs/9711103v1.pdf
277,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"this trend, which was not shown before, suggests that focusing on intrinsic lexical matching signals may be more effective than learning representations of both documents and queries in the long document environment where vocabulary mismatching occurs relatively less.",Unsolved,https://export.arxiv.org/pdf/2209.05917v2.pdf
278,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,we conjecture that the term weighting component is difficult to assign high weighting scores on rare words because our learning scheme heavily depends on the document corpus.,Unsolved,https://export.arxiv.org/pdf/2209.05917v2.pdf
279,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,there are three limitations in our work. how to evaluate the quality of human-annotated rationales is still open.,Unsolved,https://www.aclanthology.org/2022.conll-1.6.pdf
280,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,"however, this manner has damaged the original input distribution and brings uncontrollable factors on model behaviors.",Unsolved,https://www.aclanthology.org/2022.conll-1.6.pdf
281,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,"therefore, how to automatically and effectively evaluate the quality of human-annotated rationales should be studied in the future. we find that the interpretability of model architectures and saliency methods vary with tasks, especially with the input form of the task.",Unsolved,https://www.aclanthology.org/2022.conll-1.6.pdf
282,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,"thus our benchmark should contain more datasets of each task type ( e.g., single-sentence task, sentencepair similarity task and sentence-pair inference task) to further verify these findings.",Unsolved,https://www.aclanthology.org/2022.conll-1.6.pdf
283,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,"and we will build evaluation datasets for more tasks in the future. due to space limitation, there is no analysis of the relationships between metrics, e.g., the relationship between plausibility and accuracy, and the relationship between faithfulness and robustness.",Unsolved,https://www.aclanthology.org/2022.conll-1.6.pdf
284,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,"we will take these analyses in our future work.finally, we hope more evaluation metrics and analyses are proposed based on our benchmark.",Unsolved,https://www.aclanthology.org/2022.conll-1.6.pdf
285,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,efficiently integrating knowledge from different retrieval tasks in ugr has the potential to save significant time and computational resources in both academic and industrial environments.,Unsolved,https://export.arxiv.org/pdf/2304.14856v1.pdf
286,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,"however, ugr needs a complex scoring function to solve the identifier repetition problem; we encourage future work that explores other effective and efficient semantic identifiers for generative retrieval.",Unsolved,https://export.arxiv.org/pdf/2304.14856v1.pdf
287,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,"beyond kilt, training a more general unified generative retrieval model to serve different retrieval applications under multiple corpora and modalities seems a promising future direction.",Unsolved,https://export.arxiv.org/pdf/2304.14856v1.pdf
288,218487313,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"in addition of improving qa, we hypothesize that these simple unsupervised components of air will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria.",Unsolved,https://www.aclweb.org/anthology/2020.acl-main.414.pdf
289,248218748,Towards Fine-grained Causal Reasoning and QA,conclusion,Insight-tree,"experimental results using the state-of-the-art pre-trained language models provide the evidence that there is much room for improvement on causal reasoning tasks, and a need for designing better solutions to correlation discovery related to event causality analysis and why/what-if qa tasks.",Unsolved,https://arxiv.org/pdf/2204.07408v1.pdf
290,234479457,"A Multilingual Modeling Method for Span-Extraction Reading Comprehension CCS CONCEPTS  Computing methodologies  Natural language processing; Multilingual modeling KEYWORDS multilingual modeling, span-extraction reading comprehension, multilingual attention, self-adaptive attention",conclusion,Insight-tree,"despite tremendous advances have been made in the field of spanextraction reading comprehension in recent years, large-scale highquality extractive qa datasets in languages other than english remain scarce, and collecting such a sufficient amount of training data for each language is costly, and even impossible, making training reading comprehension systems in other languages challenging.",Unsolved,https://arxiv.org/pdf/2105.14880v1.pdf
291,256827818,Few-shot Reranking for Multi-hop QA via Language Model Prompting,conclusion,Insight-tree,future avenues of exploration include combining promptrank with efficient tuning techniques such as prefix tuning and efficient strategies for instruction search.,Unsolved,https://export.arxiv.org/pdf/2205.12650v2.pdf
292,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,we believe that modifying data samples can significantly help the model improve performance.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.302.pdf
293,246294995,Reasoning Like Program Executors,conclusion & future work,Insight-tree,"and we will also investigate the causes of the reasoning transfer with more insightful experiments, since we still do not know how the reasoning transfer occurs.",Unsolved,https://www.aclanthology.org/2022.emnlp-main.48.pdf
294,3520779,ATTac-2000: An Adaptive Autonomous Bidding Agent,conclusion and future work,Insight-tree,"while it was a very successful event, some minor improvements would increase its interest from a multiagent learning perspective.",Unsolved,https://arxiv.org/pdf/1106.0678v1.pdf
295,237605111,BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles,conclusion,Insight-tree,the huge gap between the human and machine leaves much room for improvement.,Unsolved,https://arxiv.org/pdf/2109.11087v2.pdf
296,237605111,BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles,conclusion,Insight-tree,"in future work, we plan to extend birdqa with riddles in other languages and incorporate figurative language understanding into riddle solving.",Unsolved,https://arxiv.org/pdf/2109.11087v2.pdf
297,237605111,BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles,conclusion,Insight-tree,we hope that birdqa will stir more research for question answering on riddles.,Unsolved,https://arxiv.org/pdf/2109.11087v2.pdf
298,250390947,Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering,conclusion and future work,Insight-tree,"in future work, we hope to extend the range and precision of the entity relations used, and we hope to extend our model to accommodate more complex multi-hop questions with unknown number of hops and non-linear reasoning.",Unsolved,https://www.aclanthology.org/2022.dlg4nlp-1.8.pdf
299,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"however, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task.",Unsolved,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
300,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,we believe that in future they can be used more directly to yield better performance gains.,Unsolved,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
301,248780551,CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation,conclusion,Insight-tree,the mqg task is more challenging and worthy of exploration compared with conventional shallow qg.,Unsolved,https://www.aclanthology.org/2022.acl-long.475.pdf
302,226262208,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,conclusion,Insight-tree,"our baseline model, built on top of state-ofthe-art models for the most closely related existing datasets, performs quite poorly in this setting, even when given oracle retrieval results, and especially when combined with other reading comprehension datasets.",Unsolved,https://arxiv.org/pdf/2011.07127v1.pdf
303,226262208,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,conclusion,Insight-tree,iirc both provides a promising new avenue for studying complex reading and retrieval problems and demonstrates that much more research is needed in this area.,Unsolved,https://arxiv.org/pdf/2011.07127v1.pdf
304,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,"in the future, we hope our analysis could inspire more explorations on the unified qa methods, or the unification of distinct tasks with complex inputs modeling by the structural prompt.",Unsolved,https://www.aclanthology.org/2022.naacl-main.313.pdf
305,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,we also hope structural prompt can be further utilized into the unification of more tasks with complex inputs.,Unsolved,https://www.aclanthology.org/2022.naacl-main.313.pdf
306,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,limitations of realfp.,Insight-tree,"our realfp dataset includes only one explanation program to a given fp whereas in practice, there can be multiple creative decompositions that lead to the correct answer.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
307,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,limitations of realfp.,Insight-tree,"to encourage models that are capable of capturing this diversity in the output space, it would be interesting to (a) collect alternative solutions similar to say, image captioning datasets where it is the norm to train and evaluate against multiple ground truth candidates and (b) increasing the number of templates in the synthfp dataset, thereby biasing the model towards exploring multiple solutions by pre-training on a richer synthetic dataset.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
308,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,limitations of realfp.,Insight-tree,"further, the work doesn't include other variants of fps -e.g. binary yes/no questions, comparisons, or fps involving probability and risk quantification.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
309,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,limitations of realfp.,Insight-tree,"finally, note that our real-world dataset, by virtue of how it is collected, has a high us-centric bias, both in terms of cultural context and vocabulary.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
310,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,"the baseline models we provide, despite being based on state-of-the-art language models and even with substantial fine-tuning, struggle on our challenge tasks.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
311,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,"they are, on average, off by two orders of magnitude from the correct estimate and perform only slightly better than predicting a constant number.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
312,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,we thus hope to establish fermi problems as a hard reasoning challenge that motivates further advances in ai reasoning systems.,Unsolved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
313,249062555,Teaching Broad Reasoning Skills via Decomposition-Guided Contexts,conclusions,Insight-tree,"despite these abilities and the availability of large scale multihop qa datasets, large lm-based qa models do not reliably learn to use such reasoning skills for answering complex questions.",Unsolved,https://arxiv.org/pdf/2205.12496v1.pdf
314,215768725,A Simple Yet Strong Pipeline for HotpotQA,conclusion,Insight-tree,"this result shows that supporting sentence identification in hotpotqa is itself not a multi-hop problem, and suggests focusing on other multi-hop datasets to demonstrate the value of more complex retrieval techniques.",Unsolved,https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf
315,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,our additional analysis also proves that it is possible to get more robust qa models via tasa in the future.,Unsolved,https://www.aclanthology.org/2022.emnlp-main.821.pdf
316,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,future work includes the extension of other network architectures and tasks like vision recognition.,Unsolved,https://export.arxiv.org/pdf/2303.01610v1.pdf
317,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"we hope to use the screenqa task and the dataset to encourage the community to look into this screen content understanding problem, as it enables new technologies and new user experiences.",Unsolved,https://export.arxiv.org/pdf/2209.08199v1.pdf
318,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,"we have argued that question answering is well-suited for this purpose and that by targeting a typologically diverse set of languages, progress on the tydi qa dataset is more likely to generalize on the breadth of linguistic phenomena found throughout the world's languages.",Unsolved,https://arxiv.org/pdf/2003.05002v1.pdf
319,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,discussion and limitations,Insight-tree,"we believe the model prediction was in these cases impacted by biases in positional embeddings (e.g., the answer tends to occur in the first retrieved paragraph), which cannot be indicated by current attribution methods. we believe this is a useful avenue for future investigation.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.447.pdf
320,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,discussion and limitations,Insight-tree,"by first thinking about what kind of counterfactuals and what kind of behaviours we want to explain, we can motivate the development of new explanation techniques to serve these needs.",Unsolved,https://www.aclanthology.org/2021.emnlp-main.447.pdf
321,237592852,Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering,conclusion,Insight-tree,"for future work, we plan to build on our insights to improve the performance of multi-hop models by combining the strengths of lexical and dense retrieval.",Unsolved,https://www.aclanthology.org/2021.sustainlp-1.7.pdf
322,237592852,Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering,conclusion,Insight-tree,"also, we aim to develop less computationally expensive multi-hop retrieval models.",Unsolved,https://www.aclanthology.org/2021.sustainlp-1.7.pdf
323,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,we introduce both limitations of the current research and intrinsic in plms.,Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
324,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"firstly, there are gaps in defeasible reasoning and reasoning path evaluation. research gap on defeasible reasoning.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
325,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"while defeasible reasoning is widely used in our daily life, this topic is still under-explored in nlp.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
326,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"[4] found that it is more challenging for chatgpt to perform abductive reasoning and inductive reasoning than deduction, among which induction is the much more difficult one.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
327,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,lack of effective ways to evaluate reasoning paths.,Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
328,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,it is still challenging to automatically evaluate generated reasoning paths without ground truth.,Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
329,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"evaluating reasoning paths might become increasingly important to build explainable and reliable ai systems, especially when more people contact and use chatgpt-like products nowadays.secondly, there are also limitations intrinsic to plms. soft deduction can produce invalid conclusions.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
330,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"transformers can only predict conclusions with probability, irrespective of whether the conclusion of deductive reasoning is necessarily true in nature, which might prevent it from precise reasoning.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
331,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,this characteristic can result in a sub-optimal solution to deductive problems (including arithmetic reasoning and symbolic reasoning).,Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
332,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"for example, while chatgpt is impressive on reasoning tasks, it still fails to achieve perfect performance on the simplest one-step deductive inference task [4]. biases on content.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
333,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"while llms have made huge progress in reasoning, [32] found that llms are biased by content like humans when performing deduction.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
334,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"for example, they perform worse in abstract or counterfactual situations than the realistic ones.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
335,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"such biases will hinder them from actual reasoning and lead to wrong answers, degrading downstream performance.",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
336,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"more severely, it might cause harmful societal influences due to some social biases such as gender, which also exist in gpt4",Unsolved,https://export.arxiv.org/pdf/2303.14725v2.pdf
337,224803601,OPEN QUESTION ANSWERING OVER TABLES AND TEXT,conclusion,Insight-tree,one interesting question we would like to ask in the future is: can we extend open question answering system to more modalities?,Unsolved,https://arxiv.org/pdf/2010.10439v1.pdf
338,224803601,OPEN QUESTION ANSWERING OVER TABLES AND TEXT,conclusion,Insight-tree,"some questions can be better answered by images and other resources, but the task can be drastically more challenging by including more modalities, as we have learned from this paper.",Unsolved,https://arxiv.org/pdf/2010.10439v1.pdf
339,224803601,OPEN QUESTION ANSWERING OVER TABLES AND TEXT,conclusion,Insight-tree,"finally, we believe the techniques we proposed might be useful for other open-qa setting, especially the comparisons between iterative retriever and fusion retriever.",Unsolved,https://arxiv.org/pdf/2010.10439v1.pdf
340,258823156,Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval,conclusion and future work,Insight-tree,future work will explore relevance feedback for token-level query representations as well as disentangling term importance scores from query representations for better interpretability.,Unsolved,https://export.arxiv.org/pdf/2305.11744v1.pdf
341,252819220,KHANQ: A Dataset for Generating Deep Questions in Education,conclusions and future works,Insight-tree,"we find that although it is feasible for the model to generate fluent and complex questions, the ability to understand and reason over the context and the prompt is still far from reaching the human level.",Unsolved,https://www.aclanthology.org/2022.coling-1.518.pdf
342,196170479,ELI5: Long Form Question Answering,conclusion,Insight-tree,"proposed models are far from human performance, in part due to the inability to exploit the long full web text.",Unsolved,https://arxiv.org/pdf/1907.09190v1.pdf
343,196170479,ELI5: Long Form Question Answering,conclusion,Insight-tree,"we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers.",Unsolved,https://arxiv.org/pdf/1907.09190v1.pdf
344,208201969,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,conclusion,Insight-tree,"the experimental results suggest that for benchmarking sophisticated nlu, datasets should be more carefully designed to ensure that questions correctly evaluate the intended skills.",Unsolved,https://ojs.aaai.org/index.php/AAAI/article/download/6422/6278
345,208201969,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,conclusion,Insight-tree,"in future work, we will develop a skill-oriented method for crowdsourcing questions.",Unsolved,https://ojs.aaai.org/index.php/AAAI/article/download/6422/6278
346,230770437,SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering,conclusion,Insight-tree,"we expect to further improve our framework by including more models in both ranker and reader side, and encourage community contributions to the project as well.",Unsolved,https://www.aclweb.org/anthology/2021.eacl-demos.2.pdf
347,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,"since context pair selection scales quadratically with the number of contexts, it is not ideal for scenarios that involve a large number of possible contexts.",Unsolved,https://arxiv.org/pdf/2104.08744v1.pdf
348,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,"however, it allows for deeper inter-document interaction as compared to other approaches that use summarized document representations.",Unsolved,https://arxiv.org/pdf/2104.08744v1.pdf
349,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,this work does not deal with any social impacts of biases in natural language processing systems.,Unsolved,https://arxiv.org/pdf/2104.08744v1.pdf
350,258865693,Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,conclusion,Insight-tree,we raise concerns about the use of parallelintegrated methods to address context length restriction: (1) pcw is functionally equal with a simple weighted sum ensemble on label distribution among context windows; (2) pcw degrades the multi-step reasoning capabilities of llms in complex tasks requiring knowledge understanding.,Unsolved,https://export.arxiv.org/pdf/2305.15262v1.pdf
351,258865693,Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,conclusion,Insight-tree,"despite the fact that parallel-integrated methods sometimes show better classification performance when the label space is large, they merely brute-force ensemble each window's context, consequently weakening logical reasoning and knowledge comprehension.",Unsolved,https://export.arxiv.org/pdf/2305.15262v1.pdf
352,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,"in future work, we will investigate the effect of web content in detail and consider applying our model to more types of downstream tasks.",Unsolved,https://export.arxiv.org/pdf/2305.10998v2.pdf
353,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"since the web is not curated and well-structured like wikipedia, we may encounter unexpected safety issues, including misinformation and harmful contents.",Unsolved,https://export.arxiv.org/pdf/2305.10998v2.pdf
354,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"while we have relied on the security control of the search engine, more attention should be paid to better understand the risks and provide effective ways to mitigate them.",Unsolved,https://export.arxiv.org/pdf/2305.10998v2.pdf
355,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"to encourage the community to investigate the question and ensure reproducibility, after the reviewing process, we will release the search urls used in our experiments.as for the potential concern, since we use the search engine to access real-time information, we do not have a tight control over retrieved results as traditional end-to-end retrieval (guu et al., 2020;lewis et al., 2020b).",Unsolved,https://export.arxiv.org/pdf/2305.10998v2.pdf
356,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"not only the changes of search engine logic, but also the newly published information, might create discrepancies over the course of time.",Unsolved,https://export.arxiv.org/pdf/2305.10998v2.pdf
357,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,this is also an issue we have to tackle to build a stable web-based solution for llms.,Unsolved,https://export.arxiv.org/pdf/2305.10998v2.pdf
358,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,commonsense reasoning is intended to utilize commonsense knowledge to reason about certain aspects of the given text .,Unsolved,https://export.arxiv.org/pdf/2305.10998v2.pdf
359,258959022,Plug-and-Play Document Modules for Pre-trained Models 5 Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,the extensive experiments prove that our proposed plugd can significantly reduce the computational cost and effectively inject document knowledge into ptms to improve performance.,Solved,https://export.arxiv.org/pdf/2305.17660v1.pdf
360,258436815,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",conclusion,Insight-tree,"in this paper, we propose the largest chinese medical qa dataset to date, consisting of 26 million medical qa pairs, expanding the size of existing datasets by more than 2 orders of magnitude.",Solved,https://export.arxiv.org/pdf/2305.01526v1.pdf
361,258436815,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",conclusion,Insight-tree,"at the same time, we benchmark many existing works based on the data set and found that these methods still have a lot of room for improvement in medical qa scenarios.",Solved,https://export.arxiv.org/pdf/2305.01526v1.pdf
362,258436815,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",conclusion,Insight-tree,we also demonstrate the possible uses of the dataset in practice.,Solved,https://export.arxiv.org/pdf/2305.01526v1.pdf
363,258436815,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",conclusion,Insight-tree,the experimental results show that the dataset contains rich medical knowledge that can be very helpful to existing datasets and tasks.,Solved,https://export.arxiv.org/pdf/2305.01526v1.pdf
364,258436815,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",conclusion,Insight-tree,"we hope that the huatuo-26m dataset can not only help promote the research of medical qa, but also practically help doctors and patients.",Solved,https://export.arxiv.org/pdf/2305.01526v1.pdf
365,251718892,Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, we study the task of multi-hop question answering and propose to stepwise locate the single-hop supporting sentences and generate more fact-grounded single-hop questions for better interpretable multi-hop reasoning.",Solved,https://www.aclanthology.org/2022.coling-1.142.pdf
366,251718892,Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering,conclusion,Insight-tree,we present a stepwise reasoning framework to incorporate both single-hop supporting sentence identification and the corresponding single-hop question generation for each intermediate step until inferring a final result.,Solved,https://www.aclanthology.org/2022.coling-1.142.pdf
367,251718892,Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering,conclusion,Insight-tree,"it employs a pre-trained simple question generator and takes the identified single-hop supporting sentences as base to generate the single-hop question, which obviates the necessity of constructed supervision and helps generate more fact-based single-hop questions.",Solved,https://www.aclanthology.org/2022.coling-1.142.pdf
368,251718892,Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering,conclusion,Insight-tree,it utilizes a unified reader to jointly learn both intermediate hop reasoning and final hop inference for better fault tolerance.,Solved,https://www.aclanthology.org/2022.coling-1.142.pdf
369,251718892,Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering,conclusion,Insight-tree,experimental results validate the general effectiveness and interpretability of our stepreasoner.,Solved,https://www.aclanthology.org/2022.coling-1.142.pdf
370,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,conclusion,Insight-tree,the dynamic adaptive reasoner improves generalization to unseen sub-questions.,Solved,https://www.aclanthology.org/2022.emnlp-main.356.pdf
371,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,conclusion,Insight-tree,our method successfully learns the intermediate reasoning process and shows better interpretability.,Solved,https://www.aclanthology.org/2022.emnlp-main.356.pdf
372,253244513,Natural Language Deduction with Incomplete Information,conclusion,Insight-tree,"in this work, we tackle the generation of missing premise statements in textual reasoning through the use of abduction.",Solved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
373,253244513,Natural Language Deduction with Incomplete Information,conclusion,Insight-tree,"we introduce a new system capable of abductive and deductive step generation, which yields inferred missing premises while building a proof showing its reasoning.",Solved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
374,253244513,Natural Language Deduction with Incomplete Information,conclusion,Insight-tree,"furthermore, we propose a novel validation method that reduces hallucination and other common failure modes in end-to-end and stepwise searches.",Solved,https://www.aclanthology.org/2022.emnlp-main.564.pdf
375,255545881,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,conclusion and future work,Insight-tree,"to study the zero-shot capability of the logical reasoning models, we propose the first benchmark for the generalized zero-shot logical reasoning, named zslr.",Solved,https://export.arxiv.org/pdf/2301.02983v1.pdf
376,255545881,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,conclusion and future work,Insight-tree,it includes six splits sampled with three strategies and two metrics to comprehensively evaluate the performances.,Solved,https://export.arxiv.org/pdf/2301.02983v1.pdf
377,255545881,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,conclusion and future work,Insight-tree,"also, we propose a model taco to enhance the reasoning type perception through the heuristic input reconstruction and the type-aware contrastive learning.",Solved,https://export.arxiv.org/pdf/2301.02983v1.pdf
378,255545881,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,conclusion and future work,Insight-tree,"also, we conduct extensive experiments on the zero-shot splits, full-data setting as well as other dataset.",Solved,https://export.arxiv.org/pdf/2301.02983v1.pdf
379,255545881,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,conclusion and future work,Insight-tree,superior results illustrate the effectiveness and generalization capability of the proposed modules.,Solved,https://export.arxiv.org/pdf/2301.02983v1.pdf
380,253157925,DyREx: Dynamic Query Representation for Extractive Question Answering,conclusion,Insight-tree,"in this paper, we propose dyrex, a method to dynamically compute query representations to calculate the start and end positions of answer spans in extractive question answering.",Solved,https://export.arxiv.org/pdf/2210.15048v1.pdf
381,253157925,DyREx: Dynamic Query Representation for Extractive Question Answering,conclusion,Insight-tree,"our approach consistently outperforms the dominant approach on a wide range of qa datasets, and the gain is even more significant in a few-shot scenario.",Solved,https://export.arxiv.org/pdf/2210.15048v1.pdf
382,240353952,Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections,conclusion,Insight-tree,we present dcqa that connects pieces in a document via open-ended questions and full-sentence answers.,Solved,https://www.aclanthology.org/2022.emnlp-main.806.pdf
383,240353952,Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections,conclusion,Insight-tree,dcqa is collected via a new paradigm that regards the main purpose of a new sentence as an answer to a free-form question evoked earlier in the context.,Solved,https://www.aclanthology.org/2022.emnlp-main.806.pdf
384,240353952,Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections,conclusion,Insight-tree,"consequently, this paradigm yields both discourse and semantic links across all sentences in a document.",Solved,https://www.aclanthology.org/2022.emnlp-main.806.pdf
385,240353952,Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections,conclusion,Insight-tree,"dcqa is introduced with the goal of providing a more scalable data collection paradigm, also as initial resource, for answering open-ended questions for discourse comprehension.",Solved,https://www.aclanthology.org/2022.emnlp-main.806.pdf
386,240353952,Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections,conclusion,Insight-tree,our experiments showed that dcqa provides valuable supervision for such tasks.,Solved,https://www.aclanthology.org/2022.emnlp-main.806.pdf
387,252407569,Dynamic Relevance Graph Network for Knowledge-Aware Question Answering,conclusion,Insight-tree,"in this paper, we propose a novel dynamic relevance graph network (drgn) architecture for commonsense question answering given an external source of knowledge in the form of a knowledge graph.",Solved,https://www.aclanthology.org/2022.coling-1.116.pdf
388,252407569,Dynamic Relevance Graph Network for Knowledge-Aware Question Answering,conclusion,Insight-tree,"our model learns the graph node representation while a) exploits the existing relations in kg, b) re-scales the importance of the neighbor nodes in the graph based on training a dynamic relevance matrix, c) establishes direct connections between graph nodes based on measuring the relevance scores of the nodes dynamically during training.",Solved,https://www.aclanthology.org/2022.coling-1.116.pdf
389,252407569,Dynamic Relevance Graph Network for Knowledge-Aware Question Answering,conclusion,Insight-tree,the dynamic relevance edges help in finding the chain of reasoning when there are missing edges in the original kg.,Solved,https://www.aclanthology.org/2022.coling-1.116.pdf
390,252407569,Dynamic Relevance Graph Network for Knowledge-Aware Question Answering,conclusion,Insight-tree,our quantitative and qualitative analysis shows that the proposed approach facilitates answering the complex questions that need multiple hops of reasoning.,Solved,https://www.aclanthology.org/2022.coling-1.116.pdf
391,252407569,Dynamic Relevance Graph Network for Knowledge-Aware Question Answering,conclusion,Insight-tree,"furthermore, since drgn uses the relevance between the question node and graph entities, it exploits the richer semantic context of the question in graph reasoning which leads to improvements in the performance on the negative questions.",Solved,https://www.aclanthology.org/2022.coling-1.116.pdf
392,252407569,Dynamic Relevance Graph Network for Knowledge-Aware Question Answering,conclusion,Insight-tree,"our proposed approach shows competitive performance on two qa benchmarks, including commonsenseqa and openbookqa.",Solved,https://www.aclanthology.org/2022.coling-1.116.pdf
393,89838681,Detection of non specific toll-like receptor 3 in the marine and freshwater fishes,conclusion,Insight-tree,the results of phylogenetic analysis showed that the tlr3 gene of the catfish was closely related to i. punctatus and c. batracus as well as some freshwater fish and seawater.,Solved,
394,250729995,ScienceQA: a novel resource for question answering on scholarly articles,conclusion and future work,Insight-tree,"in this paper, we present scienceqa, a novel dataset for benchmark evaluation of methods in the mrc (qa and qg in particular) task on scholarly articles.",Solved,
395,250729995,ScienceQA: a novel resource for question answering on scholarly articles,conclusion and future work,Insight-tree,"the dataset is created semi-automatically, consisting of over 100k triples of context-question-answer.",Solved,
396,250729995,ScienceQA: a novel resource for question answering on scholarly articles,conclusion and future work,Insight-tree,the developed qa system could provide valuable evidence in managing the vast number of scholarly submissions.,Solved,
397,250729995,ScienceQA: a novel resource for question answering on scholarly articles,conclusion and future work,Insight-tree,"we offer a baseline and two more models, viz., (i). vanilla bert, (ii). science bert (i.e., scib-ert), and (iii). combination of bert and bi-daf.",Solved,
398,250729995,ScienceQA: a novel resource for question answering on scholarly articles,conclusion and future work,Insight-tree,our proposed models are competitive compared to the existing state-of-the-art models.,Solved,
399,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"the continued growth of probing-based papers has also led to recent work examining best practices for probes, and how to interpret their results.",Solved,https://export.arxiv.org/pdf/2103.11072v2.pdf
400,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,discussion and conclusion,Insight-tree,this paper focused on the local interpretable methods commonly used for natural language processing models.,Solved,https://export.arxiv.org/pdf/2103.11072v2.pdf
401,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,discussion and conclusion,Insight-tree,"in this survey, we have divided these methods into three different categories based on their underlying characteristics: 1) explaining the model's outputs from the input features, where these features could be identified through rationale extraction, perturbing inputs, traditional attribution methods, and attention weight extraction; 2) generating the natural language explanations corresponding to each inputs; 3) using diagnostic classifiers to analyse the hidden information stored within a model.",Solved,https://export.arxiv.org/pdf/2103.11072v2.pdf
402,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,discussion and conclusion,Insight-tree,"for each method type, we have also outlined the common datasets used for different nlp tasks and different evaluation methods for examining the validity and efficacy of the explanations provided.",Solved,https://export.arxiv.org/pdf/2103.11072v2.pdf
403,250390687,MultiSpanQA: A Dataset for Multi-Span Question Answering,conclusion,Insight-tree,"we present multispanqa, a reading comprehension dataset where answers consist of multiple discrete spans.",Solved,https://www.aclanthology.org/2022.naacl-main.90.pdf
404,250390687,MultiSpanQA: A Dataset for Multi-Span Question Answering,conclusion,Insight-tree,"as part of this, we proposed a method for classifying the semantic structure of answers, based on the semantic relation between answer spans.",Solved,https://www.aclanthology.org/2022.naacl-main.90.pdf
405,250390687,MultiSpanQA: A Dataset for Multi-Span Question Answering,conclusion,Insight-tree,"we also provide an expanded version of the dataset which includes unanswerable questions and single-answer questions, to make it both more challenging and more realistic.",Solved,https://www.aclanthology.org/2022.naacl-main.90.pdf
406,250390687,MultiSpanQA: A Dataset for Multi-Span Question Answering,conclusion,Insight-tree,"we additionally presented a number of models for multi-span qa extraction, and found that the best-performing model was sequence tagging-based, augmented by a span number prediction module and span adjustment module.",Solved,https://www.aclanthology.org/2022.naacl-main.90.pdf
407,238856994,Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,conclusion and future work,Insight-tree,"in this work, we present elue, which is a public benchmark and platform for efficient models, and elasticbert, which is a strong baseline (backbone) for efficient static (dynamic) models.",Solved,https://www.aclanthology.org/2022.naacl-main.240.pdf
408,235313620,Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins,conclusion,Insight-tree,"we demonstrate how seemingly unrelated tasks spanning data integration, search, and recommendation can all be viewed as instantiations of context enrichment.",Solved,https://arxiv.org/pdf/2106.01501v1.pdf
409,235313620,Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins,conclusion,Insight-tree,"we propose keyless joins as a unifying abstraction that can power a system for general context enrichment, which allows us to view context enrichment as a data management problem.",Solved,https://arxiv.org/pdf/2106.01501v1.pdf
410,235313620,Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins,conclusion,Insight-tree,"consequently, we developed and applied ember, a first-of-its kind system that performs no-code context enrichment via keyless joins.",Solved,https://arxiv.org/pdf/2106.01501v1.pdf
411,235313620,Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins,conclusion,Insight-tree,"we evaluate how developing a keyless join enrichment layer empowers a single system to generalize to five downstream applications, with no ml code written by the user.",Solved,https://arxiv.org/pdf/2106.01501v1.pdf
412,214802134,Graph Sequential Network for Reasoning over Sequences,conclusion,Insight-tree,this paper proposes graph sequential network as a novel neural architecture to facilitate reasoning over graphs with sequential data on the nodes.,Solved,https://arxiv.org/pdf/2004.02001v1.pdf
413,214802134,Graph Sequential Network for Reasoning over Sequences,conclusion,Insight-tree,we develop a new message passing algorithm based on co-attention between two sequences on graph nodes.,Solved,https://arxiv.org/pdf/2004.02001v1.pdf
414,214802134,Graph Sequential Network for Reasoning over Sequences,conclusion,Insight-tree,"the scheme avoids the information loss inherent in the pooling based early summarization of existing gnn-based models, and improve the reasoning ability on sentence level.",Solved,https://arxiv.org/pdf/2004.02001v1.pdf
415,214802134,Graph Sequential Network for Reasoning over Sequences,conclusion,Insight-tree,"through experiments on hotpotqa and fever, both of which require the model to perform multi-hop reasoning, we show that our proposed gsn attains better performance than existing gnns on different types of tasks.",Solved,https://arxiv.org/pdf/2004.02001v1.pdf
416,258378176,Combining Parameter-efficient Modules for Task-level Generalisation,conclusions,Insight-tree,"to this end, we proposed a model where a subset of latent, discrete skills from a fixed inventory is allocated to each task in an end-to-end fashion.",Solved,https://www.aclanthology.org/2023.eacl-main.49.pdf
417,258378176,Combining Parameter-efficient Modules for Task-level Generalisation,conclusions,Insight-tree,"the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters.",Solved,https://www.aclanthology.org/2023.eacl-main.49.pdf
418,258378176,Combining Parameter-efficient Modules for Task-level Generalisation,conclusions,Insight-tree,we evaluate the sample efficiency of our model on multitask instruction following through reinforcement learning and its few-shot adaptability on multitask text-to-text generation through supervised learning.,Solved,https://www.aclanthology.org/2023.eacl-main.49.pdf
419,258378176,Combining Parameter-efficient Modules for Task-level Generalisation,conclusions,Insight-tree,"in both experiments, we surpass competitive baselines such as conditional parameter generation (hyperformer) and mixture of experts (task-moe).",Solved,https://www.aclanthology.org/2023.eacl-main.49.pdf
420,258378176,Combining Parameter-efficient Modules for Task-level Generalisation,conclusions,Insight-tree,"finally, we show that modularity helps interpret multi-task models by inferring explicit relationships between tasks according to the skills they share.",Solved,https://www.aclanthology.org/2023.eacl-main.49.pdf
421,216562779,MAVEN: A Massive General Domain Event Detection Dataset,conclusion and future work,Insight-tree,"in this paper, we present a massive general domain event detection dataset (maven), which significantly alleviates the data sparsity and low coverage problems of existing datasets.",Solved,https://arxiv.org/pdf/2004.13590v1.pdf
422,216562779,MAVEN: A Massive General Domain Event Detection Dataset,conclusion and future work,Insight-tree,"we conduct a thorough evaluation of the state-of-the-art models on our maven and observe an obvious performance drop, which indicates that our maven is challenging and may facilitate further research on ed.",Solved,https://arxiv.org/pdf/2004.13590v1.pdf
423,237450610,Flexible Generation of Natural Language Deductions,conclusion,Insight-tree,building systems that use natural language as a medium for reasoning will require operations to logically combine and transform natural language statements.,Solved,https://www.aclanthology.org/2021.emnlp-main.506.pdf
424,237450610,Flexible Generation of Natural Language Deductions,conclusion,Insight-tree,"in this work, we present parapattern, a method for creating such models with minimal manual effort by finetuning pretrained sequence-to-sequence language models on data generated through a three-step process of syntactic retrieval, template expansion, and automatic paraphrasing.",Solved,https://www.aclanthology.org/2021.emnlp-main.506.pdf
425,237450610,Flexible Generation of Natural Language Deductions,conclusion,Insight-tree,"our experimental results show that parapattern yields operation models capable of generating consistent logical transformations over a diverse range of natural language inputs, matching the performance of models trained with in-domain human supervision.",Solved,https://www.aclanthology.org/2021.emnlp-main.506.pdf
426,202558815,A Discrete Hard EM Approach for Weakly Supervised Question Answering,conclusion,Insight-tree,"in this paper, we demonstrated that, for many qa tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option.",Solved,https://www.aclweb.org/anthology/D19-1284.pdf
427,202558815,A Discrete Hard EM Approach for Weakly Supervised Question Answering,conclusion,Insight-tree,"then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution.",Solved,https://www.aclweb.org/anthology/D19-1284.pdf
428,202558815,A Discrete Hard EM Approach for Weakly Supervised Question Answering,conclusion,Insight-tree,"we showed that this approach significantly outperforms previous approaches on six qa tasks including reading comprehension, opendomain qa, discrete reasoning task and semantic parsing, achieving absolute gains of 2-10% and setting the new state-of-the-art on five wellstudied datasets.",Solved,https://www.aclweb.org/anthology/D19-1284.pdf
429,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"in this paper, we propose a novel method to improve the accuracy, credibility and traceability of large language models (llm) for complex knowledge-intensive tasks called searchain, which is a framework for deep interaction between llm and information retrieval (ir).",Solved,https://export.arxiv.org/pdf/2304.14732v1.pdf
430,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"experiments on multi-hop question answering datasets show that searchain not only has strong knowledge-reasoning ability when faced with complex questions but also can effectively exploit the interaction with ir to supplement and correct its knowledge, so as to improve the accuracy and credibility of the generated contents.",Solved,https://export.arxiv.org/pdf/2304.14732v1.pdf
431,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"besides, searchain effectively decouples the knowledge of llm and ir, which avoids the misleading of llm and can also accurately explain whether the knowledge involved in solving knowledge-intensive tasks comes from parameters or external ir.",Solved,https://export.arxiv.org/pdf/2304.14732v1.pdf
432,236477844,PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval,conclusion and future work,Insight-tree,this paper presented a novel dense passage retrieval approach that leverages both query-centric and passage-centric similarity relations for capturing more comprehensive semantic relations.,Solved,https://export.arxiv.org/pdf/2108.06027v2.pdf
433,236477844,PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval,conclusion and future work,Insight-tree,"to implement our approach, we made three important technical contributions in the loss formulation, training data augmentation and effective training procedure.",Solved,https://export.arxiv.org/pdf/2108.06027v2.pdf
434,236477844,PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval,conclusion and future work,Insight-tree,extensive results demonstrated the effectiveness of our approach.,Solved,https://export.arxiv.org/pdf/2108.06027v2.pdf
435,236477844,PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval,conclusion and future work,Insight-tree,"to our knowledge, it is the first time that passage-centric similarity relation has been considered for dense passage retrieval.",Solved,https://export.arxiv.org/pdf/2108.06027v2.pdf
436,225067214,READONCE Transformers: Reusable Representations of Text for Transformers,conclusion,Insight-tree,"this work introduced readonce transformers, a novel approach for using large scale transformerbased language models to both build and consume reusable document representations.",Solved,https://www.aclanthology.org/2021.acl-long.554.pdf
437,230799347,Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies,conclusion,Insight-tree,"we present strategyqa, the first dataset of implicit multi-step questions requiring a wide-range of reasoning skills.",Solved,https://arxiv.org/pdf/2101.02235v1.pdf
438,230799347,Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies,conclusion,Insight-tree,"to build strategyqa, we introduced a novel annotation pipeline for eliciting creative questions that use simple language, but cover a challenging range of diverse strategies.",Solved,https://arxiv.org/pdf/2101.02235v1.pdf
439,230799347,Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies,conclusion,Insight-tree,"questions in strategyqa are annotated with decomposition into reasoning steps and evidence paragraphs, to guide the ongoing research towards addressing implicit multi-hop reasoning.",Solved,https://arxiv.org/pdf/2101.02235v1.pdf
440,252993059,RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses,conclusion,Insight-tree,"in this paper, we study to utilize pretrained t5 models on text ranking.",Solved,https://export.arxiv.org/pdf/2210.10634v1.pdf
441,252993059,RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses,conclusion,Insight-tree,"unlike monot5, which simply converts a text ranking problem into a t5-compatible token generation task, our paper presents two model variants that output numerical numbers based on the t5 architecture: an encoderdecoder model and an encoder-only model.",Solved,https://export.arxiv.org/pdf/2210.10634v1.pdf
442,252993059,RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses,conclusion,Insight-tree,we thereby propose fine-tuning t5 with ranking losses to maximize ranking metrics.,Solved,https://export.arxiv.org/pdf/2210.10634v1.pdf
443,252993059,RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses,conclusion,Insight-tree,we demonstrate significant improvement when ranking losses are used to fine-tune the t5 model on the ms marco and the nq data sets.,Solved,https://export.arxiv.org/pdf/2210.10634v1.pdf
444,252993059,RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses,conclusion,Insight-tree,we further illustrate that such improvement can be maintained in the zero-shot setting on out-of-domain data sets.,Solved,https://export.arxiv.org/pdf/2210.10634v1.pdf
445,258822799,HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"we introduce helma, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing and improving hallucinations.",Solved,https://export.arxiv.org/pdf/2305.11747v1.pdf
446,258822799,HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"to automatically generate large-scale samples, we propose a chatgpt-based two-step approach, i.e., samplingthen-filtering.",Solved,https://export.arxiv.org/pdf/2305.11747v1.pdf
447,258822799,HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one.,Solved,https://export.arxiv.org/pdf/2305.11747v1.pdf
448,258822799,HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.",Solved,https://export.arxiv.org/pdf/2305.11747v1.pdf
449,257427664,Challenges in Explanation Quality Evaluation,conclusion,Insight-tree,this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face.,Solved,https://export.arxiv.org/pdf/2210.07126v2.pdf
450,257427664,Challenges in Explanation Quality Evaluation,conclusion,Insight-tree,"we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate the discussed characteristics.",Solved,https://export.arxiv.org/pdf/2210.07126v2.pdf
451,257427664,Challenges in Explanation Quality Evaluation,conclusion,Insight-tree,we support our arguments with empirical evidence of a crowdsourced case study that we conducted for the example of explainable question answering systems from the hotpotqa leaderboard.,Solved,https://export.arxiv.org/pdf/2210.07126v2.pdf
452,254246737,What is Not in the Context? Evaluation of Few-shot Learners with Informative Demonstrations,conclusion,Insight-tree,"this work introduces a task of conceptual few-shot learning, that reflects on language models' ability to learn in-context and apply a specific, possibly novel reasoning concept.",Solved,https://export.arxiv.org/pdf/2212.01692v1.pdf
453,215238846,Multi-Step Inference for Reasoning Over Paragraphs,conclusion,Insight-tree,we propose a multi-step reading comprehension model that performs chained inference over natural language text.,Solved,https://arxiv.org/pdf/2004.02995v2.pdf
454,215238846,Multi-Step Inference for Reasoning Over Paragraphs,conclusion,Insight-tree,"we have demonstrated that our model substantially outperforms prior work on ropes, a challenging new reading comprehension dataset.",Solved,https://arxiv.org/pdf/2004.02995v2.pdf
455,245219136,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,conclusion,Insight-tree,we introduce a multi-task learning objective the combines answer generation and evidentiality prediction.,Solved,https://www.aclanthology.org/2022.naacl-main.162.pdf
456,245219136,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,conclusion,Insight-tree,we propose task-agnostic data mining techniques to obtain silver evidentiality labels to enable this auxiliary training.,Solved,https://www.aclanthology.org/2022.naacl-main.162.pdf
457,245219136,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,conclusion,Insight-tree,"our experiments across five datasets show large performance improvements over baselines and our evidentialityguided generator advances the state-of-the-art performance on faviq-ambig, fever and wow.",Solved,https://www.aclanthology.org/2022.naacl-main.162.pdf
458,245219136,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,conclusion,Insight-tree,our analysis shows that multi-task learning and silver evidentiality mining both contribute to the performance improvements by helping the model learn to focus on and generate answers from more relevant passages.,Solved,https://www.aclanthology.org/2022.naacl-main.162.pdf
459,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,we believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks.,Solved,https://arxiv.org/pdf/2009.09139v3.pdf
460,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,"each ca-mtl module that adapts a transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances.",Solved,https://arxiv.org/pdf/2009.09139v3.pdf
461,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,this evidence shows that ca-mtl is able to mitigate task interference and promote more efficient parameter sharing.,Solved,https://arxiv.org/pdf/2009.09139v3.pdf
462,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,we showed that mt-uncertainty is able to avoid degrading performances of low resource tasks.,Solved,https://arxiv.org/pdf/2009.09139v3.pdf
463,220302524,Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval,conclusion,Insight-tree,ance fundamentally eliminates the discrepancy between the representation learning of texts and their usages in dense retrieval.,Solved,https://arxiv.org/pdf/2007.00808v1.pdf
464,220302524,Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval,conclusion,Insight-tree,"our ance trained dense retrieval model, the vanilla bert-siamese, convincingly outperforms all dense retrieval and sparse retrieval baselines in our large scale document retrieval and passage retrieval experiments.",Solved,https://arxiv.org/pdf/2007.00808v1.pdf
465,220302524,Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval,conclusion,Insight-tree,it nearly matches the ranking accuracy of the state-of-theart cascade sparse retrieval and bert reranking pipeline.,Solved,https://arxiv.org/pdf/2007.00808v1.pdf
466,220302524,Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval,conclusion,Insight-tree,"more importantly, all these advantages are achieved with a standard transformer encoder at a 1% online inference latency, using a simple dot-product in the ance-learned representation space.",Solved,https://arxiv.org/pdf/2007.00808v1.pdf
467,252368291,Learning to Answer Semantic Queries over Code,conclusions and future work,Insight-tree,we presented the codequeries dataset which tests the ability of neural models for code understanding on the proposed problem of answering semantic queries over code.,Solved,https://export.arxiv.org/pdf/2209.08372v1.pdf
468,258053774,Cancer-Specific Survival after Limb Salvage versus Amputation in Children and Adolescents with Osteosarcoma: A Population-Based Analysis with Propensity Score Matching,conclusion,Insight-tree,"to summarize, we established and validated a novel nomogram for os patients of cya, which could serve as concise and practical tools for clinicians to anticipate the 1-, 3-, and 5-years css.",Solved,
469,258053774,Cancer-Specific Survival after Limb Salvage versus Amputation in Children and Adolescents with Osteosarcoma: A Population-Based Analysis with Propensity Score Matching,conclusion,Insight-tree,lss for patients with os exhibited signifcant beneft on css compared with amputation.,Solved,
470,236771976,MuSiQue: Multihop Questions via Single-hop Question Composition,conclusion,Insight-tree,a bottom-up process of constructing multihop from single-hop questions allows systematic exploration of a large space of multihop candidates and greater control over which questions we compose.,Solved,https://www.aclanthology.org/2022.tacl-1.31.pdf
471,236771976,MuSiQue: Multihop Questions via Single-hop Question Composition,conclusion,Insight-tree,"we showed how to use such a carefully controlled process to create a challenging dataset that, by design, requires connected reasoning by reducing potential reasoning shortcuts, minimizing train-test leakage, and including harder distractor contexts.",Solved,https://www.aclanthology.org/2022.tacl-1.31.pdf
472,236771976,MuSiQue: Multihop Questions via Single-hop Question Composition,conclusion,Insight-tree,empirical results show that -ans has a substantially higher human-model gap and is significantly less cheatable via disconnected reasoning than previous datasets.,Solved,https://www.aclanthology.org/2022.tacl-1.31.pdf
473,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,"in this paper, we propose reasoning circuits, a new framework suited to real-world scenarios where the nlp task at hand requires multiple steps of structured reasoning, with only a limited number of available labelled examples, and a small annotation budget, also only a modest deep learning computational infrastructure/budget is accessible.",Solved,https://export.arxiv.org/pdf/2211.08466v1.pdf
474,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,"in this work, we apply this framework to the task of fewshot multi-hop question generation which fits all these criteria.",Solved,https://export.arxiv.org/pdf/2211.08466v1.pdf
475,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,we identify structured multi-step rationales that break down this problem into many discrete reasoning steps.,Solved,https://export.arxiv.org/pdf/2211.08466v1.pdf
476,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,we show that fine-tuning with only around 64 to 128 labelled rationale examples with our approach is enough to improve automatic evaluation metrics compared to a baseline trained without rationales on the hot-potqa dataset.,Solved,https://export.arxiv.org/pdf/2211.08466v1.pdf
477,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,"more importantly, with human evaluation, we find that this framework can strongly improve the central objective of multi-hop qg, to generate challenging questions which cannot be answered from reading only a single passage.",Solved,https://export.arxiv.org/pdf/2211.08466v1.pdf
478,241583187,FaBULOUS: Fact-checking Based on Understanding of Language Over Unstructured and Structured information,conclusion,Insight-tree,"overall, this multi-modal task creates a plethora of new challenges to overcome and opens exciting avenues of research for the future of automated fact verification.",Solved,https://www.aclanthology.org/2021.fever-1.4.pdf
479,243986045,Recent Advances in Automated Question Answering In Biomedical Domain,conclusion,Insight-tree,"we provided a review of general domain question answering using knowledge bases, texts or both, before moving on to biomedical question answering systems.",Solved,https://arxiv.org/pdf/2111.05937v1.pdf
480,243986045,Recent Advances in Automated Question Answering In Biomedical Domain,conclusion,Insight-tree,we explored current state of the art biomedical qa systems and discussed their limitations.,Solved,https://arxiv.org/pdf/2111.05937v1.pdf
481,243986045,Recent Advances in Automated Question Answering In Biomedical Domain,conclusion,Insight-tree,we finally provided an overall analysis of limitations of bqa systems along with ways to overcome them.,Solved,https://arxiv.org/pdf/2111.05937v1.pdf
482,243986045,Recent Advances in Automated Question Answering In Biomedical Domain,conclusion,Insight-tree,we finally explore the potential areas of focus for further research.,Solved,https://arxiv.org/pdf/2111.05937v1.pdf
483,252818979,Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering,conclusion,Insight-tree,"this paper proposes hdrn, a novel model for representation learning and reasoning for biomedical question answering.",Solved,https://www.aclanthology.org/2022.coling-1.127.pdf
484,252818979,Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering,conclusion,Insight-tree,"first, we construct hierarchical representations to obtain a deep understanding of the biomedical evidences. then, we perform multi-step dynamic reasoning to solve complex biomedical questions.",Solved,https://www.aclanthology.org/2022.coling-1.127.pdf
485,252818979,Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering,conclusion,Insight-tree,we evaluate our model on three bioqa datasets and achieve new state-of-theart performances.,Solved,https://www.aclanthology.org/2022.coling-1.127.pdf
486,247476426,Relation Leakage in Elicited Natural Language Inference Datasets,conclusion,Insight-tree,"we have introduced useful tools and techniques for analyzing elicited sentence relation leakage bias in nli datasets, and applied them to a large, representative set of popular current datasets.",Solved,https://arxiv.org/pdf/2112.09237v2.pdf
487,235678938,Controllable Open-ended Question Generation with A New Question Type Ontology,conclusion,Insight-tree,we present a new question type ontology which better captures the nuances of questions to support the study of open-ended question generation.,Solved,https://www.aclanthology.org/2021.acl-long.502.pdf
488,235678938,Controllable Open-ended Question Generation with A New Question Type Ontology,conclusion,Insight-tree,"we further annotate a new dataset with 4,959 questions based on the proposed ontology.",Solved,https://www.aclanthology.org/2021.acl-long.502.pdf
489,235678938,Controllable Open-ended Question Generation with A New Question Type Ontology,conclusion,Insight-tree,"we describe a joint question focus detection and question generation framework with a novel semantic graphaugmented representation, which is directly built on large pre-trained models.",Solved,https://www.aclanthology.org/2021.acl-long.502.pdf
490,235678938,Controllable Open-ended Question Generation with A New Question Type Ontology,conclusion,Insight-tree,"based on this framework, we also enhance the controllability and diversity of generated questions by employing template exemplars or automatically generated templates.",Solved,https://www.aclanthology.org/2021.acl-long.502.pdf
491,235678938,Controllable Open-ended Question Generation with A New Question Type Ontology,conclusion,Insight-tree,"experiments on two large datasets show that questions generated by our models have better quality and higher diversity than non-trivial comparisons, with similar results rated by human judges.",Solved,https://www.aclanthology.org/2021.acl-long.502.pdf
492,199668753,Towards Knowledge-Based Recommender Dialog System,conclusion,Insight-tree,"in this paper, we propose a novel end-to-end framework, kbrd, which bridges the gap between the recommender system and the dialog system via knowledge propagation.",Solved,https://www.aclweb.org/anthology/D19-1189.pdf
493,199668753,Towards Knowledge-Based Recommender Dialog System,conclusion,Insight-tree,"through a series of experiments, we show that kbrd can reach better performances in both recommendation and dialog generation in comparison with the baselines. we also discuss how the two systems benefit each other.",Solved,https://www.aclweb.org/anthology/D19-1189.pdf
494,253510370,World Knowledge in Multiple Choice Reading Comprehension,limitations,Insight-tree,we propose an approach that can automatically flag questions that can be answered without contextual information.,Solved,https://export.arxiv.org/pdf/2211.07040v2.pdf
495,243756815,Extracting a Knowledge Base of COVID-19 Events from Social Media,conclusion,Insight-tree,"in this paper, we presented a corpus of 10,000 tweets annotated with 5 types of events and 28 slots.",Solved,https://export.arxiv.org/pdf/2006.02567v4.pdf
496,243756815,Extracting a Knowledge Base of COVID-19 Events from Social Media,conclusion,Insight-tree,we showed that our corpus supports automatic extraction of covid-19 events using supervised learning.,Solved,https://export.arxiv.org/pdf/2006.02567v4.pdf
497,243756815,Extracting a Knowledge Base of COVID-19 Events from Social Media,conclusion,Insight-tree,"by aggregating extractions over millions of tweets, our approach can accurately answer a range of structured queries about events that are publicly reported in real-time on twitter.",Solved,https://export.arxiv.org/pdf/2006.02567v4.pdf
498,243756815,Extracting a Knowledge Base of COVID-19 Events from Social Media,conclusion,Insight-tree,"our knowledge base could be a useful tool for epidemiologists, journalists and policymakers to more efficiently track the spread of this new disease.",Solved,https://export.arxiv.org/pdf/2006.02567v4.pdf
499,243756815,Extracting a Knowledge Base of COVID-19 Events from Social Media,conclusion,Insight-tree,this work also presents a case-study on how an information extraction system can be rapidly developed for a new domain in response to an emerging crisis.,Solved,https://export.arxiv.org/pdf/2006.02567v4.pdf
500,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"we have shown evidence that our method, qlora, can replicate 16-bit full finetuning performance with a 4-bit base model and low-rank adapters (lora).",Solved,https://export.arxiv.org/pdf/2305.14314v1.pdf
501,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,we see that the average score in guanaco-65b is much lower than other raw pretrained models.,Solved,https://export.arxiv.org/pdf/2305.14314v1.pdf
502,202539540,QUARTZ: An Open-Domain Dataset of Qualitative Relationship Questions,conclusion,Insight-tree,"quartz aims to fill this gap by providing the first open-domain dataset of qualitative relationship questions, along with the requisite qualitative knowledge and a rich set of annotations.",Solved,https://www.aclweb.org/anthology/D19-1608.pdf
503,202539540,QUARTZ: An Open-Domain Dataset of Qualitative Relationship Questions,conclusion,Insight-tree,"specifically, quartz removes the requirement, present in all previous qualitative reasoning work, that a fixed set of qualitative relationships be formally pre-specified.",Solved,https://www.aclweb.org/anthology/D19-1608.pdf
504,202539540,QUARTZ: An Open-Domain Dataset of Qualitative Relationship Questions,conclusion,Insight-tree,"instead, quartz tests the ability of a system to find and apply an arbitrary relationship on the fly to answer a question, including when simple reasoning (arguments, polarities) is required.",Solved,https://www.aclweb.org/anthology/D19-1608.pdf
505,247518803,Synthetic Question Value Estimation for Domain Adaptation of Question Answering,conclusion,Insight-tree,we propose a question value estimator to estimate the usefulness of synthetic questions and select useful ones for improving target-domain qa train- 7,Solved,https://www.aclanthology.org/2022.acl-long.95.pdf
506,247518803,Synthetic Question Value Estimation for Domain Adaptation of Question Answering,conclusion,Insight-tree,"we treat it as a binary classification problem here: if a question is selected, the prediction is 1; 0 otherwise.",Solved,https://www.aclanthology.org/2022.acl-long.95.pdf
507,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,we conducted an extensive exploratory study comparing app issue classification tools and pre-trained transformer-based models in various settings.,Solved,https://arxiv.org/pdf/2104.05861v3.pdf
508,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,"we conducted the experiments on six available datasets and a highly imbalanced dataset, which is a combination of the six datasets.",Solved,https://arxiv.org/pdf/2104.05861v3.pdf
509,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,domain-specific ptms were trained using different sizes of app review data we collected from google play and these customized ptms were also studied here.,Solved,https://arxiv.org/pdf/2104.05861v3.pdf
510,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,"our results confirm that ptms are achieving higher scores in binary and multi-class classification compared to prior approaches, but the over-the-shelf ptms are not always the best models to be used in all scenarios.",Solved,https://arxiv.org/pdf/2104.05861v3.pdf
511,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,"instead, cptms have the highest scores and are able to perform better than other models in all settings.",Solved,https://arxiv.org/pdf/2104.05861v3.pdf
512,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,"moreover, incorporating app specific data in the pre-training of ptms reduces the prediction time.",Solved,https://arxiv.org/pdf/2104.05861v3.pdf
513,256846917,"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",conclusion,Insight-tree,"to this end, we systematically and comprehensively survey state-of-the-art research studies on backdoor attacks and defenses.",Solved,https://export.arxiv.org/pdf/2302.06801v1.pdf
514,256846917,"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",conclusion,Insight-tree,"additionally, we thoroughly review and analyze various aspects of backdoor learning, including techniques, model architectures, evaluation metrics, and benchmark datasets.",Solved,https://export.arxiv.org/pdf/2302.06801v1.pdf
515,165163607,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,conclusion,Insight-tree,"we have introduced boolq, a new reading comprehension dataset of naturally occurring yes/no questions.",Solved,https://www.aclweb.org/anthology/N19-1300.pdf
516,165163607,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,conclusion,Insight-tree,we have shown these questions are challenging and require a wide range of inference abilities to solve.,Solved,https://www.aclweb.org/anthology/N19-1300.pdf
517,165163607,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,conclusion,Insight-tree,"we have also studied how transfer learning performs on this task, and found crowd-sourced entailment datasets can be leveraged to boost performance even on top of language model pre-training.",Solved,https://www.aclweb.org/anthology/N19-1300.pdf
518,231883811,Unification-based Reconstruction of Multi-hop Explanations for Science Questions,conclusion,Insight-tree,this paper proposed a novel framework for multihop explanation reconstruction based on explanatory unification.,Solved,https://www.aclweb.org/anthology/2021.eacl-main.15.pdf
519,231883811,Unification-based Reconstruction of Multi-hop Explanations for Science Questions,conclusion,Insight-tree,an extensive evaluation on the worldtree corpus led to the following conclusions:,Solved,https://www.aclweb.org/anthology/2021.eacl-main.15.pdf
520,216553210,Semantic Graphs for Generating Deep Questions,conclusion and future works,Insight-tree,we propose the problem of dqg to generate questions that requires reasoning over multiple disjoint pieces of information.,Solved,https://www.aclweb.org/anthology/2020.acl-main.135.pdf
521,216553210,Semantic Graphs for Generating Deep Questions,conclusion and future works,Insight-tree,"to this end, we propose a novel framework which incorporates semantic graphs to enhance the input document representations and generate questions by jointly training with the task of content selection.",Solved,https://www.aclweb.org/anthology/2020.acl-main.135.pdf
522,216553210,Semantic Graphs for Generating Deep Questions,conclusion and future works,Insight-tree,"experiments on the hotpotqa dataset demonstrate that introducing semantic graph significantly reduces the semantic errors, and content selection benefits the selection and reasoning over disjoint relevant contents, leading to questions with better quality.",Solved,https://www.aclweb.org/anthology/2020.acl-main.135.pdf
523,258866004,Machine Reading Comprehension using Case-based Reasoning,conclusion,Insight-tree,"we present cbr-mrc, a semi-parametric model for machine reading comprehension that is simple, accurate, and interpretable.",Solved,https://export.arxiv.org/pdf/2305.14815v1.pdf
524,258866004,Machine Reading Comprehension using Case-based Reasoning,conclusion,Insight-tree,"our model stores a collection of cases, retrieves the most relevant cases for a given test question, and then explicitly reuses the reasoning patterns encoded in the embeddings of these cases to predict an answer.",Solved,https://export.arxiv.org/pdf/2305.14815v1.pdf
525,212644640,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,conclusion,Insight-tree,"in this paper, we introduce a novel framework to characterise machine reading comprehension gold standards.",Solved,https://www.aclweb.org/anthology/2020.lrec-1.660.pdf
526,212644640,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,conclusion,Insight-tree,"furthermore we applied the framework to analyse popular state-of-the-art gold standards for machine reading comprehension: we reveal issues with their factual correctness, show the presence of lexical cues and we observe that semantics-altering grammatical modifiers are missing in all of the investigated gold standards.",Solved,https://www.aclweb.org/anthology/2020.lrec-1.660.pdf
527,227334750,KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning,conclusion,Insight-tree,we have proposed a pre-training method by cooperatively modeling the generative and discriminative knowledge injecting approaches.,Solved,https://arxiv.org/pdf/2012.03551v1.pdf
528,227334750,KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning,conclusion,Insight-tree,our model can be easily extended to larger pre-training corpus and does not introduce any modifications for downstream tasks during finetuning.,Solved,https://arxiv.org/pdf/2012.03551v1.pdf
529,227334750,KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning,conclusion,Insight-tree,"experiments show our model consistently outperforms all base models on a variety of question answering datasets, demonstrating that our kgplm is a preferred choice for the knowledge intensive nlp tasks.  ",Solved,https://arxiv.org/pdf/2012.03551v1.pdf
530,233296243,Learning with Instance Bundles for Reading Comprehension,conclusion,Insight-tree,"we have presented a way to use contrastive estimation in a supervised manner to learn from distinguishing cues between multiple related qa pairs, or instance bundles.",Solved,https://www.aclanthology.org/2021.emnlp-main.584.pdf
531,233296243,Learning with Instance Bundles for Reading Comprehension,conclusion,Insight-tree,"our experiments with multiple ce-based loss functions, defined over a joint neighborhood of questions and answers, have shown that these models outperform existing methods on two datasets: ropes and hotpotqa.",Solved,https://www.aclanthology.org/2021.emnlp-main.584.pdf
532,233296243,Learning with Instance Bundles for Reading Comprehension,conclusion,Insight-tree,"apart from presenting several ways to create instance bundles, we also explore theoretical connections between unlikelihood training and contrastive estimation, and initial exploration into when instance bundles are likely to be effective with these methods.",Solved,https://www.aclanthology.org/2021.emnlp-main.584.pdf
533,254366618,Text Embeddings by Weakly-Supervised Contrastive Pre-training,conclusion,Insight-tree,"in this work, we train a general-purpose text embedding model e5 from weak supervision signals.",Solved,https://export.arxiv.org/pdf/2212.03533v1.pdf
534,254366618,Text Embeddings by Weakly-Supervised Contrastive Pre-training,conclusion,Insight-tree,we adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web.,Solved,https://export.arxiv.org/pdf/2212.03533v1.pdf
535,254366618,Text Embeddings by Weakly-Supervised Contrastive Pre-training,conclusion,Insight-tree,"when further customized for downstream tasks, e5 achieves superior fine-tuned performance compared to existing embedding models with 40 more parameters on the large, 56-task mteb benchmark datasets.",Solved,https://export.arxiv.org/pdf/2212.03533v1.pdf
536,248476403,Inferring Implicit Relations with Language Models,conclusion,Insight-tree,"in this work, we propose the task of implicit relation inference, which decouples the inference of reasoning steps from their execution.",Solved,https://arxiv.org/pdf/2204.13778v1.pdf
537,248476403,Inferring Implicit Relations with Language Models,conclusion,Insight-tree,"we introduce implicitrelations, a benchmark that includes over 600 questions implicit reasoning questions along with more than 2,000 annotated implicit relations.",Solved,https://arxiv.org/pdf/2204.13778v1.pdf
538,248476403,Inferring Implicit Relations with Language Models,conclusion,Insight-tree,"we show that large lms can infer implicit relations well in the in-context setup across multiple types of questions and reasoning skills, but that this success does not transfer to an improvement in the downstream task of answering implicit reasoning questions.",Solved,https://arxiv.org/pdf/2204.13778v1.pdf
539,248476403,Inferring Implicit Relations with Language Models,conclusion,Insight-tree,"our work sheds light on the types of capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for further improving the ability of models to infer implicit relations.",Solved,https://arxiv.org/pdf/2204.13778v1.pdf
540,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,"in this paper, we systematically study the robustness of mrc models to entity name substitution.",Solved,https://www.aclanthology.org/2022.naacl-main.37.pdf
541,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,"specifically, we propose a substitution framework along with candidate names of different implications.",Solved,https://www.aclanthology.org/2022.naacl-main.37.pdf
542,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,we experiment with three pretrained language models on five mrc datasets.,Solved,https://www.aclanthology.org/2022.naacl-main.37.pdf
543,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,"we find that models trained on distantly-supervised datasets are susceptible to entity name substitution, while models trained on human-annotated datasets are relatively robust, with gpe renaming harder than per and org renaming.",Solved,https://www.aclanthology.org/2022.naacl-main.37.pdf
544,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,the lack of robustness can be further attributed to model's overreliance on entity knowledge and name clues.,Solved,https://www.aclanthology.org/2022.naacl-main.37.pdf
545,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,"we also find that spanbert, which is pretrained using span-level objectives, shows better robustness than bert and roberta.",Solved,https://www.aclanthology.org/2022.naacl-main.37.pdf
546,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,"leveraging these insights, we study defense approaches based on continual pretraining and demonstrate that entity-based masking policies are beneficial to model's robustness.",Solved,https://www.aclanthology.org/2022.naacl-main.37.pdf
547,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,conclusion,Insight-tree,"we propose a novel extension of soft prompt tuning (pt) -vector quantized input-contextualized prompt tuning (vip), designed to have two desirable characteristics -(i) contextualizing the soft prompt tokens w.r.t input text using a learnable sentence encoder (ii) discretizing the contextual prompts using a vector quantization network.",Solved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
548,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,conclusion,Insight-tree,"further, our generalization studies on out-of-domain evaluations of qa and nli and multi-task settings over 4 tasks also show that vip is able to learn richer and more robust prompt representations than pt.",Solved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
549,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"in appendix a.2 we show that by reducing the number of vip-prompt tokens and codebooksize, we can reduce the parameter size to onethird while compromising performance slightly on superglue.",Solved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
550,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"while vip needs additional hyperparameters, in all our experiments across 20 training datasets from 5 tasks, we fix all hyperparameters related to codebook and sentence-encoder.",Solved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
551,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,this shows that our model is indeed not sensitive to the hyperparameters and does not need very specific tuning for each task/setting.,Solved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
552,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"indeed, by reducing the parameter size of vip, we achieve much better performance on cb. t5-base as backbone plm.",Solved,https://www.aclanthology.org/2022.emnlp-main.455.pdf
553,235446913,A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections,conclusions and future work,Insight-tree,our contributions can be summarized as follows:,Solved,https://www.aclanthology.org/2021.acl-long.301.pdf
554,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,"in this paper, we propose a sequence labeling based question answering (slqa) approach to construct nl-pl pairs in a pl-agnostic way.",Solved,https://arxiv.org/pdf/2203.10744v1.pdf
555,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,"we propose a model that can produce multiple code blocks as solutions of a post's question, which is achieved by using bio sequence tagging.",Solved,https://arxiv.org/pdf/2203.10744v1.pdf
556,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,we also propose to incorporate the global textual context as pl-independent supplementary information.,Solved,https://arxiv.org/pdf/2203.10744v1.pdf
557,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,"to validate the capacity of our method, we manually annotate a challenging cross-pl multi-block dataset, named lang2code-human.",Solved,https://arxiv.org/pdf/2203.10744v1.pdf
558,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,substantial experiments on the single-pl single-block staqc-human and our lang2code-human benchmarks demonstrate the effectiveness and cross-pl transferability of our method.,Solved,https://arxiv.org/pdf/2203.10744v1.pdf
559,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,"finally, we present lang2code, the largest-to-date nl-pl corpus to the best of our knowledge, containing over 1.4 million pairs spanning six pls.",Solved,https://arxiv.org/pdf/2203.10744v1.pdf
560,233296016,BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models,conclusions and future work,Insight-tree,"in this work, we presented beir: a heterogeneous benchmark for information retrieval.",Solved,https://arxiv.org/pdf/2104.08663v4.pdf
561,233296016,BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models,conclusions and future work,Insight-tree,we provided a broader selection of target tasks ranging from narrow expert domains to open domain datasets.,Solved,https://arxiv.org/pdf/2104.08663v4.pdf
562,233296016,BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models,conclusions and future work,Insight-tree,we included nine different retrieval tasks spanning 18 diverse datasets.,Solved,https://arxiv.org/pdf/2104.08663v4.pdf
563,225040262,Stronger Transformers for Neural Multi-Hop Question Generation,conclusion,Insight-tree,"to effectively encode the context documents and the answer, we introduce answer type embeddings and a new sublayer to incorporate the extracted entity-centric graph.",Solved,https://arxiv.org/pdf/2010.11374v1.pdf
564,225040262,Stronger Transformers for Neural Multi-Hop Question Generation,conclusion,Insight-tree,we also propose an auxiliary contrastive objective to identify the supporting facts and a data filtering approach to balance the training-test distribution mismatch.,Solved,https://arxiv.org/pdf/2010.11374v1.pdf
565,225040262,Stronger Transformers for Neural Multi-Hop Question Generation,conclusion,Insight-tree,experiments on the hotpotqa dataset show that our models outperform the current best approaches by a substantial margin of 5 bleu points.,Solved,https://arxiv.org/pdf/2010.11374v1.pdf
566,225040262,Stronger Transformers for Neural Multi-Hop Question Generation,conclusion,Insight-tree,"our analysis further reveals that graph-based components may not be the most critical in improving the performance, but can render complementary strengths to the transformer.  ",Solved,https://arxiv.org/pdf/2010.11374v1.pdf
567,207847640,Blockwise Self-Attention for Long Document Understanding,conclusion,Insight-tree,"in this work, we study the lightweight bert model with the goal of achieving both efficiency and effectiveness.",Solved,https://www.aclweb.org/anthology/2020.findings-emnlp.232.pdf
568,207847640,Blockwise Self-Attention for Long Document Understanding,conclusion,Insight-tree,"we profile and analyze the memory bottlenecks of bert and focus on optimize dotproduct self-attention, which consumes quadratic memory with respect to the sequence length.",Solved,https://www.aclweb.org/anthology/2020.findings-emnlp.232.pdf
569,207847640,Blockwise Self-Attention for Long Document Understanding,conclusion,Insight-tree,"to reduce both time and memory consumption, we present blockbert, which sparsifies the attention matrices to be sparse block matrices.",Solved,https://www.aclweb.org/anthology/2020.findings-emnlp.232.pdf
570,207847640,Blockwise Self-Attention for Long Document Understanding,conclusion,Insight-tree,the proposed model achieves time and memory saving without significant loss of performance.,Solved,https://www.aclweb.org/anthology/2020.findings-emnlp.232.pdf
571,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,in this work we motivate a new challenge task of solving complex task by communicating with existing ai agents.,Solved,https://arxiv.org/pdf/2110.08542v1.pdf
572,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,"towards this goal, we introduce a new benchmark dataset commaqa which involves multihop questions with three multi-hop reasoning challenges, all solvable by composing four qa agents.",Solved,https://arxiv.org/pdf/2110.08542v1.pdf
573,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,each agent has an internal knowledge base (similar to ai assistants or large lms) that can be queried via natural language queries.,Solved,https://arxiv.org/pdf/2110.08542v1.pdf
574,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,we hope this dataset will enable future work on learning to communicate with agents without relying on this additional supervision.,Solved,https://arxiv.org/pdf/2110.08542v1.pdf
575,248299683,STANDING ON THE SHOULDERS OF GIANT FROZEN LANGUAGE MODELS,conclusions,Insight-tree,"this paper has shown that a better alternative exists: freezing a single, huge pretrained lm and learning much smaller neural modules that specialize the lm to different tasks.",Solved,https://arxiv.org/pdf/2204.10019v1.pdf
576,248299683,STANDING ON THE SHOULDERS OF GIANT FROZEN LANGUAGE MODELS,conclusions,Insight-tree,"while prompt tuning, prefix tuning, and other existing frozen model methods cited above can be seen as a simple instantiations of this idea, this paper shows that much more complex architectures can achieve much stronger performance.",Solved,https://arxiv.org/pdf/2204.10019v1.pdf
577,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,"we produced unpredictable, a dataset of 413,299 diverse few-shot learning tasks from internet tables.",Solved,https://export.arxiv.org/pdf/2208.01009v2.pdf
578,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,finetuning on unpredictable improves the fsl ability of lms.,Solved,https://export.arxiv.org/pdf/2208.01009v2.pdf
579,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,"we find that certain narrow datasets (even ones made of trivia) are even more helpful than diverse, curated nlp datasets.",Solved,https://export.arxiv.org/pdf/2208.01009v2.pdf
580,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,"finetuning on these narrow datasets leads to strong improvements on the same test tasks as finetuning on diverse, curated nlp datasets.",Solved,https://export.arxiv.org/pdf/2208.01009v2.pdf
581,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,"we hope our work spurs investigation on what data causes few-shot learning to emerge, both to develop better datasets and to better understand how training data leads to unexpected behaviors or failures.  ",Solved,https://export.arxiv.org/pdf/2208.01009v2.pdf
582,236447339,QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension,conclusion,Insight-tree,"we have presented the most extensive survey of the field to date, identifying the key dimensions along which the current datasets vary.",Solved,https://export.arxiv.org/pdf/2107.12708v2.pdf
583,253117139,Analyzing Multi-Task Learning for Abstractive Text Summarization,conclusion & future work,Insight-tree,"in this work, we studied the influence of multi-task learning combinations of task families during the pre-finetuning stage for english abstractive text summarization.",Solved,https://export.arxiv.org/pdf/2210.14606v2.pdf
584,253117139,Analyzing Multi-Task Learning for Abstractive Text Summarization,conclusion & future work,Insight-tree,"we trained three different training strategies, six task families composed of 18 tasks, and evaluated two downstream tasks.",Solved,https://export.arxiv.org/pdf/2210.14606v2.pdf
585,211832082,"A Survey of Antimicrobial Resistance Determinants in Category A Select Agents, Exempt Strains, and Near-Neighbor Species",conclusions,Insight-tree,"here, we used two complementary technologies-microarray analysis and hrma-to survey 127 select agents, exempt strains, and near-neighbor species for a broad variety of resistance mechanisms acquired through both horizontal transfer and gene mutations.",Solved,
586,211832082,"A Survey of Antimicrobial Resistance Determinants in Category A Select Agents, Exempt Strains, and Near-Neighbor Species",conclusions,Insight-tree,"to our knowledge, this is the largest survey of category a agents, exempt strains, and near-neighbor species for genes covering multiple mechanisms of amr.",Solved,
587,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"the focus on word rather than e.g., pdfs or web pages also allowed us to obtain questions about documents at various stages of development, not just finalized manuscripts.",Solved,https://export.arxiv.org/pdf/2203.15073v2.pdf
588,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"we observed that some users posted multiple questions in succession and close together in time, e.g., in the span of a few minutes.",Solved,https://export.arxiv.org/pdf/2203.15073v2.pdf
589,247779047,Understanding Questions that Arise When Working with Business Documents,conclusion,Insight-tree,"to understand users' actual needs, it was important to collect their document-centric questions in-situ. therefore, we conducted two user studies.",Solved,https://export.arxiv.org/pdf/2203.15073v2.pdf
590,247779047,Understanding Questions that Arise When Working with Business Documents,conclusion,Insight-tree,"in the first study, we performed experience sampling of users' questions via a microsoft word add-in as users were working with their documents.",Solved,https://export.arxiv.org/pdf/2203.15073v2.pdf
591,247779047,Understanding Questions that Arise When Working with Business Documents,conclusion,Insight-tree,"in the second, users submitted their questions via an add-in and received answers from a human-in-the-loop document q&a system that complemented a question-answering ai with human intelligence.",Solved,https://export.arxiv.org/pdf/2203.15073v2.pdf
592,247779047,Understanding Questions that Arise When Working with Business Documents,conclusion,Insight-tree,"we characterized the distributions of questions and observed that the types of questions do indeed vary by whether the user is an author, a reviewer, or a reader of the document.",Solved,https://export.arxiv.org/pdf/2203.15073v2.pdf
593,247779047,Understanding Questions that Arise When Working with Business Documents,conclusion,Insight-tree,"in addition, the questions gave us insight into what types of request can be automated and whether particular skillsets or roles within the document are needed from human respondents in a document digital assistant that is co-powered by artificial and human intelligence.",Solved,https://export.arxiv.org/pdf/2203.15073v2.pdf
594,231627885,COARSE-GRAINED DECOMPOSITION AND FINE-GRAINED INTERACTION FOR MULTI-HOP QUESTION ANSWERING,conclusion and future work,Insight-tree,"in this paper, we propose a mutli-hop question answering model, that contains a coarse-grained decomposition strategy to divide a complex query into multiple single-hop simple queries and a fine- grained interaction strategy to better represent each word in the document and help the model find the sentences needed to answer the question.",Solved,https://arxiv.org/pdf/2101.05988v1.pdf
595,231627885,COARSE-GRAINED DECOMPOSITION AND FINE-GRAINED INTERACTION FOR MULTI-HOP QUESTION ANSWERING,conclusion and future work,Insight-tree,"in the experiments, we show that our models significantly and consistently outperform the baseline model.",Solved,https://arxiv.org/pdf/2101.05988v1.pdf
596,246015349,Natural Language Deduction through Search over Statement Compositions,discussion and conclusion,Insight-tree,"in this work, we propose a system that performs natural language reasoning through generative deduction and heuristic-guided search.",Solved,https://export.arxiv.org/pdf/2201.06028v2.pdf
597,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"the baseline approach we consider in this work, end-to-end modeling of entailment tree generation, enjoys the convenience of simple inference and quadratic complexity.",Solved,https://export.arxiv.org/pdf/2201.06028v2.pdf
598,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,our structured approach allows arbitrarily large premise sets and step counts.,Solved,https://export.arxiv.org/pdf/2201.06028v2.pdf
599,247647041,"Determination of Important Variables in Food Security Classification Using Random Forest 1 Etrk, . Gda Gvencesi Dzeyi Snflandrlmasnda Kullanlan nemli Gstergelerin Random Forest Yntemine Gre Belirlenmesi",conclusions,Insight-tree,results indicated that seasonal apricot workers suffered from food insecurity.,Solved,https://dergipark.org.tr/tr/download/article-file/2139537
600,247647041,"Determination of Important Variables in Food Security Classification Using Random Forest 1 Etrk, . Gda Gvencesi Dzeyi Snflandrlmasnda Kullanlan nemli Gstergelerin Random Forest Yntemine Gre Belirlenmesi",conclusions,Insight-tree,food security was largely dependent on the purchasing power of food.,Solved,https://dergipark.org.tr/tr/download/article-file/2139537
601,247647041,"Determination of Important Variables in Food Security Classification Using Random Forest 1 Etrk, . Gda Gvencesi Dzeyi Snflandrlmasnda Kullanlan nemli Gstergelerin Random Forest Yntemine Gre Belirlenmesi",conclusions,Insight-tree,the auc value of the rf classification model (0.846) indicated its utility in the detection of the driving forces and causation patterns behind household food (in)security of the seasonal agricultural workers in turkey and in the world.,Solved,https://dergipark.org.tr/tr/download/article-file/2139537
602,247647041,"Determination of Important Variables in Food Security Classification Using Random Forest 1 Etrk, . Gda Gvencesi Dzeyi Snflandrlmasnda Kullanlan nemli Gstergelerin Random Forest Yntemine Gre Belirlenmesi",conclusions,Insight-tree,the findings of this study showed how the families of agricultural workers perceiving food security and necessity of developing different intervention strategies for different populations.,Solved,https://dergipark.org.tr/tr/download/article-file/2139537
603,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,"in this work, we investigate performing it with unlabeled data for zero-shot cross-task generalization.",Solved,https://www.aclanthology.org/2022.emnlp-main.105.pdf
604,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,"then, we propose udit to take better advantage of the instructions by constructing pseudo-labeled data from the unlabeled plain texts.",Solved,https://www.aclanthology.org/2022.emnlp-main.105.pdf
605,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,"through udit, it is possible to perform it with unlabeled data when there are few or no humanannotated samples, which offers a better way to incorporate unlabeled data compared with other approaches.",Solved,https://www.aclanthology.org/2022.emnlp-main.105.pdf
606,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,"through comprehensive analysis, we find that the domain diversity and the matching between the pseudo-labeled data and corresponding instructions are essential for udit.",Solved,https://www.aclanthology.org/2022.emnlp-main.105.pdf
607,247593935,R E L I C : Retrieving Evidence for Literary Claims,conclusion,Insight-tree,"in this work, we introduce the task of literary evidence retrieval and an accompanying dataset, relic.",Solved,https://www.aclanthology.org/2022.acl-long.517.pdf
608,247593935,R E L I C : Retrieving Evidence for Literary Claims,conclusion,Insight-tree,we find that direct quotation of primary sources in literary analysis is most commonly used as evidence for literary claims or arguments.,Solved,https://www.aclanthology.org/2022.acl-long.517.pdf
609,119407066,On the Structure of the Yang-Mills Vacuum,conclusions,Insight-tree,the results i have presented here are a rst step towards a microscopic understanding of the yang-mills vacuum.,Solved,https://export.arxiv.org/pdf/hep-lat/9506033v1.pdf
610,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,conclusion,Insight-tree,"we introduce answer-based claim decomposition, which aims to decompose a question into a series of true/false claims.",Solved,https://export.arxiv.org/pdf/2305.14750v1.pdf
611,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,conclusion,Insight-tree,"through experiments on three datasets with gpt-3.5, including a new challenge dataset obscureqa, we show how our technique can be used to perform fine-grained self-evaluation.",Solved,https://export.arxiv.org/pdf/2305.14750v1.pdf
612,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,conclusion,Insight-tree,"we find that there is a significant difference in the proportion of claims satisfied for incorrect and correct responses, but there is no indication that gpt-3.5 believes that the gold answer satisfies more abcd claims than its incorrect answers.",Solved,https://export.arxiv.org/pdf/2305.14750v1.pdf
613,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,conclusion,Insight-tree,"finally, to investigate the reliability of our approach, we conduct an error analysis and based on our findings, suggest remedies to overcome these errors.",Solved,https://export.arxiv.org/pdf/2305.14750v1.pdf
614,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,limitations,Insight-tree,"in our preliminary experiments, we apply answerbased claim decomposition to factual trivia questions where answers are entities spanning a few words.",Solved,https://export.arxiv.org/pdf/2305.14750v1.pdf
615,215238360,Query Focused Multi-Document Summarization with Distant Supervision,conclusions,Insight-tree,"in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization.",Solved,https://arxiv.org/pdf/2004.03027v1.pdf
616,215238360,Query Focused Multi-Document Summarization with Distant Supervision,conclusions,Insight-tree,we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments.,Solved,https://arxiv.org/pdf/2004.03027v1.pdf
617,215238360,Query Focused Multi-Document Summarization with Distant Supervision,conclusions,Insight-tree,experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more relevant and less redundant.,Solved,https://arxiv.org/pdf/2004.03027v1.pdf
618,215238360,Query Focused Multi-Document Summarization with Distant Supervision,conclusions,Insight-tree,"we have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query.",Solved,https://arxiv.org/pdf/2004.03027v1.pdf
619,234741852,TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,conclusion,Insight-tree,"we propose a new challenging qa dataset tat-qa, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain.",Solved,https://www.aclanthology.org/2021.acl-long.254.pdf
620,234741852,TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,conclusion,Insight-tree,"we also propose a baseline model tagop based on tat-qa, aggregating information from hybrid context and performing numerical reasoning over it with pre-defined operators to compute the final answer.",Solved,https://www.aclanthology.org/2021.acl-long.254.pdf
621,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,the collabqa task and dataset we proposed in this paper is a small step towards that direction.,Solved,https://arxiv.org/pdf/2201.09708v1.pdf
622,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,"we add a series of constraints to make the kgs more realistic, such as a person who doesn't have job gets no annual income; a person cannot be a mayor and be an employee in some company at the same time; the largest company of a city must be located in that city, and so on.",Solved,https://arxiv.org/pdf/2201.09708v1.pdf
623,204915921,QASC: A Dataset for Question Answering via Sentence Composition,conclusion,Insight-tree,"we present qasc, the first qa dataset for multi-hop reasoning beyond a single paragraph where two facts needed to answer a question are annotated for training, but questions cannot be easily syntactically decomposed into these facts.",Solved,https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175
624,204915921,QASC: A Dataset for Question Answering via Sentence Composition,conclusion,Insight-tree,"qasc is generated via a crowdsourcing process, and further enhanced via multi-adversary distractor choice selection.",Solved,https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175
625,235097535,MULTIPROVER: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,conclusion,Insight-tree,"we proposed multilabel-multiprover and iterative-multiprover, two variants of a proof-set generation model where the former performs implicit conditioning between the proofs to generate them in parallel while the latter generates a proof-set through explicit conditioning on the previously generated proofs.",Solved,https://www.aclweb.org/anthology/2021.naacl-main.287.pdf
626,235097535,MULTIPROVER: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,conclusion,Insight-tree,both models obtain strong proof f1 improvements on synthetic and humanparaphrased datasets and iterative-multiprover also obtains state-of-the-art proof f1 on a zero-shot dataset with single proofs.,Solved,https://www.aclweb.org/anthology/2021.naacl-main.287.pdf
627,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,"second, our work focuses on flan [32] instruction-finetuned language models.",Solved,https://export.arxiv.org/pdf/2305.03653v1.pdf
628,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,we chose these models due to their ability to follow instructions and the fact that these models are open-source.,Solved,https://export.arxiv.org/pdf/2305.03653v1.pdf
629,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,"third, we study specific prompt templates (see appendix a) and there may be other ways to formulate the different prompts.",Solved,https://export.arxiv.org/pdf/2305.03653v1.pdf
630,258546701,Query Expansion by Prompting Large Language Models,conclusion,Insight-tree,in this paper we study llm-based query expansions.,Solved,https://export.arxiv.org/pdf/2305.03653v1.pdf
631,258546701,Query Expansion by Prompting Large Language Models,conclusion,Insight-tree,"our proposed method is simple: we prompt a large language model and provide it a query, then we use the model's output to expand the original query with new terms that help during document retrieval.",Solved,https://export.arxiv.org/pdf/2305.03653v1.pdf
632,245769925,Does Entity Abstraction Help Generative Transformers Rea- son?,conclusion,Insight-tree,we presented various ways to incorporate abstract knowledge into transformer language models.,Solved,https://export.arxiv.org/pdf/2201.01787v2.pdf
633,245769925,Does Entity Abstraction Help Generative Transformers Rea- son?,conclusion,Insight-tree,"focusing on entity types, this work evaluated model performance on reasoning tasks requiring compositional generalization and multi-hop reasoning.",Solved,https://export.arxiv.org/pdf/2201.01787v2.pdf
634,245769925,Does Entity Abstraction Help Generative Transformers Rea- son?,conclusion,Insight-tree,"overall our results demonstrate three things: (i) incorporating abstract knowledge significantly improves reasoning and compositional generalization in both interpolation and extrapolation when the environment is formally defined in a logical reasoning setting; (ii) different ways to incorporate abstraction yields different performance boosts: enc-sum and dec-loss are generally performing better than others; (iii) abstraction is not beneficial when the task at hand is more natural, less procedural, and not requiring long reasoning chains.",Solved,https://export.arxiv.org/pdf/2201.01787v2.pdf
635,256662612,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",conclusion and discussion,Insight-tree,"multitask, multilingual, multimodalchatgpt outperforms multiple state-of-the-art zero-shot llms on various tasks and even surpasses fine-tuned models on some tasks.",Solved,https://export.arxiv.org/pdf/2302.04023v2.pdf
636,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,"then, we propose a novel framework called searchain for enabling ir and llm to interact with each other effectively.",Solved,https://export.arxiv.org/pdf/2304.14732v4.pdf
637,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,"searchain not only stimulates the knowledge-reasoning ability of llm but also uses ir to provide the knowledge that llm really needs based on the external knowledge base, which improves accuracy and credibility.",Solved,https://export.arxiv.org/pdf/2304.14732v4.pdf
638,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,"besides, searchain can mark references to supporting documents for the knowledge involved in the generated contents, which improves the traceability of the contents.",Solved,https://export.arxiv.org/pdf/2304.14732v4.pdf
639,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,"in addition, the interaction between ir and llm in searchain transforms the topology of reasoning from chain to tree, which enables llm to dynamically modify reasoning direction.",Solved,https://export.arxiv.org/pdf/2304.14732v4.pdf
640,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,experimental results on complex knowledge-intensive tasks show searchain performs better than all baselines.,Solved,https://export.arxiv.org/pdf/2304.14732v4.pdf
641,247476296,Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach,conclusion & future work,Insight-tree,"we explore an iterative prompting framework towards driving a ""train of thought"" from plms for multi-step reasoning tasks.",Solved,https://export.arxiv.org/pdf/2203.08383v2.pdf
642,247476296,Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach,conclusion & future work,Insight-tree,"we show the superiority of this iterative scheme, and also effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme.",Solved,https://export.arxiv.org/pdf/2203.08383v2.pdf
643,247476296,Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach,conclusion & future work,Insight-tree,"in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors.",Solved,https://export.arxiv.org/pdf/2203.08383v2.pdf
644,233240947,Time-Stamped Language Model: Teaching Language Models to Understand the Flow of Events,conclusion,Insight-tree,"we proposed the time-stamped language model (tslm model), a novel approach based on a simple and effective idea, which enables pre-trained qa models to process procedural texts and produce different outputs based on each step to track entities and their changes.",Solved,https://arxiv.org/pdf/2104.07635v1.pdf
645,233240947,Time-Stamped Language Model: Teaching Language Models to Understand the Flow of Events,conclusion,Insight-tree,"tslm utilizes a timestamp function that causes the attention modules in the transformer-based lm architecture to incorporate past, current, and future information by computing a timestamp embedding for each input token.",Solved,https://arxiv.org/pdf/2104.07635v1.pdf
646,233240947,Time-Stamped Language Model: Teaching Language Models to Understand the Flow of Events,conclusion,Insight-tree,our experiments show a 3.1% improvement on the f1 score and a 10.4% improvement over the recall metric on propara dataset.,Solved,https://arxiv.org/pdf/2104.07635v1.pdf
647,233240947,Time-Stamped Language Model: Teaching Language Models to Understand the Flow of Events,conclusion,Insight-tree,our model further outperforms the state-of-the-art models with a 1.55% margin in the npn-cooking dataset accuracy for the location prediction task.,Solved,https://arxiv.org/pdf/2104.07635v1.pdf
648,238259354,FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks,conclusion and future directions,Insight-tree,our key contribution is providing a thorough and insightful empirical analysis of existing federated learning algorithms in the context of nlp models.,Solved,https://arxiv.org/pdf/2104.08815v3.pdf
649,238259354,FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks,conclusion and future directions,Insight-tree,"notably, we compare typical fl methods for four nlp task formulations under multiple non-iid data partitions.",Solved,https://arxiv.org/pdf/2104.08815v3.pdf
650,238259354,FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks,conclusion and future directions,Insight-tree,our findings reveal both promise and the challenges of fl for nlp.,Solved,https://arxiv.org/pdf/2104.08815v3.pdf
651,238259354,FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks,conclusion and future directions,Insight-tree,"in addition, we also provide a suite of resources to support future research in fl for nlp (e.g., a unifying framework for connecting transformer models with popular fl methods and different non-iid partition strategies).",Solved,https://arxiv.org/pdf/2104.08815v3.pdf
652,233189566,Globalizing BERT-based Transformer Architectures for Long Document Summarization,conclusion,Insight-tree,"in this paper, we have introduced a novel transformer-based model for long document summarization based on propagation layers that spread information between multiple transformer windows.",Solved,https://www.aclweb.org/anthology/2021.eacl-main.154.pdf
653,233189566,Globalizing BERT-based Transformer Architectures for Long Document Summarization,conclusion,Insight-tree,"this model preserves the architecture of commonly used pre-trained language models, thus allowing the transfer of parameters.",Solved,https://www.aclweb.org/anthology/2021.eacl-main.154.pdf
654,233189566,Globalizing BERT-based Transformer Architectures for Long Document Summarization,conclusion,Insight-tree,"an evaluation, conducted on top of the bert model in the context of an extractive summarization task, further revealed its effectiveness in dealing with long documents compared to other adaptations of bert and previously proposed models.",Solved,https://www.aclweb.org/anthology/2021.eacl-main.154.pdf
655,256358835,Graph Attention with Hierarchies for Multi-hop Question Answering,conclusions and future work,Insight-tree,"in this paper, we proposed two extensions to hierarchical graph network (hgn) for the multihop question answering task on hotpotqa.",Solved,https://export.arxiv.org/pdf/2301.11792v1.pdf
656,256358835,Graph Attention with Hierarchies for Multi-hop Question Answering,conclusions and future work,Insight-tree,"first, we completed the hierarchical graph structure by adding new edges between the query and context sentence nodes.",Solved,https://export.arxiv.org/pdf/2301.11792v1.pdf
657,256358835,Graph Attention with Hierarchies for Multi-hop Question Answering,conclusions and future work,Insight-tree,"second, we introduced gath as the mechanism for neural node updates, a novel extension to gat that can update node representations sequentially, based on hierarchical levels.",Solved,https://export.arxiv.org/pdf/2301.11792v1.pdf
658,256358835,Graph Attention with Hierarchies for Multi-hop Question Answering,conclusions and future work,Insight-tree,"to the best of our knowledge, this is the first time the hierarchical graph structure is directly exploited in the update mechanism for information propagation.",Solved,https://export.arxiv.org/pdf/2301.11792v1.pdf
659,248476204,QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance,conclusion,Insight-tree,"to address these issues, we propose qrelscore, which measures the word-and sentence-level relevance through the off-the-shelf language models.",Solved,https://www.aclanthology.org/2022.emnlp-main.37.pdf
660,248476204,QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance,conclusion,Insight-tree,extensive experiments demonstrate that qrelscore achieves start-of-the-art correlation with human judgments and makes up for the shortcomings of existing reference-based metrics.,Solved,https://www.aclanthology.org/2022.emnlp-main.37.pdf
661,237502990,Adaptive Information Seeking for Open-Domain Question Answering,conclusion and future work,Insight-tree,"this work presents an adaptive informationseeking approach for open-domain question answering, called aiso.",Solved,https://www.aclanthology.org/2021.emnlp-main.293.pdf
662,237502990,Adaptive Information Seeking for Open-Domain Question Answering,conclusion and future work,Insight-tree,"it models the open-domain qa task as a pomdp, where the environment contains a large corpus and the agent is asked to sequentially select retrieval function and reformulate query to collect the evidence.",Solved,https://www.aclanthology.org/2021.emnlp-main.293.pdf
663,237502990,Adaptive Information Seeking for Open-Domain Question Answering,conclusion and future work,Insight-tree,"aiso achieves stateof-the-art results on two public datasets, which demonstrates the necessity of different retrieval functions for different questions.",Solved,https://www.aclanthology.org/2021.emnlp-main.293.pdf
664,254125744,NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,conclusion,Insight-tree,"with this intuition, we propose a neural information retrieval training framework called nir-prompt consisting of essential matching module (emm) and matching description module (mdm) based on the idea of decoupling the process of signal capturing and signal combination.",Solved,https://export.arxiv.org/pdf/2212.00229v2.pdf
665,254125744,NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,conclusion,Insight-tree,mdm uses the method of prompt learning to obtain the description of different tasks in the pre-trained language model.,Solved,https://export.arxiv.org/pdf/2212.00229v2.pdf
666,254125744,NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,conclusion,Insight-tree,emm is trained on diverse mixed datasets and combined with the guidance from the task descriptions in mdm to capture essential matching signals and adapt these signals to different tasks.,Solved,https://export.arxiv.org/pdf/2212.00229v2.pdf
667,254125744,NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,conclusion,Insight-tree,"based on this, a generalized neural information retrieval pipeline consisting of retrieval and reranking is constructed.",Solved,https://export.arxiv.org/pdf/2212.00229v2.pdf
668,254125744,NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,conclusion,Insight-tree,"the experimental results on eighteen public datasets and a heterogeneous benchmark for testing the generalization ability of retrieval models show that our method yields better in-domain multi-task, out-of-domain multi-task and new task adaptation performance for dense retrieval, reranking and the entire neural information retrieval pipeline compared to the traditional fine-tuning paradigm.",Solved,https://export.arxiv.org/pdf/2212.00229v2.pdf
669,248022214,QAGAN: Adversarial Approach To Learning Domain Invariant Language Features,conclusion,Insight-tree,we presented a method for training a question-answering language model in an adversarial fashion and showed through various experiments that it helps the model generalize well to out-of-domain dataset.,Solved,https://arxiv.org/pdf/2206.12388v1.pdf
670,237365386,Topic Knowledge Acquisition and Utilization for Machine Reading Comprehension in Social Media Domain,conclusion,Insight-tree,"in this paper, we focus on machine reading comprehension in social media domain.",Solved,https://www.aclanthology.org/2021.ccl-1.88.pdf
671,237365386,Topic Knowledge Acquisition and Utilization for Machine Reading Comprehension in Social Media Domain,conclusion,Insight-tree,we propose a novel method to address the problem of lacking in background knowledge in this task.,Solved,https://www.aclanthology.org/2021.ccl-1.88.pdf
672,237365386,Topic Knowledge Acquisition and Utilization for Machine Reading Comprehension in Social Media Domain,conclusion,Insight-tree,"utilizing the nature of clustering of social media, we retrieve and refine topic knowledge from the relevant messages, and then integrate the knowledge into an mrc model, tkr. experimental results show that our proposed method outperforms the recently proposed models and the bert-based baselines, which proves the method effective overall.",Solved,https://www.aclanthology.org/2021.ccl-1.88.pdf
673,237365386,Topic Knowledge Acquisition and Utilization for Machine Reading Comprehension in Social Media Domain,conclusion,Insight-tree,"by introducing different amount of topic knowledge, we demonstrate the effectiveness of our refined knowledge.",Solved,https://www.aclanthology.org/2021.ccl-1.88.pdf
674,237365386,Topic Knowledge Acquisition and Utilization for Machine Reading Comprehension in Social Media Domain,conclusion,Insight-tree,"moreover, the ablation study further validates the contribution of the key modules of tkr for utilizing the knowledge.",Solved,https://www.aclanthology.org/2021.ccl-1.88.pdf
675,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,conclusions,Insight-tree,we have demonstrated the utility of question decomposition as an effective means to probe pretrained multi-hop question-answering models for supporting evidence.,Solved,https://export.arxiv.org/pdf/2204.07693v2.pdf
676,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,conclusions,Insight-tree,"through simulatability experiments, we show the effectiveness of this explanation form at allowing humans to predict model behavior, a sign that it helps humans to form an accurate mental model of the machine learning system (jacovi et al., 2022).",Solved,https://export.arxiv.org/pdf/2204.07693v2.pdf
677,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,our simulability study results (section 4) are conducted on silver labels.,Solved,https://export.arxiv.org/pdf/2204.07693v2.pdf
678,254044526,Dense Text Retrieval based on Pretrained Language Models: A Survey,conclusion,Insight-tree,"in this survey, we thoroughly review the recent progress of dense retrieval based on pretrained language models (plm).",Solved,https://export.arxiv.org/pdf/2211.14876v1.pdf
679,254044526,Dense Text Retrieval based on Pretrained Language Models: A Survey,conclusion,Insight-tree,"as an important evolution of language intelligence techniques, plms empower dense retrieval models with excellent modeling capacities to capture and represent text semantics for relevance matching.",Solved,https://export.arxiv.org/pdf/2211.14876v1.pdf
680,254044526,Dense Text Retrieval based on Pretrained Language Models: A Survey,conclusion,Insight-tree,"our survey has extensively discussed the key issues and the mainstream solutions in four major aspects to develop dense retrieval systems, including architecture, training, indexing and integration.",Solved,https://export.arxiv.org/pdf/2211.14876v1.pdf
681,254044526,Dense Text Retrieval based on Pretrained Language Models: A Survey,conclusion,Insight-tree,"next, we briefly summarize the discussions of this survey and introduce some remaining issues for dense retrieval.",Solved,https://export.arxiv.org/pdf/2211.14876v1.pdf
682,218502712,Query Reformulation using Query History for Passage Retrieval in Conversational Search,conclusion,Insight-tree,"we present hqe and ntr, both conversational query reformulation methods stacked on a successful multi-stage ir pipeline.",Solved,https://arxiv.org/pdf/2005.02230v1.pdf
683,218502712,Query Reformulation using Query History for Passage Retrieval in Conversational Search,conclusion,Insight-tree,"the effectiveness of our methods are attested by experiments on the cast benchmark dataset, the results of which suggest that the two methods have different advantages in fusing context information into conversational user utterances for downstream ir models.",Solved,https://arxiv.org/pdf/2005.02230v1.pdf
684,218502712,Query Reformulation using Query History for Passage Retrieval in Conversational Search,conclusion,Insight-tree,"finally, this work elevates the state of the art in cast benchmarks and provides simple but effectives baselines for future research.",Solved,https://arxiv.org/pdf/2005.02230v1.pdf
685,253018873,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,conclusion,Insight-tree,"this paper stimulates the compositional reasoning process of humans in decision-making, and makes the following hypotheses: (1) the intuitive perception system (system 1) and cognitive reasoning system (system 2) can be decoupled and (2) the complex decision-making can be disentangled into multi-step execution of fundamental reasoning skills.",Solved,https://export.arxiv.org/pdf/2210.11265v2.pdf
686,253018873,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,conclusion,Insight-tree,"correspondingly, we propose reason-former, a compositional general-purpose reasoning framework.",Solved,https://export.arxiv.org/pdf/2210.11265v2.pdf
687,253018873,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,conclusion,Insight-tree,"reasonformer decouples the representation module and reasoning modules, which are pre-trained to expert in fundamental reasoning skills.",Solved,https://export.arxiv.org/pdf/2210.11265v2.pdf
688,253018873,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,conclusion,Insight-tree,the reasoning modules are dynamically composed in parallel and cascaded manner to form a whole reasoning process.,Solved,https://export.arxiv.org/pdf/2210.11265v2.pdf
689,253018873,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,conclusion,Insight-tree,reasonformer is endto-end and unified in solving multiple tasks with one model.,Solved,https://export.arxiv.org/pdf/2210.11265v2.pdf
690,253018873,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,conclusion,Insight-tree,extensive experiments on 11 tasks reveal the compositional reasoning ability of reason-former and disentangling of representation and reasoning modules.,Solved,https://export.arxiv.org/pdf/2210.11265v2.pdf
691,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,"in this work, we present a novel framework, which can diagnose the correlation between the nlu dataset and a specific skill and we probe a fundamental reasoning skill, logical reasoning, on 11",Solved,https://www.aclanthology.org/2022.coling-1.147.pdf
692,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,our framework involves a logical probe to conduct diagnosis and defines a qualitative process and a quantitative process to calculate two indicators.,Solved,https://www.aclanthology.org/2022.coling-1.147.pdf
693,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,"from the results, we observe that 1) most nli datasets have a relatively strong correlation with logical reasoning.",Solved,https://www.aclanthology.org/2022.coling-1.147.pdf
694,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,2) the correlations between type 1 mrc datasets and logical reasoning are moderate because logical reasoning is not the only dominant skill in these datasets.,Solved,https://www.aclanthology.org/2022.coling-1.147.pdf
695,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,"in future works, we will focus on: 1) exploring the solution to the limitations of the proposed method; 2) build associations for different datasets that require the same nlu capabilities.",Solved,https://www.aclanthology.org/2022.coling-1.147.pdf
696,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,number of premises needed for conclusions,Insight-tree,we show the accuracy of the examples with different numbers of premises needed to reach the conclusions in figure 3.,Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
697,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,number of premises needed for conclusions,Insight-tree,"under the few-shot prompting setting, gpt-3 and  confusion matrices in figure 4 for the fine-tuning and 8-shot nl prompt results both show that lms are significantly better at making the correct predictions for conclusions with labels of true than the conclusions with labels of false or unknown.",Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
698,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,performance on conclusions with different labels,Insight-tree,the accuracy on examples with false or unknown conclusions is 54.41% with fine-tuning and 36.91% with few-shot prompting.,Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
699,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,performance on conclusions with different labels,Insight-tree,they also tend to make more predictions of true than the other labels.,Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
700,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,conclusion,Insight-tree,"we introduced folio, an expert-written dataset for first-order logic (fol) reasoning equipped with parallel fol formulas.",Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
701,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,conclusion,Insight-tree,the examples in folio are created based on real-world knowledge with natural language.,Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
702,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,conclusion,Insight-tree,it exhibits a large number of distinct logic patterns and a large vocabulary.,Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
703,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,conclusion,Insight-tree,"experiments show that the performance of one of the most capable llms publicly available is only slightly better than chance with few-shot prompting on hyblogic, a subset of folio, and llms are especially bad at predicting the correct truth values for false and unknown conclusions.",Solved,https://export.arxiv.org/pdf/2209.00840v1.pdf
704,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,this paper addresses answer uncertainty and unanswerability in multiple-choice mrc.,Solved,https://www.aclanthology.org/2022.findings-acl.82.pdf
705,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,"an electra prlm achieve competitive results on the default reclor dataset, achieving up to 67.1% accuracy on the evaluation split.",Solved,https://www.aclanthology.org/2022.findings-acl.82.pdf
706,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,ensemble-based predictive uncertainty measures are explored for both modes of operation: answer uncertainty for negative marking schemes and the presence of unanswerability.,Solved,https://www.aclanthology.org/2022.findings-acl.82.pdf
707,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,it is shown that uncertainty in the prediction such as expected entropy is correlated with the error rate of the mrc system allowing better than vanilla performance with an aggressive negative marking scheme for reclor and race.,Solved,https://www.aclanthology.org/2022.findings-acl.82.pdf
708,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,"interestingly, it is found that expected entropy from the predictions of an implicitly trained system is competitive at unanswerability detection and is able to out-compete map decoding from an explicitly trained system that has been trained with unanswerable examples for reclor.",Solved,https://www.aclanthology.org/2022.findings-acl.82.pdf
709,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,we produce several novel strategies for multilingual qa that go beyond zero-shot training and outshine the previous baseline built on top of mbert.,Solved,https://arxiv.org/pdf/2012.05958v1.pdf
710,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,we present a translation model that has 14 times more training data.,Solved,https://arxiv.org/pdf/2012.05958v1.pdf
711,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,"further, our at and laf strategies utilize translation as data augmentation to bring the language-specific embeddings of the lm closer to each other.",Solved,https://arxiv.org/pdf/2012.05958v1.pdf
712,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,these approaches help us significantly improve the cross-lingual transfer.,Solved,https://arxiv.org/pdf/2012.05958v1.pdf
713,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,"empirically, our models demonstrate strong results and all approaches improve over the previous zs strategy.",Solved,https://arxiv.org/pdf/2012.05958v1.pdf
714,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,"in this paper, we aimed to solve the qualitative reasoning task in an interpretable manner. inspired by human cognition, we first summarized the questions into two categories, prediction and comparison.",Solved,https://arxiv.org/pdf/2106.02399v1.pdf
715,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,then an end-to-end trained reasoning component that contains two reasoning chains was designed.,Solved,https://arxiv.org/pdf/2106.02399v1.pdf
716,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,both reasoning chains contained multiple neural modules that provide transparent intermediate predictions for the understanding and reasoning process.,Solved,https://arxiv.org/pdf/2106.02399v1.pdf
717,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,"the experimental results showed the effectiveness of our approach, and the analysis of each module and case study demonstrated the superior interpretability compared with the ""blackbox"" model.",Solved,https://arxiv.org/pdf/2106.02399v1.pdf
718,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,"moreover, we found that some questions could be solved by both reasoning chains, thus increasing the default tolerance and generalization capability.",Solved,https://arxiv.org/pdf/2106.02399v1.pdf
719,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,"furthermore, a human evaluation was conducted to validate the function of the synthetic text and provide an additional explanation for the superior performance achieved by our method.",Solved,https://arxiv.org/pdf/2106.02399v1.pdf
720,237373991,Interactive Machine Comprehension with Dynamic Knowledge Graphs,conclusion,Insight-tree,we explore to leverage graph representations in the challenging imrc tasks.,Solved,https://www.aclanthology.org/2021.emnlp-main.540.pdf
721,237373991,Interactive Machine Comprehension with Dynamic Knowledge Graphs,conclusion,Insight-tree,we investigate different categories of graph structures that can capture text information at various levels.,Solved,https://www.aclanthology.org/2021.emnlp-main.540.pdf
722,237373991,Interactive Machine Comprehension with Dynamic Knowledge Graphs,conclusion,Insight-tree,we describe methods that dynamically generate the graphs during information gathering.,Solved,https://www.aclanthology.org/2021.emnlp-main.540.pdf
723,237373991,Interactive Machine Comprehension with Dynamic Knowledge Graphs,conclusion,Insight-tree,experiment results show that graph representations provide consistent improvement across settings.,Solved,https://www.aclanthology.org/2021.emnlp-main.540.pdf
724,237373991,Interactive Machine Comprehension with Dynamic Knowledge Graphs,conclusion,Insight-tree,this evinces our hypothesis that graph representations are proper inductive biases in imrc.,Solved,https://www.aclanthology.org/2021.emnlp-main.540.pdf
725,202572622,PubMedQA: A Dataset for Biomedical Research Question Answering,conclusion,Insight-tree,"we present pubmedqa, a novel dataset aimed at biomedical research question answering using yes/no/maybe, where complex quantitative reasoning is required to solve the task.",Solved,https://www.aclweb.org/anthology/D19-1259.pdf
726,202572622,PubMedQA: A Dataset for Biomedical Research Question Answering,conclusion,Insight-tree,pubmedqa has substantial automatically collected instances as well as the largest size of expert annotated yes/no/maybe questions in biomedical domain.,Solved,https://www.aclweb.org/anthology/D19-1259.pdf
727,252280663,Prompt-based Conservation Learning for Multi-hop Question Answering,conclusions and future work,Insight-tree,"in this paper, we introduce a novel prompt-based conservation learning framework for multi-hop qa -a framework that retains knowledge from previous component tasks -able to answer questions in a principled way that matches human expectations by answering sub-questions and integrating the answers.",Solved,https://www.aclanthology.org/2022.coling-1.154.pdf
728,252280663,Prompt-based Conservation Learning for Multi-hop Question Answering,conclusions and future work,Insight-tree,"by developing soft prompts related to reasoning types during training, we also show that we can condition plms to stimulate and apply the reasoning knowledge required for specific multihop questions.",Solved,https://www.aclanthology.org/2022.coling-1.154.pdf
729,252280663,Prompt-based Conservation Learning for Multi-hop Question Answering,conclusions and future work,Insight-tree,experimental results on multiple multi-hop qa datasets demonstrate the improved performance of pcl over previous multi-hop qa models in multi-hop qa.,Solved,https://www.aclanthology.org/2022.coling-1.154.pdf
730,258866060,Prompt Optimization of Large Language Model for Interactive Tasks without Gradient and Demonstrations,conclusion,Insight-tree,"we propose llm-po , a prompt optimization method, to make llm learn to solve interactive tasks without gradient computation or in-context demonstrations.",Solved,https://export.arxiv.org/pdf/2305.15064v1.pdf
731,258866060,Prompt Optimization of Large Language Model for Interactive Tasks without Gradient and Demonstrations,conclusion,Insight-tree,experiments show that llm-po is competitive to icl baselines in task success rates while requiring no diverse and high-quality demonstrations and achieves at least twice as lower inference cost.,Solved,https://export.arxiv.org/pdf/2305.15064v1.pdf
732,256597851,LIQUID: A Framework for List Question Answering Dataset Generation,conclusion,Insight-tree,"herein, we introduced liquid, a framework that automatically generates list qa datasets from unlabeled corpora to alleviate the data scarcity problem in this field.",Solved,https://export.arxiv.org/pdf/2302.01691v2.pdf
733,256597851,LIQUID: A Framework for List Question Answering Dataset Generation,conclusion,Insight-tree,our synthetic data significantly improved the performance of the current supervised models on five benchmark datasets.,Solved,https://export.arxiv.org/pdf/2302.01691v2.pdf
734,256597851,LIQUID: A Framework for List Question Answering Dataset Generation,conclusion,Insight-tree,we thoroughly analyzed the effect of each component in liq-uid and generated data quantitatively and qualitatively.,Solved,https://export.arxiv.org/pdf/2302.01691v2.pdf
735,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,limitations,Insight-tree,"at its core, reflexion is an optimization technique that uses natural language to do policy optimization.",Solved,https://export.arxiv.org/pdf/2303.11366v2.pdf
736,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,limitations,Insight-tree,"policy optimization is a powerful approach to improve action choice through experience, but it may still succumb to non-optimal local minima solutions.",Solved,https://export.arxiv.org/pdf/2303.11366v2.pdf
737,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,conclusion,Insight-tree,"in this work, we present reflexion, an approach that leverages verbal reinforcement to teach agents to learn from past mistakes.",Solved,https://export.arxiv.org/pdf/2303.11366v2.pdf
738,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,conclusion,Insight-tree,we empirically show that reflexion agents significantly outperform currently widely-used decision-making approaches by utilizing self-reflection.,Solved,https://export.arxiv.org/pdf/2303.11366v2.pdf
739,233296336,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,conclusion,Insight-tree,"in this paper, we propose an evaluation pipeline for pretrained models by testing their transferability without word identity information.",Solved,https://arxiv.org/pdf/2104.08410v1.pdf
740,233296336,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,conclusion,Insight-tree,"specifically, we take an english pretrained bert off-the-shelf and fine-tune it with a scrambled english dataset.",Solved,https://arxiv.org/pdf/2104.08410v1.pdf
741,233296336,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,conclusion,Insight-tree,we conduct analyses across six tasks covering both classification and sequence labeling.,Solved,https://arxiv.org/pdf/2104.08410v1.pdf
742,233296336,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,conclusion,Insight-tree,"we find considerable transfer for bert as compared to even powerful baselines, by only for classification tasks.",Solved,https://arxiv.org/pdf/2104.08410v1.pdf
743,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"we propose a two-branch graph transformer network for logical reasoning of text, which is named as logiformer.",Solved,https://arxiv.org/pdf/2205.00731v2.pdf
744,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"firstly, we introduce two different strategies to construct the logical graph and syntax graph respectively.",Solved,https://arxiv.org/pdf/2205.00731v2.pdf
745,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"especially for the logical graph, we are the first to model both causal relations and negations in the logical reasoning task.",Solved,https://arxiv.org/pdf/2205.00731v2.pdf
746,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"secondly, we feed the extracted node sequences to the fully connected graph transformer for each graph.",Solved,https://arxiv.org/pdf/2205.00731v2.pdf
747,233297051,Explaining Answers with Entailment Trees,summary and conclusion,Insight-tree,"our goal is to enable machines to generate richer, more systematic explanations.",Solved,https://www.aclanthology.org/2021.emnlp-main.585.pdf
748,233297051,Explaining Answers with Entailment Trees,summary and conclusion,Insight-tree,"to this end, we have developed a novel formulation of explanations as multistep entailment trees, and created entail-mentbank, the first large dataset of such trees.",Solved,https://www.aclanthology.org/2021.emnlp-main.585.pdf
749,258685562,Answering Complex Questions over Text by Hybrid Question Parsing and Execution,conclusion,Insight-tree,"we propose hpe for answering complex questions over text, which combines the strengths of neural network approaches and symbolic approaches.",Solved,https://export.arxiv.org/pdf/2305.07789v1.pdf
750,258685562,Answering Complex Questions over Text by Hybrid Question Parsing and Execution,conclusion,Insight-tree,we parse the question into h-expressions followed by the hybrid execution to get the final answer.,Solved,https://export.arxiv.org/pdf/2305.07789v1.pdf
751,258685562,Answering Complex Questions over Text by Hybrid Question Parsing and Execution,conclusion,Insight-tree,"our extensive empirical results demonstrate that hpe has a strong performance on various datasets under supervised, few-shot, and zero-shot settings.",Solved,https://export.arxiv.org/pdf/2305.07789v1.pdf
752,258685562,Answering Complex Questions over Text by Hybrid Question Parsing and Execution,conclusion,Insight-tree,"moreover, our model has a strong interpretability exposing its underlying reasoning process, which facilitates understanding and possibly fixing its errors. by replacing our text reader with kb or",Solved,https://export.arxiv.org/pdf/2305.07789v1.pdf
753,237194607,MeDiaQA: A Question Answering Dataset on Medical Dialogues,conclusion,Insight-tree,"in this paper, we construct mediaqa, a novel dataset for qa on medical dialogues and propose a method media-bert based on the state-of-the-art pretrained language model.",Solved,https://arxiv.org/pdf/2108.08074v1.pdf
754,237194607,MeDiaQA: A Question Answering Dataset on Medical Dialogues,conclusion,Insight-tree,the proposed dataset shows the distinctiveness of medical dialogues compared with other normal domains in the context of qa.,Solved,https://arxiv.org/pdf/2108.08074v1.pdf
755,249888962,A Dense Representation Framework for Lexical and Semantic Matching,conclusions and future work,Insight-tree,we present a simple yet effective approach to densifying lexical representations for passage retrieval.,Solved,https://export.arxiv.org/pdf/2206.09912v2.pdf
756,249888962,A Dense Representation Framework for Lexical and Semantic Matching,conclusions and future work,Insight-tree,this work introduces a dense representation framework and proposes a new scoring function to compute relevance scores between dense lexical representations (dlrs) derived from queries and passages.,Solved,https://export.arxiv.org/pdf/2206.09912v2.pdf
757,249888962,A Dense Representation Framework for Lexical and Semantic Matching,conclusions and future work,Insight-tree,"using our framework, we can combine lexical and semantic representations into dense hybrid representations (dhrs) for hybrid retrieval.",Solved,https://export.arxiv.org/pdf/2206.09912v2.pdf
758,249888962,A Dense Representation Framework for Lexical and Semantic Matching,conclusions and future work,Insight-tree,"our experiments show that dlrs can accurately approximate any ""off-the-shelf"" lexical model.",Solved,https://export.arxiv.org/pdf/2206.09912v2.pdf
759,249888962,A Dense Representation Framework for Lexical and Semantic Matching,conclusions and future work,Insight-tree,"furthermore, when combined with other semantic representations (as dhrs), the resulting models can achieve comparable effectiveness to existing state-of-the-art hybrid retrieval methods.",Solved,https://export.arxiv.org/pdf/2206.09912v2.pdf
760,236428903,Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy,conclusion,Insight-tree,"this paper proposes a novel ""select-to-guide"" model (s2g) for multi-hop reading comprehension in more effective and convenient way.",Solved,https://arxiv.org/pdf/2107.11823v1.pdf
761,236428903,Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy,conclusion,Insight-tree,"as an alternative of the existing graph modeling, the proposed graph-free s2g model consists of an evidence paragraph retrieval module which selects evidence paragraphs in a step-by-step multihop manner, and a multi-task module that simultaneously extracts evidence sentences and answer spans.",Solved,https://arxiv.org/pdf/2107.11823v1.pdf
762,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,conclusion,Insight-tree,we propose a few-shot data synthesis framework to training smaller models for efficient open domain multi-hop question answering with less than 10 human annotations.,Solved,https://export.arxiv.org/pdf/2305.13691v1.pdf
763,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,conclusion,Insight-tree,"our framework consists of generation functions parameterized by llms and prompts, which requires less hand-crafted features than prior work while still achieving better performance.",Solved,https://export.arxiv.org/pdf/2305.13691v1.pdf
764,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,conclusion,Insight-tree,we show that our approach is general by extending to fact verification tasks.,Solved,https://export.arxiv.org/pdf/2305.13691v1.pdf
765,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,conclusion,Insight-tree,"in experiments, we benchmark our approach on three multihop question answering and one fact verification benchmarks.",Solved,https://export.arxiv.org/pdf/2305.13691v1.pdf
766,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,conclusion,Insight-tree,the results show that our approach leads to significantly better models that rival the performance of previous methods employing models nearly three times larger in terms of parameter counts.,Solved,https://export.arxiv.org/pdf/2305.13691v1.pdf
767,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,conclusion,Insight-tree,the analysis shows the importance of the filtering steps and our approach benefits models of various sizes.,Solved,https://export.arxiv.org/pdf/2305.13691v1.pdf
768,256827065,Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering,conclusion,Insight-tree,we analyze the effectiveness of the underlying reasoning tasks using two multi-hop datasets: 2wiki and hotpotqa-small.,Solved,https://export.arxiv.org/pdf/2302.05963v1.pdf
769,256827065,Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering,conclusion,Insight-tree,the results reveal that the underlying reasoning tasks can improve qa performance.,Solved,https://export.arxiv.org/pdf/2302.05963v1.pdf
770,256827065,Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering,conclusion,Insight-tree,"using four debiased sets, we demonstrate that the underlying reasoning tasks can reduce the reasoning shortcuts of the qa task.",Solved,https://export.arxiv.org/pdf/2302.05963v1.pdf
771,256827065,Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering,conclusion,Insight-tree,"the results also reveal that the underlying reasoning tasks do not make the models more robust on adversarial examples, such as sub-questions and inverted questions.",Solved,https://export.arxiv.org/pdf/2302.05963v1.pdf
772,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,we provide insights into an rnn-based iterative memory model that incorporates gate attention on multi-step reasoning over natural language.,Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
773,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,"instead of using the original gru and dotproduct attention, we integrate gate attention to update hidden states.",Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
774,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,the experiment results show the model with gate attention achieves generally better performance than the original rnnbased iterative-memory model with dot-product attention and other rnn-based models.,Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
775,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,the performance of our model is comparable or better than the much larger and pretrained roberta-large in some scenarios.,Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
776,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,"furthermore, our model shows better out-of-distribution generalisation performance than the pretained roberta.",Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
777,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,"to address the issue of depth-imbalance in the existing datasets on multi-step reasoning over natural language, we develop a large-scale multi-step reasoning dataset called pararule-plus, with more examples of deep reasoning depths than previous datasets.",Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
778,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,"we find that the performance of the models in our experiments improves when we add pararule-plus in the training, especially on examples that require deeper reasoning depths and extra out-of-distribution examples.",Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
779,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,we use pararules with all depths as the training set for all models and then test them on examples with different reasoning depths (d).,Solved,https://export.arxiv.org/pdf/2207.14000v1.pdf
780,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,"in this paper, we present a chain-centric reasoning and pre-training (carp) framework for table-andtext question answering.",Solved,https://arxiv.org/pdf/2201.05880v1.pdf
781,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,"when answering the questions given retrieved table and passages, carp first extracts explicit hybrid chain to reveal the intermediate reasoning process leading to the answer across table and text.",Solved,https://arxiv.org/pdf/2201.05880v1.pdf
782,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,"the hybrid chain provides a guidance for qa, and explanation of the intermediate reasoning process.",Solved,https://arxiv.org/pdf/2201.05880v1.pdf
783,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,"to enhance the extraction model with better reasoning ability and alleviate data sparsity problem, we design a novel chaincentric pre-training method.",Solved,https://arxiv.org/pdf/2201.05880v1.pdf
784,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,"this method synthesizes the reasoning corpus in a larger scale and of higher reasoning complexity, which is achieved by automatically synthesizing heterogeneous reasoning paths from tables and passages in wikipedia and reversely generating multi-hop questions.",Solved,https://arxiv.org/pdf/2201.05880v1.pdf
785,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,"we find that the pre-training task boosts performance on the hybrid chain extraction model, especially for questions requiring more complex reasoning, which leads to significant improvement on the performance of the qa model.",Solved,https://arxiv.org/pdf/2201.05880v1.pdf
786,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,the hybrid chain also provides better interpretability of the reasoning process.,Solved,https://arxiv.org/pdf/2201.05880v1.pdf
787,253237103,RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees,conclusion,Insight-tree,"we presented rlet, a rl-based entailment tree generation framework, which contains sentences selection and deduction generation modules and can be trained with cumulative signals across the entire reasoning tree.",Solved,https://www.aclanthology.org/2022.emnlp-main.483.pdf
788,253237103,RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees,conclusion,Insight-tree,experiments show that rlet outperforms existing baselines on structure correctness and is applicable in practical scenarios.,Solved,https://www.aclanthology.org/2022.emnlp-main.483.pdf
789,58008636,Genetic variability of myostatin and prolactin genes in popular goat breeds in Egypt,conclusion,Insight-tree,"in the end, this study is considered to be a step advancing for further studies that may add to give additional information about the genetic polymorphism of meat and growth characters of egyptian goat breeds and the improvement of these economically important traits.",Solved,
790,14182996,Integrating miRNA and mRNA Expression Profiling Uncovers miRNAs Underlying Fat Deposition in Sheep,conclusion,Insight-tree,"in conclusion, we identified a number of mirnas that are differentially expressed between the fat-tailed and short-tailed sheep breeds.",Solved,
791,14182996,Integrating miRNA and mRNA Expression Profiling Uncovers miRNAs Underlying Fat Deposition in Sheep,conclusion,Insight-tree,"we further highlighted gene targets of related mirnas that may be involved in regulating fat deposition and adiposeness, in sheep and other livestock.",Solved,
792,14182996,Integrating miRNA and mRNA Expression Profiling Uncovers miRNAs Underlying Fat Deposition in Sheep,conclusion,Insight-tree,"our results also provide evidence for the interaction of mirnas and genes in the regulation of obesity and metabolic syndromes, which suggests that this may serve as an animal model for human' obesity and metabolic syndromes researches.",Solved,
793,160009340,Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction,conclusion,Insight-tree,"we consider that the main contributions of our study are (1) the proposed qfe model that is based on a summarization model for the explainable multi-hop qa, (2) the dependency among the evidence and the coverage of the question due to the usage of the summarization model, and (3) the state-of-the-art performance in evidence extraction in both rc and rte tasks.",Solved,https://arxiv.org/pdf/1905.08511v1.pdf
794,102352338,Tracking Discrete and Continuous Entity State for Process Understanding,conclusion,Insight-tree,"in this paper, we present a structured architecture for entity tracking which leverages both the discrete and continuous characterization of the entity evolution.",Solved,https://www.aclweb.org/anthology/W19-1502.pdf
795,102352338,Tracking Discrete and Continuous Entity State for Process Understanding,conclusion,Insight-tree,we use a neural crf approach to model our discrete constraints while tracking entities and locations recurrently.,Solved,https://www.aclweb.org/anthology/W19-1502.pdf
796,102352338,Tracking Discrete and Continuous Entity State for Process Understanding,conclusion,Insight-tree,our model achieves state of the art results on the propara dataset.,Solved,https://www.aclweb.org/anthology/W19-1502.pdf
797,234334015,ExpMRC: explainability evaluation for machine reading comprehension,conclusion,Insight-tree,"in this paper, we propose a comprehensive benchmark for evaluating the explainability of mrc systems.",Solved,https://arxiv.org/pdf/2105.04126v1.pdf
798,234334015,ExpMRC: explainability evaluation for machine reading comprehension,conclusion,Insight-tree,"the proposed expmrc benchmark contains four datasets, including squad, cmrc 2018, race + , c 3 , covering span-extraction mrc and multiple-choice mrc in both en- glish and chinese.",Solved,https://arxiv.org/pdf/2105.04126v1.pdf
799,234334015,ExpMRC: explainability evaluation for machine reading comprehension,conclusion,Insight-tree,expmrc aims to evaluate the mrc system to give not only correct predictions on the final answer but also extract correct evidence for the answer.,Solved,https://arxiv.org/pdf/2105.04126v1.pdf
800,234334015,ExpMRC: explainability evaluation for machine reading comprehension,conclusion,Insight-tree,we set up several baseline systems to thoroughly evaluate the difficulties of expmrc.,Solved,https://arxiv.org/pdf/2105.04126v1.pdf
801,234334015,ExpMRC: explainability evaluation for machine reading comprehension,conclusion,Insight-tree,"the experimental results show that both traditional and state-of-the-art pre-trained language models still underperform human performance by a large margin on most of the subsets, indicating that more efforts should be made on designing an effective approach for evidence extraction.",Solved,https://arxiv.org/pdf/2105.04126v1.pdf
802,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,we propose to approximate a pomdp by using a region observable pomdp.,Solved,https://arxiv.org/pdf/cs/9711103v1.pdf
803,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,"a method for determining approximation quality is described, which allows one to make the tradeo between approximation quality and computational time by starting with a coarse approximation and re ning it gradually.",Solved,https://arxiv.org/pdf/cs/9711103v1.pdf
804,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,"simulation experiments have shown that when there is not much uncertainty in the e ects of actions and observations are informative, a pomdp can be accurately approximated by a region observable pomdp that can be solved exactly.",Solved,https://arxiv.org/pdf/cs/9711103v1.pdf
805,247762238,Lite Unified Modeling for Discriminative Reading Comprehension,conclusion,Insight-tree,"in this work, we propose pos-enhanced iterative co-attention network (poi-net), as a lightweight unified modeling for multiple subcategories of discriminative mrc.",Solved,https://www.aclanthology.org/2022.acl-long.594.pdf
806,247762238,Lite Unified Modeling for Discriminative Reading Comprehension,conclusion,Insight-tree,"poi-net utilizes pos embedding to encode pos attributes for the preciseness of answer boundary, and iterative co-attention mechanism with integration strategy is employed to highlight and integrate critical information at decoding aspect, with almost no additional parameter.",Solved,https://www.aclanthology.org/2022.acl-long.594.pdf
807,247762238,Lite Unified Modeling for Discriminative Reading Comprehension,conclusion,Insight-tree,"as the first effective and unified modeling with pertinence for different types of discriminative mrc, evaluation results on four extractive and multi-choice mrc benchmarks consistently indicate the general effectiveness and applicability of our model.",Solved,https://www.aclanthology.org/2022.acl-long.594.pdf
808,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"in this paper, we proposed a novel uni-encoder model, sparse retriever using a dual document encoder (spade), to alleviate the trade-off between effectiveness and efficiency of the ir system.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
809,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,we adopted a dual document encoder for lexical and semantic matching and developed a co-training strategy to mitigate the training intervention between encoders.,Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
810,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"we also utilized document-and corpus-level pruning during model training, enabling efficient retrieval using the inverted index.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
811,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"experimental results showed that spade achieves state-of-the-art performance among uni-encoder models with acceptable query latency, notably preferable for commercial ir systems.  ",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
812,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"we use official metrics for ms marco document ranking task, i.e., mrr@100 and recall@100.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
813,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"as a result, spade achieves the best performance among baselines using sparse representations.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
814,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"it may seem similar to the passage ranking task results, but spade's effectiveness is more highlighted in long documents.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
815,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"for example, in the passage ranking, mrr@10 performance of unicoil and spade were 0.351 and 0.353, respectively, but in the document ranking, spade significantly outperforms it by 0.369 versus 0.353.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
816,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"secondly, ance shows high ranking performance, i.e., mrr, compared to spade, but spade has better recall performance.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
817,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"if compared considering the same mrr performance, uni-encoder outperforms bi-encoder in terms of recall.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
818,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"to reduce the query latency, we use corpus-level pruning introduced in section 3.4.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
819,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"specifically, during the model training, terms with high document frequency are pruned, and the model learns document representations only with the remaining terms.",Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
820,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,our analysis found that incorrect queries of spade on trec dl 2019 mostly include relatively rare words.,Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
821,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,spade can identify important terms and expand new terms of spade and deepimpact [43].,Solved,https://export.arxiv.org/pdf/2209.05917v2.pdf
822,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,we provide a new interpretability evaluation benchmark which contains three tasks with both english and chinese annotated data.,Solved,https://www.aclanthology.org/2022.conll-1.6.pdf
823,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,we have several annotators to perform quality control based on human intuitions and experiences.,Solved,https://www.aclanthology.org/2022.conll-1.6.pdf
824,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,"meanwhile, we compare model behaviors on full inputs and human annotated rationales to evaluate the sufficiency and comprehensiveness of rationales, as shown in table 4 and table 7.",Solved,https://www.aclanthology.org/2022.conll-1.6.pdf
825,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,conclusion,Insight-tree,"we propose a new fine-grained interpretability evaluation benchmark, containing token-level rationales, a new evaluation metric and corresponding perturbed examples for three typical nlp tasks, i.e., sentiment analysis, textual similarity and machine reading comprehension.",Solved,https://www.aclanthology.org/2022.conll-1.6.pdf
826,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,conclusion,Insight-tree,the experimental results on three models and three saliency methods prove that our benchmark can be used to evaluate interpretability of both models and saliency methods.,Solved,https://www.aclanthology.org/2022.conll-1.6.pdf
827,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,conclusion,Insight-tree,"we will release this benchmark and hope it can facilitate progress on several directions, such as better interpretability evaluation metrics and causal analysis of nlp models.",Solved,https://www.aclanthology.org/2022.conll-1.6.pdf
828,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,"we have proposed ugr, a novel unified generative retriever, which can robustly serve different retrieval tasks for knowledge-intensive language tasks.",Solved,https://export.arxiv.org/pdf/2304.14856v1.pdf
829,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,"to unify retrieval tasks, we formulated the retrieval problem as a conditional generation problem and introduced an n-gram-based identifier for relevant contexts at different levels of granularity.",Solved,https://export.arxiv.org/pdf/2304.14856v1.pdf
830,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,empirical results on the kilt benchmark demonstrated the superiority of the proposed method.,Solved,https://export.arxiv.org/pdf/2304.14856v1.pdf
831,218487313,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"we introduced a simple, unsupervised approach for evidence retrieval for question answering.",Solved,https://www.aclweb.org/anthology/2020.acl-main.414.pdf
832,218487313,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"our approach combines three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using glove embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, and (c) a simple stopping condition that concludes the iterative process when all terms in the given question and candidate answers are covered by the retrieved justifications.",Solved,https://www.aclweb.org/anthology/2020.acl-main.414.pdf
833,218487313,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"overall, despite its simplicity, unsupervised nature, and its sole reliance on glove embeddings, our approach outperforms all previous methods (including supervised ones) on the evidence selection task on two datasets: multirc and qasc.",Solved,https://www.aclweb.org/anthology/2020.acl-main.414.pdf
834,218487313,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"when these evidence sentences are fed into a roberta answer classification component, we achieve the best qa performance on these two datasets.",Solved,https://www.aclweb.org/anthology/2020.acl-main.414.pdf
835,218487313,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"further, we show that considerable improvements can be obtained by aggregating knowledge from parallel evidence chains retrieved by our method.",Solved,https://www.aclweb.org/anthology/2020.acl-main.414.pdf
836,244799249,ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,conclusion,Insight-tree,"we introduced colbertv2, a retriever that advances the quality and space efficiency of multivector representations.",Solved,https://www.aclanthology.org/2022.naacl-main.272.pdf
837,244799249,ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,conclusion,Insight-tree,we hypothesized that cluster centroids capture context-aware semantics of the token-level representations and proposed a residual representation that leverages these patterns to dramatically reduce the footprint of multi-vector systems off-the-shelf.,Solved,https://www.aclanthology.org/2022.naacl-main.272.pdf
838,244799249,ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,conclusion,Insight-tree,we then explored improved supervision for multi-vector retrieval and found that their quality improves considerably upon distillation from a cross-encoder system.,Solved,https://www.aclanthology.org/2022.naacl-main.272.pdf
839,244799249,ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,conclusion,Insight-tree,"the proposed colbertv2 considerably outperforms existing retrievers in within-domain and out-of-domain evaluations, which we conducted extensively across 28 datasets, establishing state-of-the-art quality while exhibiting competitive space footprint.",Solved,https://www.aclanthology.org/2022.naacl-main.272.pdf
840,254877343,HYRR: Hybrid Infused Reranking for Passage Retrieval,conclusion,Insight-tree,we proposed a generic training framework for rerankers based on a hybrid retriever.,Solved,https://export.arxiv.org/pdf/2212.10528v1.pdf
841,254877343,HYRR: Hybrid Infused Reranking for Passage Retrieval,conclusion,Insight-tree,"the proposed approach is robust and outperforms several strong baselines on ms marco passage ranking task and beir benchmark dataset, which demonstrates that it is practical and generalized.",Solved,https://export.arxiv.org/pdf/2212.10528v1.pdf
842,254877343,HYRR: Hybrid Infused Reranking for Passage Retrieval,conclusion,Insight-tree,"we observe that a model trained with robust training instances (in this case, from the hybrid retriever) produces a reranker that outperforms matched-training rerankers for termbased or neural retrievers.",Solved,https://export.arxiv.org/pdf/2212.10528v1.pdf
843,257921404,Rethinking the Role of Token Retrieval in Multi-Vector Retrieval,conclusion,Insight-tree,"in this paper, we propose xtr that simplifies the existing three-stage inference of multivector models by improving the initial token retrieval stage.",Solved,https://export.arxiv.org/pdf/2304.01982v2.pdf
844,257921404,Rethinking the Role of Token Retrieval in Multi-Vector Retrieval,conclusion,Insight-tree,"as a result, xtr achieves state-of-the-art performances on zero-shot information retrieval benchmarks while greatly reducing the flops of the scoring stage.",Solved,https://export.arxiv.org/pdf/2304.01982v2.pdf
845,257921404,Rethinking the Role of Token Retrieval in Multi-Vector Retrieval,conclusion,Insight-tree,"we further show that indeed our objective encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query.",Solved,https://export.arxiv.org/pdf/2304.01982v2.pdf
846,232380161,DAGN: Discourse-Aware Graph Network for Logical Reasoning,conclusion,Insight-tree,"in this paper, we introduce a discourse-aware graph network (dagn) to addressing logical reasoning qa tasks.",Solved,https://www.aclweb.org/anthology/2021.naacl-main.467.pdf
847,232380161,DAGN: Discourse-Aware Graph Network for Logical Reasoning,conclusion,Insight-tree,dagn then learns the discourse-based features and enhances them with contextual token embeddings.,Solved,https://www.aclweb.org/anthology/2021.naacl-main.467.pdf
848,232380161,DAGN: Discourse-Aware Graph Network for Logical Reasoning,conclusion,Insight-tree,dagn reaches competitive performances on two recent logical reasoning datasets reclor and logiqa.,Solved,https://www.aclweb.org/anthology/2021.naacl-main.467.pdf
849,252819276,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,conclusion,Insight-tree,this paper presents a novel method to guide the mrc model to better perform logical reasoning tasks.,Solved,https://www.aclanthology.org/2022.coling-1.126.pdf
850,252819276,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,conclusion,Insight-tree,we propose a holistic graph-based system to model hierarchical logical reasoning chains.,Solved,https://www.aclanthology.org/2022.coling-1.126.pdf
851,252819276,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,conclusion,Insight-tree,"to our best knowledge, we are the first to deal with context at both discourse level and phrase level as the basis for logical reasoning.",Solved,https://www.aclanthology.org/2022.coling-1.126.pdf
852,252819276,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,conclusion,Insight-tree,"to decouple the interaction between the node features and type features, we apply hierarchical interaction mechanism to yield the appropriate representation for reading comprehension.",Solved,https://www.aclanthology.org/2022.coling-1.126.pdf
853,252819276,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,conclusion,Insight-tree,"on the logical qa benchmarks (reclor, logiqa) and natural language inference benchmarks (snli and anli), our proposed model has been shown effective by significantly outperforming the strong baselines.",Solved,https://www.aclanthology.org/2022.coling-1.126.pdf
854,248218748,Towards Fine-grained Causal Reasoning and QA,conclusion,Insight-tree,we explored the efficacy of current state-of-the-art methods for causal reasoning tasks by considering a novel fine-grained reasoning setting and developing a dataset with rich human labels.,Solved,https://arxiv.org/pdf/2204.07408v1.pdf
855,202573071,CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION,conclusion,Insight-tree,"with 1.63 billion parameters, ctrl is the largest publicly released language model to date.",Solved,https://arxiv.org/pdf/1909.05858v2.pdf
856,202573071,CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION,conclusion,Insight-tree,it is trained with control codes so that text generation can be more easily controlled by human users.,Solved,https://arxiv.org/pdf/1909.05858v2.pdf
857,202573071,CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION,conclusion,Insight-tree,"these codes allow users to explicitly specify domain, subdomain, entities, relationships between entities, dates, and task-specific behavior.",Solved,https://arxiv.org/pdf/1909.05858v2.pdf
858,235727345,Springer Nature 2021 L A T E X template A Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension,conclusion,Insight-tree,"we present the cluereader, a heterogeneous graph attention network for multi-hop machine reading comprehension, which is inspired by the grandmother cells concept from cognitive neuroscience.",Solved,https://export.arxiv.org/pdf/2107.00841v2.pdf
859,235727345,Springer Nature 2021 L A T E X template A Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension,conclusion,Insight-tree,we take the reasoning nodes and mention nodes to complete the process and use document nodes to add supernumerary semantic information.,Solved,https://export.arxiv.org/pdf/2107.00841v2.pdf
860,235727345,Springer Nature 2021 L A T E X template A Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension,conclusion,Insight-tree,"we apply our methodology in qanga-roo, a multi-hop mrc dataset, and the official evaluation supports the effectiveness of our model in open-domain qa and molecular biology domain usage.",Solved,https://export.arxiv.org/pdf/2107.00841v2.pdf
861,234479457,"A Multilingual Modeling Method for Span-Extraction Reading Comprehension CCS CONCEPTS  Computing methodologies  Natural language processing; Multilingual modeling KEYWORDS multilingual modeling, span-extraction reading comprehension, multilingual attention, self-adaptive attention",conclusion,Insight-tree,"to make full use of all existing extractive training datasets in various languages and to learn the rich hidden semantic knowledge from different language families, we propose xlrc, a multilingual extractive reading comprehension method, to simultaneously model the existing extractive reading comprehension training data in a multilingual environment by using multilingual bert and multilingual attention.",Solved,https://arxiv.org/pdf/2105.14880v1.pdf
862,234479457,"A Multilingual Modeling Method for Span-Extraction Reading Comprehension CCS CONCEPTS  Computing methodologies  Natural language processing; Multilingual modeling KEYWORDS multilingual modeling, span-extraction reading comprehension, multilingual attention, self-adaptive attention",conclusion,Insight-tree,experimental results demonstrate the effectiveness of our proposed multilingual modelling by transferring the semantic knowledge learned from various existing datasets in different languages.,Solved,https://arxiv.org/pdf/2105.14880v1.pdf
863,256827818,Few-shot Reranking for Multi-hop QA via Language Model Prompting,conclusion,Insight-tree,"we introduced promptrank, a method to perform few-shot reranking of multi-document paths for multi-hop question answering based on large language models.",Solved,https://export.arxiv.org/pdf/2205.12650v2.pdf
864,256827818,Few-shot Reranking for Multi-hop QA via Language Model Prompting,conclusion,Insight-tree,experiments on a standard multihop qa benchmark show the strong performance of promptrank in the few-shot setting compared to fully-supervised multi-hop reranking systems.,Solved,https://export.arxiv.org/pdf/2205.12650v2.pdf
865,253157773,COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,conclusion,Insight-tree,coco-dr improves zerodr accuracy by combating the distribution shifts using continuous contrastive learning and implicit distributionally robust optimization.,Solved,https://export.arxiv.org/pdf/2210.15212v2.pdf
866,253157773,COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,conclusion,Insight-tree,coco helps models better capture the sequence representations of target corpora in pretraining.,Solved,https://export.arxiv.org/pdf/2210.15212v2.pdf
867,253157773,COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,conclusion,Insight-tree,implicit dro improves model robustness by reweighting query clusters in fine-tuning.,Solved,https://export.arxiv.org/pdf/2210.15212v2.pdf
868,253080447,Efficiently Tuned Parameters are Task Embeddings,conclusion,Insight-tree,"in this paper, we show that efficiently tuned parameters are highly predictive for inter-task transferability and thus can be used as off-the-shelf task embeddings for source task selection in intermediatetask transfer learning.",Solved,https://www.aclanthology.org/2022.emnlp-main.334.pdf
869,253080447,Efficiently Tuned Parameters are Task Embeddings,conclusion,Insight-tree,our empirical investigation with three parameter-efficient tuning methods on 22 nlp tasks demonstrates that our approach outperforms prior works on inter-task transferability prediction despite being more efficient.,Solved,https://www.aclanthology.org/2022.emnlp-main.334.pdf
870,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,we study the effect of question decomposition (qd) on a diverse set of tasks.,Solved,https://www.aclanthology.org/2022.emnlp-main.302.pdf
871,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,we decompose questions manually and significantly improve model performance (24% for gpt3 and 29% for roberta-squad along with a symbolic calculator).,Solved,https://www.aclanthology.org/2022.emnlp-main.302.pdf
872,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,our findings indicate that human-in-the-loop question decomposition (hqd) can potentially provide an alternate path to building large lms.,Solved,https://www.aclanthology.org/2022.emnlp-main.302.pdf
873,253080620,LittleBird: Efficient Faster & Longer Transformer for Question Answering,conclusion,Insight-tree,"we propose littlebird, which is more efficient in terms of memory and computational time than existing transformer models for long sequences, and its effective way to train.",Solved,https://www.aclanthology.org/2022.emnlp-main.352.pdf
874,253080620,LittleBird: Efficient Faster & Longer Transformer for Question Answering,conclusion,Insight-tree,"it combines a novel position encoding method, bialibi, and pack & unpack with sliding window attention to achieve high speed and accuracy, particularly in question answering tasks for long documents.",Solved,https://www.aclanthology.org/2022.emnlp-main.352.pdf
875,253080620,LittleBird: Efficient Faster & Longer Transformer for Question Answering,conclusion,Insight-tree,"we demonstrated through experiments that the accuracy of question answering improves as the model is fed a longer input, and we achieved state-of-the-art performance in korquad2.0 using littlebird.",Solved,https://www.aclanthology.org/2022.emnlp-main.352.pdf
876,246294995,Reasoning Like Program Executors,conclusion & future work,Insight-tree,"we introduce poet, a new pre-training paradigm for boosting reasoning capability of language models via imitating program executors.",Solved,https://www.aclanthology.org/2022.emnlp-main.48.pdf
877,246294995,Reasoning Like Program Executors,conclusion & future work,Insight-tree,"experimental results on six datasets demonstrate that poet can significantly boost existing language models on several reasoning skills, including numerical, logical and multi-hop reasoning.",Solved,https://www.aclanthology.org/2022.emnlp-main.48.pdf
878,246294995,Reasoning Like Program Executors,conclusion & future work,Insight-tree,our best language model under poet can reach highly competitive performance with previous specialized models.,Solved,https://www.aclanthology.org/2022.emnlp-main.48.pdf
879,248227284,TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval,conclusion,Insight-tree,we introduce a method to train bi-encoders on unstructured text and knowledge graph types through a type-enforced contrastive loss.,Solved,https://www.aclanthology.org/2022.findings-acl.169.pdf
880,248227284,TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval,conclusion,Insight-tree,"our loss can improve retrieval of rare entities for ambiguous mentions, while maintaining strong overall performance on open-domain nlp tasks.",Solved,https://www.aclanthology.org/2022.findings-acl.169.pdf
881,221292890,Continual Domain Adaptation for Machine Reading Com-prehension,conclusion,Insight-tree,we introduce the continual domain adaptation task for mrc.,Solved,https://arxiv.org/pdf/2008.10874v1.pdf
882,221292890,Continual Domain Adaptation for Machine Reading Com-prehension,conclusion,Insight-tree,"so far as we know, this is the first study on the continual learning perspective of mrc.",Solved,https://arxiv.org/pdf/2008.10874v1.pdf
883,221292890,Continual Domain Adaptation for Machine Reading Com-prehension,conclusion,Insight-tree,"we build two datasets cda-q and cda-c for the cda task, by re-organizing existing mrc collections into different domains with respect to the question type and passage type.",Solved,https://arxiv.org/pdf/2008.10874v1.pdf
884,221292890,Continual Domain Adaptation for Machine Reading Com-prehension,conclusion,Insight-tree,we conduct preliminary experiments showing the existence of catastrophic forgetting (cf) phenomenon of existing mrc models under the cda setting.,Solved,https://arxiv.org/pdf/2008.10874v1.pdf
885,221292890,Continual Domain Adaptation for Machine Reading Com-prehension,conclusion,Insight-tree,"further, we propose regularization-based regbertqa and dynamic-architecture progbertqa to tackle the cda for mrc.",Solved,https://arxiv.org/pdf/2008.10874v1.pdf
886,221292890,Continual Domain Adaptation for Machine Reading Com-prehension,conclusion,Insight-tree,we conduct extensive experiments to analysis the effectiveness of both methods and validate that the proposed dynamic-architecture based model achieves the best performance.,Solved,https://arxiv.org/pdf/2008.10874v1.pdf
887,252090148,Interactive Question Answering Systems: Literature Review,conclusion,Insight-tree,"in conclusion, we have reviewed a substantial collection of interactive question answering systems (iqass)-related literature published during the past decade.",Solved,https://export.arxiv.org/pdf/2209.01621v1.pdf
888,252090148,Interactive Question Answering Systems: Literature Review,conclusion,Insight-tree,"we discovered the literature to be diverse, beginning with adopted methodologies for addressing multiple qa tasks and concluding with a vast array of diverse resources (i.e. knowledge source, and datasets) that are typically utilized to create and evaluate question answering systems (qass).",Solved,https://export.arxiv.org/pdf/2209.01621v1.pdf
889,252090148,Interactive Question Answering Systems: Literature Review,conclusion,Insight-tree,"despite the fact that the state-of-the-art is defined by several types of qa solutions, we were able to determine the characteristics shared by the suggested systems that constitute a shared framework.",Solved,https://export.arxiv.org/pdf/2209.01621v1.pdf
890,252090148,Interactive Question Answering Systems: Literature Review,conclusion,Insight-tree,"to the best of our knowledge, we are the first to present a unified and comprehensive design that emphasizes the fundamental components and functions of iqass.",Solved,https://export.arxiv.org/pdf/2209.01621v1.pdf
891,250264890,Rationale-Augmented Ensembles in Language Models,conclusion,Insight-tree,"in this paper, we have presented a unified framework for rationale-augmented ensembles, and found that rationale sampling in the output space is a key component for achieving improved performance in natural language processing tasks.",Solved,https://arxiv.org/pdf/2207.00747v1.pdf
892,250264890,Rationale-Augmented Ensembles in Language Models,conclusion,Insight-tree,"by sampling diverse rationales and ensembling the results, we have shown that rational-ensembling methods in the proposed framework can reliably outperform standard prompting and rationale-based few-shot prompting, across a wide range of natural language tasks and alternative language models.",Solved,https://arxiv.org/pdf/2207.00747v1.pdf
893,250264890,Rationale-Augmented Ensembles in Language Models,conclusion,Insight-tree,"overall, rationale-augmented ensembling appears to be a reliable way to shift from the paradigm of (input  output) pairs to (input, rationale  output) pairs to achieve more accurate and interpretable natural language processing.",Solved,https://arxiv.org/pdf/2207.00747v1.pdf
894,237605111,BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles,conclusion,Insight-tree,"in this paper, we introduce birdqa, a large-scale, bilingual multiple-choice question answering dataset to facilitate the development of qa systems capable of solving tricky riddles.",Solved,https://arxiv.org/pdf/2109.11087v2.pdf
895,250390947,Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering,conclusion and future work,Insight-tree,"in this paper, we apply explicit graph reasoning to extracted knowledge and contextual information for multi-hop reasoning.",Solved,https://www.aclanthology.org/2022.dlg4nlp-1.8.pdf
896,250390947,Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering,conclusion and future work,Insight-tree,experimental results on hotpotqa dataset show the effectiveness of our model.,Solved,https://www.aclanthology.org/2022.dlg4nlp-1.8.pdf
897,248827723,A STEP towards Interpretable Multi-Hop Reasoning: Bridge Phrase Identification and Query Expansion,conclusion,Insight-tree,we proposed an unsupervised approach for the identification of bridge phrases in multi-hop question answering.,Solved,
898,248827723,A STEP towards Interpretable Multi-Hop Reasoning: Bridge Phrase Identification and Query Expansion,conclusion,Insight-tree,"our method can be coupled with any downstream qa component, i.e., it can be used as query expansion for evidence retrieval; it can be used to generate enhanced context for answer prediction; and it can be used to generate post-hoc explanations for given answers.",Solved,
899,248827723,A STEP towards Interpretable Multi-Hop Reasoning: Bridge Phrase Identification and Query Expansion,conclusion,Insight-tree,"using the hotpotqa dataset, we demonstrate that our method yields improved results in all these scenarios, for multiple types of downstream components.",Solved,
900,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,we show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection.,Solved,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
901,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,we proposed a simple semi-supervision technique to expose the model to these annotations.,Solved,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
902,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,we have also released these annotations for the research community at https: //github.com/ddua/intermediate_annotations.,Solved,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
903,248780551,CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation,conclusion,Insight-tree,"to address the complexity control problem of mqg, we propose a simple control framework cqg, which consists of a gat-based key entity extractor and a controlled generated.",Solved,https://www.aclanthology.org/2022.acl-long.475.pdf
904,248780551,CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation,conclusion,Insight-tree,cqg greatly improves the performance and we hope our model will help researchers to study the mqg task.,Solved,https://www.aclanthology.org/2022.acl-long.475.pdf
905,202583433,Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering,conclusion,Insight-tree,we introduce an entity-centric approach to ir that finds relevant evidence required to answer multihop questions from a corpus containing millions of paragraphs leading to significant improvement to an existing qa system.,Solved,https://www.aclweb.org/anthology/D19-5816.pdf
906,207852944,Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering,conclusion,Insight-tree,we proposed a general approach for open-domain question answering (qa) that models interactions between paragraphs using structural information from a knowledge base.,Solved,https://arxiv.org/pdf/1911.03868v2.pdf
907,207852944,Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering,conclusion,Insight-tree,"unlike standard approaches where a model retrieves and reads a set of passages, we integrate graph structure at every stage to construct, retrieve and read a graph of passages.",Solved,https://arxiv.org/pdf/1911.03868v2.pdf
908,207852944,Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering,conclusion,Insight-tree,"our approach consistently outperforms competitive baselines in three open-domain qa datasets, webquestions, natural ques-tions and triviaqa, and we also include a detailed qualitative analysis to illustrate where the cross paragraph reading contributes the most to the overall system performance.",Solved,https://arxiv.org/pdf/1911.03868v2.pdf
909,248405719,A Thorough Examination on Zero-shot Dense Retrieval,conclusion and future work,Insight-tree,"in this paper, we thoroughly examine the zeroshot capability of dr models.",Solved,https://export.arxiv.org/pdf/2204.12755v2.pdf
910,248405719,A Thorough Examination on Zero-shot Dense Retrieval,conclusion and future work,Insight-tree,we conduct empirical analysis by extensively studying the effect of various factors on the retrieval performance.,Solved,https://export.arxiv.org/pdf/2204.12755v2.pdf
911,248405719,A Thorough Examination on Zero-shot Dense Retrieval,conclusion and future work,Insight-tree,"in particular, we find that the factors of vocabulary overlap, query type distribution, and data scale are likely to affect the zero-shot performance of dense retriever.",Solved,https://export.arxiv.org/pdf/2204.12755v2.pdf
912,248405719,A Thorough Examination on Zero-shot Dense Retrieval,conclusion and future work,Insight-tree,"overall, we find that the zero-shot performance of dense retrieval models still has room to improve and deserves further study.",Solved,https://export.arxiv.org/pdf/2204.12755v2.pdf
913,226262208,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,conclusion,Insight-tree,"we introduced iirc, a new dataset of incompleteinformation reading comprehension questions.",Solved,https://arxiv.org/pdf/2011.07127v1.pdf
914,219687051,Self-supervised Learning: Generative or Contrastive,conclusion,Insight-tree,"this survey comprehensively reviews the existing selfsupervised representation learning approaches in natural language processing (nlp), computer vision (cv), graph learning, and beyond.",Solved,https://arxiv.org/pdf/2006.08218v5.pdf
915,219687051,Self-supervised Learning: Generative or Contrastive,conclusion,Insight-tree,"despite the diversity of algorithms, we categorize all self-supervised methods into three classes: generative, contrastive, and generative contrastive according to their essential training objectives.",Solved,https://arxiv.org/pdf/2006.08218v5.pdf
916,219687051,Self-supervised Learning: Generative or Contrastive,conclusion,Insight-tree,we introduce typical and representative methods in each category and sub-categories.,Solved,https://arxiv.org/pdf/2006.08218v5.pdf
917,219687051,Self-supervised Learning: Generative or Contrastive,conclusion,Insight-tree,"moreover, we discuss the pros and cons of each category and their unique application scenarios.",Solved,https://arxiv.org/pdf/2006.08218v5.pdf
918,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization.",Solved,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
919,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments.,Solved,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
920,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more  gpus with 11gb memory.,Solved,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
921,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"we obtained 90 and 110 evidence sentences from the sentence selection and span selection models, respectively.",Solved,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
922,235294052,On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study,conclusion,Insight-tree,"in this paper, we demonstrated that across a variety of models and datasets, training on adversarial data leads to better performance on evaluation sets created in a similar fashion, but tends to yield worse performance on out-of-domain evaluation sets not created adversarially.",Solved,https://www.aclanthology.org/2021.acl-long.517.pdf
923,235294052,On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study,conclusion,Insight-tree,"additionally, our results suggest that the adc process (regardless of the outcome) might matter more than successfully fooling a model.",Solved,https://www.aclanthology.org/2021.acl-long.517.pdf
924,235294052,On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study,conclusion,Insight-tree,"we also identify key qualitative differences between data generated via adc and sdc, particularly the kinds of questions created.",Solved,https://www.aclanthology.org/2021.acl-long.517.pdf
925,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,"we introduce proqa, a unified qa paradigm that adopts a single model for solving various qa tasks with the bridge of a structural prompt.",Solved,https://www.aclanthology.org/2022.naacl-main.313.pdf
926,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,"we further conduct structural prompt-based pre-training, seeking to empower the model with general qa-centric ability and injects the semantic knowledge of the structural prompt into the pre-training model.",Solved,https://www.aclanthology.org/2022.naacl-main.313.pdf
927,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,experimental results on 11 qa benchmarks demonstrate that proqa can significantly boost performance on all settings.,Solved,https://www.aclanthology.org/2022.naacl-main.313.pdf
928,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,"further analyses show that our method can better mitigate the catastrophic forgetting issue during continual learning, and our method can be adapted to a newly involved task more quickly, by taking the advantages of the structural prompt.",Solved,https://www.aclanthology.org/2022.naacl-main.313.pdf
929,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,"in this work, we propose fermi problems (fps) as a reasoning challenge for ai systems.",Solved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
930,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,we collect two datasets -realfp with 1k real-world questions and syn-thfp with 10k templated questions.,Solved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
931,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,"based on these datasets, we propose three concrete tasks of increasing difficulty that encompass the fp challenge.",Solved,https://www.aclanthology.org/2021.emnlp-main.582.pdf
932,249062555,Teaching Broad Reasoning Skills via Decomposition-Guided Contexts,conclusions,Insight-tree,large language models demonstrate impressive reading comprehension abilities and a wide variety of reasoning skills.,Solved,https://arxiv.org/pdf/2205.12496v1.pdf
933,249062555,Teaching Broad Reasoning Skills via Decomposition-Guided Contexts,conclusions,Insight-tree,"in this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions.",Solved,https://arxiv.org/pdf/2205.12496v1.pdf
934,249062555,Teaching Broad Reasoning Skills via Decomposition-Guided Contexts,conclusions,Insight-tree,our transfer results on actual qa datasets also add to the line of work that shows synthetic datasets can be used to inject useful skills that transfer over to real natural language tasks.,Solved,https://arxiv.org/pdf/2205.12496v1.pdf
935,249062555,Teaching Broad Reasoning Skills via Decomposition-Guided Contexts,conclusions,Insight-tree,"given the artifact issues in real datasets (specifically, in their contexts) and the difficulty in controlling for them via perturbations, leveraging existing multihop questions for their broad reasoning patterns but using synthetic contexts appears to be a viable alternative for carefully constructing teaching datasets, where models can learn the right way to reason.",Solved,https://arxiv.org/pdf/2205.12496v1.pdf
936,215768725,A Simple Yet Strong Pipeline for HotpotQA,conclusion,Insight-tree,"our work shows that on the hotpotqa tasks, a simple pipeline model can do as well as or better than more complex solutions, such as graph networks, cross-document attention, or ner.",Solved,https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf
937,215768725,A Simple Yet Strong Pipeline for HotpotQA,conclusion,Insight-tree,"powerful pre-trained models allow us to score sentences one at a time, without looking at other paragraphs.",Solved,https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf
938,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,"we present tasa, an automatic adversarial attack method for qa models.",Solved,https://www.aclanthology.org/2022.emnlp-main.821.pdf
939,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,"in experiments, tasa achieves remarkable attack performance on five datasets and three victim models with satisfactory sample quality.",Solved,https://www.aclanthology.org/2022.emnlp-main.821.pdf
940,253384010,NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering,conclusion,Insight-tree,"in this paper, we present a non-autoregressive program generation (napg) framework for numerical reasoning, which facilitates program generation in parallel.",Solved,https://export.arxiv.org/pdf/2211.03462v1.pdf
941,253384010,NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering,conclusion,Insight-tree,our framework independently generates complete program tuples containing both the operator and its operands.,Solved,https://export.arxiv.org/pdf/2211.03462v1.pdf
942,253384010,NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering,conclusion,Insight-tree,"compared to previous autoregressive decoding methods, napg does not suffer from exposure bias, and can significantly boost the program generation speed.",Solved,https://export.arxiv.org/pdf/2211.03462v1.pdf
943,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"in this paper, we present a novel plug-and-play smoe-dropout strategy for training overparameterized transformers in full-capacity settings without collapse.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
944,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,we design a fixed and randomly initialized router to assign experts and gradually increase their number along with the training.,Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
945,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"as a result, our proposal provides an appealing ""self-slimmable"" property to large transformers during inference and downstream fine-tuning, depending on available resources.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
946,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,it implies alleviated representation collapse and delivers an in-situ trade-off between efficiency and performance.,Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
947,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"extensive experiments across various combinations of network backbone and dataset, consistently demonstrate the significantly improved performance and training time savings from our algorithm.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
948,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"the results are reported in table a4, from which we can observe that our smoe-dropout achieves a statistically significant improvement of 0.93%  1.17% accuracy gains compared with other smoe-variants and the dense network, where there is no overlap between the error bars (one standard deviation).",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
949,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"table a5 demonstrates that both random routing policy and progressively increasing the number of activated experts are beneficial for alleviating representation collapse and providing ""selfslimmable"" property, yet not as good as combining both.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
950,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"to be specific, when applying the strategy of progressively enlarging the number of activated experts, the learnable smoes suffer less representation collapse and achieve better performance, i.e., 0.31% higher accuracy.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
951,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"meanwhile, we find that learnable smoe with curriculum learning has the ""self-slimmable"" property only when activating experts from k = 1 to k",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
952,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"we conduct a further transfer study of the pre-trained bert networks on a multi-hop questionanswering dataset, hotpotqa yang et al. (2018). and we use exact match (em) accuracy to assess networks' performance.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
953,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"following the same metric in press et al. (2022), we calculate the compositionality gap, i.e., the gap of em accuracy between multi-hop question answering and its all single-hop sub-questions , of each network.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
954,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"as shown in table a6, our smoe-dropout is beneficial for reducing the compositionality gap, which achieves the best performance with up to 0.30% higher em score and 0.30% narrower compositionality gap, compared with the learnable smoe and its dense counterpart.",Solved,https://export.arxiv.org/pdf/2303.01610v1.pdf
955,256105660,Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,conclusion,Insight-tree,"in this paper, we propose to model the structured knowledge in contexts and perform structure reasoning over them for complex reasoning.",Solved,https://export.arxiv.org/pdf/2301.08913v1.pdf
956,256105660,Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,conclusion,Insight-tree,"to accomplish this objective, we present a unified framework combining structure reasoning and language modeling.",Solved,https://export.arxiv.org/pdf/2301.08913v1.pdf
957,256105660,Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,conclusion,Insight-tree,it extracts four types of elementary knowledge structures from contexts to construct structured queries and adopts the box embedding method for explicit structure reasoning along the constructed queries.,Solved,https://export.arxiv.org/pdf/2301.08913v1.pdf
958,256105660,Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,conclusion,Insight-tree,we utilize the contextual language representations of knowledge for structure reasoning and obtain a structure-embedded language representation.,Solved,https://export.arxiv.org/pdf/2301.08913v1.pdf
959,256105660,Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning,conclusion,Insight-tree,experimental results show the effectiveness of our model on complex reasoning tasks over both language and kgs.,Solved,https://export.arxiv.org/pdf/2301.08913v1.pdf
960,257405222,AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation,discussion and conclusion,Insight-tree,"in this study, a series of scalable augmentation methods are proposed to produce surrogate queries for training dense retrievers without using any annotated query-document pairs.",Solved,https://export.arxiv.org/pdf/2212.08841v2.pdf
961,257405222,AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation,discussion and conclusion,Insight-tree,"we achieve state-of-the-art performance on two collections of widely used benchmarks (beir and six odqa datasets), demonstrating that the efficacy of synthetic querydocument pairs for training dense retrievers, greatly bridging the gap between unsupervised dense models and bm25 and inspiring us to rethink the necessity of using real queries.",Solved,https://export.arxiv.org/pdf/2212.08841v2.pdf
962,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"in this work, we proposed the screenqa task.",Solved,https://export.arxiv.org/pdf/2209.08199v1.pdf
963,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"we annotated a large-scale screenqa dataset, which contains more than 80,000 question-answer pairs.",Solved,https://export.arxiv.org/pdf/2209.08199v1.pdf
964,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"compared to other vision-language multimodal problems, such as document image understanding and visual question answering, screenqa poses its unique challenges: rich in text, diverse in apps, and blended with icons and symbols.",Solved,https://export.arxiv.org/pdf/2209.08199v1.pdf
965,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"a) question annotation was performed in a sequential manner, the later and non-overlapping annotators can see all previous questions to diversify question framing and avoid duplication.",Solved,https://export.arxiv.org/pdf/2209.08199v1.pdf
966,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,we also used the sequential process to provide more feedback and training to the annotators for quality improvement.,Solved,https://export.arxiv.org/pdf/2209.08199v1.pdf
967,233297028,Question Decomposition with Dependency Graphs,conclusion,Insight-tree,"in this work, we propose to represent qdmr structures with a dependency graph over the input tokens, and propose a graph parser and a seq2seq model that uses graph supervision as an auxiliary loss.",Solved,https://arxiv.org/pdf/2104.08647v1.pdf
968,233297028,Question Decomposition with Dependency Graphs,conclusion,Insight-tree,"we show that a graph parser is 16x faster than a seq2seq model, and that it exhibits better sample coplexity.",Solved,https://arxiv.org/pdf/2104.08647v1.pdf
969,233297028,Question Decomposition with Dependency Graphs,conclusion,Insight-tree,"moreover, using graphs as auxiliary supervision improves out-of-domain generalization and leads to better performance on questions that represent a long sequence of computational steps.",Solved,https://arxiv.org/pdf/2104.08647v1.pdf
970,233297028,Question Decomposition with Dependency Graphs,conclusion,Insight-tree,"last, we propose a new evaluation metric for qdmr parsing and show it better corresponds to human intuitions.",Solved,https://arxiv.org/pdf/2104.08647v1.pdf
971,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,"by avoiding data collection procedures reliant on translation and multilingual modeling, we greatly mitigate the risk of sampling bias.",Solved,https://arxiv.org/pdf/2003.05002v1.pdf
972,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,"because we believe mt may be a fruitful research direction for tydi qa, we do not release any automatic translations.",Solved,https://arxiv.org/pdf/2003.05002v1.pdf
973,222125277,Under review AUTOREGRESSIVE ENTITY RETRIEVAL,conclusions,Insight-tree,"in this work, we propose genre, a novel paradigm to addresses entity retrieval: generate entity names autoregressively.",Solved,https://arxiv.org/pdf/2010.00904v1.pdf
974,222125277,Under review AUTOREGRESSIVE ENTITY RETRIEVAL,conclusions,Insight-tree,"the autoregressive formulation allows us to directly capture some of these properties, leading to several advantages with respect to current solutions, including an efficient way to cross encode men-tion context and entity candidates, a much smaller memory footprint, and the ability to compute an exact softmax without the need to subsample negative data.",Solved,https://arxiv.org/pdf/2010.00904v1.pdf
975,222125277,Under review AUTOREGRESSIVE ENTITY RETRIEVAL,conclusions,Insight-tree,"we empirically show that these characteristics, combined with constrained decoding strategies, led to state-of-the-art performance on a plethora of entity retrieval datasets, spanning entity disambiguation, end-to-end entity linking, and page-level document retrieval, while resulting in systems with a remarkably contained memory footprint, a space reduction by a factor of twenty on average.",Solved,https://arxiv.org/pdf/2010.00904v1.pdf
976,222125277,Under review AUTOREGRESSIVE ENTITY RETRIEVAL,conclusions,Insight-tree,we additionally demonstrate that new entities can be effectively considered in our system by simply appending their unambiguous name to the candidate set.,Solved,https://arxiv.org/pdf/2010.00904v1.pdf
977,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,discussion and limitations,Insight-tree,we show that feature attributions can reveal known dataset biases and reasoning shortcuts in hotpotqa without having to perform a detailed manual analysis.,Solved,https://www.aclanthology.org/2021.emnlp-main.447.pdf
978,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,discussion and limitations,Insight-tree,this confirms the suitability of our attribution methods for at least this use case: model designers can look at them in a semi-automated way and determine how robust the model is going to be when faced with counterfactuals.our analysis also highlights the limitations of current explanation techniques.,Solved,https://www.aclanthology.org/2021.emnlp-main.447.pdf
979,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,discussion and limitations,Insight-tree,"we experimented with other counterfactuals by permuting the order of the paragraphs in the context, which often gave rise to different predictions.",Solved,https://www.aclanthology.org/2021.emnlp-main.447.pdf
980,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,conclusion,Insight-tree,we have presented a new methodology using explanations to understand model behavior on realistic counterfactuals.,Solved,https://www.aclanthology.org/2021.emnlp-main.447.pdf
981,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,conclusion,Insight-tree,"we show explanations can indeed be connected to model behavior, and therefore we can compare explanations to understand which ones truly give us actionable insights about what our models are doing.",Solved,https://www.aclanthology.org/2021.emnlp-main.447.pdf
982,237592852,Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering,conclusion,Insight-tree,"in this work, we provided insights on the performance of state-of-the-art dense retrieval for multihop questions.",Solved,https://www.aclanthology.org/2021.sustainlp-1.7.pdf
983,237592852,Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering,conclusion,Insight-tree,"we showed that rerank+dpr 2 (our hybrid model) outperforms mdr (the state-of-theart multi-hop dense retrieval model) in the low re-source setting, and it is competitive with mdr in the setting where mdr uses considerably more computational resources.",Solved,https://www.aclanthology.org/2021.sustainlp-1.7.pdf
984,237592852,Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering,conclusion,Insight-tree,"finally, we highlighted that fully dense retrieval models get harmed when using limited computational resources.",Solved,https://www.aclanthology.org/2021.sustainlp-1.7.pdf
985,247447492,What Makes Reading Comprehension Questions Difficult?,conclusion,Insight-tree,we crowdsource multiple-choice reading comprehension questions for passages extracted from seven different sources and analyze the effects of passage source on question difficulty and diversity.,Solved,https://www.aclanthology.org/2022.acl-long.479.pdf
986,247447492,What Makes Reading Comprehension Questions Difficult?,conclusion,Insight-tree,"although we expect that the difficulty of a passage affects the difficulty of questions about that passage, the collected questions do not show any strong correlation between the human-machine performance gap and passage source, length, or readability measures.",Solved,https://www.aclanthology.org/2022.acl-long.479.pdf
987,247447492,What Makes Reading Comprehension Questions Difficult?,conclusion,Insight-tree,our manual annotation of comprehension types reveals that questions requiring numerical or logical reasoning are relatively difficult.,Solved,https://www.aclanthology.org/2022.acl-long.479.pdf
988,247447492,What Makes Reading Comprehension Questions Difficult?,conclusion,Insight-tree,we also find several trends between passage sources and comprehension types.,Solved,https://www.aclanthology.org/2022.acl-long.479.pdf
989,251594672,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,conclusion,Insight-tree,"in this paper, we have proposed corpusbrain, a novel pre-trained generative retrieval model to encode all information about the corpus into its parameters.",Solved,https://export.arxiv.org/pdf/2208.07652v1.pdf
990,251594672,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,conclusion,Insight-tree,"to train such a strong generative model, we delicately devised a set of pre-training tasks to emphasize different aspects of semantics between queries and documents.",Solved,https://export.arxiv.org/pdf/2208.07652v1.pdf
991,251594672,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,conclusion,Insight-tree,corpusbrain just needs to pre-train one model and could be then adapted to improve a diversity of downstream kilt tasks without the need of constructing additional index.,Solved,https://export.arxiv.org/pdf/2208.07652v1.pdf
992,251594672,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,conclusion,Insight-tree,"through experiments on the kilt benchmark in terms of the retrieval task, corpusbrain achieved significant improvements over strong baseline approaches.",Solved,https://export.arxiv.org/pdf/2208.07652v1.pdf
993,251594672,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,conclusion,Insight-tree,we also showed that corpusbrain can achieve strong performance under both the zero-and low-resource settings.,Solved,https://export.arxiv.org/pdf/2208.07652v1.pdf
994,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,we give a comprehensive review of large-scale multi-modal pre-trained models (mm-ptms) in this paper.,Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
995,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,"firstly, we introduce the background of mm-ptms, with a focus on conventional deep learning, and pre-training in nlp, cv, and speech.",Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
996,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,"then, the task definition, key challenges, and benefits of mm-ptms are discussed.",Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
997,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,"after that, we dive into the reviews of mm-ptms and discuss the pre-training data, objectives, networks, knowledge enhanced pre-training, etc.",Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
998,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,"we review the downstream tasks including generative, classification, and regression tasks, and also give an overview of model parameters of mm-ptms and hardware for the pre-training.",Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
999,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,experimental results of several representative tasks are also discussed and visualized.,Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
1000,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,"finally, we point out some research directions that are worth to be focused on.",Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
1001,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,we summarize this paper and hope our survey can provide some useful insights for the mm-ptms.,Solved,https://export.arxiv.org/pdf/2302.10035v1.pdf
1002,226955938,Unsupervised Explanation Generation for Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we aim to improve the explainability for the machine reading comprehension task, which is different from most of the previous works that were only striving for better objective evaluation scores.",Solved,https://arxiv.org/pdf/2011.06737v1.pdf
1003,226955938,Unsupervised Explanation Generation for Machine Reading Comprehension,conclusion,Insight-tree,"to achieve this goal, we propose a novel mechanism called recursive dynamic gating (rdg) to gradually refine the amount of the input information in each layer of the pre-trained language model.",Solved,https://arxiv.org/pdf/2011.06737v1.pdf
1004,226955938,Unsupervised Explanation Generation for Machine Reading Comprehension,conclusion,Insight-tree,"also, we propose an attention smoothing technique that will increase the accuracy of the rdg mechanism.",Solved,https://arxiv.org/pdf/2011.06737v1.pdf
1005,226955938,Unsupervised Explanation Generation for Machine Reading Comprehension,conclusion,Insight-tree,"experimental results on three multiple-choice machine reading comprehension datasets show that the proposed rdg mechanism could not only improve the objective evaluation scores, but also show an advantage over the traditional attention mechanism in explainability.",Solved,https://arxiv.org/pdf/2011.06737v1.pdf
1006,224803601,OPEN QUESTION ANSWERING OVER TABLES AND TEXT,conclusion,Insight-tree,we focus on the problem of performing open question answering over tables and text in this paper.,Solved,https://arxiv.org/pdf/2010.10439v1.pdf
1007,253708231,CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,conclusion,Insight-tree,"this paper proposes a novel multi-vector retrieval method that achieves state-of-the-art performance on several benchmark datasets while being 40 faster than colbert-v2 and 17 faster than the most efficient multi-vector retrieval library to date, plaid.",Solved,https://export.arxiv.org/pdf/2211.10411v1.pdf
1008,253708231,CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,conclusion,Insight-tree,"by jointly optimizing for the token index size and load balancing, our new dynamic lexical routing scheme greatly reduces the redundancy in all-to-all token interaction of colbert while bridging the word-mismatch problem in coil.",Solved,https://export.arxiv.org/pdf/2211.10411v1.pdf
1009,253708231,CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,conclusion,Insight-tree,experiments on both in-domain and out-of-domain datasets demonstrate the effectiveness and efficiency of our model.,Solved,https://export.arxiv.org/pdf/2211.10411v1.pdf
1010,258823156,Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval,conclusion and future work,Insight-tree,we demonstrate that query representations can be improved using feedback from a cross-encoder reranker at inference time for better performance of dual-encoder retrieval.,Solved,https://export.arxiv.org/pdf/2305.11744v1.pdf
1011,258823156,Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval,conclusion and future work,Insight-tree,this work proposes for distillation using relevance feedback from the re-ranker as a better and faster alternative to the traditional strategy of re-ranking a larger pool of candidates for improving recall.,Solved,https://export.arxiv.org/pdf/2305.11744v1.pdf
1012,258823156,Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval,conclusion and future work,Insight-tree,"our proposed distillation process is lightweight and im-proves retrieval accuracy across different domains, languages and modalities over a state-of-the-art retrieve-and-rerank pipeline with comparable latency.",Solved,https://export.arxiv.org/pdf/2305.11744v1.pdf
1013,252819220,KHANQ: A Dataset for Generating Deep Questions in Education,conclusions and future works,Insight-tree,"in this paper, we propose khanq, a dataset for generating in-depth educational questions.",Solved,https://www.aclanthology.org/2022.coling-1.518.pdf
1014,252819220,KHANQ: A Dataset for Generating Deep Questions in Education,conclusions and future works,Insight-tree,"each sample in khanq is carefully annotated as context, prompt, and question to form a clean dataset.",Solved,https://www.aclanthology.org/2022.coling-1.518.pdf
1015,252819220,KHANQ: A Dataset for Generating Deep Questions in Education,conclusions and future works,Insight-tree,we evaluate the performance of state-of-the-art question generation models on khanq.,Solved,https://www.aclanthology.org/2022.coling-1.518.pdf
1016,196170479,ELI5: Long Form Question Answering,conclusion,Insight-tree,we introduce the first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers.,Solved,https://arxiv.org/pdf/1907.09190v1.pdf
1017,196170479,ELI5: Long Form Question Answering,conclusion,Insight-tree,we show that abstractive models generate coherent answers and are competitive with extractive models in human evaluation.,Solved,https://arxiv.org/pdf/1907.09190v1.pdf
1018,128345225,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,conclusions,Insight-tree,pullnet is a novel integrated qa framework for (1) learning what to retrieve from a kb and/or corpus and (2) reasoning with this heterogeneous data to find the best answer.,Solved,https://www.aclweb.org/anthology/D19-1242.pdf
1019,128345225,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,conclusions,Insight-tree,this iterative process makes it possible to retrieve a small graph that contains just the information relevant to a multi-hop question.,Solved,https://www.aclweb.org/anthology/D19-1242.pdf
1020,258841405,"HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision",conclusion,Insight-tree,"we present hug, a probabilistic, principled approach for explainable multi-hop reasoning without rationale supervision.",Solved,https://export.arxiv.org/pdf/2305.14237v1.pdf
1021,258841405,"HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision",conclusion,Insight-tree,hug explicitly models multi-hop reasoning by considering the dependency between documents and between sentences within a document.,Solved,https://export.arxiv.org/pdf/2305.14237v1.pdf
1022,258841405,"HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision",conclusion,Insight-tree,experimental results demonstrate that hug outperforms other state-of-the-art methods that do not rely on rationale labels.,Solved,https://export.arxiv.org/pdf/2305.14237v1.pdf
1023,208201969,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,conclusion,Insight-tree,"by contrast, in this work, we proposed an analysis methodology for the bench-marking capacity of datasets.",Solved,https://ojs.aaai.org/index.php/AAAI/article/download/6422/6278
1024,208201969,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,conclusion,Insight-tree,we exemplified 12 skills and analyzed 10 datasets.,Solved,https://ojs.aaai.org/index.php/AAAI/article/download/6422/6278
1025,198229624,SpanBERT: Improving Pre-training by Representing and Predicting Spans,conclusion,Insight-tree,"we presented a new method for span-based pretraining which extends bert by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it.",Solved,https://www.aclweb.org/anthology/2020.tacl-1.5.pdf
1026,198229624,SpanBERT: Improving Pre-training by Representing and Predicting Spans,conclusion,Insight-tree,"together, our pretraining process yields models that outperform all bert baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.",Solved,https://www.aclweb.org/anthology/2020.tacl-1.5.pdf
1027,230770437,SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering,conclusion,Insight-tree,"in conclusion, this paper presents sf-qa, a novel evaluation framework to make open-domain qa research simple and fair.",Solved,https://www.aclweb.org/anthology/2021.eacl-demos.2.pdf
1028,230770437,SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering,conclusion,Insight-tree,"this framework fixes the gap among researchers from different fields, and make the open-domain qa more accessible.",Solved,https://www.aclweb.org/anthology/2021.eacl-demos.2.pdf
1029,230770437,SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering,conclusion,Insight-tree,we show the robustness of this framework by successfully reproducing several existing models in opendomain qa research.,Solved,https://www.aclweb.org/anthology/2021.eacl-demos.2.pdf
1030,234470046,Encoding Explanatory Knowledge for Zero-shot Science Question Answering,conclusion,Insight-tree,"in this paper, we proposed a neural encoding mechanism for explanatory knowledge acquisition and transfer, n-xkt.",Solved,https://www.aclanthology.org/2021.iwcs-1.5.pdf
1031,234470046,Encoding Explanatory Knowledge for Zero-shot Science Question Answering,conclusion,Insight-tree,we evaluated the impact of the encoding mechanism on downstream science qa.,Solved,https://www.aclanthology.org/2021.iwcs-1.5.pdf
1032,234470046,Encoding Explanatory Knowledge for Zero-shot Science Question Answering,conclusion,Insight-tree,the proposed model delivers better generalisation and accuracy for qa tasks that require multi-hop and explanatory inference.,Solved,https://www.aclanthology.org/2021.iwcs-1.5.pdf
1033,234470046,Encoding Explanatory Knowledge for Zero-shot Science Question Answering,conclusion,Insight-tree,"the proposed encoding mechanism can be used to deliver zero-shot inference capabilities, providing comparable performance when compared to supervised models on qa.",Solved,https://www.aclanthology.org/2021.iwcs-1.5.pdf
1034,234470046,Encoding Explanatory Knowledge for Zero-shot Science Question Answering,conclusion,Insight-tree,these results supports the hypothesis that pretraining tasks targeting abstract and explanatory knowledge acquisition can constitute and impor-,Solved,https://www.aclanthology.org/2021.iwcs-1.5.pdf
1035,251320151,Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models,conclusions,Insight-tree,"in this paper, we propose to separately evaluate interpolation and extrapolation capabilities of neural retrieval models.",Solved,https://export.arxiv.org/pdf/2204.11447v2.pdf
1036,251320151,Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models,conclusions,Insight-tree,"based on the definition, we investigate the bias in popular benchmarks, design associated evaluation methods, and revisit existing neural ranking models.",Solved,https://export.arxiv.org/pdf/2204.11447v2.pdf
1037,251320151,Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models,conclusions,Insight-tree,we observe that the popular benchmarks are biased towards interpolation and thus may not reflect how models extrapolate.,Solved,https://export.arxiv.org/pdf/2204.11447v2.pdf
1038,202541222,Large Scale Question Answering using Tourism Data,conclusion,Insight-tree,"in the spirit of defining a question answering challenge that is closer to a real-world qa setting, we introduce the novel task of identifying the correct entity answer to a given user question based on a collection of unstructured reviews describing entities.",Solved,https://arxiv.org/pdf/1909.03527v2.pdf
1039,202541222,Large Scale Question Answering using Tourism Data,conclusion,Insight-tree,"we harvest a dataset of over 48,000 qa pairs, which enables end to end training of models.",Solved,https://arxiv.org/pdf/1909.03527v2.pdf
1040,252907386,Can language representation models think in bets?,conclusion and future work,Insight-tree,"in this article, we addressed the question of whether such lrms can be adapted for (approximately) rational decision-making and preference elicitation.",Solved,https://export.arxiv.org/pdf/2210.07519v1.pdf
1041,252907386,Can language representation models think in bets?,conclusion and future work,Insight-tree,"given the near human-like performance of lrms on language-based problems, we formulated a set of rqs to specifically test whether: (i) lrms have a distinct preference for high-value items over lowvalue items, especially when the items were not seen during training, and after stratifying by the format of the questions, (ii) lrms can make, or be taught to make, (approximately rational) bets in a generalizable manner, including when an lrm has been fine-tuned on one 'modality' of bet but is evaluated on another modality.",Solved,https://export.arxiv.org/pdf/2210.07519v1.pdf
1042,247594506,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,conclusion,Insight-tree,"in this paper, we proposed fairr, a faithful and robust deductive reasoning model based on three modular components: rule selection, fact selection, and knowledge composition.",Solved,https://www.aclanthology.org/2022.acl-long.77.pdf
1043,247594506,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,conclusion,Insight-tree,fairr ensures causality from proof generation to entailment prediction by design.,Solved,https://www.aclanthology.org/2022.acl-long.77.pdf
1044,247594506,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,conclusion,Insight-tree,we established the effectiveness of our approach through experiments on testing robustness to language variations and demonstrating the interpretability of the errors made by our model.,Solved,https://www.aclanthology.org/2022.acl-long.77.pdf
1045,247594506,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,conclusion,Insight-tree,we also show that fairr is faster and more precise at deductive reasoning than prior baselines.,Solved,https://www.aclanthology.org/2022.acl-long.77.pdf
1046,254854129,Visconde: Multi-document QA with GPT-3 and Neural Reranking,conclusion,Insight-tree,this paper describes a system for multi-document question answering that uses a passage reranker to retrieve documents and large language models to reason over them and compose an answer.,Solved,https://export.arxiv.org/pdf/2212.09656v1.pdf
1047,254854129,Visconde: Multi-document QA with GPT-3 and Neural Reranking,conclusion,Insight-tree,"our system rivals state-of-the-art supervised models in three datasets: iirc, qasper, and strategyqa.",Solved,https://export.arxiv.org/pdf/2212.09656v1.pdf
1048,254854129,Visconde: Multi-document QA with GPT-3 and Neural Reranking,conclusion,Insight-tree,"our results suggest that using gpt-3 as a reader is close to human-level performance as long as relevant passages are provided, while current retrievers are the main bottleneck.",Solved,https://export.arxiv.org/pdf/2212.09656v1.pdf
1049,254854129,Visconde: Multi-document QA with GPT-3 and Neural Reranking,conclusion,Insight-tree,we also show that inducing the model to give explanations before answering a question improves effectiveness.,Solved,https://export.arxiv.org/pdf/2212.09656v1.pdf
1050,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,we have presented a generative formulation of context pair selection in multi-hop question answering models.,Solved,https://arxiv.org/pdf/2104.08744v1.pdf
1051,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,"by encouraging the context selection model to explain the entire question, it is less susceptible to bias, performing substantially better on adversarial data than existing methods that use discriminative selection.",Solved,https://arxiv.org/pdf/2104.08744v1.pdf
1052,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,this paper focuses on biases found in question answering models that make its reasoning capabilities brittle.,Solved,https://arxiv.org/pdf/2104.08744v1.pdf
1053,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,it uses an existing method of testing model performance on adversarial held-out set as an evaluation metric.,Solved,https://arxiv.org/pdf/2104.08744v1.pdf
1054,247188085,Read before Generate! Faithful Long Form Question Answering with Machine Reading,conclusion,Insight-tree,we propose a new end-to-end framework rbg that jointly models answer generation and machine reading to tackle the faithfulness issue in lfqa.,Solved,https://www.aclanthology.org/2022.findings-acl.61.pdf
1055,247188085,Read before Generate! Faithful Long Form Question Answering with Machine Reading,conclusion,Insight-tree,"experiments on two lfqa datasets, eli5 and ms marco, demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics.",Solved,https://www.aclanthology.org/2022.findings-acl.61.pdf
1056,247188085,Read before Generate! Faithful Long Form Question Answering with Machine Reading,conclusion,Insight-tree,"the detailed analysis further proves the competency of our method in generating fluent, relevant, and more faithful answers.",Solved,https://www.aclanthology.org/2022.findings-acl.61.pdf
1057,211126663,Transformers as Soft Reasoners over Language,conclusion,Insight-tree,"the result, at least within our experiments, appears to be both nat-ural and robust, in a way distinct from working with the original formalization.",Solved,https://arxiv.org/pdf/2002.05867v2.pdf
1058,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,"this paper presented a unified web-augmented framework for a wide range of knowledge-intensive tasks, called uniweb.",Solved,https://export.arxiv.org/pdf/2305.10998v2.pdf
1059,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,we convert 16 tasks into a text-to-text generation task for training.,Solved,https://export.arxiv.org/pdf/2305.10998v2.pdf
1060,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,we propose a search engine assisted learning method to selectively retrieve documents from the web through google search.,Solved,https://export.arxiv.org/pdf/2305.10998v2.pdf
1061,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,"furthermore, to reduce the discrepancy between the encoded and retrieved knowledge, we design a pre-training task, i.e., continual knowledge learning, to integrate the retrieved knowledge into llms.",Solved,https://export.arxiv.org/pdf/2305.10998v2.pdf
1062,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,experiments on 16 tasks show the effectiveness of our web-augmented model compared to previous retrieval-augmented models.,Solved,https://export.arxiv.org/pdf/2305.10998v2.pdf
1063,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"in our model, we used a commercial search engine as the retriever to work with the whole web as a knowledge source.",Solved,https://export.arxiv.org/pdf/2305.10998v2.pdf
1064,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"to mitigate the huge disparity between dataset sizes, we follow  to use the temperature-scaled mixing strategy with a rate of t = 2 for setting the proportion of data coming from each task.",Solved,https://export.arxiv.org/pdf/2305.10998v2.pdf
1065,247158312,Combining Modular Skills in Multitask Learning,conclusions,Insight-tree,"the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters.",Neutral,https://arxiv.org/pdf/2202.13914v2.pdf
1066,221971009,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,conclusion,Insight-tree,"also, current method only uses a bag-ofword features for the query.",Neutral,https://www.aclweb.org/anthology/2021.naacl-main.47.pdf
1067,221971009,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,conclusion,Insight-tree,we expect further gain by incorporating word-order information.,Neutral,https://www.aclweb.org/anthology/2021.naacl-main.47.pdf
1068,202773198,Answering Complex Open-domain Questions Through Iterative Query Generation,conclusion,Insight-tree,we start from the wikipedia dump file containing the introductory paragraphs used in hotpotqa that yang et al.,Neutral,https://www.aclweb.org/anthology/D19-1261.pdf
1069,202773198,Answering Complex Open-domain Questions Through Iterative Query Generation,conclusion,Insight-tree,"(2018) provide, 10 and add the fields corresponding to wikipedia page titles and the introductory paragraphs (text) into the index.",Neutral,https://www.aclweb.org/anthology/D19-1261.pdf
1070,221655732,Multi-Hop Fact Checking of Political Claims,conclusions,Insight-tree,"in the first setting, the models are trained for 4 epochs on liar-plus.",Neutral,https://arxiv.org/pdf/2009.06401v3.pdf
1071,221655732,Multi-Hop Fact Checking of Political Claims,conclusions,Insight-tree,"in the second setting, the models are trained for 8 epochs on politihop.",Neutral,https://arxiv.org/pdf/2009.06401v3.pdf
1072,221655732,Multi-Hop Fact Checking of Political Claims,conclusions,Insight-tree,"in the third setting, models are trained for 4 epochs on liar-plus, followed by 4 epochs on politihop.",Neutral,https://arxiv.org/pdf/2009.06401v3.pdf
1073,221655732,Multi-Hop Fact Checking of Political Claims,conclusions,Insight-tree,"in every setting, models are evaluated on the dev set and the model with the best label prediction macro-f1 score is saved, which enables early stopping.",Neutral,https://arxiv.org/pdf/2009.06401v3.pdf
1074,221655732,Multi-Hop Fact Checking of Political Claims,conclusions,Insight-tree,"for the fourth setting, we pre-train the model for 2 epochs on the fever dataset, followed by 4 epochs on liar-plus, the fine-tune on politihop for 4 epochs.",Neutral,https://arxiv.org/pdf/2009.06401v3.pdf
1075,237490850,"Extract, Integrate, Compete: Towards Verification Style Reading Comprehension",conclusion,Insight-tree,we believe vgaokao is a challenging test-bed for natural language understanding in chinese and encourage further research in verification style reading comprehensionn.,Neutral,https://arxiv.org/pdf/2109.05149v1.pdf
1076,211572557,TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing,conclusion and future work,Insight-tree,we have conducted a series of experiments.,Neutral,https://www.aclweb.org/anthology/2020.acl-demos.2.pdf
1077,254877499,Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,conclusions,Insight-tree,one-step question based retrieval is insufficient for such tasks as what information is needed for later steps is not evident from the question alone.,Neutral,https://export.arxiv.org/pdf/2212.10509v1.pdf
1078,254877499,Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,conclusions,Insight-tree,"therefore, for the experiments in the main paper, we go with this choice.",Neutral,https://export.arxiv.org/pdf/2212.10509v1.pdf
1079,254877499,Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,conclusions,Insight-tree,note though that the trends discussed in  5 (ircot qa > oner qa > zeror qa) hold regardless of the choice of the reader.,Neutral,https://export.arxiv.org/pdf/2212.10509v1.pdf
1080,245218995,Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants,discussion and conclusion,Insight-tree,"in the standard data collection setting, and under the assumption of no access to adversarially-collected data, gaas with prompts sampled based on likelihood provide annotation speed-ups, while prompts sampled by adversarial performance or uncertainty metrics provide benefits to both the model error rates on the collected data as well as subsequent downstream qa performance.",Neutral,https://www.aclanthology.org/2022.naacl-main.275.pdf
1081,244954717,Does Structure Matter? Leveraging Data-to-Text Generation for Answering Complex Information Needs,conclusion,Insight-tree,"traditionally, ir approaches solving complex information needs focused on leveraging multi-turn interactions to provide optimal rankings of candidate documents at each turn.",Neutral,https://arxiv.org/pdf/2112.04344v1.pdf
1082,253370208,Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts,conclusions,Insight-tree,"despite large lms' impressive reading abilities and the availability of large scale multi-step qa datasets requiring a rich set of reasoning skills, lmbased qa models do not reliably learn to use such skills for answering complex questions.",Neutral,https://www.aclanthology.org/2022.emnlp-main.439.pdf
1083,254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,conclusion,Insight-tree,"in this regard, after presenting the multi-hop mrc problem definition, the multi-hop mrc techniques had been explained based on 31 studies from 2018 to 2022.",Neutral,https://export.arxiv.org/pdf/2212.04072v1.pdf
1084,254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,conclusion,Insight-tree,"in addition to categorize the approaches based on the main technique, they also were reviewed in detail including the architecture, superiority, and motivations.",Neutral,https://export.arxiv.org/pdf/2212.04072v1.pdf
1085,254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,conclusion,Insight-tree,"in the following, a fine-grain comprehension of the approaches and techniques was prepared, and finally, some open issues in this field were discussed.",Neutral,https://export.arxiv.org/pdf/2212.04072v1.pdf
1086,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,exponentially growing perturbations do not exist in a homogeneous inviscid keplerian flow provided that there are no conditions for the magneto-rotational instability.,Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1087,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,"nevertheless, observations suggest that also in this case angular momentum should be somehow transported outwards.",Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1088,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,"at least, this implies that there should be some mechanism of energy transfer from the regular rotational motion to hydrodynamical perturbations.",Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1089,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,in spectrally stable flows the transient growth mechanism is responsible for this.,Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1090,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,here it was introduced by a simple example of two-dimensional vortices and it was discussed that the reason for their growth is the shortening of the length of leading spirals by the differential rotation of the flow,Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1091,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,"physically, the energy growth of vortices takes place due to their own angular momentum conservation, which in the local limit is expressed by the conservation of their potential vorticity and the existence of the invariant i (see section 2.2).",Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1092,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,here we considered both small-scale (k y  1) and large-scale (k y  1) vortices and compared their optimal growth with account for non-zero effective viscosity in the disk,Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1093,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,"we have discussed in detail that the transient growth is a consequence of non-normality of the governing dynamical operator of the problem and non-orthogonality of its eigenvectors, i.e. modes of perturbations",Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1094,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,"the matrix method requires a discrete representation of the dynamical operator, for example, in the basis of its eigenvectors.",Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1095,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,"the variational method is reduced to iterative integration of the system of direct and adjoint equations forward and backward in time, respectively.",Neutral,https://arxiv.org/pdf/1512.08897v1.pdf
1096,57721315,Multi-Style Generative Reading Comprehension,conclusion,Insight-tree,"in particular, the capability of copying words from the question and passages can be shared among the styles, while the capability of controlling the mixture weights for the generative and copy distributions can be acquired for each style.",Neutral,https://arxiv.org/pdf/1901.02262v2.pdf
1097,235399966,FINQA: A Dataset of Numerical Reasoning over Financial Data,conclusion and future work,Insight-tree,we believe finqa should serve as a valuable resource for the research community.,Neutral,https://www.aclanthology.org/2021.emnlp-main.300.pdf
1098,201058633,Reasoning Over Paragraph Effects in Situations,conclusion,Insight-tree,we hope that ropes will aide efforts in tying language and reasoning together for more comprehensive understanding of text.,Neutral,https://www.aclweb.org/anthology/D19-5808.pdf
1099,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,conclusion,Insight-tree,"the choice of the training dataset and loss functions are guided by the goal to include more reasoning-related sentences and training signals, respectively.",Neutral,https://export.arxiv.org/pdf/2212.09282v2.pdf
1100,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,conclusion,Insight-tree,"through experiments on two logical reasoning datasets and ablation studies, we demonstrate the effectiveness of our proposed approach.",Neutral,https://export.arxiv.org/pdf/2212.09282v2.pdf
1101,245334850,MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding,conclusions and future work,Insight-tree,"the task is challenging in the requirement of cross-media grounding over images, captions, and news body text.",Neutral,https://arxiv.org/pdf/2112.10728v2.pdf
1102,254853987,Source-Free Domain Adaptation for Question Answering with Masked Self-training,limitation,Insight-tree,"since the use of source-free uda is mostly discussed in the medical field at the moment, ideally, more experiments on medical datasets would be more convincing.",Neutral,https://export.arxiv.org/pdf/2212.09563v1.pdf
1103,256846551,STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK,conclusion,Insight-tree,we aim to enable machines to perform multi-step reasoning while explaining their answers.,Neutral,https://export.arxiv.org/pdf/2302.06729v1.pdf
1104,256846551,STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK,conclusion,Insight-tree,we believe that teaching machines how to manipulate premises and reach conclusions can be an important step towards true language understanding.,Neutral,https://export.arxiv.org/pdf/2302.06729v1.pdf
1105,211258652,Training Question Answering Models From Synthetic Data,conclusion,Insight-tree,"doing so required us to scale model size for our answer generators, question generators, and filtration models.",Neutral,https://arxiv.org/pdf/2002.09599v1.pdf
1106,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,conclusion,Insight-tree,"as a result, models lie closer to the y = x diagonal than on other distribution shifts.",Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1107,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,experimenting with different in-distribution datasets.,Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1108,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,"as an example andor et al. (2019) improved performance on drop (dua et al., 2019) by using arithmetic programs to improve a model's mathematical reasoning.",Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1109,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,for fine-tuned models we include a single data-point for each model.,Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1110,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,"however, previous work (phang et al., 2018;dodge et al., 2020) has shown that different data ordering and weight initialization can lead to large variance in model performance.",Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1111,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,"as at most other universities, notre dame's students run a number of news media outlets.",Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1112,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,"the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals.",Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1113,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,question: how many student news papers are found at notre dame?,Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1114,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,"answer:  a sample from squad with the input formatting used for fine-tuning decoder-only models, incontext learning, and zero-shot inference.",Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1115,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,"a sample from squad with the input formatting used for fine-tuning decoder-only models, incontext learning, and zero-shot inference.",Neutral,https://export.arxiv.org/pdf/2210.12517v1.pdf
1116,247475874,E-KAR : A Benchmark for Rationalizing Natural Language Analogical Reasoning,conclusion and discussion,Insight-tree,preliminary experiments show that this benchmark provides a rather difficult challenge for prevailing language models.,Neutral,https://www.aclanthology.org/2022.findings-acl.311.pdf
1117,255186555,DEMONSTRATE-SEARCH-PREDICT: Composing retrieval and language models for knowledge-intensive NLP,conclusion,Insight-tree,"for a long time, the dominant paradigm for building models in ai has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration.",Neutral,https://export.arxiv.org/pdf/2212.14024v2.pdf
1118,255186555,DEMONSTRATE-SEARCH-PREDICT: Composing retrieval and language models for knowledge-intensive NLP,conclusion,Insight-tree,"however, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems.",Neutral,https://export.arxiv.org/pdf/2212.14024v2.pdf
1119,5106665,A Model for Processing Temporal References in Chinese,conclusions and future work,Insight-tree,the mapping mechanism was validated.,Neutral,
1120,247518809,Ask to Understand: Question Generation for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, inspired by human cognitive behavior, we believe that asking questions is an important indication to testify whether the model truly understands the input text.",Neutral,https://arxiv.org/pdf/2203.09073v1.pdf
1121,258587884,Automatic Evaluation of Attribution by Large Language Models,conclusion,Insight-tree,the insights gained from this study and the provided testbed and modeling methodology can inspire future studies on this important problem.,Neutral,https://export.arxiv.org/pdf/2305.06311v1.pdf
1122,258587884,Automatic Evaluation of Attribution by Large Language Models,limitations,Insight-tree,the current version of attributionscore is primarily trained on a simulated or repurposed error dataset from similar tasks.,Neutral,https://export.arxiv.org/pdf/2305.06311v1.pdf
1123,258587884,Automatic Evaluation of Attribution by Large Language Models,limitations,Insight-tree,"however, this dataset still has gaps compared to the real attribution scenario.",Neutral,https://export.arxiv.org/pdf/2305.06311v1.pdf
1124,258587884,Automatic Evaluation of Attribution by Large Language Models,limitations,Insight-tree,these additional cases will be used to further fine-tune and evaluate attributionscore.,Neutral,https://export.arxiv.org/pdf/2305.06311v1.pdf
1125,246638887,E M E R G I N G T R E N D S Emerging Trends: SOTA-Chasing,conclusions/recommendations,Insight-tree,sota-chasing comes with many costs.,Neutral,
1126,246638887,E M E R G I N G T R E N D S Emerging Trends: SOTA-Chasing,conclusions/recommendations,Insight-tree,we discussed three costs:,Neutral,
1127,258959022,Plug-and-Play Document Modules for Pre-trained Models 5 Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,"in this setting, we can get rid of encoding the same document multiple times for different tasks.",Neutral,https://export.arxiv.org/pdf/2305.17660v1.pdf
1128,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,conclusion,Insight-tree,we take a step forward in constructing the explainable method for multi-hop question answering by proposing two effective improvements.,Neutral,https://www.aclanthology.org/2022.emnlp-main.356.pdf
1129,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,conclusion,Insight-tree,the global differentiable learning strategy learns optimal reasoning paths by exploring latent probability space to alleviate the problem of semantic space mismatch and error propagation.,Neutral,https://www.aclanthology.org/2022.emnlp-main.356.pdf
1130,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,limitations,Insight-tree,question decomposition is the pre-stage of building interpretable models.,Neutral,https://www.aclanthology.org/2022.emnlp-main.356.pdf
1131,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,limitations,Insight-tree,"to the best of our knowledge, there is only one largescale question decomposition dataset (wolfson et al., 2020), and the performance of existing automatic decomposition models is far below human performance.",Neutral,https://www.aclanthology.org/2022.emnlp-main.356.pdf
1132,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,limitations,Insight-tree,inaccurate question decomposition leads to errors in reasoning.,Neutral,https://www.aclanthology.org/2022.emnlp-main.356.pdf
1133,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,limitations,Insight-tree,"therefore, exploring better question decomposition techniques is a challenging and rewarding direction. existing interpretable models (min et al., 2019;jiang and bansal, 2019;ding et al., 2019;khot et al., 2021;wolfson et al., 2020), including our approach, focus on solving complex questions, ignoring a simple question with a complex context that requires a deep understanding of the context to reason out the answer. the dynamic adaptive reasoner introduces a small number of additional parameters in the router, which can increase the computational cost.",Neutral,https://www.aclanthology.org/2022.emnlp-main.356.pdf
1134,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"end-to-end models are able to produce a single generation per example reducing the time complexity for sufficiently small sets of premises.step-by-step models like our search procedure in this work are capable of handling sets of any size of premises for the search, but do increase the execution time per example, especially when using validators that require doing generation themselves.",Neutral,https://www.aclanthology.org/2022.emnlp-main.564.pdf
1135,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"nevertheless, validators do reduce the total time required for running a set of examples due to their ability of pruning the search space and thus removing numerous heuristic and generation calls.",Neutral,https://www.aclanthology.org/2022.emnlp-main.564.pdf
1136,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"however, we do not follow in the steps of delphi (jiang et al., 2021) in making any claims about its ability to make systems ethical or say anything about ""values"" encoded in pre-trained models.",Neutral,https://www.aclanthology.org/2022.emnlp-main.564.pdf
1137,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"we do not support its use as part of any user-facing system at this time.step signaturestep typex, x  y d deductive x, y d  y d deductive g, x  ya abductive g, y d  ya abductive ya, x  ya abductive ya, y d  ya abductive table 6: a list of possible input statement types each step model can take.",Neutral,https://www.aclanthology.org/2022.emnlp-main.564.pdf
1138,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"x refers to a premise, y d refers to an intermediate deductive conclusion, g refers to the goal, and y a refers to an abductive hypothesis.",Neutral,https://www.aclanthology.org/2022.emnlp-main.564.pdf
1139,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"note that the deductive model can accept inputs in any order but the abductive model cannot, as the abduction operation is not commutative.",Neutral,https://www.aclanthology.org/2022.emnlp-main.564.pdf
1140,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"also note that deductive outputs can be used as inputs to abductive steps, but not the other way around; allowing deductive steps to accept abductive generations could result in vacuous proofs.",Neutral,https://www.aclanthology.org/2022.emnlp-main.564.pdf
1141,222272049,Case Study: Deontological Ethics in NLP,conclusion,Insight-tree,"through the four case studies discussed in this paper, we demonstrate how these principles can be used to evaluate the decisions made by nlp systems and to identify the missing aspects.",Neutral,https://www.aclweb.org/anthology/2021.naacl-main.297.pdf
1142,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"hewitt and liang [66] considered how to ensure that a probe is truly reflective of the underlying information present in a model, and proposed the use of a control task, a randomised version of a probe task in which high performance is only possible by memorisation of inputs.",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1143,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"hence, a faithful probe should perform well on a probe task and poorly on a corresponding control task if the underlying model does indeed contain the information being probed for.",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1144,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"the authors found that most probes (including linear classifiers)are over-parameterised, and discuss methods for constraining complex probes (e.g. multilayer perceptrons) to improve faithfulness while still allowing them to achieve similar results.while most papers we have discussed above follow the intuition that probes should avoid complex probes to prevent memorisation",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1145,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"pimentel et al. [127] suggest that instead the probe with the best score on a given task should be chosen as the tightest estimate, since simpler models may simply be unable to extract the linguistic information present in a model, and such linguistic information cannot be 'added' by more complex probes (since their only input are hidden representations).",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1146,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"recent work has also presented methods that avoid making assumptions about probe complexity, such as mdl probing [101,170], which directly measures 'amount of effort' needed to achieve some extraction task, or directprobe [198], which directly examines intermediate representations of models to avoid having to deal with additional classifiers",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1147,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"finally, hall maudslay et al.[59] compared the structural probe",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1148,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"[67] with a lightweight dependency parser (both given the same inputs), and demonstrate that the parser is generally able to extract more syntactic information from bert embedding.",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1149,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"in contrast, the probe performs better with a different metric, showing that the choice of metric is important for probes: when testing for evidence of linguistic information, one should not only consider the nature of the probe, but also the metric used to evaluate it.",Neutral,https://export.arxiv.org/pdf/2103.11072v2.pdf
1150,258378176,Combining Parameter-efficient Modules for Task-level Generalisation,conclusions,Insight-tree,we argued that a modular design is crucial to ensure that neural networks can learn from a few examples and generalise robustly across tasks by recombining autonomous facets of knowledge.,Neutral,https://www.aclanthology.org/2023.eacl-main.49.pdf
1151,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"in searchain, when llm faces complex questions, it constructs a chain called chain-of-query (coq), each node of the coq is an ir-oriented query and llm-generated answer.",Neutral,https://export.arxiv.org/pdf/2304.14732v1.pdf
1152,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"ir interacts with each node on the chain, judges whether the answer is correct, and provides llm with its unknown knowledge.",Neutral,https://export.arxiv.org/pdf/2304.14732v1.pdf
1153,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,llm generates a new coq according to the feedback of ir.,Neutral,https://export.arxiv.org/pdf/2304.14732v1.pdf
1154,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"ir interacts with llm for multiple rounds, gradually helping llm generate the correct coq, and finally solve the complex question.",Neutral,https://export.arxiv.org/pdf/2304.14732v1.pdf
1155,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"besides, the contents returned to the user include not only the final answer but also the reasoning process for the question, that is, the coq and the supporting documents retrieved by ir for each node of the coq, which improves the credibility and traceability of the contents generated by llm.",Neutral,https://export.arxiv.org/pdf/2304.14732v1.pdf
1156,236477844,PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval,conclusion and future work,Insight-tree,we believe such an idea itself is worth exploring in designing new ranking mechanism.,Neutral,https://export.arxiv.org/pdf/2108.06027v2.pdf
1157,225067214,READONCE Transformers: Reusable Representations of Text for Transformers,conclusion,Insight-tree,"akin to humans' ability to read a document and extract useful information without knowing the enduse, readonce representations are compact, information-capturing document representations that can be pre-computed once, in a task-and example-independent fashion.",Neutral,https://www.aclanthology.org/2021.acl-long.554.pdf
1158,245219136,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,conclusion,Insight-tree,"augmenting pre-trained generation models with retrievers has shown to be effective in many knowledge-intensive tasks; however, they often rely on spurious cues or generate hallucinations during inference.",Neutral,https://www.aclanthology.org/2022.naacl-main.162.pdf
1159,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,"in a large-scale 24-task nlp experiment, ca-mtl outperforms fully tuned single task models by 2.3% for bert large and by 1.2% for roberta large using 1.12 times the number of parameters, while single task fine-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters.",Neutral,https://arxiv.org/pdf/2009.09139v3.pdf
1160,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,"when a bert vanilla mtl model sees its performance drop as the number of tasks increases, ca-mtl scores continue to climb.",Neutral,https://arxiv.org/pdf/2009.09139v3.pdf
1161,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,performance gains are not driven by a single task as it is often the case in mtl.,Neutral,https://arxiv.org/pdf/2009.09139v3.pdf
1162,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,"tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting.",Neutral,https://arxiv.org/pdf/2009.09139v3.pdf
1163,252368291,Learning to Answer Semantic Queries over Code,conclusions and future work,Insight-tree,it requires the models to perform single-or multi-hop reasoning.,Neutral,https://export.arxiv.org/pdf/2209.08372v1.pdf
1164,258053774,Cancer-Specific Survival after Limb Salvage versus Amputation in Children and Adolescents with Osteosarcoma: A Population-Based Analysis with Propensity Score Matching,conclusion,Insight-tree,"while new chemotherapy regimens will be required to increase survivorship in the setting of os, patients with tumor features suitable to lss had a much higher survival rate than those suitable for amputation.",Neutral,
1165,236771976,MuSiQue: Multihop Questions via Single-hop Question Composition,conclusion,Insight-tree,constructing multihop datasets is a tricky process. it can introduce shortcuts and artifacts that models can exploit to circumvent the need for multihop reasoning.,Neutral,https://www.aclanthology.org/2022.tacl-1.31.pdf
1166,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,"each step in these rationales is treated as a single ""task"" within a mixture of similar ""tasks"".",Neutral,https://export.arxiv.org/pdf/2211.08466v1.pdf
1167,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,"the individual tasks can be categorized into control tasks, which control the flow of information between tasks, and generative tasks, that generate free-form text for successive tasks in the reasoning circuit.",Neutral,https://export.arxiv.org/pdf/2211.08466v1.pdf
1168,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,conclusion,Insight-tree,"the framework is relatively easy to implement, since only a single generative model is fine-tuned with a mixture of all reasoning steps; at inference time, the same model can generate all reasoning steps sequentially.",Neutral,https://export.arxiv.org/pdf/2211.08466v1.pdf
1169,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,limitations,Insight-tree,the proposed reasoning circuits framework intends to replace the need for thousands of annotated examples with a strong inductive bias of structured rationales.,Neutral,https://export.arxiv.org/pdf/2211.08466v1.pdf
1170,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,limitations,Insight-tree,"there is two issues with this approach at a conceptual level: 1. it may not always be possible to break down a multi-step reasoning problem cleanly into discrete reasoning steps, and another related issue it increasing complexity of the circuit with the complexity of the task.",Neutral,https://export.arxiv.org/pdf/2211.08466v1.pdf
1171,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,limitations,Insight-tree,"2. for the design of these reasoning circuits a researcher must develop a thorough understanding of this reasoning task, so that the final circuit design broadly covers all possible types of reasoning problems expected to be solved.",Neutral,https://export.arxiv.org/pdf/2211.08466v1.pdf
1172,253553165,Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,limitations,Insight-tree,"an under-or illdesigned reasoning circuit may cause the system to either not support a certain portion of problems or produce non-sensical outputs.essentially, there is trade off between a tighter control over reasoning by investing in a deep understanding of the problem leading to a comprehensive reasoning circuit design and lower annotations budget, versus, less control over logic and depending on a large number of annotations which allow the model to discover this logic on its own at much higher cost of large scale annotations budget.",Neutral,https://export.arxiv.org/pdf/2211.08466v1.pdf
1173,243986045,Recent Advances in Automated Question Answering In Biomedical Domain,conclusion,Insight-tree,"in this review, we focused on the recent advances in biomedical question answering.",Neutral,https://arxiv.org/pdf/2111.05937v1.pdf
1174,199668753,Towards Knowledge-Based Recommender Dialog System,conclusion,Insight-tree,"dialog information is effective for the recommender system especially in the setting of cold start, and the introduction of knowledge can strengthen the recommendation performance significantly.",Neutral,https://www.aclweb.org/anthology/D19-1189.pdf
1175,199668753,Towards Knowledge-Based Recommender Dialog System,conclusion,Insight-tree,information from the recommender system that contains the user preference and the relevant knowledge can enhance the consistency and diversity of the generated dialogs.,Neutral,https://www.aclweb.org/anthology/D19-1189.pdf
1176,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"for example, flan v2 is similar to mmlu, but dissimilar to chatbot benchmarks and vice versa for the chip2 dataset and both models score accordingly on the mmlu and vicuna benchmarks.",Neutral,https://export.arxiv.org/pdf/2305.14314v1.pdf
1177,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction.",Neutral,https://export.arxiv.org/pdf/2305.14314v1.pdf
1178,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,we evaluate the likelihood of guanaco-65b to generate a socially biased sequence of tokens compared to other models in table 8.,Neutral,https://export.arxiv.org/pdf/2305.14314v1.pdf
1179,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"as such, it seems that finetuning on the oasst1 dataset reduces the bias of the llama base model.",Neutral,https://export.arxiv.org/pdf/2305.14314v1.pdf
1180,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,nan,Neutral,https://export.arxiv.org/pdf/2305.14314v1.pdf
1181,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,since finetuning after quantization seems to recover most of the information that is lost during quantization this might enable much more aggressive quantization.,Neutral,https://export.arxiv.org/pdf/2305.14314v1.pdf
1182,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"for example, 3-bit gptq quantization of the basemodel with lora might also yield 16-bit full finetuning performance after finetuning.",Neutral,https://export.arxiv.org/pdf/2305.14314v1.pdf
1183,202539540,QUARTZ: An Open-Domain Dataset of Qualitative Relationship Questions,conclusion,Insight-tree,"understanding and applying textual qualitative knowledge is an important skill for questionanswering, but has received limited attention, in part due the lack of a broad-coverage dataset to study the task.",Neutral,https://www.aclweb.org/anthology/D19-1608.pdf
1184,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,"large general-purpose language models, such as gpt family of models [brown et al.2020;openai 2023] , lamda [thoppilan et al. 2022], palm [anil et al. 2023;chowdhery et al. 2022], and others, have propagated into informationseeking workflows of a general audience.",Neutral,https://export.arxiv.org/pdf/2305.14331v1.pdf
1185,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,"a vast host of existing and ongoing work in nlp examine the deficiencies of these language models, ranging from hallucinated generations  with background with background and highlights.",Neutral,https://export.arxiv.org/pdf/2305.14331v1.pdf
1186,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,"users' subjective rating of the system for the usefulness of highlights, background, their confidence in the ai system, selfconfidence, and satisfaction with ai.",Neutral,https://export.arxiv.org/pdf/2305.14331v1.pdf
1187,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,users rate self-confidence marginally higher in the condition with background than the condition without (left).,Neutral,https://export.arxiv.org/pdf/2305.14331v1.pdf
1188,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,"however, users rate their satisfaction with ai in the condition with background slightly lower than without.",Neutral,https://export.arxiv.org/pdf/2305.14331v1.pdf
1189,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,"users' satisfaction rating is slightly lower even after introducing highlights with the background (right), with a slightly higher rating of background utility in the condition with highlights than without.",Neutral,https://export.arxiv.org/pdf/2305.14331v1.pdf
1190,256846917,"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",conclusion,Insight-tree,this work extensively covers research efforts on backdoor learning for nlp.,Neutral,https://export.arxiv.org/pdf/2302.06801v1.pdf
1191,256846917,"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",conclusion,Insight-tree,"we argue that for backdoor learning to contribute to actual robustness, research studies should take into account an expansive view and strive to answer questions related to why such attacks and defenses are successful.",Neutral,https://export.arxiv.org/pdf/2302.06801v1.pdf
1192,256846917,"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",conclusion,Insight-tree,it is crucial to determine whether any given technique is booming due to limitations and weaknesses associated with the target model (inherent incapability arising from intrinsic properties of a target model) or whether it is due to weaknesses or limitations in the dataset itself.,Neutral,https://export.arxiv.org/pdf/2302.06801v1.pdf
1193,256846917,"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",conclusion,Insight-tree,"finally, we offer insights into open challenges and future research directions worth pursuing.  ",Neutral,https://export.arxiv.org/pdf/2302.06801v1.pdf
1194,165163607,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,conclusion,Insight-tree,likewise we compute p * from the premise.,Neutral,https://www.aclweb.org/anthology/N19-1300.pdf
1195,233296243,Learning with Instance Bundles for Reading Comprehension,conclusion,Insight-tree,the code is available at https://github.com/ddua/ contrastive-estimation.,Neutral,https://www.aclanthology.org/2021.emnlp-main.584.pdf
1196,254366618,Text Embeddings by Weakly-Supervised Contrastive Pre-training,conclusion,Insight-tree,"e5 offers strong off-the-shelf performance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching.",Neutral,https://export.arxiv.org/pdf/2212.03533v1.pdf
1197,254366618,Text Embeddings by Weakly-Supervised Contrastive Pre-training,conclusion,Insight-tree,"the ""others"" category includes ""sim-plewiki"", ""gooaq"", ""wikihow"", ""yahoo answers"" from https://huggingface.co/datasets/ sentence-transformers/embedding-training-data.",Neutral,https://export.arxiv.org/pdf/2212.03533v1.pdf
1198,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,conclusion,Insight-tree,"on an extensive set of language understanding tasks -superglue, qa, nli, ner, and relation classification, vip outperforms pt baseline.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1199,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"in this section, we point out the limitations of vip and its potential future directions. pretraining prompt contextualizer.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1200,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,the sentence encoder in vip is trained from scratch for each downstream task.,Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1201,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"vip framework demands a larger parameter size than the baseline soft prompt tuning, owing mainly to the codebook.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1202,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"other than the standard hyperparameters of the sentence encoder, the quantizer introduces new hyperparameters -codebook-size, multinomial sample size, and the temperature constant  to scale logits.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1203,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,learning the codebook requires an ema style updating scheme instead of the standard gradient update.,Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1204,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"with the plm being frozen, this needs more careful handlingfor e.g. a critical hyperparameter is the value of the temperature constant  .",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1205,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,a very high value can lead to representation collapse of the codebook while very low values can lead to sparse codebook usage.,Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1206,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"however, as discussed above,  is independent of the task and depends on the initial norm of codebook vectors.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1207,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"we posit that due to the larger parameter size of vip, it performs worse than pt in tasks with lesser training data, e.g. scores on the cb dataset in table 1.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1208,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,this is due to the larger parameter size of vip.,Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1209,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"due to resource limitations, in all our experiments we use t5base as the backbone.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1210,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"following lester et al. (2021) where larger plms are shown to improve prompt-tuning performance, we speculate vip to showcase a similar effect.",Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1211,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,it is likely that additional biases are also embedded within the t5-base plm that was used as the backbone of vip.,Neutral,https://www.aclanthology.org/2022.emnlp-main.455.pdf
1212,238744204,Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?,conclusion,Insight-tree,"this is achieved by training a dense lexical model  to imitate the behavior of the teacher sparse retriever, the feasibility of which remained unknown until this work.",Neutral,https://export.arxiv.org/pdf/2110.06918v3.pdf
1213,238744204,Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?,conclusion,Insight-tree,"this way, we can avoid imitating the errors of the sparse retriever, and devise new ways of training dense retrievers that can potentially surpass hybrid models.",Neutral,https://export.arxiv.org/pdf/2110.06918v3.pdf
1214,252780709,Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering,conclusion,Insight-tree,further directions include better approximations of multi-hop questions and advanced modeling techniques that encourage compositional ability.,Neutral,https://www.aclanthology.org/2022.coling-1.152.pdf
1215,249395505,On the Advance of Making Language Models Better Reasoners,conclusion and future work,Insight-tree,"for example, with codedavinci-002, diverse achieves new state-of-theart results in most of the reasoning tasks, outperforming the 540b palm model combined with previous prompting approaches.",Neutral,https://arxiv.org/pdf/2206.02336v2.pdf
1216,252873674,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,discussion & conclusion,Insight-tree,"caveats and risks of explanations from large language models our analysis suggests that llms' internal ""reasoning"" does not always align with explanations that it generates, as shown by our consistency results.",Neutral,https://export.arxiv.org/pdf/2205.03401v2.pdf
1217,252873674,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,discussion & conclusion,Insight-tree,section 6 of bender et al. (2021) discusses these risks in additional detail.,Neutral,https://export.arxiv.org/pdf/2205.03401v2.pdf
1218,252873674,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,discussion & conclusion,Insight-tree,"the fact that language models can hallucinate explanations is also found in other work (zhou and tan, 2021).",Neutral,https://export.arxiv.org/pdf/2205.03401v2.pdf
1219,252873674,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,discussion & conclusion,Insight-tree,"this result is unsurprising in some sense: without sufficient supervision or grounding, language models do not learn meaning as distinct from form (bender and koller, 2020), so we should not expect their explanations to be strongly grounded.",Neutral,https://export.arxiv.org/pdf/2205.03401v2.pdf
1220,255440689,InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval,conclusion,Insight-tree,the synthetic data and finetuned models were publicly released.,Neutral,https://export.arxiv.org/pdf/2301.01820v4.pdf
1221,248426967,HYBRIDIALOGUE: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data,conclusion,Insight-tree,"while previous work has combined table and text modality in the question-answering space, this has not been utilized in the dialogue setting.",Neutral,https://www.aclanthology.org/2022.findings-acl.41.pdf
1222,233289699,What to Pre-Train on? Efficient Intermediate Task Selection,conclusion,Insight-tree,"in this work we have established that intermediate pre-training can yield gains in adapter-based setups, however, around 44% of all transfer combinations result in decreased performances.",Neutral,https://www.aclanthology.org/2021.emnlp-main.827.pdf
1223,207852656,Meta Answering for Machine Reading,conclusion,Insight-tree,"further, the task brings to the surface, yet again but from a novel perspective, limitations of the current nlu paradigm.",Neutral,https://arxiv.org/pdf/1911.04156v2.pdf
1224,257532206,Semantic matching based legal information retrieval system for COVID-19 pandemic,conclusion,Insight-tree,"the system can identify: (1) the crime cases entered by the user and find the most similar cases to be pushed to the user as answers, and (2) the crime cases documented by the user and give the reference legal gist applicable to the case.",Neutral,
1225,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,this work focuses on testing if cft outperforms end-to-end learning and chain of thought prompting in two very different domains.,Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1226,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,both are second-order research questions that can be pursued once compositionality has been confirmed to improve performance.,Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1227,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"importantly, both questions have been left open in the initial chain of thought work as well.",Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1228,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"for example, a dataset focused on compositional generalization may include many different types of questions, each requiring different types of intermediate steps.",Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1229,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"for world cities, we have a = {temperature, population} where average city temperatures are obtained from wikipedia 4 and city populations from sim-plemaps 2019.",Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1230,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"5 after merging items from both sources, we end with 347 well-known cities (>50k inhabitans) from around the globe, such that d c = (i f ull c , {temperature, population}) and |i f ull c | = 347.",Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1231,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"for local restaurants, we randomly sample 240 restaurants from the city with most restaurants in the yelp dataset 6 , toronto.",Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1232,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"with that, we have d r = (i f ull r , {price, distance}) and |i f ull r | = 240.in terms of component tasks, we have 694 factual statements for the cities domain and 480 for restaurants, covering two attributes per item.",Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1233,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"whenever factual statements are provided in cft, they always cover i f ull entirely in order to give the lm full knowledge of the attribute values.",Neutral,https://export.arxiv.org/pdf/2210.12607v1.pdf
1234,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,nan,Neutral,https://export.arxiv.org/pdf/2211.08380v1.pdf
1235,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,we hypothesize the following reasons: 1) a large portion of our current model's improvement relies on the weakly supervised relation pre-training.,Neutral,https://export.arxiv.org/pdf/2211.08380v1.pdf
1236,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"to do it, we construct a k-hop (k=2 now) subgraph, and sample dependency graph based on it.",Neutral,https://export.arxiv.org/pdf/2211.08380v1.pdf
1237,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"the larger k we choose, the more noise is included into the generated relation label, in an exponential increasing speed.",Neutral,https://export.arxiv.org/pdf/2211.08380v1.pdf
1238,220403560,KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base,conclusions,Insight-tree,"extensive experiments reveal a huge gap between machines and humans, demonstrating that kqa pro is very challenging.",Neutral,https://arxiv.org/pdf/2007.03875v1.pdf
1239,220403560,KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base,conclusions,Insight-tree,we hope that these additional information can help machines develop the compositional reasoning ability and learn to tackle complex questions like a human.,Neutral,https://arxiv.org/pdf/2007.03875v1.pdf
1240,248722227,Exploring Universal Intrinsic Task Subspace via Prompt Tuning,conclusion and discussion,Insight-tree,it first finds a subspace by jointly decomposing the adaptive parameters of multiple tasks and then tunes parameters within the subspace for unseen data and tasks.,Neutral,https://export.arxiv.org/pdf/2110.07867v3.pdf
1241,257365136,MULTITASK PROMPT TUNING ENABLES PARAMETER-EFFICIENT TRANSFER LEARNING,conclusion,Insight-tree,the shared component is then transferred and adapted to target tasks for further tuning.,Neutral,https://export.arxiv.org/pdf/2303.02861v1.pdf
1242,257365136,MULTITASK PROMPT TUNING ENABLES PARAMETER-EFFICIENT TRANSFER LEARNING,conclusion,Insight-tree,"empirically we found this approach enables parameter-efficient transfer learning to target downstream tasks across diverse nlp benchmarks, even outperforming the full finetuning baseline in some cases, despite tuning much fewer task-specific parameters.",Neutral,https://export.arxiv.org/pdf/2303.02861v1.pdf
1243,256662717,Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories,conclusion,Insight-tree,"(october 10, december 10, 1978) was an american filmmaker, actor, writer, producer, and director.",Neutral,https://export.arxiv.org/pdf/2302.03754v1.pdf
1244,256662717,Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories,conclusion,Insight-tree,hard negatives retrieved from the mixture of augmentation corpora.,Neutral,https://export.arxiv.org/pdf/2302.03754v1.pdf
1245,253080775,WIKIWHY: ANSWERING AND EXPLAINING CAUSE-AND-EFFECT QUESTIONS,conclusion,Insight-tree,"finally, we motivate the development of new automatic metrics that are better able to handle the complexities of generated reasoning.",Neutral,https://export.arxiv.org/pdf/2210.12152v2.pdf
1246,227231411,Answer-driven Deep Question Generation based on Reinforcement Learning,conclusion and future work,Insight-tree,deep question generation aims to generate complex questions that require reasoning over multiple pieces of information.,Neutral,https://www.aclweb.org/anthology/2020.coling-main.452.pdf
1247,227231411,Answer-driven Deep Question Generation based on Reinforcement Learning,conclusion and future work,Insight-tree,an answer-aware initialization module with a gated connection layer and a semantic-rich fusion attention mechanism are designed to incorporate document and answer information into the generation process.,Neutral,https://www.aclweb.org/anthology/2020.coling-main.452.pdf
1248,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,conclusion,Insight-tree,"mr. right covers three types of text-based search queries with different modality information, including text-related, image-related, and mixed, to simulate real-world search situations.",Neutral,https://export.arxiv.org/pdf/2209.13764v1.pdf
1249,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,limitations and future work,Insight-tree,"further, mr. right focuses on the materials in wikipedia.",Neutral,https://export.arxiv.org/pdf/2209.13764v1.pdf
1250,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,conclusion,Insight-tree,"supervised dense compression algorithms have been popular and effective in-domain in recent times, however can have difficulties to generalize well to unseen domains.",Neutral,https://arxiv.org/pdf/2205.11498v1.pdf
1251,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,conclusion,Insight-tree,"the algorithms are memory efficient, but lack in performance when evaluated in specialized domains which contain no training data.",Neutral,https://arxiv.org/pdf/2205.11498v1.pdf
1252,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,limitations and future work,Insight-tree,even though we find the gpl technique to provide a boost with memory compressed models: bpr and jpq.,Neutral,https://arxiv.org/pdf/2205.11498v1.pdf
1253,245616876,Using Bloom's Taxonomy to Classify Question Complexity,conclusion and future work,Insight-tree,"the diagonal of the matrix is a determinant for defining complex questions, ranging from simple questions in the upper left to complex questions on the bottom right.",Neutral,https://www.aclanthology.org/2021.icnlsp-1.34.pdf
1254,222141025,PROVER: Proof Generation for Interpretable Reasoning over Rules,conclusion,Insight-tree,"prover's modeling is relatively generic, and similar proof generation methods can be explored in traditional multi-hop qa tasks.",Neutral,https://arxiv.org/pdf/2010.02830v1.pdf
1255,222141025,PROVER: Proof Generation for Interpretable Reasoning over Rules,conclusion,Insight-tree,prover can also be a helpful aid to formal reasoners in scenarios where rules are fuzzy and creating rule-bases in a formal language is tedious or infeasible.,Neutral,https://arxiv.org/pdf/2010.02830v1.pdf
1256,258615731,Active Retrieval Augmented Generation,limitation,Insight-tree,the lm needs to be activated multiple times (once for each retrieval) and a cachingfree implementation will also require recomputing the previous activation each time after a retrieval.,Neutral,https://export.arxiv.org/pdf/2305.06983v1.pdf
1257,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,limitations of reflexion,Insight-tree,reflexion relies on the emergent property of self-reflection that is present in several large language models.,Neutral,https://export.arxiv.org/pdf/2303.11366v1.pdf
1258,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,limitations of reflexion,Insight-tree,"the agent achieved a 33% 34% accuracy improvement in the baseline run and a mere 33% 35% accuracy improvement in the reflexion run, which suggests that the agent only successfully completed 1 additional task relative to the baseline agent's performance.however, after analyzing the failed trajectories, we noted that the chance of a successful item purchase for an agent in a webshop environment was not necessarily dependent on the agent's ability to plan and execute a correct sequence of actions, but rather on the quality of the webshop search engine's results.",Neutral,https://export.arxiv.org/pdf/2303.11366v1.pdf
1259,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,conclusion,Insight-tree,reflexion is a highly applicable method to improve performance between trials on decision-making and knowledge-intensive tasks due to its sole dependence on a binary reward model.,Neutral,https://export.arxiv.org/pdf/2303.11366v1.pdf
1260,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,conclusion,Insight-tree,"in the alfworld and hotpotqa experiments, we constrained the reward model to imitate environments in which informative reward models may be difficult to design or compute.",Neutral,https://export.arxiv.org/pdf/2303.11366v1.pdf
1261,258436694,Explain-able Conversational Question Answering over Heterogeneous Sources via Iterative Graph Neural Networks,conclusion,Insight-tree,"finally, for several systems in ir and nlp, performance, efficiency, and explainability are seen as trade-offs.",Neutral,https://export.arxiv.org/pdf/2305.01548v1.pdf
1262,248780469,MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,limitations and future work,Insight-tree,"for example, a pre-trained model with specialized table structure-aware mechanisms cheng et al., 2021a;yang et al., 2022) can be utilized in the facts retrieving module to better understand hierarchical tables; and 4) leveraging structured knowledge (xie et al., 2022) to inject external financial knowledge to models.",Neutral,https://www.aclanthology.org/2022.acl-long.454.pdf
1263,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,"while it is ideal to explore more natural theories, it makes the systematic logical perturbation process very challenging.",Neutral,https://export.arxiv.org/pdf/2205.12598v2.pdf
1264,249062828,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,we use fairly simple logical rules and constructs for robustlr.,Neutral,https://export.arxiv.org/pdf/2205.12598v2.pdf
1265,186346769,The Connection Commonalities in the Mathematical Content of Lesson Sequences,conclusion,Insight-tree,"from the interview, x-t2 stressed that it was essential for all his students to see the connections of what reallife examples can bring to the topic of statistics.",Neutral,
1266,186346769,The Connection Commonalities in the Mathematical Content of Lesson Sequences,conclusion,Insight-tree,"at the same time, he wanted his students to enjoy and learn mathematics from the enthusiasm he brought into his mathematics teaching.",Neutral,
1267,186346769,The Connection Commonalities in the Mathematical Content of Lesson Sequences,conclusion,Insight-tree,"from the observations made, the same level of technique (the technique used to implement making connections problems) made by x-t2 was not detected in the lessons of the other three brunei teachers.",Neutral,
1268,248377036,Evaluating Extrapolation Performance of Dense Retrieval,conclusions,Insight-tree,"with the proposed evaluation method, we first revisit how existing dr models perform in the extrapolation regimes.",Neutral,https://arxiv.org/pdf/2204.11447v1.pdf
1269,248377036,Evaluating Extrapolation Performance of Dense Retrieval,conclusions,Insight-tree,then we further interpret our extrapolation performance by investigating its relationship with the domain transfer ability.,Neutral,https://arxiv.org/pdf/2204.11447v1.pdf
1270,252519203,Towards Faithful Model Explanation in NLP: A Survey,conclusion,Insight-tree,"despite being a fundamental principle of model explanation methods, faithfulness does not have a well-established technical definition or evaluation framework.",Neutral,https://export.arxiv.org/pdf/2209.11326v2.pdf
1271,252519203,Towards Faithful Model Explanation in NLP: A Survey,conclusion,Insight-tree,"consequently, it is difficult to compare different methods in terms of faithfulness, and many of them do not report any quantitative faithfulness evaluation results.",Neutral,https://export.arxiv.org/pdf/2209.11326v2.pdf
1272,211258645,Unsupervised Question Decomposition for Question Answering,conclusion,Insight-tree,"overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems.",Neutral,https://arxiv.org/pdf/2002.09758v1.pdf
1273,258556908,VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens,conclusions,Insight-tree,we provide a table 4 of notations that are used for more than once so that the readers can refer to their definition easily.,Neutral,https://export.arxiv.org/pdf/2305.04241v2.pdf
1274,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,discussion and conclusion,Insight-tree,"while wic and english wsd rely exclusively on dictionaries (pilehvar and camacho-collados, 2019) to obtain word senses and example sentences, our data collection depends on wikipedia, wic, & nlp models and our annotation depends on experts.",Neutral,https://www.aclanthology.org/2023.eacl-main.1.pdf
1275,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,limitations,Insight-tree,hyperparameters we train each bert-based classifier for a maximum of 100 epochs with early stopping monitored on validation accuracy (patience of 10 epochs).,Neutral,https://www.aclanthology.org/2023.eacl-main.1.pdf
1276,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,limitations,Insight-tree,"we use a batch size of 200 and adam optimizer with learning rate  = 0.0001,  1 = 0.9,  2 = 0.999, and  = 10 8 .",Neutral,https://www.aclanthology.org/2023.eacl-main.1.pdf
1277,233231436,AR-LSAT: Investigating Analytical Reasoning of Text,conclusion,Insight-tree,further discussions are made to shed light on important future directions.,Neutral,https://arxiv.org/pdf/2104.06598v2.pdf
1278,248157463,ASQA: Factoid Questions Meet Long-Form Answers,conclusion,Insight-tree,"thus, we believe that asqa is an appealing task for the qa community.",Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1279,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"we now make two remarks that we urge the reader to consider when interpreting the results of this work.inter-annotator agreement in section 3.3, we observed that inter-annotator agreement in asqa is higher than in eli5.",Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1280,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,high inter-annotator agreement in asqa is contingent upon the high inter-annotator agreement in the ambigqa dataset.,Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1281,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"indeed, ambigqa disambiguations serve as a shared source of information between the two asqa annotators working on the same instance, potentially inflating the level of agreement.that said, min et al.",Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1282,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"(2020) observe that human annotators have a decent level of agreement in constructing the disambiguations in ambigqa, thereby supporting the observation that asqa is more objective than eli5.evaluation metrics second, we caveat that our accuracy metrics (str-em and disambig-f1) only measure the recall of the required information in the long answers.",Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1283,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"in cases where the long answer hallucinates incorrect disambiguations or facts, the accuracy metrics may still be high as long as the correct disambiguations are included.",Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1284,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,and receive an answer within a day.,Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1285,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,the compensation rate for the task was set to be $17.8/hour which is higher than the minimum hourly wage in the us.,Neutral,https://www.aclanthology.org/2022.emnlp-main.566.pdf
1286,254685782,MASTER: MULTI-TASK PRE-TRAINED BOTTLE- NECKED MASKED AUTOENCODERS ARE BETTER DENSE RETRIEVERS,conclusion,Insight-tree,we compare our approach with a variety of methods: of bert on these nlu tasks.,Neutral,https://export.arxiv.org/pdf/2212.07841v1.pdf
1287,218610721,Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction,conclusion,Insight-tree,jointly in the context of lp.,Neutral,https://www.aclweb.org/anthology/2020.acl-main.209.pdf
1288,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,"developing approaches for this challenge, we argue, could allow for more generalizable, privacy-preserving and efficient models.",Neutral,https://arxiv.org/pdf/2110.08542v1.pdf
1289,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,"in contrast, a model that is able to learn to communicate with the agents, albeit using annotated decompositions, is able to solve this task.",Neutral,https://arxiv.org/pdf/2110.08542v1.pdf
1290,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,this indicates the need and potential of such approaches to solve complex tasks.,Neutral,https://arxiv.org/pdf/2110.08542v1.pdf
1291,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,theory 1: what movies have people from the country $1 acted in?,Neutral,https://arxiv.org/pdf/2110.08542v1.pdf
1292,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,"however, the size of our dataset is not the key factor in its success.",Neutral,https://export.arxiv.org/pdf/2208.01009v2.pdf
1293,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,our results question common wisdom that task diversity is necessary for adapting lms to fsl.,Neutral,https://export.arxiv.org/pdf/2208.01009v2.pdf
1294,236447339,QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension,conclusion,Insight-tree,the number of qa/rc datasets produced by the nlp community is large and growing rapidly.,Neutral,https://export.arxiv.org/pdf/2107.12708v2.pdf
1295,211832082,"A Survey of Antimicrobial Resistance Determinants in Category A Select Agents, Exempt Strains, and Near-Neighbor Species",conclusions,Insight-tree,both pcr and microarrays are valuable tools for the tracking the genetic underpinnings of amr resistance.,Neutral,
1296,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"in this work we focused only on word documents as a common document type where users can author, copy-edit, or read content.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1297,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"indeed, the organization where we deployed our studies primarily uses word for business documents.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1298,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"because each file type is used for different purposes (e.g., excel documents for long-term book-keeping [27]) and possibly containing content at different levels of abstraction, the extent to which question answering in these documents can be automated and the kinds of expertise knowledge workers need may be different from the word documents in our study.because all questions had to first pass through the knowledge workers' system, the majority of the responses that participants received from the q&a system had a delay.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1299,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"therefore, it is possible that participants would have asked different questions if the responses had been provided instantaneously.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1300,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"to understand if the types of questions by a user changed over time as they gained familiarity with the system, we examined the questions that were posted by the same user both across different documents or on the same document.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1301,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"although these users would realize that the system does not provide answers instantaneously, their cluster of initial questions would not be impacted by the expectation of a long delay, their perceived capabilities of the system, or the quality of the answers.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1302,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"in fact, although we had specified in the consent form that there may be a delay in the responses that participants would receive from the q&a system, some end-of-study survey responses indicated that a number of participants had in fact not noticed this point and had asked their first few questions expecting instantaneous answers-""i was initially confused by the delay of asking the question in the document and then waiting for an email that told me to go back to the document.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1303,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,it seemed a bit redundant to get an email about it vs. just a notification in the word doc itself and telling me it was working on it or something.,Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1304,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"for a plug-in, however, i would expect less of a delay. """,Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1305,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"therefore, the first few questions from these participants could help with the generalizability of our results.the delay or the quality of answers however, may have influenced those users who submitted questions after receiving answers from the system.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1306,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"upon examination, we found that in many instances, when users received answers to their previous questions, they explored increasingly more sophisticated questions of various types, e.g., content-related, concerning metadata, or seeking external information.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1307,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"for instance, one participant started by asking simple metadata questions (""who is the author?"") and proceeded to ask questions on a scanned document that needed not only complex reasoning, but also optical character recognition (ocr): ""what is the total score?"" on a document where handwritten scores were given to each question.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1308,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,this finding suggests that the answers may have in fact encouraged users to be liberal with the types of questions they wished to subsequently ask.,Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1309,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"this exploration could be due to users gaining confidence that the system can in fact handle the types of questions with which they need support or could be because they wished to test the limits of its abilities.nevertheless, the characterization that we present in the paper also includes the questions users submitted in the experience sampling phase, where participants could imagine a sophisticated system with any or no delay.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1310,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"the set of questions in that phase of the study could further help with generalizability of our results, e.g., to settings where not all questions necessarily experience a delay.it is conceivable that the types of questions about a document may vary with the document type.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1311,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"in phase 2 where we had access to the documents, we observed that the document distribution was in fact very varied and included project proposals and timelines, value propositions, design specifications, service instructions, management training, protocols, faqs, whitepaper reports, strategy planning, customer feedback, research findings, etc. from various domains.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1312,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"for the same reason, we did not track a user's modification of the document before or after the user posted a question; the metadata was collected upon the user's submission of a question.",Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1313,247779047,Understanding Questions that Arise When Working with Business Documents,conclusion,Insight-tree,we studied users' information needs when working with their business documents as a first step towards building document assistants that can handle a variety of user requests.,Neutral,https://export.arxiv.org/pdf/2203.15073v2.pdf
1314,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"in practice, we limit our search to a finite horizon and find that this suffices to provide a practical wall clock runtime, never exceeding 5 seconds for any single example.",Neutral,https://export.arxiv.org/pdf/2201.06028v2.pdf
1315,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,"in contrast, noises in individual task clusters and colossal data amount are less influential.",Neutral,https://www.aclanthology.org/2022.emnlp-main.105.pdf
1316,247593935,R E L I C : Retrieving Evidence for Literary Claims,conclusion,Insight-tree,"we train a dense retriever model for our task; while it significantly outperforms baselines, human performance indicates a large room for improvement.",Neutral,https://www.aclanthology.org/2022.acl-long.517.pdf
1317,119407066,On the Structure of the Yang-Mills Vacuum,conclusions,Insight-tree,many of the intuitive ideas we have developed in the last years seem to prove to be correct nally.,Neutral,https://export.arxiv.org/pdf/hep-lat/9506033v1.pdf
1318,119407066,On the Structure of the Yang-Mills Vacuum,conclusions,Insight-tree,"so, most likely, we are on the right track.",Neutral,https://export.arxiv.org/pdf/hep-lat/9506033v1.pdf
1319,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,limitations,Insight-tree,"however, given that our results were statistically significant ( 5.1), we believe that the number of questions selected were sufficient for our study.",Neutral,https://export.arxiv.org/pdf/2305.14750v1.pdf
1320,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"economically creating complex, interactive environments that simulate a significant fraction of real world interactions is still well beyond current simulators or libraries -but required for higher-fidelity interactive worlds that have multiple meaningful paths toward achieving task goals.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1321,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"generating these environments semi-automatically (e.g. ammanabrolu et al., 2020a) may offer a partial solution.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1322,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"independent of tooling, libraries and other middleware offer near-term solutions to more complex environment modeling, much in the same way 3d game engines are regularly coupled with physics engine middleware to dramatically reduce the time required to implement forces, collisions, lighting, and other physics-based modeling.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1323,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"the addition of a chemistry engine that knows ice warmed above the freezing point will change to liquid water, or a generator engine that knows the sun is a source of sunlight during sunny days, or an observation engine that knows tools (like microscopes or thermometers) can change the observation model of a pomdp -may offer tractability in the form of modularization.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1324,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"efforts using large-scale crowdsourcing to construct knowledge bases of commonsense knowledge (e.g., atomic, sap et al., 2019) may be required to support these efforts.current planning languages offer a partial solution for environment modelling.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1325,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"while simulators partially implement facilities for world modeling, some (e.g. ct et al., 2018;shridhar et al., 2020b) suggest using mature planning languages like strips (fikes and nilsson, 1971) or pddl (mcdermott et al., 1998) for more full-featured modeling.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1326,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"this would not be without significant development effort -existing implementations of planning languages typically assume full-world observability (in conflict with pomdp modelling), and primarily agent-directed state-space changes, making complex world modeling with partial ob-servability, and complex environment processes (such as plants that require water and light to survive, or a sun that rises and sets causing different items to be observable in day versus night) outside the space of being easily implemented with off-the-shelf solutions.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1327,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"text world articles frequently emphasize agent modeling contributions over environment, methodological, or analysis contributions -but these contributions are critical, especially in the early stages of this subfield.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1328,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"agent performance in easy environments has increased incrementally, while medium-to-hard environments have seen comparatively modest improvements.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1329,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"transfer learning in the form of training on one set of environments and testing on others has become a standard feature of benchmarks (e.g. hausknecht et al., 2020), but focused contributions that work to precisely characterize the limits of what can be learned from (for example) omniquest and transferred to zork, and what capacities must be learned elsewhere, will help inform research programs in agent modeling and environment design.transfer learning between text world and 3d environments.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1330,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"tasks learned at a high-level in text worlds help speed learning when those same models are transferred to more complex 3d environments (shridhar et al., 2020b).",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1331,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,this framing of transfer learning may resemble how humans can converse about plans for future actions in locations remote from those eventual actions (as when we apply knowledge learned in classrooms to the real world).,Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1332,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"as such, text-plus-3d environment rendering shows promise as a manner of controlling for different sources of complexity in multi-modal task learning (from high-level task-specific knowledge to low-level perceptual knowledge), and appears a promising research methodology for imparting complex task knowledge on agents that are able to navigate high-fidelity virtual environments.",Neutral,https://www.aclanthology.org/2022.wordplay-1.1.pdf
1333,234741852,TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,conclusion,Insight-tree,"to answer questions in tat-qa, the close relation between table and paragraphs and numerical reasoning are required.",Neutral,https://www.aclanthology.org/2021.acl-long.254.pdf
1334,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,the fact that knowledge are not shared gives rise to individual diversity and motivates collaboration.,Neutral,https://arxiv.org/pdf/2201.09708v1.pdf
1335,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,we believe natural-language based collaboration system is a domain that has practical implication and holds scientific values.,Neutral,https://arxiv.org/pdf/2201.09708v1.pdf
1336,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,figure 6 shows the structure and examples in our proposed knowledge graphs.,Neutral,https://arxiv.org/pdf/2201.09708v1.pdf
1337,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,g 1 contains a list of person entities.,Neutral,https://arxiv.org/pdf/2201.09708v1.pdf
1338,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,the value of a property of the entity is randomly generated within a reasonable range.,Neutral,https://arxiv.org/pdf/2201.09708v1.pdf
1339,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,"for example, the value of a person's height is randomly sampled in the range [160cm, 200cm].",Neutral,https://arxiv.org/pdf/2201.09708v1.pdf
1340,204915921,QASC: A Dataset for Question Answering via Sentence Composition,conclusion,Insight-tree,"instead, models must learn to retrieve and compose candidate pieces of knowledge.",Neutral,https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175
1341,204915921,QASC: A Dataset for Question Answering via Sentence Composition,conclusion,Insight-tree,"state-of-the-art bert models, even with massive fine-tuning on over 100k questions from previous relevant datasets and using our proposed two-step retrieval, leave a large margin to human performance levels, thus making qasc a new challenge for the community.",Neutral,https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175
1342,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,[31] has already studied this setting in more detail and we leave the analysis of our prompts for a dense retrieval setting as future work.,Neutral,https://export.arxiv.org/pdf/2305.03653v1.pdf
1343,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,"our work can naturally be extended to other language models [3,5,9,28] and we leave the study of such models as a topic for future research.",Neutral,https://export.arxiv.org/pdf/2305.03653v1.pdf
1344,258546701,Query Expansion by Prompting Large Language Models,conclusion,Insight-tree,"in contrast to traditional prf-based query expansion, llms are not restricted to the initial retrieved set of documents and may be able to generate expansion terms not covered by traditional methods.",Neutral,https://export.arxiv.org/pdf/2305.03653v1.pdf
1345,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,"in this paper, we point out the challenges that should be considered in introducing ir into llm from perspectives of reasoning and knowledge.",Neutral,https://export.arxiv.org/pdf/2304.14732v4.pdf
1346,238259354,FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks,conclusion and future directions,Insight-tree,"thus, we believe our wellmaintained open-source codebase to support future work in this area.",Neutral,https://arxiv.org/pdf/2104.08815v3.pdf
1347,233189566,Globalizing BERT-based Transformer Architectures for Long Document Summarization,conclusion,Insight-tree,a baselines: implementation details bertsumext:,Neutral,https://www.aclweb.org/anthology/2021.eacl-main.154.pdf
1348,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,conclusions,Insight-tree,"this ability to predict system performance occurs at the instance level instead of a sense of trust of the overall system, which can be important if the accuracy of the system is variable based on the question.",Neutral,https://export.arxiv.org/pdf/2204.07693v2.pdf
1349,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,multi-hop questions afford the use of a decompositional probing strategy.,Neutral,https://export.arxiv.org/pdf/2204.07693v2.pdf
1350,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,3) the dependences of type 2 mrc datasets are not always exactly consistent with their intended purpose.,Neutral,https://www.aclanthology.org/2022.coling-1.147.pdf
1351,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,"based on the analysis, although there are several limitations in our proposed method, this work is still a reasonable attempt to deeply understand the relationship between the dataset and a specific nlu skill.",Neutral,https://www.aclanthology.org/2022.coling-1.147.pdf
1352,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,measures of answer uncertainty are required to identify examples that the system may struggle to get correct and hence should abstain from answering such questions.,Neutral,https://www.aclanthology.org/2022.findings-acl.82.pdf
1353,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,unanswerability detection is required for when the answer cannot be deduced using the information provided.,Neutral,https://www.aclanthology.org/2022.findings-acl.82.pdf
1354,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,"in this work, we highlight open challenges in the existing multilingual approach by (lewis et al. 2020) and (clark et al. 2020).",Neutral,https://arxiv.org/pdf/2012.05958v1.pdf
1355,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,the annotated data and models are shared publicly.,Neutral,https://arxiv.org/pdf/2106.02399v1.pdf
1356,258887816,Linguistic Properties of Truthful Response,limitation,Insight-tree,"that is, we have manually confirmed gpt-judge labels on davinci responses, and extrapolated the system to ada, babbage, and curie.",Neutral,https://export.arxiv.org/pdf/2305.15875v2.pdf
1357,256597851,LIQUID: A Framework for List Question Answering Dataset Generation,conclusion,Insight-tree,table 10: distribution of answer types for the synthetic and bioasq 9b data.,Neutral,https://export.arxiv.org/pdf/2302.01691v2.pdf
1358,256597851,LIQUID: A Framework for List Question Answering Dataset Generation,conclusion,Insight-tree,table 9 presents the distribution of the number of answers.,Neutral,https://export.arxiv.org/pdf/2302.01691v2.pdf
1359,256597851,LIQUID: A Framework for List Question Answering Dataset Generation,conclusion,Insight-tree,"similar to the results in the general domain (table 5), the synthetic data were more skewed toward smaller numbers of answers than the labeled data, but some answers (14.2%) had four or more spans.",Neutral,https://export.arxiv.org/pdf/2302.01691v2.pdf
1360,237940861,Paradigm Shift in Natural Language Processing,conclusion,Insight-tree,"recently, prompt-based tuning, which is to formulate some nlp task into a (m)lm task, has exploded in popularity.",Neutral,https://arxiv.org/pdf/2109.12575v2.pdf
1361,237940861,Paradigm Shift in Natural Language Processing,conclusion,Insight-tree,they can achieve considerable performance with much less training data.,Neutral,https://arxiv.org/pdf/2109.12575v2.pdf
1362,233296336,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,conclusion,Insight-tree,"by evaluating performance against multiple baselines, we aim to assess where bert can transfer knowledge even without word identities.",Neutral,https://arxiv.org/pdf/2104.08410v1.pdf
1363,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,the topology of the graph is utilized to form the attention bias for the self-attention layers.,Neutral,https://arxiv.org/pdf/2205.00731v2.pdf
1364,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"thirdly, a dynamic gate mechanism is applied to make a fusion of the features from two branches.",Neutral,https://arxiv.org/pdf/2205.00731v2.pdf
1365,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"to improve the awareness of different question types, the question feature is updated based on the self-attention module.",Neutral,https://arxiv.org/pdf/2205.00731v2.pdf
1366,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"finally, the concatenated text sequence is passed through the feed forward layer and obtains the answer prediction.",Neutral,https://arxiv.org/pdf/2205.00731v2.pdf
1367,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"the whole reasoning process provides the interpretability, reflected by logical units with explicit relations and the visualization of the attention maps.",Neutral,https://arxiv.org/pdf/2205.00731v2.pdf
1368,237194607,MeDiaQA: A Question Answering Dataset on Medical Dialogues,conclusion,Insight-tree,we hope our dataset will lead a deeper research on machine reading comprehension in medical domain.,Neutral,https://arxiv.org/pdf/2108.08074v1.pdf
1369,256739246,Recent Advances in Long Documents Classification Using Deep-Learning,conclusion and future directions,Insight-tree,"it is beyond doubt that transformer architecture changed the way linguistic analysis is performed, and in a very short time bert has been widely accepted as the golden standard of semantic understanding.",Neutral,https://www.aclanthology.org/2022.icnlsp-1.12.pdf
1370,256739246,Recent Advances in Long Documents Classification Using Deep-Learning,conclusion and future directions,Insight-tree,"however, the greatest value of this concept may be tied to its flexibility, as it allows for extensive customization and specialization with only minimal modifications of the training procedure.",Neutral,https://www.aclanthology.org/2022.icnlsp-1.12.pdf
1371,256739246,Recent Advances in Long Documents Classification Using Deep-Learning,conclusion and future directions,Insight-tree,"while there have been numerous adaptations of successful transformer models in the past, it's highly likely that the number and quality of derivative work will increase in the near future.",Neutral,https://www.aclanthology.org/2022.icnlsp-1.12.pdf
1372,256739246,Recent Advances in Long Documents Classification Using Deep-Learning,conclusion and future directions,Insight-tree,"figuring out ways to improve an already impressive model is not easy, but growing presence of this topic in the online forums and greater availability of research papers dealing with some of the outstanding challenges could power the next wave of research in this direction.",Neutral,https://www.aclanthology.org/2022.icnlsp-1.12.pdf
1373,256739246,Recent Advances in Long Documents Classification Using Deep-Learning,conclusion and future directions,Insight-tree,"this process is already underway, and a breakthrough achieved with transformers is being actively exploited by research teams from around the world.",Neutral,https://www.aclanthology.org/2022.icnlsp-1.12.pdf
1374,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,table 4 we use glove,Neutral,https://export.arxiv.org/pdf/2207.14000v1.pdf
1375,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,[16] as the word vector representation.,Neutral,https://export.arxiv.org/pdf/2207.14000v1.pdf
1376,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,"comparison among our ima-glove-ga, ima-glove, mac-glove, dmn-glove, imasm-glove, lstm-glove, and roberta-large on pararules test sets with different reasoning depths.",Neutral,https://export.arxiv.org/pdf/2207.14000v1.pdf
1377,14182996,Integrating miRNA and mRNA Expression Profiling Uncovers miRNAs Underlying Fat Deposition in Sheep,conclusion,Insight-tree,"this occurs via the key signaling pathways including focal adhesion, pyruvate metabolism, and the mapk, foxo, and tnf signaling pathway.",Neutral,
1378,234334015,ExpMRC: explainability evaluation for machine reading comprehension,conclusion,Insight-tree,"we hope the release of the dataset will further accelerate the research on the explainability and interpretability of mrc systems, especially for the unsupervised approaches.",Neutral,https://arxiv.org/pdf/2105.04126v1.pdf
1379,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,the region observable pomdp has more informative observations and hence is easier to solve.,Neutral,https://arxiv.org/pdf/cs/9711103v1.pdf
1380,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,"other approximate methods need to be incorporated in order to solve region observable pomdps whose radiuses are not small. proof of proposition 3: because of proposition 1 and lemma 4, it su ces to show that 3.",Neutral,https://arxiv.org/pdf/cs/9711103v1.pdf
1381,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,"if x , return nil, else return b.",Neutral,https://arxiv.org/pdf/cs/9711103v1.pdf
1382,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,a document retrieval table 7 shows the performance of spade with other baselines on the ms marco document ranking dataset.,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1383,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"dr (random neg) and dr (bm25 neg) represent dense retrieval models trained with random and bm25 negatives, respectively.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1384,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,all dense retrieval models use roberta-base [36] as an encoder and [cls] token embedding for query and document representations.,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1385,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"the experimental results of them are copied from [74,75]. unicoil and unicoil-dt5q performed indexing on segmented passages, and the score of the segmented passage that obtained the highest score is used as the score of the document, i.e., maxp technique [10].",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1386,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"other models, including spade, use only the first part of the document truncated to bert's max length.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1387,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"in other words, as the first-stage retriever in document ranking, the uni-encoder method may be more favorable than the bi-encoder method.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1388,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,b effects of figure 5 shows the effect of the aggregating hyperparameter .,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1389,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"using the dual encoder, i.e., 0 < < 1, mostly shows better performance than using a mere single encoder, i.e., = 0 or = 1, depicting that each encoder only captures complementary information to another.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1390,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"spade (k=10) shows the best performance at = 0.4 with 0.352 in mrr@10, implying that the term weighting encoder is more dominant than the term expansion encoder.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1391,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,figure 6 shows the effect of corpus-level pruning of spade.,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1392,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"since new relevant terms are appended through the term expansion component without cutoff based on an approximate document frequency, the number of elements in the term-document matrix is enormous, at about 1.1 billion, implying query processing is very costly, e.g., 510 ms per query.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1393,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,there is a trade-off between effectiveness and efficiency depending on the cutoff ratio.,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1394,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,when the threshold is set   figure 7 shows the ndcg@10 on each query of spade and deep-impact [43].,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1395,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"while deepimpact [43] is better than spade in trec dl 2019, spade is better than deepimpact [43] in trec dl 2020.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1396,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,table 8 shows the weighted document terms and expanded terms.,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1397,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"for the query ""what is a nonconformity? earth science"", spade gives high scores for the important terms, e.g., ""nonconformity"" and expanding the terms, e.g., ""science"", which are matched to the query.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1398,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"owing to enriched document representations, spade can rank a given relevant document in the 27th place, where the core terms of the query, e.g., ""earth"", ""science"", do not appear in the document.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1399,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"meanwhile, even though deepimpact [43] identifies important document terms, the given document is ranked 77th since query terms are not properly expanded by doct5query",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1400,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,spade accurately expands the query terms by inferring the relevant words.,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1401,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"for the query ""dog day afternoon meaning"", spade weights for the important terms, e.g., ""dog"", ""day"", ""afternoon"".",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1402,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"moreover, spade successfully expands terms such as ""warm"", ""weather"", ""hottest"", which are highly related to the query.",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1403,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"for the query ""what is the most popular food in switzerland"", both deepimpact and spade give high weights for the important terms and expand relevant terms which are matched to the query, e.g., ""switzerland"", ""popular"", ""most"".",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1404,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,1 lack of conformity ; nonconformity .,Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1405,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval Young-In Song a Dual Document Encoder for First-stage Retrieval. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management * Equal contribution  Corresponding author Figure 1: MRR@10 and average query latency (in ms) on MS MARCO development set of SpaDE varying the size of top-masking for the document-level pruning and existing retrievers with sparse representations. The latency is mea- sured with PISA [44] using Block-Max WAND [14],conclusion,Insight-tree,"2 geology a surface between successive strata representing a missing interval in the geologic record of time , and produced either by an interruption in deposition or by the erosion of depositionally continuous strata followed by renewed deposition .",Neutral,https://export.arxiv.org/pdf/2209.05917v2.pdf
1406,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,and we hope our benchmark can facilitate the research progress of interpertability.,Neutral,https://www.aclanthology.org/2022.conll-1.6.pdf
1407,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,conclusion,Insight-tree,"the rationales in this benchmark meet primary properties that a rationale should satisfy, i.e., sufficiency, compactness and comprehensiveness.",Neutral,https://www.aclanthology.org/2022.conll-1.6.pdf
1408,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,"to learn different retrieval tasks with a single model, we mapped the descriptions of tasks to a few prompt tokens for keeping task specifications.",Neutral,https://export.arxiv.org/pdf/2304.14856v1.pdf
1409,254877343,HYRR: Hybrid Infused Reranking for Passage Retrieval,conclusion,Insight-tree,"while the hybrid retriever is composed of term-based and neural models, the reranker is a neural cross-attention model which learns from negatives examples generated by the hybrid retriever.",Neutral,https://export.arxiv.org/pdf/2212.10528v1.pdf
1410,257921404,Rethinking the Role of Token Retrieval in Multi-Vector Retrieval,conclusion,Insight-tree,multi-vector retrieval leverages query and document token representations for effective information retrieval.,Neutral,https://export.arxiv.org/pdf/2304.01982v2.pdf
1411,257921404,Rethinking the Role of Token Retrieval in Multi-Vector Retrieval,conclusion,Insight-tree,"specifically, xtr scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens.",Neutral,https://export.arxiv.org/pdf/2304.01982v2.pdf
1412,232380161,DAGN: Discourse-Aware Graph Network for Logical Reasoning,conclusion,Insight-tree,we first treat elementary discourse units (edus) that are split by discourse relations as basic reasoning units.,Neutral,https://www.aclweb.org/anthology/2021.naacl-main.467.pdf
1413,232380161,DAGN: Discourse-Aware Graph Network for Logical Reasoning,conclusion,Insight-tree,we then build discourse-based logic graphs with edus as nodes and discourse relations as edges.,Neutral,https://www.aclweb.org/anthology/2021.naacl-main.467.pdf
1414,202573071,CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION,conclusion,Insight-tree,"we hope that the release of this model at https://github.com/salesforce/ctrl pushes towards more controllable, general models for natural language processing, and we encourage future discussion about artificial generation with our team by emailing ctrl-monitoring@salesforce.com.   table 3).",Neutral,https://arxiv.org/pdf/1909.05858v2.pdf
1415,202573071,CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION,conclusion,Insight-tree,"for all the reddit data, the secondary code can be title: or text:, which is the title and text of the article, respectively.",Neutral,https://arxiv.org/pdf/1909.05858v2.pdf
1416,235727345,Springer Nature 2021 L A T E X template A Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension,conclusion,Insight-tree,the network contains several clue reading paths from the subject of the question and ends with the given candidates' entities.,Neutral,https://export.arxiv.org/pdf/2107.00841v2.pdf
1417,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,the recent trend of building large lms may not be sustainable to solve evolving benchmarks.,Neutral,https://www.aclanthology.org/2022.emnlp-main.302.pdf
1418,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,our approach provides a viable option to involve people in nlp research.,Neutral,https://www.aclanthology.org/2022.emnlp-main.302.pdf
1419,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,we hope our work will encourage the community to develop human-centric solutions that actively involve humans while leveraging nlp resources.,Neutral,https://www.aclanthology.org/2022.emnlp-main.302.pdf
1420,253080620,LittleBird: Efficient Faster & Longer Transformer for Question Answering,conclusion,Insight-tree,the distillation and training method with padding insertion allows the model to be trained by reusing the existing pre-trained language model for short inputs and work well for long inputs even if trained on short inputs.,Neutral,https://www.aclanthology.org/2022.emnlp-main.352.pdf
1421,246294995,Reasoning Like Program Executors,conclusion & future work,Insight-tree,"in the future, we hope our work could inspire more transference of reasoning knowledge from program executors to models.",Neutral,https://www.aclanthology.org/2022.emnlp-main.48.pdf
1422,248227284,TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval,conclusion,Insight-tree,we hope our work inspires future work on integrating structured data into pretrained models.,Neutral,https://www.aclanthology.org/2022.findings-acl.169.pdf
1423,248227284,TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval,conclusion,Insight-tree,government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon.,Neutral,https://www.aclanthology.org/2022.findings-acl.169.pdf
1424,248227284,TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval,conclusion,Insight-tree,"any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of nih, onr, or the u.s. government.",Neutral,https://www.aclanthology.org/2022.findings-acl.169.pdf
1425,3520779,ATTac-2000: An Adaptive Autonomous Bidding Agent,conclusion and future work,Insight-tree,tac-2000 was the rst autonomous bidding agent competition.,Neutral,https://arxiv.org/pdf/1106.0678v1.pdf
1426,250390947,Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering,conclusion and future work,Insight-tree,"we extract clues at multiple levels of granularity relating entity nodes, and construct a semantic graph from these clues.",Neutral,https://www.aclanthology.org/2022.dlg4nlp-1.8.pdf
1427,250390947,Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering,conclusion and future work,Insight-tree,we then combine a masked attention mechanism and two-stage graph reasoning to perform interpretable inference over the semantic graph.,Neutral,https://www.aclanthology.org/2022.dlg4nlp-1.8.pdf
1428,248827723,A STEP towards Interpretable Multi-Hop Reasoning: Bridge Phrase Identification and Query Expansion,conclusion,Insight-tree,"our method constructs a graph of noun phrases from the question and the available context, and applies the steiner tree algorithm to identify the minimal subgraph that connects all question phrases.",Neutral,
1429,248827723,A STEP towards Interpretable Multi-Hop Reasoning: Bridge Phrase Identification and Query Expansion,conclusion,Insight-tree,we extract as bridge phrases nodes in this graph that are not any of the question phrases.,Neutral,
1430,248827723,A STEP towards Interpretable Multi-Hop Reasoning: Bridge Phrase Identification and Query Expansion,conclusion,Insight-tree,"5. ravi sethi: he is best known as one of three authors of the classic computer science textbook """", also known as the ""dragon book"".",Neutral,
1431,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"motteux was also without heirs and bequeathed sandringham, together with another norfolk estate and a property in surrey, to the third son of his close friend, emily lamb, the wife of lord palmerston.",Neutral,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
1432,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"at the time of his inheritance in 1843, charles spencer cowper was a bachelor diplomat, resident in paris.",Neutral,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
1433,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"on succeeding to motteux's estates, he sold the other properties and based himself at sandringham.",Neutral,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
1434,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"he undertook extensions to the hall, employing samuel sanders teulon to add an elaborate porch and conservatory.",Neutral,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
1435,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"cowper's style of living was extravagant he and his wife spent much of their time on the continent and within 10 years the estate was mortgaged for 89,000.",Neutral,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
1436,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"the death of their only child, mary harriette, from cholera in 1854 led the couple to spend even more time abroad, mainly in paris, and by the early 1860s cowper was keen to sell the estate.",Neutral,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
1437,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"figure 9: predicted relevant spans for question answered correctly with annotation (prediction:""charles spencer cowper"") and incorrectly without annotations (prediction:""lord palmerston"") by xlnet on quoref",Neutral,https://www.aclweb.org/anthology/2020.acl-main.497.pdf
1438,248405719,A Thorough Examination on Zero-shot Dense Retrieval,conclusion and future work,Insight-tree,"besides, the performance between bm25 and dr models varies significantly on different target datasets, where the dataset bias (e.g., a dataset is created based on exact match) is likely to make such comparison unfair.",Neutral,https://export.arxiv.org/pdf/2204.12755v2.pdf
1439,226262208,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,conclusion,Insight-tree,"these questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways.",Neutral,https://arxiv.org/pdf/2011.07127v1.pdf
1440,219687051,Self-supervised Learning: Generative or Contrastive,conclusion,Insight-tree,self-supervised learning is the present and future of deep learning due to its supreme ability to utilize web-scale unlabeled data to train feature extractors and context generators efficiently.,Neutral,https://arxiv.org/pdf/2006.08218v5.pdf
1441,219687051,Self-supervised Learning: Generative or Contrastive,conclusion,Insight-tree,"finally, fundamental problems and future directions of self-supervised learning are listed.",Neutral,https://arxiv.org/pdf/2006.08218v5.pdf
1442,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"for the answer sentence selection model, bert was fine-tuned with a learning rate of 3  10 6 and a batch size of 16 for 3 epochs .",Neutral,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
1443,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"for span selection, we adopted a learning rate of 3  10 5 and a batch size of 64 for 5 epochs.",Neutral,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
1444,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"during inference, the confidence threshold for the relevance estimator was set to  = 0.75 (kratzwald and feuerriegel, 2018) for both sentence and passage retrieval.",Neutral,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
1445,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"for the evidence estimator, k qa was tuned on the development set.",Neutral,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
1446,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"for the centrality estimator, the influence of the query was set to  = 0.15 (wan, 2008;wan and zhang, 2014",Neutral,https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
1447,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,"structural prompt simultaneously models the common ability required for various tasks and keeps the speciality of each task, through a structurally designed learnable input schema.",Neutral,https://www.aclanthology.org/2022.naacl-main.313.pdf
1448,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,"apart from introducing abstraction as a crucial reasoning skill, our work requires the combined application of various reasoning skills including creative decomposition of problems, commonsense reasoning, mathematical reasoning, etc.",Neutral,https://www.aclanthology.org/2021.emnlp-main.582.pdf
1449,215768725,A Simple Yet Strong Pipeline for HotpotQA,conclusion,Insight-tree,"by operating jointly over these sentences chosen from multiple paragraphs, we arrive at answers and supporting sentences on par with state-of-theart approaches.",Neutral,https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf
1450,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,"it generates twin answer sentences, perturbed answer sentence (pas), and distracting answer sentence (das), to construct a new adversarial context in a qa sample.",Neutral,https://www.aclanthology.org/2022.emnlp-main.821.pdf
1451,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,it can deceive models and misguide them to an incorrect answer based on their pitfalls that overly rely on matching sensitive keywords during predicting answers.,Neutral,https://www.aclanthology.org/2022.emnlp-main.821.pdf
1452,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,"during fine-tuning bert on different qa datasets, we set the maximum input sequence length as 384, using an adam optimizer whose initial learning rate is 6.25e5 with the batch size 32.",Neutral,https://www.aclanthology.org/2022.emnlp-main.821.pdf
1453,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,the epoch number is 3 and the final model after all epochs will be saved as the victim model.,Neutral,https://www.aclanthology.org/2022.emnlp-main.821.pdf
1454,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,"spanbert we also use the huggingfancetransformers to implement the model, along with spanbert-large-cased version 5 to initialize the weights.",Neutral,https://www.aclanthology.org/2022.emnlp-main.821.pdf
1455,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,it contains 24 layers with a hidden size of 1024.,Neutral,https://www.aclanthology.org/2022.emnlp-main.821.pdf
1456,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,a linear layer is added to predict the start and end positions of the answer span.,Neutral,https://www.aclanthology.org/2022.emnlp-main.821.pdf
1457,253384010,NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering,conclusion,Insight-tree,"hybrid tabular-textual question answering (qa) requires reasoning from heterogeneous information, and the types of reasoning can be categorized into numerical reasoning and span extraction.",Neutral,https://export.arxiv.org/pdf/2211.03462v1.pdf
1458,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"smoe-dropout (k = n 2 ) 82.03  0.26 smoe-dropout (k = n) 82.32  0.14 to evaluate the stability of the improvement obtained by our smoe-dropout, we carry out further experiments of transformer-xl on sst-2.",Neutral,https://export.arxiv.org/pdf/2303.01610v1.pdf
1459,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"however, the performance starts to degrade if using more experts like k = 16.",Neutral,https://export.arxiv.org/pdf/2303.01610v1.pdf
1460,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"as for our smoe-dropout with a random routing, it enjoys a better ""self-slimmable"" property from k = 1 to k = 16 (full model capacity), with up to 0.87% higher accuracy on sst-2 across all scenarios, compared to its learnable variants.",Neutral,https://export.arxiv.org/pdf/2303.01610v1.pdf
1461,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,figure 4: data annotation interfaces for question and answer collection.,Neutral,https://export.arxiv.org/pdf/2209.08199v1.pdf
1462,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,b) the answer annotators were tasked to determine if the question is valid and if the question is answerable from the screen context.,Neutral,https://export.arxiv.org/pdf/2209.08199v1.pdf
1463,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"if both are positive, the annotators need to answer the questions by 1) selecting or drawing the bounding boxes of ui elements, 2) fill the text for each selected/drawn bounding box on right right, and 3) ranking them appropriately.",Neutral,https://export.arxiv.org/pdf/2209.08199v1.pdf
1464,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,the annotators were also tasked to review and make necessary corrections if the question has grammatical errors or typos.,Neutral,https://export.arxiv.org/pdf/2209.08199v1.pdf
1465,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"a) the two question annotation passes were capped at five and three questions, respectively, resulting in the maximum eight questions in total.",Neutral,https://export.arxiv.org/pdf/2209.08199v1.pdf
1466,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"b) the cases when a single bounding box forms a sufficient answer amount to 92% of the questions, hence removed from the chart for the clarity of the long tail.",Neutral,https://export.arxiv.org/pdf/2209.08199v1.pdf
1467,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"anything beyond 11 bounding boxes is less than 0.05%, accumulatively less than 0.1%.",Neutral,https://export.arxiv.org/pdf/2209.08199v1.pdf
1468,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,"confidently making progress on multilingual models requires challenging, trustworthy evaluations.",Neutral,https://arxiv.org/pdf/2003.05002v1.pdf
1469,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,we look forward to the many ways the research community finds to improve the quality of multilingual models.,Neutral,https://arxiv.org/pdf/2003.05002v1.pdf
1470,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,"in the past, this seems to have stymied innovation around translation as applied to multilingual datasets.",Neutral,https://arxiv.org/pdf/2003.05002v1.pdf
1471,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,we will happily share our annotation protocol on request.,Neutral,https://arxiv.org/pdf/2003.05002v1.pdf
1472,222125277,Under review AUTOREGRESSIVE ENTITY RETRIEVAL,conclusions,Insight-tree,"entity names have several properties that might help (even humans) retrieving them, including a compositional structure and a predictable interaction with the context.",Neutral,https://arxiv.org/pdf/2010.00904v1.pdf
1473,247447492,What Makes Reading Comprehension Questions Difficult?,conclusion,Insight-tree,"to make an nlu benchmark useful, it has to consist of examples that are linguistically diverse and difficult enough to discriminate among state-ofthe-art models.",Neutral,https://www.aclanthology.org/2022.acl-long.479.pdf
1474,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,plms make their prediction based on context.,Neutral,https://export.arxiv.org/pdf/2303.14725v2.pdf
1475,251594672,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,conclusion,Insight-tree,the key idea is to sample a context from one document as a pseudo query and generate the document identifiers of source or destination documents based on hyperlinks.,Neutral,https://export.arxiv.org/pdf/2208.07652v1.pdf
1476,128345225,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,conclusions,Insight-tree,"unlike prior work, pullnet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question.",Neutral,https://www.aclweb.org/anthology/D19-1242.pdf
1477,128345225,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,conclusions,Insight-tree,"in each iteration, a graph cnn is used to identify subgraph nodes that should be expanded using ""pull"" operations on the corpus and/or kb.",Neutral,https://www.aclweb.org/anthology/D19-1242.pdf
1478,208201969,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,conclusion,Insight-tree,existing analysis work in mrc is largely concerned with evaluating the capabilities of systems.,Neutral,https://ojs.aaai.org/index.php/AAAI/article/download/6422/6278
1479,208201969,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,conclusion,Insight-tree,"our methodology consists of input-ablation tests, in which each ablation method is associated with a skill requisite for mrc.",Neutral,https://ojs.aaai.org/index.php/AAAI/article/download/6422/6278
1480,230770437,SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering,conclusion,Insight-tree,we hope that sf-qa can make the open-domain qa research more accessible and make the evaluation easier.,Neutral,https://www.aclweb.org/anthology/2021.eacl-demos.2.pdf
1481,251320151,Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models,conclusions,Insight-tree,"considering the dynamics of queries in web search, we define them based on whether the training and test queries are similar or not.",Neutral,https://export.arxiv.org/pdf/2204.11447v2.pdf
1482,252907386,Can language representation models think in bets?,conclusion and future work,Insight-tree,"modern lrms, based on transformer neural networks, have rapidly exceeded the previous state-of-theart on a range of natural language understanding tasks, including question answering, text summarization and information extraction [104][105][106].",Neutral,https://export.arxiv.org/pdf/2210.07519v1.pdf
1483,252907386,Can language representation models think in bets?,conclusion and future work,Insight-tree,"in the cognitive science literature, such decision-making is often evaluated using bets.",Neutral,https://export.arxiv.org/pdf/2210.07519v1.pdf
1484,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,our proposed model is simple to implement and can be used with any existing (or future) answering model; we will release code to support this integration.,Neutral,https://arxiv.org/pdf/2104.08744v1.pdf
1485,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,"with more reasoning steps, selecting relevant documents given only the question becomes challenging, increasing the need for inter-document interaction.",Neutral,https://arxiv.org/pdf/2104.08744v1.pdf
1486,247188085,Read before Generate! Faithful Long Form Question Answering with Machine Reading,conclusion,Insight-tree,"we also propose to evaluate the factual correctness of lfqa model by answering questions of extractive qa tasks (e.g., natural questions), which may be helpful to evaluate the faithfulness of lfqa model efficiently.",Neutral,https://www.aclanthology.org/2022.findings-acl.61.pdf
1487,211126663,Transformers as Soft Reasoners over Language,conclusion,Insight-tree,"just as mccarthy advocated 60 years ago for machines reasoning (""taking advice"") in logic, we have shown (in a restricted setting) that machines can by trained to reason over language.",Neutral,https://arxiv.org/pdf/2002.05867v2.pdf
1488,211126663,Transformers as Soft Reasoners over Language,conclusion,Insight-tree,"while we have assumed a particular semantics of inference, the methodology we have used is general: characterize the desired behavior in a formal way, synthesize examples, generate linguistic equivalents, and train a model.",Neutral,https://arxiv.org/pdf/2002.05867v2.pdf
1489,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"for web-augmented models including our work, the deterioration of search results from search engine highlights the importance of deriving an effective method to interact with the huge web.search engines are often perceived as black-box and non-transparent for end users.",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1490,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"therefore, many works proposed ""leaning to search"" to decompose complex questions into simpler queries, which may improve the performance of web-based models (nakano et al., 2021;komeili et al., 2021).",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1491,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,we hope our simple approach and strong results could encourage more future work by the community to tackle these questions.,Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1492,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"therefore, we consider the given text as input and the prediction as output. natural language inference is the task of determining whether the given ""hypothesis"" logically follows from the ""premise"" (storks et al., 2019).it acquires deep knowledge about the relationship between hypothesis and premise.",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1493,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"we consider the premise as input and the hypothesis as output.for each category, we choose several representative tasks to construct our pretraining corpus.",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1494,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,the detailed information of these included tasks is listed in table 6.,Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1495,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"during pretraining, for each task example, we use bm25 to retrieve top-10 passages from ccnet as our external knowledge.",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1496,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,the input texts are concatenated with the retrieved passages using manually-written prompts.,Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1497,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"the final input is constructed in the following format: the ""option"" string is applied only when the input text is provided with several candidate answers.",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1498,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"the blanks ""[passage n ]"" and ""[option n ]"" is filled with the retrieved passages and candidate answers.",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1499,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"the blank ""[task instruction]"" aims to indicate the task for our model, which is task-specific and detailed in table 7.",Neutral,https://export.arxiv.org/pdf/2305.10998v2.pdf
1500,252819325,CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers,conclusion,Insight-tree,"in this paper, we propose a new challenge: conditional question answering with hierarchical multispan answers, which might be widespread in multispan qa in real-world scenarios.",,https://www.aclanthology.org/2022.coling-1.146.pdf
1501,252819325,CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers,conclusion,Insight-tree,"moreover, we introduce cmqa, which contains conditional and hierarchical samples to study the new proposed task.",,https://www.aclanthology.org/2022.coling-1.146.pdf
1502,252819325,CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers,conclusion,Insight-tree,"data analysis and experimental results show the main characteristics and challenges of cmqa, and the poor model performance demonstrates that the proposed task is challenging for the community to solve.",,https://www.aclanthology.org/2022.coling-1.146.pdf
1503,252819325,CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers,conclusion,Insight-tree,we believe cmqa can serve as a benchmark to study the new proposed task and help build more reliable and sophisticated qa systems.,,https://www.aclanthology.org/2022.coling-1.146.pdf
1504,252819325,CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers,conclusion,Insight-tree,figure 4: the amount of labels in cmqa.,,https://www.aclanthology.org/2022.coling-1.146.pdf
1505,252819325,CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers,conclusion,Insight-tree,c-a refers to condition-answer and c-f refers to coarse-fine.,,https://www.aclanthology.org/2022.coling-1.146.pdf
1506,258987552,UKP-SQuARE: An Interactive Tool for Teaching Question Answering,conclusion,Insight-tree,"in this paper, we present a novel method to teach question-answering to postgraduate nlp students following the learner-centered method of flipped classrooms.",,https://export.arxiv.org/pdf/2305.19748v2.pdf
1507,258987552,UKP-SQuARE: An Interactive Tool for Teaching Question Answering,conclusion,Insight-tree,we propose to provide reading materials to the students before the class and use the ukp-square platform as a driving tool to conduct the class.,,https://export.arxiv.org/pdf/2305.19748v2.pdf
1508,258987552,UKP-SQuARE: An Interactive Tool for Teaching Question Answering,conclusion,Insight-tree,this platform integrates the most popular qa pipelines and an ecosystem of tools to analyze the available models.,,https://export.arxiv.org/pdf/2305.19748v2.pdf
1509,258987552,UKP-SQuARE: An Interactive Tool for Teaching Question Answering,conclusion,Insight-tree,"these tools include explainability methods, behavioral tests, adversarial attacks, and graph visualizations.",,https://export.arxiv.org/pdf/2305.19748v2.pdf
1510,258987552,UKP-SQuARE: An Interactive Tool for Teaching Question Answering,conclusion,Insight-tree,"we provide a series of use cases for teaching based on the provided models and methods by ukp-square, showing that classes can become much more interactive by using ukp-square than in conventional lectures.",,https://export.arxiv.org/pdf/2305.19748v2.pdf
1511,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,in this work we presented a comprehensive survey on the latest progress of open-domain qa (openqa) systems.,,https://arxiv.org/pdf/2101.00774v3.pdf
1512,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"in particular, we first reviewed the development of openqa and illustrated a ""retriever-reader"" architecture.",,https://arxiv.org/pdf/2101.00774v3.pdf
1513,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"moreover, we reviewed a variety of existing openqa systems as well as their different approaches.",,https://arxiv.org/pdf/2101.00774v3.pdf
1514,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"finally, we discussed some salient challenges towards openqa followed by a summary of various qa benchmarks, hoping to reveal the research gaps so as to push further progress in this field.",,https://arxiv.org/pdf/2101.00774v3.pdf
1515,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"based on  our review of prior research, we claim that openqa would continue to be a research hot-spot.",,https://arxiv.org/pdf/2101.00774v3.pdf
1516,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"in particular, single-step and multi-step neural retrievers will attract increasing attention due to the demand for more accurate retrieval of related documents.",,https://arxiv.org/pdf/2101.00774v3.pdf
1517,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"also, more end-to-end openqa systems will be developed with the advancement of deep learning techniques.",,https://arxiv.org/pdf/2101.00774v3.pdf
1518,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,knowledge enhanced openqa is very promising not only because it is helpful to generating the answer but also because it serves as the source for interpreting the obtained answer.,,https://arxiv.org/pdf/2101.00774v3.pdf
1519,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"however, how to represent and make full use of the knowledge for openqa still needs more research efforts.",,https://arxiv.org/pdf/2101.00774v3.pdf
1520,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"furthermore, to equip openqa with a dialogue-like interface that enables interaction between human users and the system for information exchange is expected to attract increasing attention, which well aligns with real world application scenarios.  ",,https://arxiv.org/pdf/2101.00774v3.pdf
1521,225040142,Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval,conclusion,Insight-tree,we provide the first in-depth analysis on information-seeking qa datasets to inspect where unanswerability arises and quantify the remaining modeling challenges.,,https://www.aclanthology.org/2021.acl-long.118.pdf
1522,225040142,Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval,conclusion,Insight-tree,"our controlled experiments identifies two remaining headrooms, answerability prediction and paragraph selection.",,https://www.aclanthology.org/2021.acl-long.118.pdf
1523,225040142,Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval,conclusion,Insight-tree,"observing a large percentage of questions are unanswerable, we provide manual analysis studying why questions are unanswerable and make suggestions to improve answer coverage: (1) going beyond wikipedia textual information as the only source of information, (2) addressing ambiguous queries instead of simply marking and leaving the questions as is, (3) enable accessing multiple documents and introducing abstractive answers for non-factoid questions.",,https://www.aclanthology.org/2021.acl-long.118.pdf
1524,225040142,Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval,conclusion,Insight-tree,"together, our work shed light on future work for information-seeking qa, both for modeling and dataset design.",,https://www.aclanthology.org/2021.acl-long.118.pdf
1525,208193847,D-NET: A Simple Framework for Improving the Generalization of Machine Reading Comprehension,conclusions,Insight-tree,"in this paper, we describe a simple baseline system that baidu submitted for the mrqa 2019 shared task.",,https://www.aclweb.org/anthology/D19-5828.pdf
1526,208193847,D-NET: A Simple Framework for Improving the Generalization of Machine Reading Comprehension,conclusions,Insight-tree,"our system is built on a framework of pre-training and fine-tuning, namely d-net.",,https://www.aclweb.org/anthology/D19-5828.pdf
1527,208193847,D-NET: A Simple Framework for Improving the Generalization of Machine Reading Comprehension,conclusions,Insight-tree,d-net employs the techniques of pre-trained lan-guage models and multi-task learning to improve the generalization of mrc models and we conduct the experiments to examine the effectiveness of these strategies.,,https://www.aclweb.org/anthology/D19-5828.pdf
1528,258841216,LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models,conclusion,Insight-tree,"in this paper, we first analysed the performance of chatgpt on logical reasoning benchmarks, as well as that using logically enhanced chain-of-thought prompting.",,https://export.arxiv.org/pdf/2305.13718v2.pdf
1529,258841216,LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models,conclusion,Insight-tree,"from the results we can conclude that though language modeling can help compress all data both with and without supervision into neural models and accept human instructions, it is still really weak in perform logical reasoning.",,https://export.arxiv.org/pdf/2305.13718v2.pdf
1530,258841216,LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models,conclusion,Insight-tree,"specifically, llms often fail at reaching correct deductions based on the given facts and rules, and cannot distil the logical reasoning structure from observed prompts to generalize to new problems.",,https://export.arxiv.org/pdf/2305.13718v2.pdf
1531,258841216,LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models,conclusion,Insight-tree,we hope these observations could bring some insights to future research on introducing logic prior into large language models.,,https://export.arxiv.org/pdf/2305.13718v2.pdf
1532,258841216,LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models,conclusion,Insight-tree,"one step further, we combine previous data-driven approach, i.e., merit+, a selfsupervised pre-training method for logical reasoning, with flan-t5-3b, and the results have also demonstrated its effectiveness.",,https://export.arxiv.org/pdf/2305.13718v2.pdf
1533,257557511,Secret-Keeping in Question Answering,conclusion and future work,Insight-tree,"we have introduced the task of secret-keeping as an important, and under-explored problem in question answering.",,https://export.arxiv.org/pdf/2303.09067v1.pdf
1534,257557511,Secret-Keeping in Question Answering,conclusion and future work,Insight-tree,"we identify a lack of suitable secretkeeping metrics and define secrecy, paranoia and information leakage to address the gap.",,https://export.arxiv.org/pdf/2303.09067v1.pdf
1535,257557511,Secret-Keeping in Question Answering,conclusion and future work,Insight-tree,"we design and implement a secret-keeping approach that is model-agnostic, only requiring access to predefined secrets, and the output of a qa system to, detect the disclosure of secrets.",,https://export.arxiv.org/pdf/2303.09067v1.pdf
1536,257557511,Secret-Keeping in Question Answering,conclusion and future work,Insight-tree,we have identified a rich field for future work in secret-keeping including:  reducing paranoia and information leakage.,,https://export.arxiv.org/pdf/2303.09067v1.pdf
1537,253098174,Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge,conclusion,Insight-tree,"in this paper, we present a new framework, core, for odqa over heterogeneous knowledge sources.",,https://export.arxiv.org/pdf/2210.12338v1.pdf
1538,231847375,Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, to tackle the multi-hop information retrieval challenge, we introduce an architecture that models a set of paragraphs as sequential data and iteratively identifies them.",,https://arxiv.org/pdf/2102.03741v1.pdf
1539,231847375,Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"specifically, we propose gated memory flow to iterative read and memorize reasoning required information without noise information interference.",,https://arxiv.org/pdf/2102.03741v1.pdf
1540,231847375,Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,we evaluate our method on both full wiki and distractor settings on the hotpotqa dataset and the method outperforms previous works by a large margin.,,https://arxiv.org/pdf/2102.03741v1.pdf
1541,231847375,Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"in the future, we will attempt to design a more complicated model to improve retrieval performance and explore more about the effect of training data with different data distribution for multi-hop information retrieval.",,https://arxiv.org/pdf/2102.03741v1.pdf
1542,211003735,BREAK It Down: A Question Understanding Benchmark,conclusion,Insight-tree,"in this paper, we presented a formalism for question understanding.",,https://www.aclweb.org/anthology/2020.tacl-1.13.pdf
1543,211003735,BREAK It Down: A Question Understanding Benchmark,conclusion,Insight-tree,"we have shown it is possible to train crowd-workers to produce such representations with high quality at scale, and created break, a benchmark for question decomposition with over 83k decompositions of questions from 10 datasets and 3 modalities (db, images, text).",,https://www.aclweb.org/anthology/2020.tacl-1.13.pdf
1544,211003735,BREAK It Down: A Question Understanding Benchmark,conclusion,Insight-tree,"we presented the utility of qdmr for both open-domain question answering and semantic parsing, and constructed a qdmr parser with reasonable performance.",,https://www.aclweb.org/anthology/2020.tacl-1.13.pdf
1545,211003735,BREAK It Down: A Question Understanding Benchmark,conclusion,Insight-tree,"qdmr proposes a promising direction for modeling question understanding, which we believe will be useful for multiple tasks in which reasoning is probed through questions.",,https://www.aclweb.org/anthology/2020.tacl-1.13.pdf
1546,248218489,Improving Passage Retrieval with Zero-Shot Question Generation,conclusions and future work,Insight-tree,"in this work, we propose upr, an approach to perform unsupervised passage re-ranking for opendomain retrieval.",,https://www.aclanthology.org/2022.emnlp-main.249.pdf
1547,248218489,Improving Passage Retrieval with Zero-Shot Question Generation,conclusions and future work,Insight-tree,"to re-rank, upr computes a relevance score for question generation conditioned on each retrieved passage using pre-trained language models.",,https://www.aclanthology.org/2022.emnlp-main.249.pdf
1548,248218489,Improving Passage Retrieval with Zero-Shot Question Generation,conclusions and future work,Insight-tree,extensive experiments across a wide range of qa datasets show that an unsupervised pipeline consisting of retriever and upr greatly outperforms strong supervised retriever models.,,https://www.aclanthology.org/2022.emnlp-main.249.pdf
1549,248218489,Improving Passage Retrieval with Zero-Shot Question Generation,conclusions and future work,Insight-tree,"in addition, upr further improves the performance of supervised retrievers.",,https://www.aclanthology.org/2022.emnlp-main.249.pdf
1550,248218489,Improving Passage Retrieval with Zero-Shot Question Generation,conclusions and future work,Insight-tree,"on the open-domain qa task, by just performing inference using re-ranked passages and a pre-trained reader model, we achieve new state-of-the-art results.",,https://www.aclanthology.org/2022.emnlp-main.249.pdf
1551,247085284,Deep Understanding based Multi-Document Machine Reading Comprehension,conclusions,Insight-tree,"in this paper, we propose a simple but effective deep understanding based multi-document mrc model.",,https://arxiv.org/pdf/2204.03494v1.pdf
1552,247085284,Deep Understanding based Multi-Document Machine Reading Comprehension,conclusions,Insight-tree,it uses neither any sophisticated technologies nor any pretrained language models.,,https://arxiv.org/pdf/2204.03494v1.pdf
1553,247085284,Deep Understanding based Multi-Document Machine Reading Comprehension,conclusions,Insight-tree,"we evaluate our model on dureader and triviaqa web, two widely used benchmark multi-document mrc datasets.",,https://arxiv.org/pdf/2204.03494v1.pdf
1554,247085284,Deep Understanding based Multi-Document Machine Reading Comprehension,conclusions,Insight-tree,experiments show that our model achieves very competitive results on both datasets.,,https://arxiv.org/pdf/2204.03494v1.pdf
1555,252873161,How (Not) To Evaluate Explanation Quality,conclusion,Insight-tree,this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face.,,https://export.arxiv.org/pdf/2210.07126v1.pdf
1556,252873161,How (Not) To Evaluate Explanation Quality,conclusion,Insight-tree,"we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate those characteristics.",,https://export.arxiv.org/pdf/2210.07126v1.pdf
1557,252873161,How (Not) To Evaluate Explanation Quality,conclusion,Insight-tree,"finally, we propose guidelines for a more effective evaluation, which we hope to inspire future work and ultimately drive the field towards reliable and meaningful explanation quality evaluation.",,https://export.arxiv.org/pdf/2210.07126v1.pdf
1558,252873161,How (Not) To Evaluate Explanation Quality,conclusion,Insight-tree,"our discussion is backed up with examples, well-known theories and empirical findings from a crowdsourced case study that we conducted for the example of explainable question answering systems.",,https://export.arxiv.org/pdf/2210.07126v1.pdf
1559,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,"this paper introduces a new graph-based recurrent retrieval approach, which retrieves reasoning paths over the wikipedia graph to answer multi-hop open-domain questions.",,https://export.arxiv.org/pdf/1911.10470v2.pdf
1560,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,our retriever model learns to sequentially retrieve evidence paragraphs to form the reasoning path.,,https://export.arxiv.org/pdf/1911.10470v2.pdf
1561,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,"subsequently, our reader model re-ranks the reasoning paths, and it determines the final answer as the one extracted from the best reasoning path.",,https://export.arxiv.org/pdf/1911.10470v2.pdf
1562,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,our experimental results significantly advance the state of the art on hotpotqa by more than 14 points absolute gain on the full wiki setting.,,https://export.arxiv.org/pdf/1911.10470v2.pdf
1563,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,"our approach also achieves the state-of-the-art performance on squad open and natural questions open without any architectural changes, demonstrating the robustness of our method.",,https://export.arxiv.org/pdf/1911.10470v2.pdf
1564,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,"our method provides insights into the underlying entity relationships, and the discrete reasoning paths are helpful in interpreting our framework's reasoning process.",,https://export.arxiv.org/pdf/1911.10470v2.pdf
1565,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,future work involves end-to-end training of our graph-based recurrent retriever and reader for improving upon our current two-stage training.,,https://export.arxiv.org/pdf/1911.10470v2.pdf
1566,5742355,Back-translation for discovering distant protein homologies,conclusions,Insight-tree,"in this paper, we addressed the problem of finding distant protein homologies, in particular affected by frameshift events, from a codon evolution perspective.",,https://arxiv.org/pdf/1001.4603v1.pdf
1567,5742355,Back-translation for discovering distant protein homologies,conclusions,Insight-tree,"we search for protein common origins by implicitly aligning all their putative coding dna sequences, stored in efficient data structures called back-translation graphs.",,https://arxiv.org/pdf/1001.4603v1.pdf
1568,5742355,Back-translation for discovering distant protein homologies,conclusions,Insight-tree,"our approach relies on a dynamic programming alignment algorithm for these graphs, which involves a non-monotonic gap penalty that handles differently frameshifts and full codon indels.",,https://arxiv.org/pdf/1001.4603v1.pdf
1569,5742355,Back-translation for discovering distant protein homologies,conclusions,Insight-tree,"we designed a powerful translation-dependent scoring function for nucleotide pairs, based on codon substitution models, whose purpose is to reflect the expected dynamics of coding dna sequences.",,https://arxiv.org/pdf/1001.4603v1.pdf
1570,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,"in this work, we propose probr, a novel probabilistic graph reasoning framework for joint question answering and proof generation.",,https://arxiv.org/pdf/2107.02418v1.pdf
1571,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,"probr defines a joint distribution over all possible answers and proofs, which can directly characterize the interaction between answers and proofs.",,https://arxiv.org/pdf/2107.02418v1.pdf
1572,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,experiments prove the effectiveness of proposed probr.,,https://arxiv.org/pdf/2107.02418v1.pdf
1573,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,1,,https://arxiv.org/pdf/2107.02418v1.pdf
1574,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,"in all ablation experiments, probr achieved the best qa performance, demonstrating that probr can capture critical information for question answering in a variety of settings.",,https://arxiv.org/pdf/2107.02418v1.pdf
1575,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,"however, since some of the dataset are artificially synthesized, it is difficult to guarantee that probr will work in the real dataset as well.",,https://arxiv.org/pdf/2107.02418v1.pdf
1576,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,we leave it as future work.,,https://arxiv.org/pdf/2107.02418v1.pdf
1577,244119330,What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension,conclusion,Insight-tree,"this paper aims at addressing the newly discovered difficulty of the boundary ambiguity between sentences and sub-sentences, which exists in many languages to different extents and essentially limits the performance of span extraction mrc models, especially in chinese environment.",,https://aclanthology.org/2021.findings-emnlp.202.pdf
1578,244119330,What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension,conclusion,Insight-tree,we apply explicit span-sentence predication (esp) to enhance model's ability of precisely locating sentences containing the target span.,,https://aclanthology.org/2021.findings-emnlp.202.pdf
1579,244119330,What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension,conclusion,Insight-tree,"our proposed model design is evaluated on chinese span extraction mrc benchmark, cmrc 2018.",,https://aclanthology.org/2021.findings-emnlp.202.pdf
1580,244119330,What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension,conclusion,Insight-tree,the experimental results show that our model significantly improves both em and f1 scores compared with strong baselines and helps achieve a new state-of-the-art performance.,,https://aclanthology.org/2021.findings-emnlp.202.pdf
1581,244119330,What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension,conclusion,Insight-tree,our method also shows generality and potential in dealing with other languages.,,https://aclanthology.org/2021.findings-emnlp.202.pdf
1582,244119330,What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension,conclusion,Insight-tree,this work highlights the research line of further improving challenging mrc by analyzing specific linguistics phenomena.,,https://aclanthology.org/2021.findings-emnlp.202.pdf
1583,248571884,Better Retrieval May Not Lead to Better Question Answering,conclusion,Insight-tree,"in this work we investigate the impact of evidence quality on question answering on two open-domain qa datasets, strategyqa and hotpotqa.",,https://arxiv.org/pdf/2205.03685v1.pdf
1584,248571884,Better Retrieval May Not Lead to Better Question Answering,conclusion,Insight-tree,results show that (1) strategyqa is less sensitive to the evidence quality than hotpotqa and (2) boolean questions are less sensitive to the evidence quality than extractive questions.,,https://arxiv.org/pdf/2205.03685v1.pdf
1585,248571884,Better Retrieval May Not Lead to Better Question Answering,conclusion,Insight-tree,further study shows this is mainly because the model does not yield a sufficiently high score on those examples with perfect evidence retrieval.,,https://arxiv.org/pdf/2205.03685v1.pdf
1586,248571884,Better Retrieval May Not Lead to Better Question Answering,conclusion,Insight-tree,"finally, we recommend that for the implicit decomposition open-domain question answering problems with boolean questions such as strategyqa, researchers start by improving qa performance given the gold paragraphs rather than improving evidence quality.",,https://arxiv.org/pdf/2205.03685v1.pdf
1587,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"in this paper we introduce cronquestions, a new dataset for temporal knowledge graph question answering.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1588,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"while there exist some temporal kgqa datasets, they are all based on non-temporal kgs (e.g., freebase) and have relatively few questions.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1589,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,our dataset consists of both a temporal kg as well as a large set of temporal questions requiring various structures of reasoning.,,https://www.aclanthology.org/2021.acl-long.520.pdf
1590,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"in order to develop such a large dataset, we used a synthetic   generation procedure, leading to a question distribution that is artificial from a semantic perspective.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1591,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"however, having a large dataset provides an opportunity to train models, rather than just evaluate them.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1592,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,we experimentally show that increasing the training dataset size steadily improves the performance of certain methods on the tkgqa task.,,https://www.aclanthology.org/2021.acl-long.520.pdf
1593,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,we first apply large pre-trained lm based qa methods on our new dataset.,,https://www.aclanthology.org/2021.acl-long.520.pdf
1594,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"then we inject kg embeddings, both temporal and non-temporal, into these lms and observe significant improvement in performance.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1595,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"we also propose a new method, cronkgqa, that is able to leverage temporal kg embeddings to perform tkgqa.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1596,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"in our experiments, cronkgqa outperforms all baselines.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1597,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"these results suggest that kg embeddings can be effectively used to perform temporal kgqa, although there remains significant scope for improvement when it comes to complex reasoning questions.",,https://www.aclanthology.org/2021.acl-long.520.pdf
1598,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,"in this work, we propose a modular model chain-of-skills (cos) that learns five reusable skills for odqa via multi-task learning.",,https://export.arxiv.org/pdf/2305.03130v2.pdf
1599,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,"to reduce task interference, we design a new parameterization for skill modules.",,https://export.arxiv.org/pdf/2305.03130v2.pdf
1600,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,we also show that skills learned by cos can be flexibly chained together to better fit the target task.,,https://export.arxiv.org/pdf/2305.03130v2.pdf
1601,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,cos can directly perform superior zero-shot retrieval using multitask self-supervision on wikipedia.,,https://export.arxiv.org/pdf/2305.03130v2.pdf
1602,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,"when finetuned on multiple datasets, cos achieves sota results across the board.",,https://export.arxiv.org/pdf/2305.03130v2.pdf
1603,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,"for future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (talmor et al., 2022) and biomedical retrieval (nentidis et al., 2020;zhang et al., 2022b).",,https://export.arxiv.org/pdf/2305.03130v2.pdf
1604,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,group at microsoft research for their helpful discussions and anonymous reviewers for their valuable suggestions on this paper.,,https://export.arxiv.org/pdf/2305.03130v2.pdf
1605,244896105,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,conclusions,Insight-tree,"in this work, we propose an alternative to multidataset models for multi-skill qa.",,https://www.aclanthology.org/2023.eacl-main.259.pdf
1606,244896105,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,conclusions,Insight-tree,we propose to combine expert agents to create a collaborative system for question answering (qa) called metaqa.,,https://www.aclanthology.org/2023.eacl-main.259.pdf
1607,244896105,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,conclusions,Insight-tree,"it considers questions, answer predictions, and confidence scores from the agents to select the best answer to a question.",,https://www.aclanthology.org/2023.eacl-main.259.pdf
1608,244896105,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,conclusions,Insight-tree,"through quantitative experiments, we show that our model avoids the limitations of multi-dataset models and outperforms the baselines thanks to the agent collaboration established.",,https://www.aclanthology.org/2023.eacl-main.259.pdf
1609,244896105,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,conclusions,Insight-tree,"additionally, since metaqa learns to match questions with answers instead of end-to-end qa, it is highly data-efficient to train.",,https://www.aclanthology.org/2023.eacl-main.259.pdf
1610,244896105,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,conclusions,Insight-tree,"we leave as future work: i) combining partially correct answer predictions to generate a better one, ii) adding new agents without retraining metaqa by fixing most of the weights and only training the weights of the new agent selection network, and iii) identifying a priori agents that are likely to give an incorrect answer to skip them at run-time.",,https://www.aclanthology.org/2023.eacl-main.259.pdf
1611,237552879,Evaluation Paradigms in Question Answering,conclusion,Insight-tree,we identify two core motivations for qa research over the past twenty years.,,https://www.aclanthology.org/2021.emnlp-main.758.pdf
1612,237552879,Evaluation Paradigms in Question Answering,conclusion,Insight-tree,"we link one to the usercentered goals of the cranfield paradigm and propose the manchester paradigm to describe research working towards building human-like, intelligent qa systems.",,https://www.aclanthology.org/2021.emnlp-main.758.pdf
1613,237552879,Evaluation Paradigms in Question Answering,conclusion,Insight-tree,"in at least the short-term, this distinction is important as it illuminates the goals of industry and academic stakeholders; ultimately, this makes it easier to ensure that both research agendas are valued.",,https://www.aclanthology.org/2021.emnlp-main.758.pdf
1614,237552879,Evaluation Paradigms in Question Answering,conclusion,Insight-tree,"in the long term, we suspect that the best qa agents will benefit from the insights of user-oriented tasks and the longer-range efforts towards natural language understanding (bender and koller, 2020; linzen, 2020).",,https://www.aclanthology.org/2021.emnlp-main.758.pdf
1615,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,conclusion,Insight-tree,this work proposes a novel method that extracts distant and incidental signals from parallel news to facilitate general question representation.,,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1616,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,conclusion,Insight-tree,such parallel news signals intuitively bridge the reasoning gap in pre-trained language models due to reporting biases.,,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1617,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,conclusion,Insight-tree,"to support this intuition, we train a model named decompt5 on such distant supervision and show that it improves 20%-30% on two semantic parsing benchmarks, namely overnight and torque, that directly evaluate query understanding.",,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1618,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,conclusion,Insight-tree,"with decompt5 as the basis, we design a well-motivated question-answering pipeline decompentail that follows a decomposition, correction, and entailment scheme.",,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1619,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,conclusion,Insight-tree,"we show that decompentail improves on strategyqa and hotpotqa by 3.7% and 8%, respectively.",,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1620,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,"in this section, we discuss some of the limitations of our work, and motivate future works.",,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1621,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,limited question formats.,,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1622,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,our proposed qa pipeline operates on binary yes/no questions.,,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1623,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,"while binary questions are very general, as most other questions can be re-written into similar forms, such transformations have not been designed or evaluated, which motivates future works.",,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1624,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,limited factual correction coverage.,,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1625,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,we use gpt-3 as the backbone for our factual correction step.,,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1626,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,"although it is shown to be effective, it is not as deterministic as wikipedia-based ir approaches, and we cannot easily interpret why it makes mistakes and understand how to improve.",,https://www.aclanthology.org/2022.emnlp-main.142.pdf
1627,211572791,DC-BERT: DECOUPLING QUESTION AND DOCUMENT FOR EFFICIENT CONTEXTUAL ENCODING,conclusion,Insight-tree,this paper introduces dc-bert to decouple question and document for efficient contextual encoding.,,https://arxiv.org/pdf/2002.12591v1.pdf
1628,211572791,DC-BERT: DECOUPLING QUESTION AND DOCUMENT FOR EFFICIENT CONTEXTUAL ENCODING,conclusion,Insight-tree,"dc-bert has been successfully applied to document retrieval, a key component in opendomain qa, achieving 10x speedup while retaining most of the qa performance.",,https://arxiv.org/pdf/2002.12591v1.pdf
1629,211572791,DC-BERT: DECOUPLING QUESTION AND DOCUMENT FOR EFFICIENT CONTEXTUAL ENCODING,conclusion,Insight-tree,"with the capability of processing high-throughput of questions each with a large collection of retrieved documents, dc-bert brings open-domain qa one step closer to serving real-world applications.",,https://arxiv.org/pdf/2002.12591v1.pdf
1630,247518855,AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension,conclusion,Insight-tree,"to meet the challenge of reasoning-based mrc, we presented a neural-symbolic approach where neural and symbolic reasoning mutually and iteratively reinforce each other via our new adalogn model.",,https://www.aclanthology.org/2022.acl-long.494.pdf
1631,247518855,AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension,conclusion,Insight-tree,we also enhanced graph-based neural reasoning with a novel subgraph-to-node message passing mechanism.,,https://www.aclanthology.org/2022.acl-long.494.pdf
1632,247518855,AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension,conclusion,Insight-tree,"since these ideas are quite general, we believe they have great potential for a variety of applications beyond mrc, e.g., link prediction.",,https://www.aclanthology.org/2022.acl-long.494.pdf
1633,258866037,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,conclusion,Insight-tree,we demonstrate the effectiveness of iter-retgen in answering questions with complex information needs.,,https://export.arxiv.org/pdf/2305.15294v1.pdf
1634,258866037,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,conclusion,Insight-tree,"despite simple, iter-retgen outperforms retrieval-augmented baselines with a more complex workflow, which we believe could serve as a strong baseline for future research on retrievalaugmented generation.",,https://export.arxiv.org/pdf/2305.15294v1.pdf
1635,258866037,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,conclusion,Insight-tree,we also show that iter-retgen can be further improved by generationaugmented retrieval adaptation.,,https://export.arxiv.org/pdf/2305.15294v1.pdf
1636,253523028,QAMELEON : Multilingual QA with Only 5 Examples,conclusions,Insight-tree,"in this work, we examined the ability of pretrained language models to generate synthetic data for bootstrapping multilingual qa systems, with as few as five examples in a new target language.",,https://export.arxiv.org/pdf/2211.08264v1.pdf
1637,235435668,Question Answering Infused Pre-training of General-Purpose Contextualized Representations,conclusion,Insight-tree,"in this work, we pre-trained token-level contextual representations that are useful for downstream fewshot learning.",,https://www.aclanthology.org/2022.findings-acl.59.pdf
1638,235435668,Question Answering Infused Pre-training of General-Purpose Contextualized Representations,conclusion,Insight-tree,our key idea was to use questionanswer pairs to define what information should be encoded in passage representations.,,https://www.aclanthology.org/2022.findings-acl.59.pdf
1639,235435668,Question Answering Infused Pre-training of General-Purpose Contextualized Representations,conclusion,Insight-tree,"we showed that these representations are useful for a variety of standard nlp tasks in zero-and few-shot settings, including paraphrase detection, named entity recognition, and sentiment analysis, across nine total datasets.",,https://www.aclanthology.org/2022.findings-acl.59.pdf
1640,235435668,Question Answering Infused Pre-training of General-Purpose Contextualized Representations,conclusion,Insight-tree,"looking forward, we hope to see more work on designing pre-training objectives that align with downstream needs for few-shot learning.",,https://www.aclanthology.org/2022.findings-acl.59.pdf
1641,221749191,Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning,conclusions,Insight-tree,progress in multi-hop qa under the reading comprehension setting relies on understanding and quantifying the types of undesirable reasoning current models may perform.,,https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf
1642,221749191,Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning,conclusions,Insight-tree,"this work introduced a formalization of disconnected reasoning, a form of bad reasoning prevalent in multi-hop models.",,https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf
1643,221749191,Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning,conclusions,Insight-tree,it showed that a large portion of current progress in multifact reasoning can be attributed to disconnected reasoning.,,https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf
1644,221749191,Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning,conclusions,Insight-tree,"using a notion of contrastive sufficiency, it showed how to automatically transform existing support-annotated multi-hop datasets to create a more difficult and less cheatable dataset that results in reduced disconnected reasoning.",,https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf
1645,246680186,Logical Reasoning for Task Oriented Dialogue Systems,conclusions,Insight-tree,"in this paper, we proposed an architecture for the integration of a reasoning model in task-oriented dialogue systems.",,https://www.aclanthology.org/2022.ecnlp-1.10.pdf
1646,246680186,Logical Reasoning for Task Oriented Dialogue Systems,conclusions,Insight-tree,"we formulated the problem as a sequence prediction problem given a user query and context, and presented an approach for generating data and fine-tuning generative models to reason over a set of facts in the dialogue context.",,https://www.aclanthology.org/2022.ecnlp-1.10.pdf
1647,246680186,Logical Reasoning for Task Oriented Dialogue Systems,conclusions,Insight-tree,we demonstrated our approach for a shopping assistant and reported experimental results for different formulations of the problem.,,https://www.aclanthology.org/2022.ecnlp-1.10.pdf
1648,246680186,Logical Reasoning for Task Oriented Dialogue Systems,conclusions,Insight-tree,"we showed that these models can learn to do logical reasoning to 1) answer questions from the dialogue context when all the information is available, 2) extract constraints when partial information is available, and 3) delegate to the dialogue policy when no reasoning is required.",,https://www.aclanthology.org/2022.ecnlp-1.10.pdf
1649,256459309,Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data,conclusion,Insight-tree,"recall the desiderata for our algorithm, expressed in the introduction: our algorithm should (1) make no assumptions on the available auxiliary data a-priori, (2) scale well with the number of auxiliary datasets, and (3) add minimal memory and computational overhead.",,https://export.arxiv.org/pdf/2302.00674v3.pdf
1650,256459309,Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data,conclusion,Insight-tree,"(1) when designing our algorithm, we purposefully formulate the problem as a multi-armed bandit.",,https://export.arxiv.org/pdf/2302.00674v3.pdf
1651,256459309,Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data,conclusion,Insight-tree,"mab algorithms, in general, make no assumptions on the quality of rewards and, in particular, exp3 even assumes that the auxiliary datasets will play an adversarial role when returning rewards.",,https://export.arxiv.org/pdf/2302.00674v3.pdf
1652,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,limitations & future work,Insight-tree,"as other parameter-efficient adaptation methods (see 4) may outperform prompttuning in specific situations, it would be interesting to test whether an approach similar to spot could extend successfully to these methods.",,https://arxiv.org/pdf/2110.07904v2.pdf
1653,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,limitations & future work,Insight-tree,"at the same time, we believe that prompttuning has its own merit.",,https://arxiv.org/pdf/2110.07904v2.pdf
1654,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,limitations & future work,Insight-tree,"as pre-trained language models become larger and larger, some advantages of prompttuning over other methods are: (1) among current methods with learnable parameters, prompttuning is the most parameter efficient, requiring less than 0.01% task-specific parameters for most model sizes.(2) prompttuning is simpler than other methods, as it does not modify the internal model architecture (cf.",,https://arxiv.org/pdf/2110.07904v2.pdf
1655,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,limitations & future work,Insight-tree,"the prefix-tuning method of li and liang (2021), which adds a prefix to each layer of both the transformer encoder and decoder); as such, prompttuning allows mixed-task inference and facilitates transfer learning between tasks.(3) as model capacity increases, prompttuning becomes more competitive with modeltuning; to the best of our knowledge, this has not been shown for other methods.",,https://arxiv.org/pdf/2110.07904v2.pdf
1656,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,limitations & future work,Insight-tree,"(4) soft prompts could possibly be interpreted as natural language instructions.additionally, since our prompt-based task embedding approach does not capture all of the factors that influence task transferability, we leave further exploration of other task embedding methods to future work.",,https://arxiv.org/pdf/2110.07904v2.pdf
1657,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"in this paper, we study transfer learning in the context of prompt tuning.",,https://arxiv.org/pdf/2110.07904v2.pdf
1658,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,we show that scale is not necessary for prompttuning to match the performance of modeltuning.,,https://arxiv.org/pdf/2110.07904v2.pdf
1659,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"on superglue, our spot approach matches or even exceeds the performance of modeltuning by a large margin across model sizes while being more parameter-efficient.",,https://arxiv.org/pdf/2110.07904v2.pdf
1660,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,our large-scale study on task transferability indicates that tasks can benefit each other via prompt transfer in various scenarios.,,https://arxiv.org/pdf/2110.07904v2.pdf
1661,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"finally, we demonstrate that task prompts can be interpreted as task embeddings to formalize the similarity between tasks.",,https://arxiv.org/pdf/2110.07904v2.pdf
1662,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,we propose a simple yet efficient retrieval approach that measures task similarity to identify which source tasks could confer benefits to a novel target task.,,https://arxiv.org/pdf/2110.07904v2.pdf
1663,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"taken as a whole, we hope that our work will spur more research into prompt-based transfer learning.   ",,https://arxiv.org/pdf/2110.07904v2.pdf
1664,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"(raffel et al., 2020), i.e., learning rate 0.001, adafactor optimizer with pre-training parameter states restored, and dropout probability 0.1.",,https://arxiv.org/pdf/2110.07904v2.pdf
1665,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"to improve the model tuning baselines, we perform a sweep over the batch size hyperparameter and select 2 16 tokens per batch, following lester et al.",,https://arxiv.org/pdf/2110.07904v2.pdf
1666,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,-2021,,https://arxiv.org/pdf/2110.07904v2.pdf
1667,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"table 5 shows the performance of our spot xxl superglue submission, along with several strong competitors from the public superglue leaderboard.",,https://arxiv.org/pdf/2110.07904v2.pdf
1668,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"apart from the human baseline, the top-7 submissions all tune >3b parameters directly on the final tasks.",,https://arxiv.org/pdf/2110.07904v2.pdf
1669,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"only three previous superglue submissions use parameter efficient adaptation, in the sense of tuning <1m parameters on the final tasks; all other submissions tune >50m parameters.",,https://arxiv.org/pdf/2110.07904v2.pdf
1670,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"17 17 the ""ailabs team, transformers"" submission is listed as tuning 3m parameters, but we suspect this is in error, as the our spot submission achieves a score of 89.2, which far exceeds all other parameter-efficient adaptation methods, including gpt-3, which benefits from over 10 more frozen parameters (although it uses no tuned parameters).",,https://arxiv.org/pdf/2110.07904v2.pdf
1671,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"compared to warp (hambardzumyan et al., 2021), our spot approach tunes 16 more parameters (410k vs. 25k), and benefits from 50 more frozen parameters.",,https://arxiv.org/pdf/2110.07904v2.pdf
1672,229923145,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,conclusion,Insight-tree,"in this paper, we develop general methods to improve the performance of large pre-trained seq2seq models for jointly producing nl rationales and answer predictions.",,https://www.aclanthology.org/2021.emnlp-main.301.pdf
1673,229923145,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,conclusion,Insight-tree,"specifically, we introduce sentence markers into seq2seq models to tackle explanation fabrication, we enable larger input passage sizes using the fusion-in-decoder architecture, and we infuse knowledge by fine-tuning on restructured qa datasets.",,https://www.aclanthology.org/2021.emnlp-main.301.pdf
1674,229923145,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,conclusion,Insight-tree,we show that a universal model can perform favourably compared to the best task-specific fine-tuned models.,,https://www.aclanthology.org/2021.emnlp-main.301.pdf
1675,229923145,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,conclusion,Insight-tree,"our methods improve the state of the art on rationale extraction metrics and task accuracy on multiple eraser benchmarks while reducing the extent to which seq2seq models fabricate explanations to justify incorrect predictions, thereby improving the reliability and verifiability of the generated rationales.",,https://www.aclanthology.org/2021.emnlp-main.301.pdf
1676,218487535,Obtaining Faithful Interpretations from Compositional Neural Networks,conclusion,Insight-tree,"we introduce the concept of module-wise faithfulness, a systematic evaluation of faithfulness in neural module networks (nmns) for visual and textual reasoning.",,https://www.aclweb.org/anthology/2020.acl-main.495.pdf
1677,218487535,Obtaining Faithful Interpretations from Compositional Neural Networks,conclusion,Insight-tree,we show that nave training of nmns does not produce faithful modules and propose several techniques to improve module-wise faithfulness in nmns.,,https://www.aclweb.org/anthology/2020.acl-main.495.pdf
1678,218487535,Obtaining Faithful Interpretations from Compositional Neural Networks,conclusion,Insight-tree,we show how our approach leads to much higher module-wise faithfulness at a low cost to performance.,,https://www.aclweb.org/anthology/2020.acl-main.495.pdf
1679,218487535,Obtaining Faithful Interpretations from Compositional Neural Networks,conclusion,Insight-tree,"we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models.",,https://www.aclweb.org/anthology/2020.acl-main.495.pdf
1680,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,"in this paper, we propose a generator of multi-hop knowledge paths, which provides structured evidence for answering commonsense questions.",,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1681,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,"the generator, learned by fine-tuning gpt-2 on random walks sampled from conceptnet, produces a path between each pair of question and answer entities.",,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1682,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,all generated paths are aggregated into a knowledge embedding and fused with a context embedding given by a text encoder for classification.,,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1683,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,our qa framework enhanced with this generator outperformes both pre-trained language models and prior kg-augmented methods on two commonsense qa benchmarks.,,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1684,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,the accuracy gain increases with less training data.,,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1685,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,"furthermore, automatic-and human-based evaluations of the generated paths yield high scores for their validity, novelty, and relevance.",,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1686,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,future research should investigate how to optimally fuse the knowledge and the context embeddings.,,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1687,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,"it should also address the ambiguity of the entity mentions in the questions, the answers, and the lexical nodes in conceptnet.",,https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
1688,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,"using propositional logic corpora to simulate a controlled language modeling setting, we ask: 1) do properties of the training corpus affect lms' abilities to differentiate the meanings of logical operators?",,https://aclanthology.org/2021.acl-short.21.pdf
1689,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,and 2) do any training corpora lead to models that differentiate these meanings to a satisfactory degree?,,https://aclanthology.org/2021.acl-short.21.pdf
1690,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,"our results imply a positive answer to (1): models trained on corpora generated with different constraints appear to perform differently at the task of separating  from . however, these differences are a function of both data and model.",,https://aclanthology.org/2021.acl-short.21.pdf
1691,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,"for example, the transformer architecture seems better able to learn from weaker signal (corpora generated only with a truthfulness constraint), while lstms require more explicit signal (direct access to truth values).",,https://aclanthology.org/2021.acl-short.21.pdf
1692,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,"on question (2), our results are largely negative for the syntactically similar operators.",,https://aclanthology.org/2021.acl-short.21.pdf
1693,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,even the most semantically transparent training data did not enable models to separate the representations of symbols with similar form but different meaning.,,https://aclanthology.org/2021.acl-short.21.pdf
1694,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,"only the small transformer trained on the explicit grounding condition can perfectly differentiate  from  at the lexical level, despite the task's controlled nature.",,https://aclanthology.org/2021.acl-short.21.pdf
1695,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,"however, every model did separate  from both  and , illustrating how syntactic differences can support differentiation of meaning.",,https://aclanthology.org/2021.acl-short.21.pdf
1696,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,in this paper we investigate whether meta-learning applied to multi-task instructional learning (mtil) can boost the generalizability of lms to unseen tasks in a zero-shot setting.,,https://export.arxiv.org/pdf/2210.11617v1.pdf
1697,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,"specifically, we evaluate mtil in three directions with maml, hnet and hnet-maml.",,https://export.arxiv.org/pdf/2210.11617v1.pdf
1698,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,"to test the generalization ability, we consider two sets of training and evaluation task sets and through extensive experiments on the niv2 dataset, show that meta-learning can significantly boost the performance by increasing the effectiveness of instructions particularly under strict zero shot conditions and for ""hard"" tasks.",,https://export.arxiv.org/pdf/2210.11617v1.pdf
1699,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,"while the models perform relatively well under zero-shot conditions, the performance is far from fully supervised models.",,https://export.arxiv.org/pdf/2210.11617v1.pdf
1700,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,it remains to be seen at what point we can match fully supervised models (for example using a k-shot setting).,,https://export.arxiv.org/pdf/2210.11617v1.pdf
1701,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,"in addition, the impact of hnet-maml on the bart-large model was lower.",,https://export.arxiv.org/pdf/2210.11617v1.pdf
1702,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,it will be interesting to see how meta-learning scales with model sizes and whether the additional bandwidth from larger models can negate the impact of meta-learning in encoding and utilizing instructions.,,https://export.arxiv.org/pdf/2210.11617v1.pdf
1703,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,this is subject of future work.,,https://export.arxiv.org/pdf/2210.11617v1.pdf
1704,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,limitations,Insight-tree,there are several limitations of the proposed metalearning based approaches in its present form. computation and memory overhead: metalearning approaches have higher resource requirements which can limit the usage specially for larger models.,,https://export.arxiv.org/pdf/2210.11617v1.pdf
1705,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,limitations,Insight-tree,"for example with bartlarge, the hnet-maml model on a single gpu is inefficient to train since we have to use small batch sizes which leads to lower performance. regressions with easy tasks: we see some regression in metrics for the easy tasks.",,https://export.arxiv.org/pdf/2210.11617v1.pdf
1706,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,limitations,Insight-tree,further analysis and research is needed to understand the factors and improve the models such that model enhancements are uniform across tasks. hyper-parameter tuning: meta learning models have more hyper-parameters and thus might be more difficult to tune than the standard training approach. overall zero-shot performance: the zero-shot performance even with the best meta-learning approaches is quite far from state-of-the-art results.,,https://export.arxiv.org/pdf/2210.11617v1.pdf
1707,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,limitations,Insight-tree,it will be interesting to see at what point (e.g. with k-shot learning) the performance can match a fully supervised model.,,https://export.arxiv.org/pdf/2210.11617v1.pdf
1708,252519173,PROMPTAGATOR : FEW-SHOT DENSE RETRIEVAL FROM 8 EXAMPLES,conclusion and discussions,Insight-tree,"in this paper, we have presented promptagator, a novel approach to few-shot retrieval.",,https://export.arxiv.org/pdf/2209.11755v1.pdf
1709,252519173,PROMPTAGATOR : FEW-SHOT DENSE RETRIEVAL FROM 8 EXAMPLES,conclusion and discussions,Insight-tree,"we showed that it is possible to create task-specific, end-to-end retrievers with only a few annotated examples.",,https://export.arxiv.org/pdf/2209.11755v1.pdf
1710,252519173,PROMPTAGATOR : FEW-SHOT DENSE RETRIEVAL FROM 8 EXAMPLES,conclusion and discussions,Insight-tree,"the few-shot examples, amplified by prompt-based llm query generation, simplifies the complexity of training neural retrievers for a new tasks and leads to promising retrieval performance gains.",,https://export.arxiv.org/pdf/2209.11755v1.pdf
1711,252519173,PROMPTAGATOR : FEW-SHOT DENSE RETRIEVAL FROM 8 EXAMPLES,conclusion and discussions,Insight-tree,"it hopefully inspires future research to further push the limit of few-shot retrieval, towards generalizable retrieval systems that can seamlessly and efficiently adapt to many tasks.",,https://export.arxiv.org/pdf/2209.11755v1.pdf
1712,252693442,"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",conclusions,Insight-tree,"in this paper, we introduce mintaka, an end-toend question answering dataset linked to wikidata.",,https://www.aclanthology.org/2022.coling-1.138.pdf
1713,252693442,"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",conclusions,Insight-tree,"mintaka addresses an important gap in qa datasets by being large-scale, complex, naturally-elicited, and multilingual.",,https://www.aclanthology.org/2022.coling-1.138.pdf
1714,252693442,"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",conclusions,Insight-tree,"our baselines show that there is room for improvement in existing methods to handle complex questions, especially in all languages.",,https://www.aclanthology.org/2022.coling-1.138.pdf
1715,252693442,"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",conclusions,Insight-tree,"with the release of mintaka, we hope to encourage researchers to continue pushing the boundaries of question answering to handle more complex questions in more languages.",,https://www.aclanthology.org/2022.coling-1.138.pdf
1716,250390946,Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering,conclusion,Insight-tree,"in this paper, we study cqa over structured and unstructured knowledge sources (i.e., kb and text particularly), and focus on studying the knowledge transfer between different knowledge sources.",,https://www.aclanthology.org/2022.suki-1.7.pdf
1717,250390946,Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering,conclusion,Insight-tree,"to facilitate the transfer, we first propose a unified cqa framework, simultqa to bridge kbqa and textqa systems.",,https://www.aclanthology.org/2022.suki-1.7.pdf
1718,250390946,Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering,conclusion,Insight-tree,empirical results show that knowledge transfer enables substantial improvements on low-resource domains.,,https://www.aclanthology.org/2022.suki-1.7.pdf
1719,250390946,Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering,conclusion,Insight-tree,"more importantly, we conduct fine-grained analyses to shed more light on how knowledge is transferred to inspire future research on knowledge transfer between sources, and we conclude the paper with insights for future cqa datasets and systems.",,https://www.aclanthology.org/2022.suki-1.7.pdf
1720,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"limited by the size of knowledge bases, some knowledge gaps cannot be filled effectively in practice.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1721,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,we are studying a creative reasoning mechanism to solve this problem.,,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1722,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"in addition, fact verification is a very open and challenging task.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1723,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"it needs not only the support of linguistic features and background knowledge, but also the support of more complex multi-dimension information, such as social content and spatiotemporal information.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1724,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"for example, claims evolve over time, and what was fake yesterday is true today.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1725,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"limited by the size of knowledge bases, some knowledge gaps cannot be filled effectively in practice.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1726,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,we are studying a creative reasoning mechanism to solve this problem.,,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1727,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"in addition, fact verification is a very open and challenging task.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1728,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"it needs not only the support of linguistic features and background knowledge, but also the support of more complex multi-dimension information, such as social content and spatiotemporal information.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1729,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"for example, claims evolve over time, and what was fake yesterday is true today.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1730,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,conclusions,Insight-tree,"in this study, a novel graph-based reasoning framework was proposed for complex fact verification (fv), which can dynamically supplement useful knowledge in the case of knowledge gaps.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1731,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,conclusions,Insight-tree,the framework retrieves and fills the knowledge gaps between the given claim and evidence to construct the collaborative graph before propagating and aggregating sequential information.,,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1732,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,conclusions,Insight-tree,"experiments have shown that dkar can effectively solve the ""not enough information"" mislabeling problem in the fv task and outperform other baselines.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1733,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,conclusions,Insight-tree,"in addition, our approach shows outstanding advantages in a small sample and heterogeneous web text sources.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1734,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,conclusions,Insight-tree,"our research first illustrates that dynamic knowledge supplementation plays an important role in complex fv tasks, which contributes to the study of reasoning methods driven by data and knowledge for fact verification.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1735,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,conclusions,Insight-tree,"it is expected that our first exploration encourages others to expand upon our work, and to further shed light on the broader and more challenging goal of complex and practical fv tasks with joint data and knowledge.",,https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
1736,250390431,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,conclusion and future work,Insight-tree,temporal reading comprehension plays a critical role in natural language understanding.,,https://www.aclanthology.org/2022.naacl-main.28.pdf
1737,250390431,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,conclusion and future work,Insight-tree,"in this paper, we propose a precise question understanding method to tackle the trc problem.",,https://www.aclanthology.org/2022.naacl-main.28.pdf
1738,250390431,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,conclusion and future work,Insight-tree,"specifically, we encode temporal ordering questions into repre-sentations of referred events and concerned temporal relations, based on which candidate answers are evaluated in terms of their temporal relations to the referred events.",,https://www.aclanthology.org/2022.naacl-main.28.pdf
1739,250390431,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,conclusion and future work,Insight-tree,"in addition, a contrastive loss is employed to empower the model to capture essential differences among temporal relations.",,https://www.aclanthology.org/2022.naacl-main.28.pdf
1740,250390431,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,conclusion and future work,Insight-tree,experimental results based on four pre-trained models verify the effectiveness of our proposed approach.,,https://www.aclanthology.org/2022.naacl-main.28.pdf
1741,250390431,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,conclusion and future work,Insight-tree,"in the future, we will investigate general approaches to handle more diverse temporal relation understanding problems and improve the passage understanding capability for temporal reading comprehension.",,https://www.aclanthology.org/2022.naacl-main.28.pdf
1742,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"we have previously solved the crystal structures of both rhodocytin and clec-2, but there is no crystallographic structure of the rhodocytin-clec-2 complex.",,
1743,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"using three dimensional structures of both rhodocytin and clec-2, we have generated models of the likely modes of interaction of the venom protein and its receptor on platelets and have investigated and analysed the computational models of the interaction which we have generated [30,32,33].",,
1744,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"using a set of analytical algorithms and approaches, we have assessed the properties of the interfacing surfaces and the contribution made to the interaction by specific intermolecular contacts, including salt bridges and hydrogen bonds.",,
1745,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"in addition to this, we have evaluated the potential flexibility of these model complexes.",,
1746,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"the model wherein two molecules of clec-2 associate with tetrameric rhodocytin provides a more plausible model in terms of the composite effects related to the number of interfacing residues, the nature of their interactions and the predicted solvation energy effects.",,
1747,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"further, it is of potential significance that the predicted dynamic motions of this complex are suggestive of a mechanism whereby this interaction might cluster the receptors on the platelet surface, which could have implications for signaling.",,
1748,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"overall, the work presented indicates that a plausible mode of binding is that of one non-disulfide linked () 2 tetramer of rhodocytin with two dimers of clec-2.",,
1749,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,this analysis will be of value in the development of further studies to characterise the interaction further with a view to developing therapeutic approaches to disrupt the rhodocytin-clec-2 interaction on the platelet surface.,,
1750,241583517,Team Papelo at FEVEROUS: Multi-hop Evidence Pursuit,conclusion,Insight-tree,"team papelo's system for feverous achieves .281 feverous score on the development set, with .658 label accuracy and .348 evidence recall.",,https://www.aclanthology.org/2021.fever-1.5.pdf
1751,241583517,Team Papelo at FEVEROUS: Multi-hop Evidence Pursuit,conclusion,Insight-tree,"the largest increase in performance over the baseline comes from the label classifier, which uses a different model architecture and is trained on extracted evidence chains including irrelevant evidence.",,https://www.aclanthology.org/2021.fever-1.5.pdf
1752,241583517,Team Papelo at FEVEROUS: Multi-hop Evidence Pursuit,conclusion,Insight-tree,"we also achieve better evidence recall through our table cell ranking module, which was trained with a multiple choice cross entropy loss similar to dpr.",,https://www.aclanthology.org/2021.fever-1.5.pdf
1753,241583517,Team Papelo at FEVEROUS: Multi-hop Evidence Pursuit,conclusion,Insight-tree,additional gains are achieved by our multi-hop evidence retrieval.,,https://www.aclanthology.org/2021.fever-1.5.pdf
1754,241583517,Team Papelo at FEVEROUS: Multi-hop Evidence Pursuit,conclusion,Insight-tree,"these modules can only be effective when given good representations of the context of sentences, list items and table cells, which we have carefully constructed.",,https://www.aclanthology.org/2021.fever-1.5.pdf
1755,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,the general objective of the present paper was to calculate the suitabilities for archeological sites in the laetanian region related to intensive and specialized roman viticulture in order to determine the underlying factors of their distribution.,,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1756,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"therefore, the archaeological dataset of 82 documented wine-pressing facilities was used as response variable in a predictive modelling approach.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1757,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"furthermore, 15 topographical and 6 socio-economic location characteristics were taken into account, which may have had, as stated by other authors, an influence on the distribution of the sites.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1758,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"in a first step, two models were developed where the variable selection was based on expert knowledge from previous studies on roman agriculture and viticulture.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1759,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"however, this work additionally aimed at semi-automating the process of variable selection.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1760,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"hence, the variables were selected using statistical distribution metrics.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1761,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,the resulting models with automated variable selection showed good performance.,,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1762,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,they serve well as a first approximation of suitability modelling when no a priori knowledge on operating processes and interactions influencing the spatial distribution of sites is available.,,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1763,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"consequently, this modeling approach can be used for theory building.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1764,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"however, it is of special interest that the best prediction performance was obtained by an expert knowledge model utilizing a combination of predictor variables that is based on the specific recommendations on viticulture by lucius junius moderatus columella, the prominent ancient roman agronomist.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1765,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,the model was used to make the first assumptions and theories about the underlying factors that had an impact on the development of viticulture in the laetanian region.,,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1766,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"the results indicate that the accessibility of a location and its connectivity to the local and regional distribution centres and trade routes, determined by terrain steepness, was decisive for the settlement of winemaking facilities.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1767,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"moreover, on the basis of the predictive model and findings on experimental archaeology, the maximum number of winegrowing facilities that could have existed in the laetanian region in roman times was extrapolated to 360.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1768,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"as the applied modeling approach focusses on the average 50% and 75% of wine pressing facilities, some site locations could not be explained by the predictive model.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1769,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"thus, future research should particularly focus on those locations to find reliable hypotheses for settlement under these ""marginal"" and ""less suitable"" conditions.",,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1770,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,this is expected to provide more detailed insights into the complexity of ancient rural settlement and viticulture in the laetanian region.,,https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
1771,231718729,Muppet: Massive Multi-task Representations with Pre-Finetuning,conclusion,Insight-tree,"in this work, we propose pre-finetuning, a stage after pre-training to further refine representations before end-task finetuning.",,https://www.aclanthology.org/2021.emnlp-main.468.pdf
1772,231718729,Muppet: Massive Multi-task Representations with Pre-Finetuning,conclusion,Insight-tree,we show that we can effectively learn more robust representations through multi-task learning (mtl) at scale.,,https://www.aclanthology.org/2021.emnlp-main.468.pdf
1773,231718729,Muppet: Massive Multi-task Representations with Pre-Finetuning,conclusion,Insight-tree,our mtl models outperform their vanilla pre-trained counterparts across several tasks.,,https://www.aclanthology.org/2021.emnlp-main.468.pdf
1774,231718729,Muppet: Massive Multi-task Representations with Pre-Finetuning,conclusion,Insight-tree,our analysis shows that properly scaling mtl with heterogeneous batches and loss scaling is critical to leveraging better representations.,,https://www.aclanthology.org/2021.emnlp-main.468.pdf
1775,231718729,Muppet: Massive Multi-task Representations with Pre-Finetuning,conclusion,Insight-tree,"we also show a critical point regarding the number of tasks when doing multi-task learning, where fewer tasks degrade representations compared to the pre-trained model, but more tasks than this point improve representations.",,https://www.aclanthology.org/2021.emnlp-main.468.pdf
1776,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"the adopted model of explanatory power relies on the availability of human-annotated explanations with specific features (e.g., explanatory facts reused across different training instances).",,https://www.aclanthology.org/2022.coling-1.134.pdf
1777,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"however, these resources might not be available in real-world scenarios and are generally costly to develop.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1778,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"moreover, since the explanatory power model relies on similarity measures and indicator functions, the model's ability to generalise might be sensitive to the incompleteness of the knowledge bases and the availability of representative explanations.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1779,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"we believe these limitations can be potentially alleviated by exploring the role of more abstract sentence representations within the cbr paradigm (bergmann and wilke, 1996).in the current implementation of cb-anli, the refine phase adopts specific assumptions to model the abstraction process required for explanation generation.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1780,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"this process, in fact, is performed by assuming that abstraction at the concept level translates in a correct mapping between hypotheses and central explanatory sentences.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1781,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"however, contextual linguistic elements can still affect the overall meaning of the specific concept being abstracted, inducing the inclusion of spurious links between sentences.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1782,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"while contextual elements are considered during the precedent phases through the use of contextualised embeddings and similar cases, additional work is still required to guarantee the correctness of the abstraction process.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1783,244800755,Case-Based Abductive Natural Language Inference,conclusion,Insight-tree,"this paper presented cb-anli, a model that integrates multi-hop and case-based reasoning (cbr) in a unified framework.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1784,244800755,Case-Based Abductive Natural Language Inference,conclusion,Insight-tree,we demonstrated the efficacy of the framework in complex abstractive and multi-hop nli tasks.,,https://www.aclanthology.org/2022.coling-1.134.pdf
1785,244800755,Case-Based Abductive Natural Language Inference,conclusion,Insight-tree,"we believe this work can open new lines of research on hybrid neurosymbolic models for explanation-based nli, and plan to investigate the efficacy of the framework on architectures that adopt richer symbolic representations in combination with neural models, further exploring the role of abstraction in case-based reasoning for improving robustness, generalisation, and explainability in nli.",,https://www.aclanthology.org/2022.coling-1.134.pdf
1786,207756753,Adversarial NLI: A New Benchmark for Natural Language Understanding,discussion & conclusion,Insight-tree,"in this work, we used a human-and-model-in-theloop training method to collect a new benchmark for natural language understanding.",,https://www.aclweb.org/anthology/2020.acl-main.441.pdf
1787,207756753,Adversarial NLI: A New Benchmark for Natural Language Understanding,discussion & conclusion,Insight-tree,the benchmark is designed to be challenging to current stateof-the-art models.,,https://www.aclweb.org/anthology/2020.acl-main.441.pdf
1788,207756753,Adversarial NLI: A New Benchmark for Natural Language Understanding,discussion & conclusion,Insight-tree,"annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into misclassifying, but that another person would correctly classify.",,https://www.aclweb.org/anthology/2020.acl-main.441.pdf
1789,207756753,Adversarial NLI: A New Benchmark for Natural Language Understanding,discussion & conclusion,Insight-tree,"we found that non-expert annotators, in this gamified setting and with appropriate incentives, are remarkably creative at finding and exploiting weaknesses.",,https://www.aclweb.org/anthology/2020.acl-main.441.pdf
1790,207756753,Adversarial NLI: A New Benchmark for Natural Language Understanding,discussion & conclusion,Insight-tree,"we collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult.",,https://www.aclweb.org/anthology/2020.acl-main.441.pdf
1791,207756753,Adversarial NLI: A New Benchmark for Natural Language Understanding,discussion & conclusion,Insight-tree,training on this new data yielded the state of the art on existing nli benchmarks.,,https://www.aclweb.org/anthology/2020.acl-main.441.pdf
1792,256389465,EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval,conclusion,Insight-tree,we propose embeddistill -a novel distillation method for ir that goes beyond simple score matching.,,https://export.arxiv.org/pdf/2301.12005v1.pdf
1793,256389465,EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval,conclusion,Insight-tree,we specialize it to distill a de model into another de model by (a) reusing the teacher's document encoder in the student and (b) aligning query embeddings of the teacher and student.,,https://export.arxiv.org/pdf/2301.12005v1.pdf
1794,256389465,EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval,conclusion,Insight-tree,"this simple approach delivers immediate quality and computational gains in practical deployments and we demonstrate them on msmarco, nq, and beir benchmarks.",,https://export.arxiv.org/pdf/2301.12005v1.pdf
1795,256389465,EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval,conclusion,Insight-tree,we show that query generation technique further improves the performance of the distilled student in most cases.,,https://export.arxiv.org/pdf/2301.12005v1.pdf
1796,256389465,EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval,conclusion,Insight-tree,we generalize the proposed approach to distill a ce model to a de model,,https://export.arxiv.org/pdf/2301.12005v1.pdf
1797,237485098,Zero-Shot Dialogue State Tracking via Cross-Task Transfer,conclusion,Insight-tree,"in this paper, we present transferqa, a unified generative model that performs dst without using any dst training data.",,https://arxiv.org/pdf/2109.04655v1.pdf
1798,237485098,Zero-Shot Dialogue State Tracking via Cross-Task Transfer,conclusion,Insight-tree,transferqa uses the textto-text transfer learning framework that seamlessly combines extractive qa and multi-choice qa for tracking both categorical slots and non-categorical slots.,,https://arxiv.org/pdf/2109.04655v1.pdf
1799,237485098,Zero-Shot Dialogue State Tracking via Cross-Task Transfer,conclusion,Insight-tree,"to enable our model to zero-shot ""none"" value slots, we introduce two effective ways to construct unanswerable questions, i.e., negative question sampling and context truncation.",,https://arxiv.org/pdf/2109.04655v1.pdf
1800,237485098,Zero-Shot Dialogue State Tracking via Cross-Task Transfer,conclusion,Insight-tree,the experimental results on the multiwoz and sgd datasets demonstrate the effectiveness of our approach in both zero-shot and few-shot settings.,,https://arxiv.org/pdf/2109.04655v1.pdf
1801,237485098,Zero-Shot Dialogue State Tracking via Cross-Task Transfer,conclusion,Insight-tree,"we also show that improving the ""none"" value slot accuracy has",,https://arxiv.org/pdf/2109.04655v1.pdf
1802,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,it is studied that clause-like text spans delimited by discourse relations can be discourse units that reveal the rhetorical structure of texts,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1803,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"[34], [41].",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1804,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"we further observe that such discourse units are essential logical propositions in logical reasoning, such as premise or conclusion.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1805,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"as the example shown in figure 3, the ""while"" in the passage indicates a comparison between the attributes of the fig.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1806,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,2,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1807,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,the discourse-aware graph networks (dagns) pipeline mainly consists of (1) logic graph construction (2) logic representation learning.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1808,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,the logic graph construction module takes a logical qa data point as input and constructs logic graphs.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1809,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,the logic representation learning module then performs graph reasoning upon the constructed logic graphs.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1810,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"besides, the encoder provides fundamental embeddings for the pipeline.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1811,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"""analog system"" and that of the ""digital system"".",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1812,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"the ""because"" in the option uncovers that ""error cannot occur in the emission of digital signals"" as a premise to the conclusion ""digital systems are the best information systems"".this observation is agreed with informal logic theories [35], [36], which study uncovering logical structure from the texts and have conventional in-line logical indicators.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1813,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"for example, acknowledged premise indicators include ""since"", ""because"", ""given that"". conclusion indicators include ""therefore"", ""so"", ""consequently"", and so forth.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1814,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,most of these indicators are discourse connectives.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1815,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"some discourse parsers [42], [43] perform discourse unit segmentation.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1816,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"however, discourse parsing is still challenging, and the parsers are not general to new data, such as logical reasoning questions.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1817,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"for example, segbot",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1818,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,[43] is good on the rst-dt dataset but does not work well on the standardized exam texts as in the reclor dataset.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1819,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"thus, we customize discourse unit delimitation strategy for logical texts.we use the penn discourse treebank (pdtb 2.0)",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1820,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,[34] to help draw discourse connectives.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1821,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"pdtb 2.0 contains discourse relations that are manually annotated on the 1 million wall street journal (wsj) corpus and are broadly characterized into ""explicit"" and ""implicit"" connectives.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1822,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"the former ones are explicitly present in sentences such as discourse adverbial ""instead"" or subordinating conjunction ""because"", whereas the latter ones are inferred by pdtb annotators between successive pairs of text spans split by punctuation marks such as "".""",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1823,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"or "";"".",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1824,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"we take all the ""explicit"" connectives as well as common punctuation marks to form our discourse-aware delimiter library, presented in table 1.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1825,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,each logical text is split into elementary discourse units (edus) by all the delimiters in the library.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1826,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,the edus are taken as graph nodes v.nodes with topic-related terms.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1827,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,the desired key terms are those real nouns or phrases that repeatedly appear in the text.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1828,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,such nouns or phrases are instantiations of logical variables in propositions.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1829,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"as a result, replacing such terms with abstract variables or terms in other topics does not change the process of reasoning.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1830,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"for example, in figure 3, the first two sentences indicate a comparison of ""signal"" between ""analog system(s)"" and ""digital system(s)"".",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1831,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"performing abstraction by replacing ""signal"" with variable , ""analog system(s)"" with variable , and ""digital system(s)"" with variable , the propositions are free from the topic of electronics, but the comparison relation is retained.we use a sliding window to collect the recurring phrases.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1832,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"given the input logical text, stemming is first applied to handle morphological diversity.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1833,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"then, the sliding window loops over ngrams and records the reoccurrence.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1834,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"next, all the stop words and overlapped substrings are filtered.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1835,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,the resulting topic-related terms are attached to the nodes according to which text segment they belong.binary node types.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1836,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"the text of logical reasoning qa consists of two possible structures: (passage, question, options) or (dialogue context, candidate responses).",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1837,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"we regard passage or dialogue context as context texts that carry the main logical reasoning structure, whereas regard (question, options) or candidate responses as candidate texts that are added to the context texts and should remain their logical consistency.according to the discourse unit delimitation, the graph nodes are naturally from the context texts or the candidate texts.",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1838,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"therefore, we define two disjoint and independent node sets: context node set v u and candidate node setv v .",,https://export.arxiv.org/pdf/2207.01450v2.pdf
1839,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,v u v v = v and v u v v,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1840,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,#NAME?,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1841,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,conclusion,Insight-tree,this paper explores a structure-based solution to textual logical reasoning that explicitly models the logical reasoning process.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1842,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,conclusion,Insight-tree,the challenges include: (1) uncovering the inference structure from plain texts for effective structural constraints.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1843,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,conclusion,Insight-tree,(2) learning the inference processes rather than the knowledge for effective logical reasoning.,,https://export.arxiv.org/pdf/2207.01450v2.pdf
1844,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"we present drkit, a differentiable module that is capable of answering multi-hop questions directly using a large entity-linked text corpus.",,https://arxiv.org/pdf/2002.10640v1.pdf
1845,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"drkit is designed to imitate traversal in kb over the text corpus, providing ability to follow relations in the ""virtual"" kb over text.",,https://arxiv.org/pdf/2002.10640v1.pdf
1846,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"we achieve state-of-the-art results on the metaqa dataset for answering natural language questions, with a 9 point increase in the 3-hop case.",,https://arxiv.org/pdf/2002.10640v1.pdf
1847,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"we also developed an efficient implementation using sparse operations and inner product search, which led to a 10-100x increase in queries/sec over baseline approaches.",,https://arxiv.org/pdf/2002.10640v1.pdf
1848,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"a metaqa: implementation details we use p = 400 dimensional embeddings for the mentions and queries, and 200-dimensional embeddings each for the start and end positions.",,https://arxiv.org/pdf/2002.10640v1.pdf
1849,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,this results in an index of size 750mb.,,https://arxiv.org/pdf/2002.10640v1.pdf
1850,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"when computing a em , the entity to mention co-occurrence matrix, we only retain mentions in the top 50 paragraphs matched with an entity, to ensure sparsity.",,https://arxiv.org/pdf/2002.10640v1.pdf
1851,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,further we initialize the first 4 layers of the question encoder with the transformer network from pre-training.,,https://arxiv.org/pdf/2002.10640v1.pdf
1852,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"for the first hop, we assign z 0 as a 1-hot vector for the least frequent entity detected in the question using an exact match.",,https://arxiv.org/pdf/2002.10640v1.pdf
1853,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"the number of nearest neighbors k and the softmax temperature  were tuned on the dev set of each task, and we found k = 10000 and  = 4 to work best.",,https://arxiv.org/pdf/2002.10640v1.pdf
1854,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"we pretrain the index on a combination of the metaqa corpus, using the kb provided with metaqa for distance data, and the wikidata corpus.   ",,https://arxiv.org/pdf/2002.10640v1.pdf
1855,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,indexing only entity-mentions in single-hop questions over all spans.,,https://arxiv.org/pdf/2002.10640v1.pdf
1856,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"note that drkit-entities has a high hits@1 performance on the rare relations subset, showing that there is generalization to less frequent data due to the natural language representations of entities and relations.",,https://arxiv.org/pdf/2002.10640v1.pdf
1857,222177127,"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment",conclusion and future work,Insight-tree,we propose a novel ea model (attrgnn) and contribute a hard experimental setting for practical evaluation.,,https://arxiv.org/pdf/2010.03249v1.pdf
1858,222177127,"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment",conclusion and future work,Insight-tree,attrgnn can integrate both attribute and relation triples with varying importance for better performance.,,https://arxiv.org/pdf/2010.03249v1.pdf
1859,222177127,"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment",conclusion and future work,Insight-tree,"experimental results under the regular and hard settings present significant improvements of our proposed model, and the severe dataset bias can be effectively alleviated in our proposed hard setting.",,https://arxiv.org/pdf/2010.03249v1.pdf
1860,258865410,Decomposing Complex Queries for Tip-of-the-tongue Retrieval,conclusion,Insight-tree,"we study a real-world information-seeking settingtip of the tongue retrieval-in which users issue long, complex queries for re-finding items despite being unable to articulate identifying details about those items.",,https://export.arxiv.org/pdf/2305.15053v1.pdf
1861,258865410,Decomposing Complex Queries for Tip-of-the-tongue Retrieval,conclusion,Insight-tree,we introduce a simple but effective approach to handling these complex queries that decomposes them into sub-queries or clues that are routed to expert retrievers for specialized scoring.,,https://export.arxiv.org/pdf/2305.15053v1.pdf
1862,258865410,Decomposing Complex Queries for Tip-of-the-tongue Retrieval,conclusion,Insight-tree,our simple framework allows for modular composition of different retrievers and leveraging of pretrained models for specific modalities such as clip for document images.,,https://export.arxiv.org/pdf/2305.15053v1.pdf
1863,258865410,Decomposing Complex Queries for Tip-of-the-tongue Retrieval,conclusion,Insight-tree,"we observe improvements of up to 7% relative gain for recall@5 when incorporating query decomposition into existing retrievers on our newly-introduced whatsthatbook, a large challenging dataset of real-world, tip-of-thetongue queries for books.",,https://export.arxiv.org/pdf/2305.15053v1.pdf
1864,258686160,Parallel Context Windows for Large Language Models,conclusion and future work,Insight-tree,"in recent years, a multitude of successful approaches have been proposed for allowing transformer-based language models to leverage large amounts of text during inference, leading to a variety of dedicated architectures.",,https://export.arxiv.org/pdf/2212.10947v2.pdf
1865,258686160,Parallel Context Windows for Large Language Models,conclusion and future work,Insight-tree,"in parallel, however, the mainstream llm production line of new models with ""regular""-up to several thousand tokens-context window sizes enjoys faster progress in the form of scaling, innovation, and data updating.",,https://export.arxiv.org/pdf/2212.10947v2.pdf
1866,258686160,Parallel Context Windows for Large Language Models,conclusion and future work,Insight-tree,this paper introduced parallel context windows (pcw): a simple approach for allowing any offthe-shelf llm to broaden the scope of text it can access during inference.,,https://export.arxiv.org/pdf/2212.10947v2.pdf
1867,258686160,Parallel Context Windows for Large Language Models,conclusion and future work,Insight-tree,"we showed the effectiveness of pcw in the framework of in-context learning, where access to a context that is larger by a factor of b implies learning from b times more training examples.",,https://export.arxiv.org/pdf/2212.10947v2.pdf
1868,258686160,Parallel Context Windows for Large Language Models,conclusion and future work,Insight-tree,"our results show that pcw is more effective than the vanilla single context window approach for in-context learning over a broad set of multi-class classification tasks, suggesting that pcw could improve in-context learning in tasks with diverse input or output spaces.",,https://export.arxiv.org/pdf/2212.10947v2.pdf
1869,258686160,Parallel Context Windows for Large Language Models,conclusion and future work,Insight-tree,we also showed promising signals for applying pcw for multiple retrieved document reading.,,https://export.arxiv.org/pdf/2212.10947v2.pdf
1870,238856959,Can Explanations Be Useful for Calibrating Black Box Models?,discussion & conclusion,Insight-tree,"limitations despite showing promising results in improving model generalization performance, our attribution-based approach does suffer from intensive computation cost.",,https://www.aclanthology.org/2022.acl-long.429.pdf
1871,238856959,Can Explanations Be Useful for Calibrating Black Box Models?,discussion & conclusion,Insight-tree,"using either lime or shap to generate attributions requires running inference a fair number of perturbations when the input size is large (see appendix for details), which limits our method's applicability.",,https://www.aclanthology.org/2022.acl-long.429.pdf
1872,238856959,Can Explanations Be Useful for Calibrating Black Box Models?,discussion & conclusion,Insight-tree,"but this doesn't undermine the main contribution of this paper, answering the question in the title, and our approach is still applicable as-is in the scenarios where we pay for access to the model but not per query.",,https://www.aclanthology.org/2022.acl-long.429.pdf
1873,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,limitations,Insight-tree,"the spin model achieves linear complexity via advanced self-attention mechanisms; this also introduces additional hyper-parameters into the model, potentially increasing tuning time.",,https://arxiv.org/pdf/2205.11718v1.pdf
1874,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,limitations,Insight-tree,"the resulting architecture remains overparametrized even after with small numbers of inducing points h, f and may overfit.the primary source of expressivity is attention between datapoints, which has a query dimension he.highly expressive models may learn to ignore the training set and operate in a fully-parametric mode; this failure mode is best avoided via regularization and large datasets.",,https://arxiv.org/pdf/2205.11718v1.pdf
1875,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,limitations,Insight-tree,"interestingly, our approach benefits from big data, while classical non-parametric models work best on small datasets due to their computational complexity.",,https://arxiv.org/pdf/2205.11718v1.pdf
1876,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,limitations,Insight-tree,"regularization via small h, f , dropout, and feature masking control overfitting; we will explore more compact architectures in future work.",,https://arxiv.org/pdf/2205.11718v1.pdf
1877,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,conclusion,Insight-tree,"in this paper, we introduce a domain-agnostic general-purpose architecture, the semi-parametric inducing point network (spin).",,https://arxiv.org/pdf/2205.11718v1.pdf
1878,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,conclusion,Insight-tree,"unlike previous semi-parametric approaches whose computational cost grows quadratically with the size of the dataset, our approach scales linearly in the size and dimensionality of the data by leveraging a cross attention mechanism between datapoints and induced latents, allowing it to scale to large datasets.",,https://arxiv.org/pdf/2205.11718v1.pdf
1879,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,we argue that progress on qa has been hindered by a lack of appropriate training and test data.,,https://www.aclweb.org/anthology/Q19-1026.pdf
1880,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,"to address this, we present the natural questions corpus.",,https://www.aclweb.org/anthology/Q19-1026.pdf
1881,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,this is the first large publicly available data set to pair real user queries with high-quality annotations of answers in documents.,,https://www.aclweb.org/anthology/Q19-1026.pdf
1882,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,"we also present metrics to be used with nq, for the purposes of evaluating the performance of question answering systems.",,https://www.aclweb.org/anthology/Q19-1026.pdf
1883,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,we demonstrate a high upper bound on these metrics and show that existing methods do not approach this upper bound.,,https://www.aclweb.org/anthology/Q19-1026.pdf
1884,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,we argue that for them to do so will require significant advances in nlu.,,https://www.aclweb.org/anthology/Q19-1026.pdf
1885,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,figure 5 shows example questions from the data set.,,https://www.aclweb.org/anthology/Q19-1026.pdf
1886,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,"figure 6 shows example question/answer pairs from the data set, together with expert judgments and statistics from the 25-way annotations.",,https://www.aclweb.org/anthology/Q19-1026.pdf
1887,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,"retrieval models, based on plm, require finetuning millions of parameters which makes them memory inefficient and non-scalable for out-of-domain adaptation.",,https://export.arxiv.org/pdf/2303.13220v1.pdf
1888,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,this motivates the need for efficient methods to adapt them to information retrieval tasks.,,https://export.arxiv.org/pdf/2303.13220v1.pdf
1889,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,"in this paper, we examine adapters for sparse retrieval models.",,https://export.arxiv.org/pdf/2303.13220v1.pdf
1890,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,"we show that with approximately 2% of training parameters, adapters can be successfully employed for splade models with comparable or even better effectiveness on benchmark ir datasets such as ms marco and trec.",,https://export.arxiv.org/pdf/2303.13220v1.pdf
1891,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,we further analyze adapter layer ablation and see a further reduction in training parameters to 1.8% retains effectiveness of full finetuning.,,https://export.arxiv.org/pdf/2303.13220v1.pdf
1892,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,"for domain adaptation, adapters are more stable and outperform finetuning, which is prone to overfitting, on tripclick dataset, adapters outperform on precision metrics torso and tail queries and performs comparably on head queries.",,https://export.arxiv.org/pdf/2303.13220v1.pdf
1893,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,we explore knowledge transfer between first stage rankers and rerankers as a final study.,,https://export.arxiv.org/pdf/2303.13220v1.pdf
1894,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,adapters underperform full finetuning when trying to reuse sparse model to rerankers.,,https://export.arxiv.org/pdf/2303.13220v1.pdf
1895,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,dense first stage rankers perform similarly for adapters and finetuning while sparse first stage rankers is less effective compared to finetuning.,,https://export.arxiv.org/pdf/2303.13220v1.pdf
1896,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,we leave this as future work.,,https://export.arxiv.org/pdf/2303.13220v1.pdf
1897,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,"as memory-efficient adapters are effective for splade, we leave for future studying larger sparse models and their generalizability.",,https://export.arxiv.org/pdf/2303.13220v1.pdf
1898,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,"finally, an interesting scenario could also be to tackle unsupervised domain adaptation with adapters.",,https://export.arxiv.org/pdf/2303.13220v1.pdf
1899,257834209,SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES,conclusion,Insight-tree,"in this paper, we introduce a domain-agnostic general-purpose architecture, the semi-parametric inducing point network (spin) and use it as the basis for induced point neural process (ipnps).",,https://export.arxiv.org/pdf/2205.11718v2.pdf
1900,257834209,SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES,conclusion,Insight-tree,"unlike previous semi-parametric approaches whose computational cost grows quadratically with the size of the dataset, our approach scales linearly in the size and dimensionality of the data by leveraging a cross attention mechanism between datapoints and induced latents.",,https://export.arxiv.org/pdf/2205.11718v2.pdf
1901,257834209,SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES,conclusion,Insight-tree,this allows our method to scale to large datasets and enables meta learning with large contexts.,,https://export.arxiv.org/pdf/2205.11718v2.pdf
1902,257834209,SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES,conclusion,Insight-tree,"we present empirical results on 10 uci datasets, a gaussian process meta learning task, and a real-world important task in genomics, genotype imputation, and show that our method can achieve competitive, if not better, performance relative to state-of-the-art methods at a fraction of the computational cost.",,https://export.arxiv.org/pdf/2205.11718v2.pdf
1903,258833599,ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,conclusion,Insight-tree,"overall, our study on explaincpe dataset demonstrated the potential of llms in medical question answering with explanations.",,https://export.arxiv.org/pdf/2305.12945v1.pdf
1904,258833599,ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,conclusion,Insight-tree,our analysis of model performance on different types of questions revealed the strengths and limitations of different llms in terms of in-context learning.,,https://export.arxiv.org/pdf/2305.12945v1.pdf
1905,258833599,ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,conclusion,Insight-tree,"our analysis of error cases pointed out the need for further improvement in llms in terms of knowledge acquisition, reliability of calculations, and text comprehension.",,https://export.arxiv.org/pdf/2305.12945v1.pdf
1906,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,"we studied the behavior of two popular functions that fuse together lexical and semantic retrieval to produce hybrid retrieval, and identified their advantages and pitfalls.",,https://export.arxiv.org/pdf/2210.11934v2.pdf
1907,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,"importantly, we investigated several questions and claims in prior work.",,https://export.arxiv.org/pdf/2210.11934v2.pdf
1908,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,we established theoretically that the choice of normalization is not as consequential as once thought for a convex combination-based fusion function.,,https://export.arxiv.org/pdf/2210.11934v2.pdf
1909,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,we found that rrf is sensitive to its parameters.,,https://export.arxiv.org/pdf/2210.11934v2.pdf
1910,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,we also observed empirically that convex combination of normalized scores outperforms rrf on in-domain and out-of-domain datasets-a finding that is in disagreement with [5].,,https://export.arxiv.org/pdf/2210.11934v2.pdf
1911,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,we believe that a convex combination with theoretical minimum-maximum normalization (tm2c2) indeed enjoys properties that are important in a fusion function.,,https://export.arxiv.org/pdf/2210.11934v2.pdf
1912,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,"its parameter, too, can be tuned sample-efficiently or set to a reasonable value based on domain knowledge.",,https://export.arxiv.org/pdf/2210.11934v2.pdf
1913,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,"in our experiments, for example, we found the range ",,https://export.arxiv.org/pdf/2210.11934v2.pdf
1914,253080555,An Analysis of Fusion Functions for Hybrid Retrieval,conclusion,Insight-tree,"[0.6, 0.8] to consistently lead to improvements.",,https://export.arxiv.org/pdf/2210.11934v2.pdf
1915,252626893,How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,conclusion,Insight-tree,we proposed a new multi-hop rc dataset for comprehensively evaluating the ability of existing models to understand date information.,,https://export.arxiv.org/pdf/2210.05208v1.pdf
1916,252626893,How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,conclusion,Insight-tree,we evaluated the top-performing models on our dataset.,,https://export.arxiv.org/pdf/2210.05208v1.pdf
1917,252626893,How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,conclusion,Insight-tree,the results revealed that the models may not possess the ability to subtract two dates even when fine-tuned on our dataset.,,https://export.arxiv.org/pdf/2210.05208v1.pdf
1918,252626893,How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,conclusion,Insight-tree,"we also found that our probing questions could help to improve qa performance, and can be used for data augmentation.",,https://export.arxiv.org/pdf/2210.05208v1.pdf
1919,252626893,How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,conclusion,Insight-tree,"for future work, we will use the hierarchical manner in our dataset to apply to other types of questions such as numerical reasoning questions in drop.",,https://export.arxiv.org/pdf/2210.05208v1.pdf
1920,219792855,Pre-trained Language Models as Symbolic Reasoners over Knowledge?,conclusion,Insight-tree,this work is a first study towards understanding bert's ability to capture knowledge seen during pre-training by investigating it's reasoning and memorization capabilities.,,https://arxiv.org/pdf/2006.10413v1.pdf
1921,219792855,Pre-trained Language Models as Symbolic Reasoners over Knowledge?,conclusion,Insight-tree,we identified factors influencing what knowledge is stored and what is forgotten and what is learnable beyond knowledge explicitly seen during training.,,https://arxiv.org/pdf/2006.10413v1.pdf
1922,219792855,Pre-trained Language Models as Symbolic Reasoners over Knowledge?,conclusion,Insight-tree,we saw that theoretically bert is able to infer facts not explicitly seen during training via symbolic rules.,,https://arxiv.org/pdf/2006.10413v1.pdf
1923,219792855,Pre-trained Language Models as Symbolic Reasoners over Knowledge?,conclusion,Insight-tree,future work should investigate how to enable bert during pre-training to use this capability.,,https://arxiv.org/pdf/2006.10413v1.pdf
1924,219792855,Pre-trained Language Models as Symbolic Reasoners over Knowledge?,conclusion,Insight-tree,we see the need to incentivize plms to capture symbolic rules and factual knowledge as this could potentially improve plm's performance also on downstream tasks where reasoning capabilities or implicit knowledge leverage is needed.,,https://arxiv.org/pdf/2006.10413v1.pdf
1925,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",conclusion,Insight-tree,"large language models struggle with hallucination, or generating incorrect information, despite the large amount of factual pre-training data they were trained on.",,https://export.arxiv.org/pdf/2305.13252v1.pdf
1926,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",conclusion,Insight-tree,"to help alleviate this problem, we proposed according-to prompts, asking language models to ground their output to their pretraining corpus.",,https://export.arxiv.org/pdf/2305.13252v1.pdf
1927,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",conclusion,Insight-tree,"to quantify the extent to which models achieve this goal, we introduced a new metric, quip-score, that efficiently and quickly measures the percent of the model's generation that exists as exact quotes in the pre-training corpus.",,https://export.arxiv.org/pdf/2305.13252v1.pdf
1928,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",conclusion,Insight-tree,we showed that prompting models with grounding prompts greatly improves the quip-score while anti-grounding prompts reduces the quip-score.,,https://export.arxiv.org/pdf/2305.13252v1.pdf
1929,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",conclusion,Insight-tree,"our analysis also shows that quip-score increases with instruction-tuning, popularity of the entity in the question, and model size.",,https://export.arxiv.org/pdf/2305.13252v1.pdf
1930,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",conclusion,Insight-tree,we hope that this work brings more attention to the positive aspects of llm memorization and encourages more work into understanding how and when language model output is grounded to its pre-training data.,,https://export.arxiv.org/pdf/2305.13252v1.pdf
1931,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",limitations,Insight-tree,"our proposed metric only accounts for exact lexical match and will miss other types of grounded statements -thus we view quip-score as a lower bound on grounding where grounding is defined only by quoting from source material.we also recognize the possibility of a discrepancy between the pre-training data of private models like chatgpt and the wikipedia version we use for analysis, due to limited information on their pretraining.",,https://export.arxiv.org/pdf/2305.13252v1.pdf
1932,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",limitations,Insight-tree,"however, this might not be a significant concern, as although wikipedia is not completely static, a substantial part of the information in this knowledge source remains consistent over a short span of years.",,https://export.arxiv.org/pdf/2305.13252v1.pdf
1933,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",limitations,Insight-tree,"furthermore, our results with chat-gpt are similar compared with models for which we do have the exact pre-training data (like gpt-j).",,https://export.arxiv.org/pdf/2305.13252v1.pdf
1934,258833449,Fact-Checking Complex Claims with Program-Guided Reasoning,conclusion and future work,Insight-tree,"we proposed programfc, a few-shot neurosymbolic model for fact-checking that learns to map input claims to a reasoning program consisting of a sequence of sub-task function calls for answering a question, for fact-checking a simple claim, and for computing a logical expression.",,https://export.arxiv.org/pdf/2305.12744v1.pdf
1935,258833449,Fact-Checking Complex Claims with Program-Guided Reasoning,conclusion and future work,Insight-tree,then factchecking is performed by executing that program.,,https://export.arxiv.org/pdf/2305.12744v1.pdf
1936,258833449,Fact-Checking Complex Claims with Program-Guided Reasoning,conclusion and future work,Insight-tree,"programfc combines the advantages of symbolic programs, such as explainability, with the flexibility of end-to-end neural models.",,https://export.arxiv.org/pdf/2305.12744v1.pdf
1937,258833449,Fact-Checking Complex Claims with Program-Guided Reasoning,conclusion and future work,Insight-tree,"using codex as the program generator, programfc demonstrates promising performance on hover and feverous with only a small number of incontext demonstrations and no additional training.",,https://export.arxiv.org/pdf/2305.12744v1.pdf
1938,258833449,Fact-Checking Complex Claims with Program-Guided Reasoning,conclusion and future work,Insight-tree,"we also investigated the impact of model size and the benefits of programs for retrieval, and we analyzed the errors.",,https://export.arxiv.org/pdf/2305.12744v1.pdf
1939,258833449,Fact-Checking Complex Claims with Program-Guided Reasoning,conclusion and future work,Insight-tree,"the results indicated that pro-gramfc effectively balances model capability, learning efficiency, and interpretability.",,https://export.arxiv.org/pdf/2305.12744v1.pdf
1940,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,we proposed a novel subtopic planning based architecture for answering how-to questions.,,https://www.aclanthology.org/2022.gem-1.3.pdf
1941,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,"our architecture is able to generate answers with better structure, higher diversity and more consistent quality.",,https://www.aclanthology.org/2022.gem-1.3.pdf
1942,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,"moreover, our subtopic selection method effectively singles out high quality subtopics with relevance and independence.",,https://www.aclanthology.org/2022.gem-1.3.pdf
1943,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,both automatic and human evaluation proved the effectiveness of our methods.,,https://www.aclanthology.org/2022.gem-1.3.pdf
1944,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,"we consider the two directions for future research: 1) improving the answer's quality by applying end-to-end retrieval-generation models, e.g. (lewis et al., 2020b).",,https://www.aclanthology.org/2022.gem-1.3.pdf
1945,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,2) developing precise metrics to evaluate long-form and non-factoid answers.,,https://www.aclanthology.org/2022.gem-1.3.pdf
1946,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,it suggests that incorporating commonsense knowledge may improve the performance of a subtopic decomposition model.,,https://www.aclanthology.org/2022.gem-1.3.pdf
1947,237439283,"On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings",conclusion,Insight-tree,"relevance performance is still undercounted: while the expert-generated relevance ratings produced in this work provide more accurate estimates of performance compared to single gold explanations when used in fully-automatic evaluations, these automatic estimates still undercount overall model performance.",,https://www.aclanthology.org/2021.emnlp-main.596.pdf
1948,237439283,"On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings",conclusion,Insight-tree,"in our experiments we show the expert ratings primarily provide a vehicle for training better models, but that automatically evaluating relevance performance still remains a challenge, even with a large targeted increase in relevance annotation.",,https://www.aclanthology.org/2021.emnlp-main.596.pdf
1949,237439283,"On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings",conclusion,Insight-tree,"further, annotators reported that determining relevance of single facts in isolation is challenging because it lacks the broader compositional context of the rest of the candidate explanation, suggesting ultimate limits to the utility of exhaustive annotation.",,https://www.aclanthology.org/2021.emnlp-main.596.pdf
1950,255570137,Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding,conclusion,Insight-tree,"in this paper, we propose cross-model comparative loss, a simple task-agnostic loss function, to improve the utility of neurons in nlu models.",,https://export.arxiv.org/pdf/2301.03765v1.pdf
1951,255570137,Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding,conclusion,Insight-tree,"comparative loss is essentially a ranking loss based on the comparison principle between the full model and its ablated models, with the expectation that the less ablation there is, the smaller the task-specific loss.",,https://export.arxiv.org/pdf/2301.03765v1.pdf
1952,255570137,Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding,conclusion,Insight-tree,"to ensure comparability among multiple ablated models, we progressively ablate the models and provide two controlled ablation methods based on dropout and context cropping, applicable to a wide range of tasks and models.",,https://export.arxiv.org/pdf/2301.03765v1.pdf
1953,236986870,How Optimal is Greedy Decoding for Extractive Question Answering?,conclusions,Insight-tree,"we investigate the optimality of greedy decoding for extractive question answering by comparing it to exact-extract, an optimal decoding algorithm that guarantees both extractiveness and exactness.",,https://export.arxiv.org/pdf/2108.05857v2.pdf
1954,236986870,How Optimal is Greedy Decoding for Extractive Question Answering?,conclusions,Insight-tree,"while the greedy algorithm lags behind exact-extract in the zero-shot setting, training the model on as few as 16 labeled examples shrinks the performance gap substantially.",,https://export.arxiv.org/pdf/2108.05857v2.pdf
1955,236986870,How Optimal is Greedy Decoding for Extractive Question Answering?,conclusions,Insight-tree,"this gap continues to narrow as more examples are available, typically converging to less than 1 point (f1) when training on 1024 examples.",,https://export.arxiv.org/pdf/2108.05857v2.pdf
1956,236986870,How Optimal is Greedy Decoding for Extractive Question Answering?,conclusions,Insight-tree,"overall, our results showcase the impressive ability of pretrained language models to adapt to extractive question answering while relying only on a naive decoding algorithm.",,https://export.arxiv.org/pdf/2108.05857v2.pdf
1957,249049412,Examining Single Sentence Label Leakage in Natural Language Inference Datasets,conclusion,Insight-tree,"in the four years since (poliak et al., 2018) single sentence relation leakage bias has proven to remain a difficult issue.",,https://arxiv.org/pdf/2112.09237v3.pdf
1958,249049412,Examining Single Sentence Label Leakage in Natural Language Inference Datasets,conclusion,Insight-tree,"efforts to debias nli have led to datasets that merely exhibit different kinds of bias than those shown before, or less saturated benchmarks that continue to exhibit cheating features.",,https://arxiv.org/pdf/2112.09237v3.pdf
1959,249049412,Examining Single Sentence Label Leakage in Natural Language Inference Datasets,conclusion,Insight-tree,future work must prioritize reducing observable bias directly using a model-driven approach.,,https://arxiv.org/pdf/2112.09237v3.pdf
1960,229923926,Coreference Reasoning in Machine Reading Comprehension,conclusions,Insight-tree,"we show that the high performance of recent models on the quoref dataset does not necessarily indicate that they are adept at performing coreference reasoning, and that qa based on coreference reasoning is a greater challenge than current scores suggest.",,https://www.aclanthology.org/2021.acl-long.448.pdf
1961,229923926,Coreference Reasoning in Machine Reading Comprehension,conclusions,Insight-tree,we then propose a methodology for creating a dataset that better presents the coreference reasoning challenge for mrc.,,https://www.aclanthology.org/2021.acl-long.448.pdf
1962,229923926,Coreference Reasoning in Machine Reading Comprehension,conclusions,Insight-tree,we provide our methodology to an annotator and create a sample dataset.,,https://www.aclanthology.org/2021.acl-long.448.pdf
1963,229923926,Coreference Reasoning in Machine Reading Comprehension,conclusions,Insight-tree,"our analysis shows that our dataset contains fewer biases compared to quoref, and the performance of state-of-the-art quoref models drops considerably on this evaluation set.",,https://www.aclanthology.org/2021.acl-long.448.pdf
1964,233219392,Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval,conclusion,Insight-tree,"we introduce a simple yet effective multi-step dense retrieval method, beamdr.",,https://www.aclweb.org/anthology/2021.naacl-main.368.pdf
1965,233219392,Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval,conclusion,Insight-tree,"by conducting beam search and globally refreshing negative chains during training, beamdr finds reasoning chains in dense space.",,https://www.aclweb.org/anthology/2021.naacl-main.368.pdf
1966,233219392,Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval,conclusion,Insight-tree,beamdr is competitive to more complex sota systems albeit not using semi-structured information.,,https://www.aclweb.org/anthology/2021.naacl-main.368.pdf
1967,254823272,FEWFEDWEIGHT: Few-shot Federated Learning Framework across Multiple NLP Tasks,conclusion,Insight-tree,"in this paper, we have presented fewfedweight, enabling few-shot learning across massive nlp tasks with federated learning.",,https://export.arxiv.org/pdf/2212.08354v1.pdf
1968,254823272,FEWFEDWEIGHT: Few-shot Federated Learning Framework across Multiple NLP Tasks,conclusion,Insight-tree,"in this new framework, the global model synthesizes pseudo samples for each client model, which are weighted by an energy-based algorithm.",,https://export.arxiv.org/pdf/2212.08354v1.pdf
1969,254823272,FEWFEDWEIGHT: Few-shot Federated Learning Framework across Multiple NLP Tasks,conclusion,Insight-tree,aggregation weights of client models are estimated according to their performance during training.,,https://export.arxiv.org/pdf/2212.08354v1.pdf
1970,254823272,FEWFEDWEIGHT: Few-shot Federated Learning Framework across Multiple NLP Tasks,conclusion,Insight-tree,experiments on 118 different tasks demonstrate the effectiveness of the proposed fewfedweight.,,https://export.arxiv.org/pdf/2212.08354v1.pdf
1971,258959258,Do Large Language Models Know What They Don't Know?,conclusion,Insight-tree,this study investigates the self-knowledge of llms by evaluating their ability to identify unanswerable questions.,,https://export.arxiv.org/pdf/2305.18153v2.pdf
1972,258959258,Do Large Language Models Know What They Don't Know?,conclusion,Insight-tree,"through the introduction of a novel dataset and an automated method for detecting uncertainty in the models' responses, we are able to accurately measure the self-knowledge of llms such as gpt-3, instructgpt and llama.",,https://export.arxiv.org/pdf/2305.18153v2.pdf
1973,258959258,Do Large Language Models Know What They Don't Know?,conclusion,Insight-tree,"our results reveal that while these models possess a certain degree of self-knowledge, there is still an apparent disparity in comparison to human selfknowledge.",,https://export.arxiv.org/pdf/2305.18153v2.pdf
1974,258959258,Do Large Language Models Know What They Don't Know?,conclusion,Insight-tree,this highlights the need for further research in this area to enhance the ability of llms to understand their own limitations on the unknows.,,https://export.arxiv.org/pdf/2305.18153v2.pdf
1975,258959258,Do Large Language Models Know What They Don't Know?,conclusion,Insight-tree,"such efforts will lead to more accurate and reliable responses from llms, which will have a positive impact on their applications in diverse fields.",,https://export.arxiv.org/pdf/2305.18153v2.pdf
1976,229923812,HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions,conclusion,Insight-tree,"in this paper, we propose the hopretriever to collect reasoning evidence over wikipedia for multi-hop question answering.",,https://arxiv.org/pdf/2012.15534v1.pdf
1977,229923812,HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions,conclusion,Insight-tree,"both the structured knowledge indicated by hyperlinks and the unstructured knowledge presented as introduc-tory documents in wikipedia, are involved and leveraged together in hopretriever to help the evidence collection.",,https://arxiv.org/pdf/2012.15534v1.pdf
1978,229923812,HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions,conclusion,Insight-tree,"the experiment on the hotpotqa dataset shows that the performance of hopretriever improved observably as a result of combining the structured knowledge with unstructured knowledge, and outperforms all the published models on the leaderboard.",,https://arxiv.org/pdf/2012.15534v1.pdf
1979,229923812,HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions,conclusion,Insight-tree,"moreover, by inspecting the proportion of the two kinds of knowledge in hops, which kind of knowledge leads the retrieving of each evidence piece can be observed directly, which also provides extra intuitive interpretations for the selection of each evidence.",,https://arxiv.org/pdf/2012.15534v1.pdf
1980,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,"in order to build effective nlp models, we must move towards evaluations that test model robustness to variations in the input.",,https://export.arxiv.org/pdf/2210.14353v2.pdf
1981,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,"we presented romqa, the first benchmark for robust, multi-evidence, multi-answer qa.",,https://export.arxiv.org/pdf/2210.14353v2.pdf
1982,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,romqa evaluates robustness of models to varying question constraints by testing for worst-case performance among clusters of related questions.,,https://export.arxiv.org/pdf/2210.14353v2.pdf
1983,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,"compared to prior qa datasets, romqa has more natural human-written questions that require reasoning over more evidence text to more answers.",,https://export.arxiv.org/pdf/2210.14353v2.pdf
1984,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,"romqa is challenging for state-of-the-art large lms in zero-shot, few-shot, and supervised settings, and provides a quantifiable test to build more robust qa methods.",,https://export.arxiv.org/pdf/2210.14353v2.pdf
1985,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,"we want questions that cover diverse topics, however wikidata has a very skewed proposition distribution, with a long tail of rare propositions.",,https://export.arxiv.org/pdf/2210.14353v2.pdf
1986,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,"hence, we down-sample frequent propositions.",,https://export.arxiv.org/pdf/2210.14353v2.pdf
1987,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,let p prop (x) denote the percentage of triples that contain the proposition x.,,https://export.arxiv.org/pdf/2210.14353v2.pdf
1988,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,we define the average proposition probability as p prop = 1 |x |,,https://export.arxiv.org/pdf/2210.14353v2.pdf
1989,235731930,FAVIQ: FAct Verification from Information-seeking Questions,conclusion & future work,Insight-tree,"we introduced faviq, a new fact verification dataset derived from ambiguous information-seeking questions.",,https://www.aclanthology.org/2022.acl-long.354.pdf
1990,235731930,FAVIQ: FAct Verification from Information-seeking Questions,conclusion & future work,Insight-tree,"we incorporate facts that real users were unaware of when posing the question, leading to false claims that are more realistic and challenging to identify without fully understanding the context.",,https://www.aclanthology.org/2022.acl-long.354.pdf
1991,235731930,FAVIQ: FAct Verification from Information-seeking Questions,conclusion & future work,Insight-tree,"our extensive analysis shows that our data contains significantly less lexical bias than previous fact checking datasets, and include refute claims that are challenging and realistic.",,https://www.aclanthology.org/2022.acl-long.354.pdf
1992,235731930,FAVIQ: FAct Verification from Information-seeking Questions,conclusion & future work,Insight-tree,"our experiments showed that the state-of-the-art models are far from solving faviq, and models trained on faviq lead to improvements in professional fact checking.",,https://www.aclanthology.org/2022.acl-long.354.pdf
1993,235731930,FAVIQ: FAct Verification from Information-seeking Questions,conclusion & future work,Insight-tree,"altogether, we believe faviq will serve as a challenging benchmark as well as support future progress in professional fact-checking.",,https://www.aclanthology.org/2022.acl-long.354.pdf
1994,202565945,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,conclusion,Insight-tree,"in this work, we proposed a self-assembling neural modular network for multi-hop qa.",,https://www.aclweb.org/anthology/D19-1455.pdf
1995,202565945,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,conclusion,Insight-tree,we designed three modules that reason between the question and text-based context.,,https://www.aclweb.org/anthology/D19-1455.pdf
1996,202565945,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,conclusion,Insight-tree,the resulting model outperforms both the single-hop baseline and the original nmn on hotpotqa .,,https://www.aclweb.org/anthology/D19-1455.pdf
1997,202565945,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,conclusion,Insight-tree,"because of the interpretable nature of our model, we presented analyses to show that our model does in fact learn to perform compositional reasoning and can dynamically assemble the modular network based on the question.",,https://www.aclweb.org/anthology/D19-1455.pdf
1998,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",limitations,Insight-tree,"for object detection, while unified-io generally produces accurate outputs (see appendix a.4), we find the recall is often poor in cluttered images.",,https://export.arxiv.org/pdf/2206.08916v2.pdf
1999,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",limitations,Insight-tree,"prior work (chen et al., 2022b) has shown this can be overcome with extensive data augmentation techniques, but these methods are not currently integrated into unified-io.",,https://export.arxiv.org/pdf/2206.08916v2.pdf
2000,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",limitations,Insight-tree,our use of a pre-trained vq-gan greatly simplifies our training and is surprisingly effective for dense prediction tasks.,,https://export.arxiv.org/pdf/2206.08916v2.pdf
2001,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",limitations,Insight-tree,"however, it does mean unified-io has limited image generation capabilities (recent works (yu et al., 2022b) have shown this method can be greatly improved but was not available at the time of development).",,https://export.arxiv.org/pdf/2206.08916v2.pdf
2002,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",limitations,Insight-tree,we also found in a small-scale study that our model does not always understand prompts not in the training data (see appendix 4.5).,,https://export.arxiv.org/pdf/2206.08916v2.pdf
2003,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",conclusion,Insight-tree,"we have presented unified-io, a unified architecture that supports a large variety of computer vision and nlp tasks with diverse inputs and outputs, including images, continuous maps, binary masks, segmentation masks, text, bounding boxes, and keypoints.",,https://export.arxiv.org/pdf/2206.08916v2.pdf
2004,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",conclusion,Insight-tree,this unification is made possible by homogenizing each of these modalities into a sequence of discrete tokens.,,https://export.arxiv.org/pdf/2206.08916v2.pdf
2005,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",conclusion,Insight-tree,"the 2.9b parameter unified-io xl model is jointly trained on 90+ datasets, is the first model to perform all 7 tasks on the grit benchmark and obtains impressive results across 16 other vision and nlp benchmarks, with no benchmark fine-tuning or task-specific modifications.",,https://export.arxiv.org/pdf/2206.08916v2.pdf
2006,258588167,Unsupervised Dense Re-trieval Training with Web Anchors,conclusion,Insight-tree,"we train an unsupervised dense retrieval model, anchor-dr, leveraging the rich web anchors.",,https://export.arxiv.org/pdf/2305.05834v1.pdf
2007,258588167,Unsupervised Dense Re-trieval Training with Web Anchors,conclusion,Insight-tree,"in particular, we design a contrastive learning task: anchor-document prediction to continuously pretrain anchor-dr. additionally, we apply predefined rules and train a query classifier to filter out uninformative anchors.",,https://export.arxiv.org/pdf/2305.05834v1.pdf
2008,258588167,Unsupervised Dense Re-trieval Training with Web Anchors,conclusion,Insight-tree,experiments on two public datasets: msmarco and beir show that anchor-dr significantly outperforms the state-of-the-art dense retrievers on unsupervised retrieval.,,https://export.arxiv.org/pdf/2305.05834v1.pdf
2009,258588167,Unsupervised Dense Re-trieval Training with Web Anchors,conclusion,Insight-tree,our analyses provide a further comparison of the patterns of information contained in our contrastive learning pairs and query-document pairs in test datasets.,,https://export.arxiv.org/pdf/2305.05834v1.pdf
2010,258841029,Improving Language Models via Plug-and-Play Retrieval Feedback,conclusion,Insight-tree,"in conclusion, this paper presents a novel pipeline, refeed, designed to improve large language models' performance in a plug-and-play framework, effectively addressing the challenges arising from knowledge-intensive tasks.",,https://export.arxiv.org/pdf/2305.14002v1.pdf
2011,258841029,Improving Language Models via Plug-and-Play Retrieval Feedback,conclusion,Insight-tree,"by employing a retrieval method to provide automatic feedback on generated outputs and integrating this feedback to refine the outputs without the need for expensive fine-tuning, refeed offers a practical and efficient solution.",,https://export.arxiv.org/pdf/2305.14002v1.pdf
2012,258841029,Improving Language Models via Plug-and-Play Retrieval Feedback,conclusion,Insight-tree,we introduce two innovative modules within the refeed pipeline: diverse answer generation and an ensemble approach.,,https://export.arxiv.org/pdf/2305.14002v1.pdf
2013,258841029,Improving Language Models via Plug-and-Play Retrieval Feedback,conclusion,Insight-tree,these two modules further enhance refeed to produce more reliable and accurate answers by considering a wider array of retrieved documents and mitigating the risk of misleading retrieval feedback.,,https://export.arxiv.org/pdf/2305.14002v1.pdf
2014,258841029,Improving Language Models via Plug-and-Play Retrieval Feedback,conclusion,Insight-tree,our extensive experiments on four challenging knowledgeintensive benchmarks demonstrate the effectiveness of refeed in achieving state-of-the-art performance under the few-shot setting.,,https://export.arxiv.org/pdf/2305.14002v1.pdf
2015,258841029,Improving Language Models via Plug-and-Play Retrieval Feedback,conclusion,Insight-tree,"we believe by continuing to refine and optimize the refeed pipeline, we can unlock its full potential and ex-pand its applicability across a diverse range of scenarios and applications.",,https://export.arxiv.org/pdf/2305.14002v1.pdf
2016,252873216,Benchmarking Long-tail Generalization with Likelihood Splits,conclusion,Insight-tree,"with the saturation of static, single-metric leaderboards, there is growing consensus for the development of holistic evaluation benchmarks.",,https://www.aclanthology.org/2023.findings-eacl.71.pdf
2017,252873216,Benchmarking Long-tail Generalization with Likelihood Splits,conclusion,Insight-tree,"this includes evaluation of systems on aspects of performance beyond just single error rate on indistribution data; aspects such as performance on out-of-distribution data (linzen, 2020), and evaluating generalizability, robustness and fairness (ethayarajh and jurafsky, 2020).",,https://www.aclanthology.org/2023.findings-eacl.71.pdf
2018,252873216,Benchmarking Long-tail Generalization with Likelihood Splits,conclusion,Insight-tree,"in this work, we describe an approach to benchmark long-tail generalization, a necessary skill for nlp systems that truly understand language.",,https://www.aclanthology.org/2023.findings-eacl.71.pdf
2019,252873216,Benchmarking Long-tail Generalization with Likelihood Splits,conclusion,Insight-tree,we demonstrate the challenge posed by our splits to state-of-the-art models on several tasks; standard evaluation overestimates model performance on long-tail utterances.,,https://www.aclanthology.org/2023.findings-eacl.71.pdf
2020,252873216,Benchmarking Long-tail Generalization with Likelihood Splits,conclusion,Insight-tree,"instead of releasing a random split as the only metric on official benchmarks, our simple method can be used, for a wide range of tasks, to expose additional challenges in the collected data at no annotation cost.",,https://www.aclanthology.org/2023.findings-eacl.71.pdf
2021,252873216,Benchmarking Long-tail Generalization with Likelihood Splits,conclusion,Insight-tree,"benchmarking long-tail generalization, in this manner, can test model behavior on a broad set of generalization challenges, which may be missed by evaluations that test specific skills in isolation.",,https://www.aclanthology.org/2023.findings-eacl.71.pdf
2022,226236740,Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps,conclusion,Insight-tree,"in this study, we presented 2wikimultihopqa-a large and high quality multi-hop dataset that provides comprehensive explanations for predictions.",,https://arxiv.org/pdf/2011.01060v2.pdf
2023,226236740,Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps,conclusion,Insight-tree,we utilized logical rules in the kb to create more natural questions that still require multi-hop reasoning.,,https://arxiv.org/pdf/2011.01060v2.pdf
2024,226236740,Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps,conclusion,Insight-tree,"through experiments, we demonstrated that our dataset ensures multi-hop reasoning while being challenging for the multi-hop models.",,https://arxiv.org/pdf/2011.01060v2.pdf
2025,226236740,Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps,conclusion,Insight-tree,we also demonstrated that bootstrapping the multi-hop mrc dataset is beneficial by utilizing large-scale available data on wikipedia and wikidata.,,https://arxiv.org/pdf/2011.01060v2.pdf
2026,258378227,MAFiD: Moving Average Equipped Fusion-in-Decoder for Question Answering over Tabular and Textual Data,conclusion,Insight-tree,"in this paper, we address long range-reasoning for the multi-hop table-and-text qa and propose mafid, which extends fid by equipping ema and the gated cross-attention layer for the encoder and decoder parts, respectively, to design an effective way of combining various types of encoded representations.",,https://www.aclanthology.org/2023.findings-eacl.177.pdf
2027,258378227,MAFiD: Moving Average Equipped Fusion-in-Decoder for Question Answering over Tabular and Textual Data,conclusion,Insight-tree,the experimental results on hy-bridqa showed that the proposed mafid achieved state-of-the-art performances in both the development and blind test sets.,,https://www.aclanthology.org/2023.findings-eacl.177.pdf
2028,258378227,MAFiD: Moving Average Equipped Fusion-in-Decoder for Question Answering over Tabular and Textual Data,conclusion,Insight-tree,"in future work, we will extend mafid to open-domain table-and-text qa and explore a unified approach that integrates single-row and multi-row reasoning.",,https://www.aclanthology.org/2023.findings-eacl.177.pdf
2029,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,we proposed a novel semantic role labeling graph reasoning network (srlgrn) to deal with multihop qa.,,https://arxiv.org/pdf/2010.03604v1.pdf
2030,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,the backbone graph of our proposed graph convolutional network (gcn) is created based on the semantic structure of the sentences.,,https://arxiv.org/pdf/2010.03604v1.pdf
2031,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,"in creating the edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts.",,https://arxiv.org/pdf/2010.03604v1.pdf
2032,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,the cross paragraph argument-predicate structure of the sentences expressed in the graph provides an explicit representation of the reasoning path and helps in both finding and explaining the multiple hops of reasoning that lead to the final answer.,,https://arxiv.org/pdf/2010.03604v1.pdf
2033,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,srlgrn exceeds most of the sota results on the hotpotqa benchmark.,,https://arxiv.org/pdf/2010.03604v1.pdf
2034,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,"moreover, we evaluate the model (excluding the paragraph selection module) on other reading comprehension benchmarks.",,https://arxiv.org/pdf/2010.03604v1.pdf
2035,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,our approach achieves competitive performance on squad v1.1 and v2.0.,,https://arxiv.org/pdf/2010.03604v1.pdf
2036,254408974,Successive Prompting for Decomposing Complex Questions,conclusion,Insight-tree,"we present a way to successively decompose complex questions into simple qa pairs, which allows for modular qd and qa systems that can be trained and queried independently.",,https://www.aclanthology.org/2022.emnlp-main.81.pdf
2037,254408974,Successive Prompting for Decomposing Complex Questions,conclusion,Insight-tree,"when performing in-context learning, we showed that successive prompting yields an improvement of 4.6 f1 over chain-of-thought prompting.",,https://www.aclanthology.org/2022.emnlp-main.81.pdf
2038,254408974,Successive Prompting for Decomposing Complex Questions,conclusion,Insight-tree,"when replacing just the in-context qa module with a fine-tuned one, which is adept at handling list type questions, we further improve the overall performance by 9.5 f1.",,https://www.aclanthology.org/2022.emnlp-main.81.pdf
2039,155100120,Dynamically Fused Graph Network for Multi-hop Reasoning,conclusion,Insight-tree,we introduce dynamically fused graph network (dfgn) to address multi-hop reasoning.,,https://arxiv.org/pdf/1905.06933v2.pdf
2040,155100120,Dynamically Fused Graph Network for Multi-hop Reasoning,conclusion,Insight-tree,"specifically, we propose a dynamic fusion reasoning block based on graph neural networks.",,https://arxiv.org/pdf/1905.06933v2.pdf
2041,155100120,Dynamically Fused Graph Network for Multi-hop Reasoning,conclusion,Insight-tree,"different from previous approaches in qa, dfgn is capable of predicting the sub-graphs dynamically at each reasoning step, and the entity-level reasoning is fused with token-level context.",,https://arxiv.org/pdf/1905.06933v2.pdf
2042,155100120,Dynamically Fused Graph Network for Multi-hop Reasoning,conclusion,Insight-tree,we evaluate dfgn on hotpotqa and achieve leading results.,,https://arxiv.org/pdf/1905.06933v2.pdf
2043,155100120,Dynamically Fused Graph Network for Multi-hop Reasoning,conclusion,Insight-tree,"besides, our analysis shows dfgn can produce reliable and explainable reasoning chains.",,https://arxiv.org/pdf/1905.06933v2.pdf
2044,155100120,Dynamically Fused Graph Network for Multi-hop Reasoning,conclusion,Insight-tree,"in the future, we may incorporate new advances in building entity graphs from texts, and solve harder reasoning problems, e.g. ""comparison"" in hot-potqa.",,https://arxiv.org/pdf/1905.06933v2.pdf
2045,231632613,Understanding in Artificial Intelligence,conclusion,Insight-tree,the paper considers the components an artificial intelligence system that understands should have.,,https://arxiv.org/pdf/2101.06573v1.pdf
2046,231632613,Understanding in Artificial Intelligence,conclusion,Insight-tree,"that is, a system that not only learns statistical relationships within the data, but is capable of forming a human-like understanding     table 1: cap1: hierarchical and compositional knowledge representation, cap2: multimodal structure-to-structure mapping, cap3: integrates symbolic and non-symbolic knowledge, cap4: supports symbolic reasoning with uncertainties of the input data.",,https://arxiv.org/pdf/2101.06573v1.pdf
2047,231632613,Understanding in Artificial Intelligence,conclusion,Insight-tree,"this is most certainly a truly difficult problem to solve and the purpose of the paper is not to claim a general solution for solving it, but to look at several research streams and some of their latest developments.",,https://arxiv.org/pdf/2101.06573v1.pdf
2048,231632613,Understanding in Artificial Intelligence,conclusion,Insight-tree,"furthermore, several benchmarks are described, which have been used to study certain characteristics of an ai that understands.",,https://arxiv.org/pdf/2101.06573v1.pdf
2049,231632613,Understanding in Artificial Intelligence,conclusion,Insight-tree,"the work also contributes to a growing interest in artificial intelligence systems that are interpretable and transparent; properties that are crucial in domains, such as medical diagnosis.",,https://arxiv.org/pdf/2101.06573v1.pdf
2050,254854493,Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation,discussion and conclusion,Insight-tree,"in this study, a series of scalable augmentation techniques are proposed to produce surrogate queries for training dense retrievers without using any annotated pairs.",,https://export.arxiv.org/pdf/2212.08841v1.pdf
2051,254854493,Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation,discussion and conclusion,Insight-tree,"we achieve state-of-the-art performance on two collections of widely used benchmarks (beir and six odqa datasets), demonstrating that the inductive bias of the synthetic querydoc pairs is effective for training dense retrievers, greatly bridging the gap between unsupervised dense models and bm25 and inspiring us to rethink the necessity of using real queries.",,https://export.arxiv.org/pdf/2212.08841v1.pdf
2052,226281978,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,"we proposed long range arena (lra), a new benchmark for evaluating progress on efficient transformer research.",,https://arxiv.org/pdf/2011.04006v1.pdf
2053,226281978,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,"our new benchmark is challenging and probes at model capabilities in dealing with diverse data types and structures such as text, mathematics, and visual data.",,https://arxiv.org/pdf/2011.04006v1.pdf
2054,226281978,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,our benchmark comprises of tasks ranging from 1k to 16k tokens.,,https://arxiv.org/pdf/2011.04006v1.pdf
2055,226281978,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,"for the first time, we conduct an extensive side-by-side comparison of ten recently proposed efficient transformer models.",,https://arxiv.org/pdf/2011.04006v1.pdf
2056,226281978,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,the experimental results show that these tasks are very challenging even for long-range transformer models.,,https://arxiv.org/pdf/2011.04006v1.pdf
2057,226281978,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,the overall results show that there is no one-size-fits-all solution and trade-offs have to be made in terms of model quality and speed/memory.,,https://arxiv.org/pdf/2011.04006v1.pdf
2058,226281978,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,"we plan to open source our code and benchmarks to facilitate future benchmarking, research and model development.",,https://arxiv.org/pdf/2011.04006v1.pdf
2059,256231203,Understanding and Improving Deep Graph Neural Networks: A Probabilistic Graphical Model Perspective,conclusion and future work,Insight-tree,"in this paper, we developed a unified theoretical framework to understand the gnn baselines as well as various deep  gnn models using a graphical model representation.",,https://export.arxiv.org/pdf/2301.10536v1.pdf
2060,256231203,Understanding and Improving Deep Graph Neural Networks: A Probabilistic Graphical Model Perspective,conclusion and future work,Insight-tree,"specifically, we obtained an iterative solution of variational inference on markov random fields, and the propagation operation of gnns can be represented as approximate forms of it.",,https://export.arxiv.org/pdf/2301.10536v1.pdf
2061,256231203,Understanding and Improving Deep Graph Neural Networks: A Probabilistic Graphical Model Perspective,conclusion and future work,Insight-tree,we also proposed a theoretically motivated and powerful gnn which performs well on both shallow and deep network layers.,,https://export.arxiv.org/pdf/2301.10536v1.pdf
2062,256231203,Understanding and Improving Deep Graph Neural Networks: A Probabilistic Graphical Model Perspective,conclusion and future work,Insight-tree,"an interesting direction for future work is to establish the connection between the approximate sampling methods, and the graph neural network to pursue a faster and more powerful sample-based gnn hamilton et al., 2017;hasanzadeh et al., 2020].",,https://export.arxiv.org/pdf/2301.10536v1.pdf
2063,256231203,Understanding and Improving Deep Graph Neural Networks: A Probabilistic Graphical Model Perspective,conclusion and future work,Insight-tree,"to complete the picture, understanding and improving the general gnn with the help of other variational methods would also be interesting.",,https://export.arxiv.org/pdf/2301.10536v1.pdf
2064,239885904,Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable,conclusion and future work,Insight-tree,"we propose a three-stage framework of relation extractor-reader and comparator (rerc), which solves the multi-hop qa task through the idea of complex question decomposition, and obtains the state-of-the-art results in the 2wikimultihopqa dataset, which is close to human performance.",,https://arxiv.org/pdf/2110.13472v1.pdf
2065,239885904,Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable,conclusion and future work,Insight-tree,our rerc framework can also provide faithful evidence with excellent interpretability.,,https://arxiv.org/pdf/2110.13472v1.pdf
2066,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,"in this paper, we explored the novel domain of document-centered digital assistance.",,https://arxiv.org/pdf/2002.00747v1.pdf
2067,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,"we focused on a consumption scenario, in which individuals are a (co-)owner of a document.",,https://arxiv.org/pdf/2002.00747v1.pdf
2068,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,"through a survey, we identified a set of primary capabilities people expect from a digital assistant in a document-centered scenario, as well as a large set of questions that gave us insight into the types of queries that people might pose about a document when they have an approximate or good idea what the document is about.",,https://arxiv.org/pdf/2002.00747v1.pdf
2069,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,"our explorations shed light on the hierarchy of questions that might be posed, and demonstrate that the types of questions people ask in a document-centered scenario are different from the factoid questions in conventional qa datasets.",,https://arxiv.org/pdf/2002.00747v1.pdf
2070,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,we show that state-of-the-art qa models can be finetuned to perform with reasonable accuracy on the new dqa data.,,https://arxiv.org/pdf/2002.00747v1.pdf
2071,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,"yet, it has proven to be an unsolved task, which makes this a fertile area for future work.",,https://arxiv.org/pdf/2002.00747v1.pdf
2072,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,this research opens a new direction for digital assistance.,,https://arxiv.org/pdf/2002.00747v1.pdf
2073,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,"avenues for future work include deeper explorations of query rewriting to better tailor document-centered questions to conventional qa systems, and also exploring ways to scale up the data to a much larger and broader range of documents.  ",,https://arxiv.org/pdf/2002.00747v1.pdf
2074,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,limitations of retrieval,Insight-tree,"our focus in this paper has been on recognizing valid chains of reasoning, assuming a retrieval step that retrieves a reasonable pool of candidates to start with (section 3.2).",,https://arxiv.org/pdf/2010.03274v1.pdf
2075,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,limitations of retrieval,Insight-tree,"however, the retrieval step itself is not perfect: for qasc, designed so that at least one valid chain always exists, the retrieved pool of 10 contains no valid chains for 24% of the questions (upper bound in table 2), capping the overall system's performance.",,https://arxiv.org/pdf/2010.03274v1.pdf
2076,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,limitations of retrieval,Insight-tree,"to gauge the performance of our model when coupled with an improved retrieval system, we ran an experiment where, at test time, we explicitly add the gold chain to the candidate pool if it does not get retrieved (and even if there is some other valid chain already in the pool).",,https://arxiv.org/pdf/2010.03274v1.pdf
2077,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,limitations of retrieval,Insight-tree,we find the p@1 score rises from 0.54 (table 2) to 0.82 (upper bound is now 1.0).,,https://arxiv.org/pdf/2010.03274v1.pdf
2078,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,limitations of retrieval,Insight-tree,"this indicates the model scoring algorithm is performing well, and that improving the retrieval system, e.g., by considering may more chains per question or modifying the search algorithm itself, is likely to have the biggest impact on improving the overall system.",,https://arxiv.org/pdf/2010.03274v1.pdf
2079,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,limitations of retrieval,Insight-tree,"note also that the corpus itself is an important component: finding valid chains requires the corpus to contain a broad diversity of general facts to build chains from, hence expanding/filtering the corpus itself is another avenue for improvement.",,https://arxiv.org/pdf/2010.03274v1.pdf
2080,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,summary and conclusion,Insight-tree,"explaining answers to multihop questions is important for understanding why an answer may be correct, but there is currently a dearth of suitable, annotated data.",,https://arxiv.org/pdf/2010.03274v1.pdf
2081,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,summary and conclusion,Insight-tree,"to address this, and promote progress in explanation, we contribute three new explanation datasets, including one with over 98k annotated reasoning chains -by far the largest repository of annotated, corpus-derived explanations to date.",,https://arxiv.org/pdf/2010.03274v1.pdf
2082,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,summary and conclusion,Insight-tree,we also have shown this data can significantly improve explanation quality on both in-domain (qasc) and out-of-domain (obqa) tasks.,,https://arxiv.org/pdf/2010.03274v1.pdf
2083,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,summary and conclusion,Insight-tree,"finally, we have proposed and explored using a lightweight method to achieve a delexicalized representation of reasoning chains.",,https://arxiv.org/pdf/2010.03274v1.pdf
2084,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,summary and conclusion,Insight-tree,"while preserving explanation quality (despite removing details), this representation appears to be more robust to certain perturbations.",,https://arxiv.org/pdf/2010.03274v1.pdf
2085,250264121,QA Is the New KR: Question-Answer Pairs as Knowledge Bases,conclusion,Insight-tree,"symbolic kbs organize information into small modular components (e.g., entities, kg triples, wikidata statements) that can be combined compositionally to answer complex queries.",,https://arxiv.org/pdf/2207.00630v1.pdf
2086,250264121,QA Is the New KR: Question-Answer Pairs as Knowledge Bases,conclusion,Insight-tree,"while many recent papers have focused on tasks like open qa, where questions are answered from text without using a kb, broad-coverage symbolic kbs continue to be widely used in practice, and, despite recent progress in methods for ""multi-hop"" qa, are still the only computationally efficient way of answering questions that combine information for multiple documents.",,https://arxiv.org/pdf/2207.00630v1.pdf
2087,250264121,QA Is the New KR: Question-Answer Pairs as Knowledge Bases,conclusion,Insight-tree,"however, the broad-coverage kbs that are currently in wide use are largely collections of information that easily collected and integrated, and need not reflect the actual infor-mation needs of users.",,https://arxiv.org/pdf/2207.00630v1.pdf
2088,250264121,QA Is the New KR: Question-Answer Pairs as Knowledge Bases,conclusion,Insight-tree,"in this position paper, we advocate for a new approach to constructing kbs, and in particular, an approach to collecting modular, compositionally-combinable knowledge components from text, driven by a sample of user's questions and answers.",,https://arxiv.org/pdf/2207.00630v1.pdf
2089,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"in this work, we propose a generative question answering (qa) approach that models multi-hop qa as a single sequence prediction task.",,https://www.aclanthology.org/2022.acl-long.69.pdf
2090,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,it learns to generate an answer along with a reasoning path to improve its capability of multi-hop reasoning.,,https://www.aclanthology.org/2022.acl-long.69.pdf
2091,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"our experiments on prominent multi-hop qa benchmarks, hotpotqa and iirc, validate the promise and effectiveness of our proposed method path-fid and its extension pathfid+.",,https://www.aclanthology.org/2022.acl-long.69.pdf
2092,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,future work will explore (1) our pathfid approach more closely with text retrieval models in open-domain qa scenarios and (2) more explicit grounding on the input information to make our approach even more interpretable and controllable.,,https://www.aclanthology.org/2022.acl-long.69.pdf
2093,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"in figure 4 and 5, we visualize the correlation between supporting evidence and answer prediction performances for comparison and bridge question types, respectively.",,https://www.aclanthology.org/2022.acl-long.69.pdf
2094,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"to obtain these plots, we first split the examples into 10 buckets where n-th bucket contains the examples with support-f1 score in (10 * (n  1), 10 * n] percentile for n = {1, 2, . .",,https://www.aclanthology.org/2022.acl-long.69.pdf
2095,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,.,,https://www.aclanthology.org/2022.acl-long.69.pdf
2096,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,", 10}.",,https://www.aclanthology.org/2022.acl-long.69.pdf
2097,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"then, we take the average answer prediction accuracy (both em and f1) over these examples for each bucket, and report this number on the y-axis of the plot at the corresponding support-f1 bucket on the x-axis, while dropping the empty buckets.",,https://www.aclanthology.org/2022.acl-long.69.pdf
2098,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,note that x = 0 corresponds to examples with support-f1 score of 0.,,https://www.aclanthology.org/2022.acl-long.69.pdf
2099,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,also note that the size of a data point on the figure reflects the number of examples in the corresponding bucket as also indicated by the legend.,,https://www.aclanthology.org/2022.acl-long.69.pdf
2100,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"from figures 4 and 5, we can observe that the accuracy of the generated answers is significantly lower, 30% for bridge and 10% for comparison, for the first bucket with zero support-f1 compared to buckets with positive support-f1 score.",,https://www.aclanthology.org/2022.acl-long.69.pdf
2101,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,this suggests that the model has a difficult time figuring out the an-swer when the supporting evidence prediction is poor.,,https://www.aclanthology.org/2022.acl-long.69.pdf
2102,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,another observation that holds for both categories is the general trend of increased answer quality as the supporting fact prediction improves.,,https://www.aclanthology.org/2022.acl-long.69.pdf
2103,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"combining these two points provide additional evidence (in addition to table 2 in the main paper) implicitly supporting the answer generation process of pathfid being grounded on the generated supporting facts, which is generated as the prefix of the answer segment in the full decoded reasoning path sequence during inference.",,https://www.aclanthology.org/2022.acl-long.69.pdf
2104,236486285,Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering,conclusion and future work,Insight-tree,"in this paper, the application of transfer learning by utilizing transformer models like roberta, al-bert and electra for question answering span prediction task was explored.",,https://www.aclanthology.org/2021.dialdoc-1.9.pdf
2105,236486285,Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering,conclusion and future work,Insight-tree,"we also experimented with pretrained models on several other datasets prior to fine-tuning it on the dialdoc21 dataset, provided as part of the dialdoc21 shared task.",,https://www.aclanthology.org/2021.dialdoc-1.9.pdf
2106,236486285,Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering,conclusion and future work,Insight-tree,maximum confidence score based ensemble techniques were employed to combine various base transformer models to further boost the per-formance.,,https://www.aclanthology.org/2021.dialdoc-1.9.pdf
2107,236486285,Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering,conclusion and future work,Insight-tree,we plan to extend our approach and experiment with other ensembling techniques for further enhancing the performance and also explore avenues for improved scalability when applied to larger datasets.,,https://www.aclanthology.org/2021.dialdoc-1.9.pdf
2108,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,limitations and difficulties of present work,Insight-tree,the test results have indicated that the system developed needs further enhancement such as optimization of the system performance and efficiency.,,
2109,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,limitations and difficulties of present work,Insight-tree,some of the suggestions are: fig.,,
2110,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,limitations and difficulties of present work,Insight-tree,-19,,
2111,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,limitations and difficulties of present work,Insight-tree,blast alignment results.,,
2112,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,conclusion,Insight-tree,the results obtained from the tests have shown that the integrated software tool performs as expected.,,
2113,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,conclusion,Insight-tree,"the work suggested that by integrating blast and fasta (two widely used and freely available algorithms), plus an additional implementation of psa and tr analysis tools, with the rest of the supporting tools (database management) developed, it is entirely possible to have an initial working version of the software tool for criminal dna analysis and de-tection work.",,
2114,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,conclusion,Insight-tree,the system has great potential and that the results obtained during the tests were satisfactory.,,
2115,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,conclusion,Insight-tree,"the following observations can be made:  psa algorithm is suitable for smaller sequence alignments as compared to blast and fasta, which are designed for large sequence database searches.",,
2116,247476217,Tracing Origins: Coreference-aware Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we present intuitive methods to solve coreference-intensive machine reading comprehension tasks by following the reading process of human in which people connect the anaphoric expressions with explicit instructions.",,https://arxiv.org/pdf/2110.07961v2.pdf
2117,247476217,Tracing Origins: Coreference-aware Machine Reading Comprehension,conclusion,Insight-tree,"we demonstrate that all our three fine-tuning methods, including coref gnn , coref addatt and coref multiatt , are superior to the pre-trained language models that incorporate the coreference information in the pretraining stage, such as corefroberta large .",,https://arxiv.org/pdf/2110.07961v2.pdf
2118,247476217,Tracing Origins: Coreference-aware Machine Reading Comprehension,conclusion,Insight-tree,"as the fine-tuning methods rely on the coreference resolution models supplied by other researchers, their performance is also constrained by the accuracy of those coreference resolution models.",,https://arxiv.org/pdf/2110.07961v2.pdf
2119,247476217,Tracing Origins: Coreference-aware Machine Reading Comprehension,conclusion,Insight-tree,"in addition, the questions that require multistep reasoning, span multiple entities or contain multiple answer items also pose the challenges to our models.",,https://arxiv.org/pdf/2110.07961v2.pdf
2120,247476217,Tracing Origins: Coreference-aware Machine Reading Comprehension,conclusion,Insight-tree,"in the future, with more in-depth study on human reasoning in reading comprehension and more progress in graph neural networks, the gnnbased coreference graph can be enriched with more edge types and diverse structures to leverage more linguistic knowledge and gain better performance.",,https://arxiv.org/pdf/2110.07961v2.pdf
2121,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,multi-hop qa has been researched quite frequently in the recent years with multiple diverse models proposed that aim to model the multi-step retrieval-reasoning process and achieve promising improvements on existing datasets and benchmarks.,,https://arxiv.org/pdf/2204.09140v1.pdf
2122,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,"such systems capable of performing multistep reasoning have a variety of applications ranging from chat-bot assistants that are capable of interactive conversations, to search engines that are capable to retrieve results that may be relevant but not reachable directly from the query text.",,https://arxiv.org/pdf/2204.09140v1.pdf
2123,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,at the same time the task of mhqa is significantly more challenging than its single hop counterpart.,,https://arxiv.org/pdf/2204.09140v1.pdf
2124,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,"since paragraphs multiple hops away from the question could share few common words and little semantic relation with the question [34], the task to retrieve such contexts is challenging and suffers from semantic drift.",,https://arxiv.org/pdf/2204.09140v1.pdf
2125,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,the ability of current models to combine multiple contexts for reasoning is also limited.,,https://arxiv.org/pdf/2204.09140v1.pdf
2126,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,"further challenges for solving mhqa is the difficult process of creating datasets that require the models to perform multi-hop reasoning, as well as the task of evaluating the models' abilities to do so without any hacks.",,https://arxiv.org/pdf/2204.09140v1.pdf
2127,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,some challenging benchmarks and evaluation methods have been recently proposed that bring out some surprising and interesting observations.,,https://arxiv.org/pdf/2204.09140v1.pdf
2128,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,these results point out to several limitations of existing systems and call for further research.,,https://arxiv.org/pdf/2204.09140v1.pdf
2129,202583429,Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering,conclusion,Insight-tree,this paper introduces an important sub-problem of bridge reasoning for the task of multi-hop qa in the open-domain setting.,,https://www.aclweb.org/anthology/D19-5806.pdf
2130,202583429,Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering,conclusion,Insight-tree,we propose a bridge reasoner that utilizes multiple types of evidence to derive the passages that cover the answers.,,https://www.aclweb.org/anthology/D19-5806.pdf
2131,202583429,Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering,conclusion,Insight-tree,the reasoner significantly improves the coverage of answer passages than ir methods.,,https://www.aclweb.org/anthology/D19-5806.pdf
2132,202583429,Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering,conclusion,Insight-tree,"with the predicted passages, we show that a standard reading comprehension model is able to achieve similar performance as the state-of-the-art method that requires bert in multiple modules.",,https://www.aclweb.org/anthology/D19-5806.pdf
2133,174799117,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,conclusion,Insight-tree,we presented a neural approach for resolving cross-document event and entity coreference.,,https://www.aclweb.org/anthology/P19-1409.pdf
2134,174799117,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,conclusion,Insight-tree,"we represent a mention using its text, context, andinspired by the joint model of lee et al.",,https://www.aclweb.org/anthology/P19-1409.pdf
2135,174799117,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,conclusion,Insight-tree,"(2012)we make an event mention representation aware of coreference clusters of entity mentions to which it is related via predicate-argument structures, and vice versa.",,https://www.aclweb.org/anthology/P19-1409.pdf
2136,174799117,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,conclusion,Insight-tree,"our model achieves state-of-the-art results, outperforming previous models by 10.5 conll f 1 points on events, and providing the first cross-document entity coreference results on ecb+.",,https://www.aclweb.org/anthology/P19-1409.pdf
2137,174799117,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,conclusion,Insight-tree,"future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions.",,https://www.aclweb.org/anthology/P19-1409.pdf
2138,249049410,Generative Multi-hop Retrieval,conclusion,Insight-tree,"in this paper, we show that the bi-encoder approach has limitations in multi-hop retrieval; the bottleneck problem becomes a more severe problem as the number of hops increases, and is more susceptible to error propagation.",,https://www.aclanthology.org/2022.emnlp-main.92.pdf
2139,249049410,Generative Multi-hop Retrieval,conclusion,Insight-tree,"we present generative multi-hop retrieval (gmr), an encoder-decoder model that performs retrieval by generating the entire target sequences with the aid of constrained decoding.",,https://www.aclanthology.org/2022.emnlp-main.92.pdf
2140,249049410,Generative Multi-hop Retrieval,conclusion,Insight-tree,we show that gmr is more robust on multi-hop retrieval tasks where it achieves higher or comparable performance in five datasets.,,https://www.aclanthology.org/2022.emnlp-main.92.pdf
2141,249049410,Generative Multi-hop Retrieval,conclusion,Insight-tree,"we also introduce two corpus memorization methods, lm memorization and multi-hop memorization, to further improve gmr's performance.",,https://www.aclanthology.org/2022.emnlp-main.92.pdf
2142,249049410,Generative Multi-hop Retrieval,conclusion,Insight-tree,"our experimental results demonstrate that in multi-hop retrieval, a generative approach is highly competitive with bi-encoder methods and deserves further explorations in the community.",,https://www.aclanthology.org/2022.emnlp-main.92.pdf
2143,252624423,eRock at Qur'an QA 2022: Contemporary Deep Neural Networks for Qur'an based Reading Comprehension Question Answers,conclusion and future work,Insight-tree,"we attempted to solve quranqa shared task using bert (devlin et al., 2018) from scratch as well as fine-tuned over two different pre-trained variants.",,https://www.aclanthology.org/2022.osact-1.11.pdf
2144,252624423,eRock at Qur'an QA 2022: Contemporary Deep Neural Networks for Qur'an based Reading Comprehension Question Answers,conclusion and future work,Insight-tree,moreover we opted for data augmentation and weight-decay regularization techniques to improve performance over the task.,,https://www.aclanthology.org/2022.osact-1.11.pdf
2145,232320760,Mitigating False-Negative Contexts in Multi-Document Question Answering with Retrieval Marginalization,conclusion,Insight-tree,we proposed a new probabilistic model for retrieving set-valued contexts for multi-document qa and show that training the qa model with marginalization over this set can help mitigate the false negatives in evidence annotations.,,https://www.aclanthology.org/2021.emnlp-main.497.pdf
2146,232320760,Mitigating False-Negative Contexts in Multi-Document Question Answering with Retrieval Marginalization,conclusion,Insight-tree,experiments on iirc and hotpotqa fullwiki show that our proposed framework can learn to retrieve unlabeled alternative contexts and improves qa f1 by 5.5 on iirc and 8.9 on hotpotqa.,,https://www.aclanthology.org/2021.emnlp-main.497.pdf
2147,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,this work contributes a novel framework for domain adaptation of qa systems in settings with limited text corpora.,,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2148,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,we develop caqa in which we combine techniques from from question generation and domain-invariant learning to answer out-of-domain questions.,,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2149,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,"different from existing works in question answering, we achieve this by proposing a contrastive adaptation loss.",,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2150,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,extensive experiments show that caqa is superior to other state-of-the-art approaches by achieving a substantially better performance on out-of-domain data.,,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2151,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,"qagen-t5: we apply lm-filtering as in (shakeri et al., 2020) and select qa pairs with highest scores for each context paragraph.",,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2152,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,"qagen-t5 models are trained similarly to aqgen and qagen, we separately keep the best qg and qa models according to validation performance on the squad dev set.",,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2153,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,"hyperparameter search: in our experiments, we empirically search for hyperparameters  and  in the contrastive adaptation loss through additional experiments.",,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2154,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,"we experiment with different values of  in the range [10 1 , 10 2 , 10 3 ] and gaussian noise n (0, ) applied on all token embeddings with standard deviation  ranging from 0 to 10 2 .",,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2155,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,"the best combination of  and  as per the training set is then selected, these numbers can be found in table 4.",,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2156,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,all parameters that have not been mentioned explicitly above were used as reported in their original paper b additional results,,https://www.aclanthology.org/2021.emnlp-main.754.pdf
2157,247594811,Simulating Bandit Learning from User Feedback for Extractive Question Answering,conclusion,Insight-tree,we present a simulation study of learning from user feedback for extractive qa.,,https://www.aclanthology.org/2022.acl-long.355.pdf
2158,247594811,Simulating Bandit Learning from User Feedback for Extractive Question Answering,conclusion,Insight-tree,we formulate the problem as contextual bandit learning.,,https://www.aclanthology.org/2022.acl-long.355.pdf
2159,247594811,Simulating Bandit Learning from User Feedback for Extractive Question Answering,conclusion,Insight-tree,"we conduct experiments to show the effectiveness of such feedback, the robustness to feedback noise, the impact of initial model performance, the trade-offs between online and offline learning, and the potential for domain adaptation.",,https://www.aclanthology.org/2022.acl-long.355.pdf
2160,247594811,Simulating Bandit Learning from User Feedback for Extractive Question Answering,conclusion,Insight-tree,our study design emphasizes the potential for reducing annotation costs by annotating few examples or by utilizing existing datasets for new domains.,,https://www.aclanthology.org/2022.acl-long.355.pdf
2161,234095341,VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension,conclusions,Insight-tree,"in this work we introduce and examine a powerful yet simple model for reading comprehension on long texts which we call vault, based on the hypothesis that with a large sequence length long answers can be classified effectively without computationally heavy graph-based models.",,https://www.aclanthology.org/2021.acl-short.131.pdf
2162,234095341,VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension,conclusions,Insight-tree,we validate our approach by showing it yields f1 scores competitive with heavier methods at a fraction of the decoding cost on two very different domain benchmark datasets that require reading long texts.,,https://www.aclanthology.org/2021.acl-short.131.pdf
2163,234095341,VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension,conclusions,Insight-tree,compare the correct answers produced by vault with the incorrect answers produced by the ablated model from the last row of table 3 (nq) and roberta baseline from the first row of table 2 (techqa).,,https://www.aclanthology.org/2021.acl-short.131.pdf
2164,234334398,READTWICE: Reading Very Large Documents with Memories,conclusion & future work,Insight-tree,"readtwice performs well on several qa tasks, particularly narrativeqa where long-range dependencies among entities appear to be very important.",,https://arxiv.org/pdf/2105.04241v2.pdf
2165,234334398,READTWICE: Reading Very Large Documents with Memories,conclusion & future work,Insight-tree,"the proposed method is conceptually simple, easy to implement and is capable of reading entire books.",,https://arxiv.org/pdf/2105.04241v2.pdf
2166,234334398,READTWICE: Reading Very Large Documents with Memories,conclusion & future work,Insight-tree,"for future work, we plan to explore new memory types, hierarchies and aggregation functions.",,https://arxiv.org/pdf/2105.04241v2.pdf
2167,234334398,READTWICE: Reading Very Large Documents with Memories,conclusion & future work,Insight-tree,"we also aim to apply the model to other tasks, particularly long text summarization, likely to benefit from a memory-forming mechanism.",,https://arxiv.org/pdf/2105.04241v2.pdf
2168,222090978,A Survey on Explainability in Machine Reading Comprehension,conclusion and open research questions,Insight-tree,this survey has proposed a systematic categorisation of benchmarks and approaches for explainability in mrc.,,https://arxiv.org/pdf/2010.00389v1.pdf
2169,222090978,A Survey on Explainability in Machine Reading Comprehension,conclusion and open research questions,Insight-tree,"lastly, we outline a set of open research questions for future work:",,https://arxiv.org/pdf/2010.00389v1.pdf
2170,252917981,PROMPTING GPT-3 TO BE RELIABLE,conclusion,Insight-tree,"our work systematically studies the reliability of gpt-3 from four key facets: generalizability, fairness, calibration, and factuality.",,https://export.arxiv.org/pdf/2210.09150v2.pdf
2171,252917981,PROMPTING GPT-3 TO BE RELIABLE,conclusion,Insight-tree,we develop effective prompting strategies to make gpt-3 outperform supervised models by large margins on these facets.,,https://export.arxiv.org/pdf/2210.09150v2.pdf
2172,252917981,PROMPTING GPT-3 TO BE RELIABLE,conclusion,Insight-tree,our work reveals new insights of llms and provides practical recommendations for users of gpt-3.,,https://export.arxiv.org/pdf/2210.09150v2.pdf
2173,252917981,PROMPTING GPT-3 TO BE RELIABLE,conclusion,Insight-tree,"we hope our work can inspire more future work to: (1) examine more facets of reliability, such as avoiding harmful generations; (2) apply the prompting methods in this paper to more real-world applications, such as incorporating human feedback for collaborative multi-step planning; (3) further explore more effective prompting strategies to improve reliability, such as post-hoc calibration on language model probabilities.",,https://export.arxiv.org/pdf/2210.09150v2.pdf
2174,234679223,QAConv: Question Answering on Informative Conversations,conclusion,Insight-tree,"qaconv is a new dataset that conducts qa on informative conversations such as emails, panels, and channels.",,https://arxiv.org/pdf/2105.06912v2.pdf
2175,234679223,QAConv: Question Answering on Informative Conversations,conclusion,Insight-tree,we show the unique challenges of our tasks in both chunk mode with oracle partial conversations and full mode with a retrieval stage.,,https://arxiv.org/pdf/2105.06912v2.pdf
2176,234679223,QAConv: Question Answering on Informative Conversations,conclusion,Insight-tree,we find that state-of-the-art qa models have limited dialogue understanding and tend to predict our answerable qa pairs as unanswerable.,,https://arxiv.org/pdf/2105.06912v2.pdf
2177,234679223,QAConv: Question Answering on Informative Conversations,conclusion,Insight-tree,we provide a new testbed for qa on conversation tasks to facilitate future research.,,https://arxiv.org/pdf/2105.06912v2.pdf
2178,232307674,Complementary Evidence Identification in Open-Domain Question Answering,conclusion,Insight-tree,"in the paper, we propose a new problem of complementary evidence identification and define the criterion of complementary evidence in vector space.",,https://www.aclweb.org/anthology/2021.eacl-main.234.pdf
2179,232307674,Complementary Evidence Identification in Open-Domain Question Answering,conclusion,Insight-tree,we further design an algorithm and a loss function to support efficient training and inference for complementary evidence selection.,,https://www.aclweb.org/anthology/2021.eacl-main.234.pdf
2180,232307674,Complementary Evidence Identification in Open-Domain Question Answering,conclusion,Insight-tree,"compared to the baseline, our approach improves more than 20% and remains to scale well to the computationally complex cases.",,https://www.aclweb.org/anthology/2021.eacl-main.234.pdf
2181,158046817,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,conclusion,Insight-tree,we propose a new gnn-based method for multihop rc across multiple documents.,,https://arxiv.org/pdf/1905.07374v1.pdf
2182,158046817,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,conclusion,Insight-tree,"we introduce the hde graph, a heterogeneous graph for multiple-hop reasoning over nodes representing different granularity levels of information.",,https://arxiv.org/pdf/1905.07374v1.pdf
2183,158046817,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,conclusion,Insight-tree,"we use co-attention and self-attention to encode candidates, documents, entities of mentions of candidates and query subjects into query-aware representations, which are then employed to initialize graph node representations.",,https://arxiv.org/pdf/1905.07374v1.pdf
2184,158046817,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,conclusion,Insight-tree,"evaluated on wiki-hop, our end-to-end trained single neural model achieves the state-of-the-art performance on the blind test set.",,https://arxiv.org/pdf/1905.07374v1.pdf
2185,158046817,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,conclusion,Insight-tree,"in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa.",,https://arxiv.org/pdf/1905.07374v1.pdf
2186,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,we consider three types of attackers with different capabilities for collecting rappor reports.the least powerful attacker has access to a single report from each user and is limited by one-time differential privacy level 1 on how much knowledge gain is possible.,,https://arxiv.org/pdf/1407.6981v2.pdf
2187,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,this attacker corresponds to an eavesdropper that has temporary ability to snoop on the users' reports.a windowed attacker is presumed to have access to one client's data over a well-defined period of time.,,https://arxiv.org/pdf/1407.6981v2.pdf
2188,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"this attacker, depending on the sophistication of her learning model, could learn more information about a user than the attacker of the first type.",,https://arxiv.org/pdf/1407.6981v2.pdf
2189,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"nevertheless, the improvement in her ability to violate privacy is strictly bounded by the longitudinal differential privacy guarantee of . this more powerful attacker may correspond to an adversary such as a malicious cloud service employee, who may have temporary access to reports, or access to a time-bounded log of reports.",,https://arxiv.org/pdf/1407.6981v2.pdf
2190,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,the third type of attacker is assumed to have unlimited collection capabilities and can learn the permanent randomized response b with absolute certainty.,,https://arxiv.org/pdf/1407.6981v2.pdf
2191,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"because of the randomization performed to obtain b from b, she is also bounded by the privacy guarantee of  and cannot improve upon this bound with more data collection.",,https://arxiv.org/pdf/1407.6981v2.pdf
2192,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"this corresponds to a worst-case adversary, but still one that doesn't have direct access to the true data values on the client.despite envisioning a completely local privacy model, one where users themselves release data in a privacy-preserving fashion, operators of rappor collections, however, can easily manipulate the process to learn more information than warranted by the nominal . soliciting users to participate more than once in a particular collection results in multiple permanent randomized responses for each user and partially defeats the benefits of memoization.",,https://arxiv.org/pdf/1407.6981v2.pdf
2193,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"in the webcentric world, users use multiple accounts and multiple devices and can unknowingly participate multiple times, releasing more information than what they expected.",,https://arxiv.org/pdf/1407.6981v2.pdf
2194,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,this problem could be mitigated to some extent by running collections per account and sharing a common permanent randomized response.,,https://arxiv.org/pdf/1407.6981v2.pdf
2195,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"notice the role of the operator to ensure that such processes are in place and the required or assumed trust on the part of the user.it is likely that some attackers will aim to target specific users by isolating and analyzing reports from that user, or a small group of users that includes them.",,https://arxiv.org/pdf/1407.6981v2.pdf
2196,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"even so, some randomly-chosen users need not fear such attacks at all: with probability 1 2 f h , clients will generate a permanent randomized response b with all 0s at the positions of set bloom filter bits.",,https://arxiv.org/pdf/1407.6981v2.pdf
2197,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"since these clients are not contributing any useful information to the collection process, targeting them individually by an attacker is counter-productive.",,https://arxiv.org/pdf/1407.6981v2.pdf
2198,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,an attacker has nothing to learn about this particular user.,,https://arxiv.org/pdf/1407.6981v2.pdf
2199,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"also, for all users, at all times, there is plausible deniability proportional to the fraction of clients providing no information.in one particular attack scenario, imagine an attacker that is interested in learning whether a given client has a particular value v, whose population frequency is known to be fv.",,https://arxiv.org/pdf/1407.6981v2.pdf
2200,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,the strongest evidence in support of v comes in the form of both bloom filter bits for v being set in the client's report (if two hash functions are used).,,https://arxiv.org/pdf/1407.6981v2.pdf
2201,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,the attacker can formulate its target set by selecting all reports with these two bits set.,,https://arxiv.org/pdf/1407.6981v2.pdf
2202,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"however, this set will miss some clients with v and include other clients who did not report v. false discovery rate (fdr) is the proportion of clients in the target set who reported a value different from v. figure 7 shows fdr as a function of fv, the frequency of the string v. notably, for relatively rare values, most clients in the target set will, in fact, have a value that is different from v, which will hopefully deter any would-be attackers.the main reason for the high fdr rate at low frequencies fv stems from the limited evidence provided by the observed bits in support of v. this is clearly illustrated by figure 8 where the probability that v was reported (1) or not reported (0) by the client is plotted as a function of fv.",,https://arxiv.org/pdf/1407.6981v2.pdf
2203,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"for relatively rare strings (those with less than 10% frequency), even when both bits corresponding to v are set in the report, the probability of v being reported is much smaller than of value v given the two bits observed in a rappor report s corresponding to the two bits set by string v. for rare strings, even when both bits are set to 1 (green lines), it is still much more likely that the client did not report v, but some other value.it not being reported.",,https://arxiv.org/pdf/1407.6981v2.pdf
2204,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"because the prior probability fv is so small, a single client's reports cannot provide sufficient evidence in favor of v.",,https://arxiv.org/pdf/1407.6981v2.pdf
2205,221970302,ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS WITH MULTI-HOP DENSE RETRIEVAL,conclusion,Insight-tree,"in this work, we generalized the recently proposed successful dense retrieval methods by extending them to the multi-hop setting.",,https://arxiv.org/pdf/2009.12756v1.pdf
2206,221970302,ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS WITH MULTI-HOP DENSE RETRIEVAL,conclusion,Insight-tree,this allowed us to handle complex multi-hop queries with much better accuracy and efficiency than the previous best methods.,,https://arxiv.org/pdf/2009.12756v1.pdf
2207,221970302,ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS WITH MULTI-HOP DENSE RETRIEVAL,conclusion,Insight-tree,"we demonstrated the versatility of our approach by applying it to two different tasks, using a variety of downstream modules.",,https://arxiv.org/pdf/2009.12756v1.pdf
2208,221970302,ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS WITH MULTI-HOP DENSE RETRIEVAL,conclusion,Insight-tree,"in addition, the simplicity of the framework and the fact that it does not depend on a corpus-dependent graph structure opens the possibility of applying such multi-hop retrieval methods more easily and broadly cross different domains and settings.",,https://arxiv.org/pdf/2009.12756v1.pdf
2209,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,"in this paper, we addressed three research questions: whether integrating external knowledge can improve explainability for nli, how effective knowledge implicitly stored in language models is for reasoning, and how humans perceive explanation quality of state-of-the-art natural language inference models.",,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2210,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,"to answer these questions, we proposed different methods of integrating various knowledge sources into deep learning models.",,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2211,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,we found that fine-tuned language models reach the highest performance on e-snli as well as the highest average accuracy within the nli stress test evaluation.,,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2212,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,"however, their performance can break down on numerical reasoning and negations.",,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2213,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,"in addition to automatic evaluation, we conducted a large-scale human crowdsourcing evaluation and found that high differences in accuracy, bleu or bleurt scores do not reflect in significant differences in human ratings of explanation correctness, commonsense inclusion, grammar or label correctness.",,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2214,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,"this highlights an alarming disconnect between automatic evaluation scores and human ratings, that puts the real-world utility of recent model improvements into question and requires to re-think automatic evaluation across the field of explainable ai.  ",,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2215,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,b study interface figure 4 shows an example of the study interface used to collect human ratings as discussed in section 5.,,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2216,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,table 4 lists the annotation guidelines used to decide on low/high levels of required external knowledge as discussed in section 5.3.,,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2217,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,table 5 shows example annotations.,,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2218,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,the entailment can be decided by matching identical parts in the premise and the hypothesis.,,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2219,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,premise: a water scene with a sunset in the background.,,https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
2220,202712552,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,conclusion,Insight-tree,we focus on the task of question answering under partial knowledge: a novel task that lies inbetween open-domain qa and reading comprehension.,,https://www.aclweb.org/anthology/D19-1281.pdf
2221,202712552,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,conclusion,Insight-tree,we identify classes of knowledge gaps when reasoning under partial knowledge and collect a dataset targeting one common class of knowledge gaps.,,https://www.aclweb.org/anthology/D19-1281.pdf
2222,202712552,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,conclusion,Insight-tree,"we demonstrate that identifying the knowledge gap first and then reasoning by filling this gap outperforms previous approaches on the openbookqa task, with and even without additional missing fact annotation.",,https://www.aclweb.org/anthology/D19-1281.pdf
2223,202712552,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,conclusion,Insight-tree,"this work opens up the possibility of focusing on other kinds of knowledge gaps and extending this approach to other datasets and tasks (e.g., span prediction).  ",,https://www.aclweb.org/anthology/D19-1281.pdf
2224,202712552,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,conclusion,Insight-tree,"boiling point means temperature above which a liquid boils figure 10: visualization of the models behavior with the predicted span, top predicted relation, and the top fact used by model.",,https://www.aclweb.org/anthology/D19-1281.pdf
2225,202712552,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,conclusion,Insight-tree,the heat map shows the confidence of the model for all the relations for each input sentence (first five) and conceptnet sentencized tuple (last but one) and the back-off tuple (last one) to capture the knowledge in the embeddings.,,https://www.aclweb.org/anthology/D19-1281.pdf
2226,207917676,Bend but Don't Break? Multi-Challenge Stress Test for QA Models,conclusion,Insight-tree,we conclude our discussion by presenting suggestions for good future practices when building and presenting new models and datasets.,,https://www.aclweb.org/anthology/D19-5818.pdf
2227,207917676,Bend but Don't Break? Multi-Challenge Stress Test for QA Models,conclusion,Insight-tree,we constructively offer these points and have no intent to criticize authors whose prior work we reference.,,https://www.aclweb.org/anthology/D19-5818.pdf
2228,235421967,A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering,conclusion,Insight-tree,"to alleviate the spurious solution problem in weakly supervised qa, we propose to explicitly exploit the semantic correlations between a question and its solution via mutual information maximization.",,https://www.aclanthology.org/2021.acl-long.318.pdf
2229,235421967,A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering,conclusion,Insight-tree,"during training, we pair a task-specific model with a question reconstructor which guides the task-specific model to predict solutions that are consistent with the questions.",,https://www.aclanthology.org/2021.acl-long.318.pdf
2230,235421967,A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering,conclusion,Insight-tree,experiments on four qa datasets demonstrate the effectiveness of our learning method.,,https://www.aclanthology.org/2021.acl-long.318.pdf
2231,235421967,A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering,conclusion,Insight-tree,"as shown by automatic and manual analyses, models trained with our method are more resistant to spurious solutions during training, and are more precise in locating information that is relevant to the questions during inference, leading to higher accuracy of both answers and solutions.",,https://www.aclanthology.org/2021.acl-long.318.pdf
2232,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,in this work we studied how distillation and parameter count influence the zero-shot effectiveness of neural retrievers.,,https://export.arxiv.org/pdf/2206.02873v5.pdf
2233,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,"we begin by showing that in-domain effectiveness, i.e., when retrievers are finetuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al.",,https://export.arxiv.org/pdf/2206.02873v5.pdf
2234,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,[14] and gupta et al.,,https://export.arxiv.org/pdf/2206.02873v5.pdf
2235,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,[9] and zhan et,,https://export.arxiv.org/pdf/2206.02873v5.pdf
2236,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,al,,https://export.arxiv.org/pdf/2206.02873v5.pdf
2237,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,[46].,,https://export.arxiv.org/pdf/2206.02873v5.pdf
2238,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,"furthermore, we show that a distilled reranker has better zeroshot effectiveness than much larger non-distilled rerankers, which is an important and desirable feature of deployed models.",,https://export.arxiv.org/pdf/2206.02873v5.pdf
2239,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,"however, our largest reranker significantly outperforms smaller rerankers and achieves a new state of the art across almost all datasets used in our zero-shot experiments.",,https://export.arxiv.org/pdf/2206.02873v5.pdf
2240,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,this suggests that a large number of parameters may play a significant role in the generalization capability of pretrained language models.,,https://export.arxiv.org/pdf/2206.02873v5.pdf
2241,236459873,Explanations for CommonsenseQA: New Dataset and Models,conclusion and future work,Insight-tree,we have presented desiderata of what constitutes an explanation in the case of common-sense qa.,,https://aclanthology.org/2021.acl-long.238.pdf
2242,236459873,Explanations for CommonsenseQA: New Dataset and Models,conclusion and future work,Insight-tree,"based on it, we generated a human-annotated explanation dataset ecqa for commonsenseqa.",,https://aclanthology.org/2021.acl-long.238.pdf
2243,236459873,Explanations for CommonsenseQA: New Dataset and Models,conclusion and future work,Insight-tree,we have also proposed models to retrieve and generate common-sense facts required to justify the answer choice.,,https://aclanthology.org/2021.acl-long.238.pdf
2244,236459873,Explanations for CommonsenseQA: New Dataset and Models,conclusion and future work,Insight-tree,we have publicly released our crowdsourced ecqa dataset and code/models.,,https://aclanthology.org/2021.acl-long.238.pdf
2245,236459873,Explanations for CommonsenseQA: New Dataset and Models,conclusion and future work,Insight-tree,"in future work, we plan to explore directions to design rlbased schemes for joint training of property ranker and property selector components in the xr system and joint training of xgp and xgf-ii to generate free-flow explanation.",,https://aclanthology.org/2021.acl-long.238.pdf
2246,236459873,Explanations for CommonsenseQA: New Dataset and Models,conclusion and future work,Insight-tree,another direction is to improve the accuracy and interpretability of the existing models for commonsenseqa using the ecqa dataset.,,https://aclanthology.org/2021.acl-long.238.pdf
2247,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,limitations and discussions,Insight-tree,"while we highlight some of the major drawbacks of instruction tuning and propose an alternative approach of instead training and retrieving experts in this paper, we do not perform experimental results over mt lms that have more than >11b parameters.",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2248,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,limitations and discussions,Insight-tree,"for example, mt lms with >11b parameters may be less susceptible to negative task transfer because of increased model capacity.",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2249,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,limitations and discussions,Insight-tree,"also, during the inference of unseen tasks, our retrieval mechanism assumes batch inference (i.e. having access to 32 samples of the target tasks without labels).",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2250,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,limitations and discussions,Insight-tree,"finally, when showing the compositional instruction experiments, we assume the two optimal experts could be retrieved from the compositional instruction (concatenation of the two seen instructions) given as the input along with the evaluation instance.",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2251,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,limitations and discussions,Insight-tree,"this might not necessarily be the case with more complex, compositional instructions, which might require a separate decomposition stage.",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2252,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,limitations and discussions,Insight-tree,we instead focus on showing the possibility merging experts can bring and leave developing novel methods of retrieving the optimal experts during inference for future work.,,https://export.arxiv.org/pdf/2302.03202v2.pdf
2253,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,conclusion,Insight-tree,"in this work, we provide an interesting finding that expert lms trained on single tasks show strong generalization capability to unseen tasks, even surpassing mt lms trained on multiple tasks (300+) by a non-trivial margin.",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2254,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,conclusion,Insight-tree,"we leverage this capability and show three main benefits of training and retrieving experts for inference over mt lms, demonstrating that our proposed distributed approach is more robust against negative task transfer, more adapt at learning new tasks, and can perform compositional instructions.",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2255,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,conclusion,Insight-tree,"to this end, we urge the research community to further explore distributed and collaborative training of experts which may have other future benefits including efficiency, privacy, and personalization not explicitly explored in this paper.",,https://export.arxiv.org/pdf/2302.03202v2.pdf
2256,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,"in this work, we try to answer why many mrc models learn shortcut tricks while ignoring the pre-designed comprehension challenges that are purposely embedded in many benchmark datasets.",,https://arxiv.org/pdf/2106.01024v1.pdf
2257,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,we argue that large proportions of shortcut questions in training data push mrc models to rely on shortcut tricks excessively.,,https://arxiv.org/pdf/2106.01024v1.pdf
2258,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,"to properly investigate, we first design two synthetic datasets where each instance has a shortcut version paired with a challenging one which requires paraphrasing, a complex reasoning skill, to answer, rather than performing question word matching or simple matching.",,https://arxiv.org/pdf/2106.01024v1.pdf
2259,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,"with these datasets, we are able to adjust the proportion of shortcut questions in both training and testing, while maintaining other factors relatively steady.",,https://arxiv.org/pdf/2106.01024v1.pdf
2260,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,"we propose two methods to examine the model training process regarding the shortcut questions, which enable us to take a closer look at the learning mechanisms of bidaf and bert under different training settings.",,https://arxiv.org/pdf/2106.01024v1.pdf
2261,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,"we find that learning shortcut questions generally requires less computational resources, and mrc models usually learn the shortcut questions at their early stage of training.",,https://arxiv.org/pdf/2106.01024v1.pdf
2262,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,"our findings reveal that, with larger proportions of shortcut questions for training, mrc models will learn the shortcut tricks quickly while ignoring the designed comprehension challenges, since the remaining truly challenging questions, usually limited in size, may not motivate models to explore sophisticated solutions in the later training stage.",,https://arxiv.org/pdf/2106.01024v1.pdf
2263,218581117,A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering,conclusion,Insight-tree,"we introduce a resource of law statutes, a dataset of hand-curated rules and cases in natural language, and a symbolic solver able to represent these rules and solve the challenge task.",,https://arxiv.org/pdf/2005.05257v2.pdf
2264,218581117,A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering,conclusion,Insight-tree,"our handbuilt solver contrasts with our baselines based on current nlp approaches, even when we adapt them to the legal domain.",,https://arxiv.org/pdf/2005.05257v2.pdf
2265,251407400,FQuAD2.0: French Question Answering and Learning When You Don't Know,conclusion & future work,Insight-tree,"in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions.",,
2266,222067190,MaP: A Matrix-based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we first investigate different approaches of span extraction in mrc.",,https://www.aclweb.org/anthology/2020.aacl-main.69.pdf
2267,222067190,MaP: A Matrix-based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension,conclusion,Insight-tree,"to improve the current vector-based conditional approach, we propose a matrix-based conditional approach.",,https://www.aclweb.org/anthology/2020.aacl-main.69.pdf
2268,222067190,MaP: A Matrix-based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension,conclusion,Insight-tree,more careful consideration of the dependencies between the start and end positions of the answer span can predict their values better.,,https://www.aclweb.org/anthology/2020.aacl-main.69.pdf
2269,222067190,MaP: A Matrix-based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension,conclusion,Insight-tree,we also propose a sampling-based training strategy to address the training process of the matrix-based conditional approach.,,https://www.aclweb.org/anthology/2020.aacl-main.69.pdf
2270,222067190,MaP: A Matrix-based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension,conclusion,Insight-tree,the final experimental results on a wide of datasets demonstrate the effectiveness of our approach and training strategy.,,https://www.aclweb.org/anthology/2020.aacl-main.69.pdf
2271,250526133,A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA,conclusion,Insight-tree,"in this paper, we propose a method that developing models that have different specialized capabilities and fuse their strengths to tackle the task objective of finqa.",,https://arxiv.org/pdf/2207.06490v1.pdf
2272,250526133,A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA,conclusion,Insight-tree,"we first analyze the data distribution of finqa by adversarial validation, and build a new validation dataset that has a similar distribution to test set by re-splitting the training and validation set.",,https://arxiv.org/pdf/2207.06490v1.pdf
2273,250526133,A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA,conclusion,Insight-tree,"besides, in order to reduce the influence of distribution gap, we develop and fuse several small models rather than train a large model which is easily over-fitting to training set.",,https://arxiv.org/pdf/2207.06490v1.pdf
2274,250526133,A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA,conclusion,Insight-tree,"furthermore, we obtain a high performance by fusing the strengths of different small model.",,https://arxiv.org/pdf/2207.06490v1.pdf
2275,250526133,A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA,conclusion,Insight-tree,"overall, we achieve the 1st place in finqa challenge with 71.93% execution accuracy and 67.03% program accuracy.",,https://arxiv.org/pdf/2207.06490v1.pdf
2276,248780575,Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking,conclusion,Insight-tree,we introduce an effective dicos-dst that dynamically selects the relevant dialogue contents corresponding to each slot from a combination of three perspectives.,,https://www.aclanthology.org/2022.acl-long.165.pdf
2277,248780575,Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking,conclusion,Insight-tree,"the dialogue collaborative selector module performs a comprehensive selection for each turn dialogue based on its relation to the slot name, its connection to the current turn dialogue, and the implicit mention oriented reasoning.",,https://www.aclanthology.org/2022.acl-long.165.pdf
2278,248780575,Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking,conclusion,Insight-tree,"then only the selected dialogue contents are fed into state generator, which explicitly minimizes the distracting information passed to the downstream state prediction.",,https://www.aclanthology.org/2022.acl-long.165.pdf
2279,248780575,Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking,conclusion,Insight-tree,"our dicos-dst model achieves new state-of-the-art performance on the multiwoz benchmark, and achieves competitive performance on most other dst benchmark datasets.",,https://www.aclanthology.org/2022.acl-long.165.pdf
2280,248780575,Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking,conclusion,Insight-tree,"the potential relationship among the above perspectives is a promising research direction, and we will explore it for more than dialogue selection in the future.",,https://www.aclanthology.org/2022.acl-long.165.pdf
2281,5669835,"Automatic extraction of dierences between spoken and written languages, and automatic translation from the written to the spoken language",conclusion,Insight-tree,"in this study, w e extracted dierences between spoken and written languages and examined the extracted dierences by using spoken and written data constructed by the communications research laboratory and the national institute for japanese language.",,http://www.lrec-conf.org/proceedings/lrec2002/pdf/27.pdf
2282,5669835,"Automatic extraction of dierences between spoken and written languages, and automatic translation from the written to the spoken language",conclusion,Insight-tree,we also tried transforming written language into spoken language by using extracted dierences as the transformation rules.,,http://www.lrec-conf.org/proceedings/lrec2002/pdf/27.pdf
2283,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,"in this survey, we reviewed 47 textual qa benchmark datasets and their corresponding evaluation metrics.",,https://arxiv.org/pdf/2109.12264v2.pdf
2284,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,a novel taxonomy of textual qa tasks is provided from an application scenario point of view.,,https://arxiv.org/pdf/2109.12264v2.pdf
2285,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,"a detailed description is provided for each dataset, which covers the task definition, contraction method, statistics, and evaluation measures.",,https://arxiv.org/pdf/2109.12264v2.pdf
2286,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,key features and detailed statistics among the benchmark datasets are summarized and compared in the form of tables.,,https://arxiv.org/pdf/2109.12264v2.pdf
2287,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,detailed description and distribution analysis of evaluation metrics are provided.,,https://arxiv.org/pdf/2109.12264v2.pdf
2288,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,"finally, we summarized the trends of the recent textual qa benchmark contraction methods and give our opinions on the future directions of the textual qa benchmark research.",,https://arxiv.org/pdf/2109.12264v2.pdf
2289,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,we hope this work serves as a good introduction to textual qa tasks.,,https://arxiv.org/pdf/2109.12264v2.pdf
2290,17412630,sRNA Antitoxins: More than One Way to Repress a Toxin,conclusions,Insight-tree,there are numerous toxin-antitoxin loci found within bacterial chromosomes; the type i and type iii pairs represent only a fraction of what has been described to date [1].,,
2291,17412630,sRNA Antitoxins: More than One Way to Repress a Toxin,conclusions,Insight-tree,"the type i and type iii loci are unique in that they utilize rna as antitoxins, yet they use their rna antitoxins very differently, illustrating the immense versatility of rna as a regulatory molecule.",,
2292,17412630,sRNA Antitoxins: More than One Way to Repress a Toxin,conclusions,Insight-tree,it is important to note that to date these antitoxins have been shown to act either through interaction with toxin mrna or protein.,,
2293,238744031,ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers,conclusion,Insight-tree,we propose a challenging dataset conditionalqa that contains questions with conditional answers.,,https://www.aclanthology.org/2022.acl-long.253.pdf
2294,238744031,ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers,conclusion,Insight-tree,table 4: em/f1 w/ conditions on the subset of questions with conditional answers.,,https://www.aclanthology.org/2022.acl-long.253.pdf
2295,238744031,ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers,conclusion,Insight-tree,"""best overall"" uses the best checkpoints/hyper-parameters on the full dataset, while ""best conditional"" uses the best ones on the subset of questions.",,https://www.aclanthology.org/2022.acl-long.253.pdf
2296,252918040,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,conclusion,Insight-tree,"in this paper, we presented a seminal knowledge prompting paradigm, based on which a novel knowledge-prompting-based plm framework kp-plm was proposed.",,https://export.arxiv.org/pdf/2210.08536v1.pdf
2297,252918040,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,conclusion,Insight-tree,we constructed contextual knowledge sub-graphs for contexts and employed continuous prompting mapping to generate knowledge prompts.,,https://export.arxiv.org/pdf/2210.08536v1.pdf
2298,252918040,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,conclusion,Insight-tree,"after that, we designed two selfsupervised pre-training tasks to learn semantic knowledge from prompts.",,https://export.arxiv.org/pdf/2210.08536v1.pdf
2299,252918040,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,conclusion,Insight-tree,"finally, we conducted extensive experiments to evaluate the model performance.",,https://export.arxiv.org/pdf/2210.08536v1.pdf
2300,252918040,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,conclusion,Insight-tree,experimental results validate the effectiveness of knowledge prompting in boosting the performance of plms.,,https://export.arxiv.org/pdf/2210.08536v1.pdf
2301,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,conclusion,Insight-tree,"in this paper, we introduce rospr, a method that enhances zero-shot generalization capabilities of a meta-trained lm by retrieving prompt-specific source prompt embeddings (soft prompts) for a given target task.",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2302,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,conclusion,Insight-tree,we accomplish this by first training the soft prompts for each hard prompt of the source tasks.,,https://export.arxiv.org/pdf/2210.03029v2.pdf
2303,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,conclusion,Insight-tree,"after training source prompt embeddings, we construct the source prompt library by storing the mean representation of training instances as keys and the corresponding prompt embeddings as values.",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2304,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,conclusion,Insight-tree,"at inference, we search for training instances stored in the library similar to sample instances from the target task, retrieve the corresponding prompt embedding, select the most frequently retrieved embedding and append it to each of the target task instances for prediction.",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2305,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,conclusion,Insight-tree,we further propose variants of retrieved embedding selection such as the interpolation of multiple source embeddings and a novel variance-based ranking to help improve accuracy and robustness to various wordings of evaluation prompts.,,https://export.arxiv.org/pdf/2210.03029v2.pdf
2306,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,conclusion,Insight-tree,"we additionally provide analysis of which factors attribute to the performance of rospr and find that heuristic cues such as the answer choice format are critical for generalization performance, implying that it may play a role similar to demonstrations in in-context learning.",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2307,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,limitations,Insight-tree,"although we show the effectiveness of rospr by applying it on t0-3b (sanh et al., 2021), we did not evaluate our method on different model scales such as the t0-11b variant and other lm architectures such as decoder-only lms due to limited computational resources.",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2308,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,limitations,Insight-tree,"this leaves future works on applying rospr to even larger lms and diverse lm architectures (wang et al., 2022a).",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2309,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,limitations,Insight-tree,"moreover, it is hard to apply var to target tasks without answer choices such as free-form generation because variance among options cannot be obtained.",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2310,252734741,Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization,limitations,Insight-tree,"however, rospr and rospr+inter can still be utilized and we leave applying rospr on zero-shot task location of free-form generation as future work (scialom et al., 2022).",,https://export.arxiv.org/pdf/2210.03029v2.pdf
2311,237513875,Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?,conclusion and future work,Insight-tree,edge probing tests are the predominant method to probe for linguistic information in large language models.,,https://www.aclanthology.org/2022.coling-1.139.pdf
2312,237513875,Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?,conclusion and future work,Insight-tree,"we use them to evaluate how the process of fine-tuning an lm for qa might change the grammatical knowledge in an encoder, and observe no significant differences between pre-trained and fine-tuned lms.",,https://www.aclanthology.org/2022.coling-1.139.pdf
2313,237513875,Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?,conclusion and future work,Insight-tree,"more importantly, we find this phenomenon in carefully designed target tasks where the models must use the said grammatical knowledge.",,https://www.aclanthology.org/2022.coling-1.139.pdf
2314,237513875,Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?,conclusion and future work,Insight-tree,"from similar ep test results, previous works have concluded that fine-tuning does not change the encoding of grammatical knowledge.",,https://www.aclanthology.org/2022.coling-1.139.pdf
2315,237513875,Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?,conclusion and future work,Insight-tree,"however, our analysis provides a 'dataset bias' explanation for the consistency of the results and provides some clues as to why any representation tends to achieve very similar results for ep tests.",,https://www.aclanthology.org/2022.coling-1.139.pdf
2316,237513875,Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?,conclusion and future work,Insight-tree,this is different from the previous task-design criticisms of the ep tests.,,https://www.aclanthology.org/2022.coling-1.139.pdf
2317,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,"in this paper, based on the observation that transformer models trained by massive tasks have better ability to generalize to unseen tasks, we hope to provide a new sight on exploring how this cross-task generalization ability is achieved and reused. inspired by the way that humans sparsely recall learned skills to solve new tasks, we explicitly model this process by resorting to a task-level mixture-of-expert model, where each expert represent different skills and tasks are routed by a router network based on the task property.",,https://arxiv.org/pdf/2205.12701v1.pdf
2318,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,"we empirically investigate several importance design choices, i.e., routing models, expert selection strategies, task representations to exploring their influence on final model.",,https://arxiv.org/pdf/2205.12701v1.pdf
2319,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,"secondly, by conducting a detailed analysis on the final routing decisions, we find it has a strongly correlation with human-defined task ontology (e.g., classification) and task characteristics (e.g., extractive, linguistic) even without any prior knowledge.",,https://arxiv.org/pdf/2205.12701v1.pdf
2320,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,we believe the result is valuable and promising in understanding the skills learned behind the black-box transformer models.,,https://arxiv.org/pdf/2205.12701v1.pdf
2321,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,model's parameters as the task embedding.,,https://arxiv.org/pdf/2205.12701v1.pdf
2322,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,"the fim provides a measure of the information a particular parameter learns about the loss corresponding to the probe model, so it is disable to represent task (wang et al., 2021,vu et al., 2020b.",,https://arxiv.org/pdf/2205.12701v1.pdf
2323,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,"given probe model m  , for example bart-base, we first calculate the loglikelihood with respect to the model parameters() as: p ",,https://arxiv.org/pdf/2205.12701v1.pdf
2324,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,#NAME?,,https://arxiv.org/pdf/2205.12701v1.pdf
2325,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,"(y | x), then fim is calculated as the covariance of gradients of the loglikelihood:",,https://arxiv.org/pdf/2205.12701v1.pdf
2326,237485084,ReasonBERT: Pre-trained to Reason with Distant Supervision,conclusion and future work,Insight-tree,"we propose reasonbert, a novel pre-training method to enhance the reasoning ability of language models.",,https://www.aclanthology.org/2021.emnlp-main.494.pdf
2327,237485084,ReasonBERT: Pre-trained to Reason with Distant Supervision,conclusion and future work,Insight-tree,"the resulting model obtains substantial improvements on multi-hop and hybrid qa tasks that require complex reasoning, and demonstrates superior few-shot performance.",,https://www.aclanthology.org/2021.emnlp-main.494.pdf
2328,237485084,ReasonBERT: Pre-trained to Reason with Distant Supervision,conclusion and future work,Insight-tree,"in the future, we plan to use our query-evidence pairs collected by distant supervision to improve the retrieval performance for open-domain qa, as well as empower reasonbert to handle more types of reasoning, like comparison and numeric reasoning, in natural language understanding.",,https://www.aclanthology.org/2021.emnlp-main.494.pdf
2329,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,"we present dragon, dense retriever trained with diverse augmentation.",,https://export.arxiv.org/pdf/2302.07452v1.pdf
2330,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,we propose a unified framework of data augmentation (da) to understand the recent progress of training dense retrievers.,,https://export.arxiv.org/pdf/2302.07452v1.pdf
2331,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,"based on the framework, we extensively study how to improve dense retrieval training through query and relevance label augmentation.",,https://export.arxiv.org/pdf/2302.07452v1.pdf
2332,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,"our experiments uncover some insights into training a dense retriever, which contradicts common wisdom that cross encoder is the most effective teacher and human-like queries are the most suitable training data for dense retrieval.",,https://export.arxiv.org/pdf/2302.07452v1.pdf
2333,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,"instead, we propose a diverse data augmentation recipe, query augmentation with the mixture of sentence cropping and generative queries, and progressive relevance label augmentation with multiple teachers.",,https://export.arxiv.org/pdf/2302.07452v1.pdf
2334,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,"with our recipe of da, we are the first to demonstrate that a single bert-base-sized dense retriever can achieve state-of-the-art effectiveness in both supervised and zero-shot retrieval tasks.",,https://export.arxiv.org/pdf/2302.07452v1.pdf
2335,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,"we believe that dragon can serve as a strong foundation retrieval model for domain adaptation retrieval tasks (wang et al., 2022;dai et al., 2022) or the existing retrieval augmented language models (izacard et al., 2022;shi et al., 2023;mallen et al., 2022).",,https://export.arxiv.org/pdf/2302.07452v1.pdf
2336,226262339,Generating Fact Checking Briefs,conclusion,Insight-tree,"we propose the concept of fact checking briefs, to be read before performing a fact check.",,https://arxiv.org/pdf/2011.05448v1.pdf
2337,226262339,Generating Fact Checking Briefs,conclusion,Insight-tree,"crucially, we develop qabriefer and release the accompanying qabriefdataset, to create qabriefs.",,https://arxiv.org/pdf/2011.05448v1.pdf
2338,226262339,Generating Fact Checking Briefs,conclusion,Insight-tree,we show in extensive empirical studies with crowdworkers and volunteers that qabriefs can improve accuracy and efficiency of fact checking.,,https://arxiv.org/pdf/2011.05448v1.pdf
2339,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,conclusion,Insight-tree,"this paper presents tacr, a",,https://export.arxiv.org/pdf/2305.14682v1.pdf
2340,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,limitations,Insight-tree,"in this paper, we focus on the hybrid qa task, where the answers to most questions can be extracted from cell values in tables and linked passages using a reading comprehension model.",,https://export.arxiv.org/pdf/2305.14682v1.pdf
2341,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,limitations,Insight-tree,"although tacr performs well in cell selection, one of its limitations is that it lacks numerical reasoning ability across different cells, such as counting and comparing.",,https://export.arxiv.org/pdf/2305.14682v1.pdf
2342,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,limitations,Insight-tree,"to enable tacr to answer numerical questions, we will further develop its numerical reasoning capabilities in future work.",,https://export.arxiv.org/pdf/2305.14682v1.pdf
2343,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,limitations,Insight-tree,another limitation of tacr is that it shows a strong ability in column selection while performing relatively worse in row selection.,,https://export.arxiv.org/pdf/2305.14682v1.pdf
2344,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,limitations,Insight-tree,"for future work, we plan to try to improve its row-selection accuracy.",,https://export.arxiv.org/pdf/2305.14682v1.pdf
2345,252089700,A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension,conclusion,Insight-tree,we have covered the shortcut identification and mitigation landscape in mrc.,,https://export.arxiv.org/pdf/2209.01824v1.pdf
2346,252089700,A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension,conclusion,Insight-tree,"the presence of shortcuts can be made clear through a variety of methods, and most researchers are aware of this issue.",,https://export.arxiv.org/pdf/2209.01824v1.pdf
2347,252089700,A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension,conclusion,Insight-tree,"mitigation methods are varied and have some degree of success, but a lot more work is needed before we can achieve models mostly free of shortcut biases.",,https://export.arxiv.org/pdf/2209.01824v1.pdf
2348,252089700,A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension,conclusion,Insight-tree,"efforts should be made to improve mrc shortcut debiasing techniques by incorporating those found in other fields such as computer vision and natural language inference, as well as finding methods with lower human and/or computation costs.",,https://export.arxiv.org/pdf/2209.01824v1.pdf
2349,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"we first adapt the widely used benchmark, hotpotqa [yang et al., 2018], to study our problem.",,https://arxiv.org/pdf/2203.11027v1.pdf
2350,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"hotpotqa contains multi-hop questions, which are each answerable by multiple wikipedia passages.",,https://arxiv.org/pdf/2203.11027v1.pdf
2351,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,we create hotpotqa-pair by splitting the wikipedia corpus into d g and d p by randomly assigning wikipedia articles to one or the other.,,https://arxiv.org/pdf/2203.11027v1.pdf
2352,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"this results in questions entirely reliant on p  d p , entirely reliant on d",,https://arxiv.org/pdf/2203.11027v1.pdf
2353,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree," d g , or reliant on a mix of one private and one public document, allowing us to evaluate performance under the pair constraints.ultimately however, d p and d g come from a single wikipedia distribution in hopotqa-pair.",,https://arxiv.org/pdf/2203.11027v1.pdf
2354,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"while it is possible that public and private data come from the same distribution (e.g., organizations routinely develop internal wikis in the style of public wikipedia), private and public data will intuitively often reflect different linguistic styles, structures, and topics, that further evolve over time",,https://arxiv.org/pdf/2203.11027v1.pdf
2355,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"[hawking, 2004].",,https://arxiv.org/pdf/2203.11027v1.pdf
2356,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,we observe all existing textual multi-hop benchmarks focus on retrieving from a single distribution (table 1).,,https://arxiv.org/pdf/2203.11027v1.pdf
2357,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"additionally, we cannot combine existing benchmarks over two different corpora because this will not yield questions requiring one passage from each domain.",,https://arxiv.org/pdf/2203.11027v1.pdf
2358,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"methodologically, in the pair setting we likely will not have access to training data from all downstream (private) domains.",,https://arxiv.org/pdf/2203.11027v1.pdf
2359,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"to evaluate with a realistically private set of information and pair set up, we create a new benchmark concurrentqa.",,https://arxiv.org/pdf/2203.11027v1.pdf
2360,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,6 following from the simple security property and *-property in the blp model.,,https://arxiv.org/pdf/2203.11027v1.pdf
2361,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,7 single-hop can also avoid performance degradations arising from using two enclaves.,,https://arxiv.org/pdf/2203.11027v1.pdf
2362,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"recall that a non-private system retrieves the top k overall passages, so if for example kp = k 2 and kg = k 2 , such that kp + kg = k, the system may not retrieve the optimal k passages that the non-private system would have retrieved (e.g., consider when the overall top k passages for a question are in dg).",,https://arxiv.org/pdf/2203.11027v1.pdf
2363,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,however letting kp ,,https://arxiv.org/pdf/2203.11027v1.pdf
2364,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"[0..k], kg ",,https://arxiv.org/pdf/2203.11027v1.pdf
2365,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"[0..k] circumvents this challenge, at the cost of retrieving a few more passages per hop.",,https://arxiv.org/pdf/2203.11027v1.pdf
2366,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,benchmark limitations,Insight-tree,"concurrentqa, like hotpotqa, faces the limitation that crowdworkers see the gold supporting passages when creating questions, which can result in textual overlap between substrings in the questions and passages [trivedi et al., 2020].",,https://arxiv.org/pdf/2203.11027v1.pdf
2367,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,benchmark limitations,Insight-tree,"we mitigate these effects through our validation task, and by limiting the allowable degree of overlap between passage pairs and questions through the frontend interface during the generation stage.",,https://arxiv.org/pdf/2203.11027v1.pdf
2368,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,benchmark limitations,Insight-tree,"further, our questions are not organic user searches as in kwiatkowski et al.",,https://arxiv.org/pdf/2203.11027v1.pdf
2369,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,benchmark limitations,Insight-tree,"[2019], however search logs do not contain questions over public and private data, and existing dialogue systems have not considered retrieval from a private corpus to our knowledge.additionally, enron was a major public corporation and many entities discussed in enron emails are public entities, so it is possible that public websites and news articles encountered during retriever and reader model pretraining, impact the distinction between public and private questions.",,https://arxiv.org/pdf/2203.11027v1.pdf
2370,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,benchmark limitations,Insight-tree,we investigate the impact of dataset leakage further in section 6.,,https://arxiv.org/pdf/2203.11027v1.pdf
2371,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,conclusion,Insight-tree,this work asks how to personalize retrieval-based systems in a privacy-preserving way and identifies that arbitrary autoregressive retrieval over public and private data poses a privacy concern.,,https://arxiv.org/pdf/2203.11027v1.pdf
2372,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,conclusion,Insight-tree,"in summary, we define the pair privacy framework, present a new multi-domain multi-hop benchmark called concurrentqa for the novel retrieval setting, and demonstrate the privacy-performance tradeoffs faced by existing open-domain systems.",,https://arxiv.org/pdf/2203.11027v1.pdf
2373,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,conclusion,Insight-tree,we finally investigate two challenges towards realizing the potential of public-private retrieval systems: using selective prediction to manage the privacy-performance tradeoff and concurrently retrieving over multiple distributions.,,https://arxiv.org/pdf/2203.11027v1.pdf
2374,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,conclusion,Insight-tree,we hope this work inspires new privacy-preserving solutions for personalized retrieval-based systems.,,https://arxiv.org/pdf/2203.11027v1.pdf
2375,238354350,Transfer Learning for Multi-lingual Tasks -a Survey,conclusion,Insight-tree,this survey provides an comprehensive overview of the existing studies on leveraging transfer learning models to tackle the multi-lingual and cross-lingual tasks.,,https://arxiv.org/pdf/2110.02052v1.pdf
2376,238354350,Transfer Learning for Multi-lingual Tasks -a Survey,conclusion,Insight-tree,"in addition to the models, we also reviewed the main available datasets in the community and investigated different approaches in term of the architectures and applications to identify the existing research challenges in the domain and later we provide few potential future directions.",,https://arxiv.org/pdf/2110.02052v1.pdf
2377,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,"this paper presented an effective unsupervised domain adaptation method, cai.",,https://www.aclanthology.org/2022.aacl-main.57.pdf
2378,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,we showed that the combination of splade with cai and the lexical approach gave a state-of-the-art performance on datasets with a large vocabulary and wordfrequency gap.,,https://www.aclanthology.org/2022.aacl-main.57.pdf
2379,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,"in addition, cai outperformed gpl and was robust enough to show high accuracy even when bow representations were used for query expression.",,https://www.aclanthology.org/2022.aacl-main.57.pdf
2380,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,"finally, our analysis showed that splade with cai addressed the problem of the exact matching of low-frequency words in training data.",,https://www.aclanthology.org/2022.aacl-main.57.pdf
2381,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,we believe that cai works on smaller mlms by distilling adalm because yao et al.,,https://www.aclanthology.org/2022.aacl-main.57.pdf
2382,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,(2021) showed that a distilled adalm achieved higher performance than bert on nlp tasks and formal et al.,,https://www.aclanthology.org/2022.aacl-main.57.pdf
2383,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,(2021) showed that the results of splade initialized with distilbert-base 14 was competitive on ms marco with other ir models initialized with bert.,,https://www.aclanthology.org/2022.aacl-main.57.pdf
2384,249191335,Learning Open Domain Multi-hop Search Using Reinforcement Learning,conclusions,Insight-tree,we proposed a focused reading methodology to automatically learn how to direct search in large corpora while iteratively building a knowledge base.,,https://www.aclanthology.org/2022.suki-1.4.pdf
2385,249191335,Learning Open Domain Multi-hop Search Using Reinforcement Learning,conclusions,Insight-tree,"the knowledge base is modeled as a graph, which in turn is used to focus the search toward documents that appear relevant.",,https://www.aclanthology.org/2022.suki-1.4.pdf
2386,249191335,Learning Open Domain Multi-hop Search Using Reinforcement Learning,conclusions,Insight-tree,our methodology complements existing information retrieval and machine tools.,,https://www.aclanthology.org/2022.suki-1.4.pdf
2387,249191335,Learning Open Domain Multi-hop Search Using Reinforcement Learning,conclusions,Insight-tree,we evaluated focused reading on a set of search problems extracted from english wikipedia and demonstrated that reinforcement learning with a state representation based on features about dynamics of the search process and the properties of the corpus is more effective and efficient than heuristic baselines.,,https://www.aclanthology.org/2022.suki-1.4.pdf
2388,249191335,Learning Open Domain Multi-hop Search Using Reinforcement Learning,conclusions,Insight-tree,"in this methodology, inference in a knowledge graph acquired during the search process is agnostic of the semantics of the concepts and their relations.",,https://www.aclanthology.org/2022.suki-1.4.pdf
2389,249191335,Learning Open Domain Multi-hop Search Using Reinforcement Learning,conclusions,Insight-tree,their quality depends on the machine reading components used to extract them.,,https://www.aclanthology.org/2022.suki-1.4.pdf
2390,119252748,Fokker-Planck equations for time-delayed systems via Markovian Embedding,conclusions,Insight-tree,"in this work, we have discussed the probabilistic description of delayed stochastic systems.",,https://arxiv.org/pdf/1903.02322v1.pdf
2391,119252748,Fokker-Planck equations for time-delayed systems via Markovian Embedding,conclusions,Insight-tree,"as a starting point, we have reviewed an earlier approach based on novikov's theorem, from which a fokker-planck description can be derived in the form of an infinite hierarchy [30][31][32].",,https://arxiv.org/pdf/1903.02322v1.pdf
2392,119252748,Fokker-Planck equations for time-delayed systems via Markovian Embedding,conclusions,Insight-tree,"the first member is the well-known fpe for the one-time pdf, which contains the two-time pdf and is thus not closed.",,https://arxiv.org/pdf/1903.02322v1.pdf
2393,119252748,Fokker-Planck equations for time-delayed systems via Markovian Embedding,conclusions,Insight-tree,"still, this equation has in the past been shown to be an important tool in the search for exact results",,https://arxiv.org/pdf/1903.02322v1.pdf
2394,119252748,Fokker-Planck equations for time-delayed systems via Markovian Embedding,conclusions,Insight-tree,"[36,37] and a valuable starting point for approximations [31,38].",,https://arxiv.org/pdf/1903.02322v1.pdf
2395,119252748,Fokker-Planck equations for time-delayed systems via Markovian Embedding,conclusions,Insight-tree,"the main purpose of our work was to shed light onto the higher members of this hierarchy, which have rarely been discussed in earlier literature.",,https://arxiv.org/pdf/1903.02322v1.pdf
2396,212747830,Invited Review . Pre-trained Models for Natural Language Processing: A Survey,conclusion,Insight-tree,"in this survey, we conduct a comprehensive overview of ptms for nlp, including background knowledge, model architecture, pre-training tasks, various extensions, adaption approaches, related resources, and applications.",,https://arxiv.org/pdf/2003.08271v4.pdf
2397,212747830,Invited Review . Pre-trained Models for Natural Language Processing: A Survey,conclusion,Insight-tree,"based on current ptms, we propose a new taxonomy of ptms from four different perspectives.",,https://arxiv.org/pdf/2003.08271v4.pdf
2398,212747830,Invited Review . Pre-trained Models for Natural Language Processing: A Survey,conclusion,Insight-tree,we also suggest several possible future research directions for ptms.,,https://arxiv.org/pdf/2003.08271v4.pdf
2399,245329570,Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering,conclusion,Insight-tree,"in this work, we propose a reasoning chain based adversarial attack for multi-hop qa.",,https://arxiv.org/pdf/2112.09658v1.pdf
2400,245329570,Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering,conclusion,Insight-tree,"by formulating the multihop reasoning process with a reasoning chain, we can identify different reasoning types and customize adversary design for each type.",,https://arxiv.org/pdf/2112.09658v1.pdf
2401,245329570,Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering,conclusion,Insight-tree,"our method allows to attack any certain hop by identifying different hop spans of the question, and making modification on relational words.",,https://arxiv.org/pdf/2112.09658v1.pdf
2402,245329570,Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering,conclusion,Insight-tree,the hop-targeted attack can inspect models' error-prone parts during the reasoning process specifically.,,https://arxiv.org/pdf/2112.09658v1.pdf
2403,245329570,Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering,conclusion,Insight-tree,"three qa models under evaluation both exhibit poor performance in face of adversaries, suggesting that they are not robust enough and have limited interpretability of conducting multi-hop reasoning.",,https://arxiv.org/pdf/2112.09658v1.pdf
2404,245329570,Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering,conclusion,Insight-tree,our adversarial evaluation can be utilized to improve models' performance by adversarial retraining as well as motivate new model development according to the weakness detected.,,https://arxiv.org/pdf/2112.09658v1.pdf
2405,252692968,RECITATION-AUGMENTED LANGUAGE MODELS,conclusion & discussion,Insight-tree,"in this paper, we propose a novel recitation-augmented generation framework to improve language models' performance in the closed-book question-answering setting.",,https://export.arxiv.org/pdf/2210.01296v2.pdf
2406,252692968,RECITATION-AUGMENTED LANGUAGE MODELS,conclusion & discussion,Insight-tree,"we hypothesize that for knowledge-intensive nlp tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory.",,https://export.arxiv.org/pdf/2210.01296v2.pdf
2407,252692968,RECITATION-AUGMENTED LANGUAGE MODELS,conclusion & discussion,Insight-tree,"in addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question.",,https://export.arxiv.org/pdf/2210.01296v2.pdf
2408,252692968,RECITATION-AUGMENTED LANGUAGE MODELS,conclusion & discussion,Insight-tree,"we show promising results over three large language models and across three different closed-book qa datasets, demonstrating the effectiveness of our proposed recite-and-answer approach.",,https://export.arxiv.org/pdf/2210.01296v2.pdf
2409,250562707,Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge,conclusion,Insight-tree,we have proposed a new approach to multi-hop question answering over tabular and textual data.,,https://aclanthology.org/2022.findings-naacl.12.pdf
2410,250562707,Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge,conclusion,Insight-tree,"the approach, referred to as dehg, takes question answering as a problem of reasoning answers on the basis of a heterogeneous information graph.",,https://aclanthology.org/2022.findings-naacl.12.pdf
2411,250562707,Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge,conclusion,Insight-tree,dehg employs bert in encoding of questions and passages respectively and generates pointers in decoding of answer generation.,,https://aclanthology.org/2022.findings-naacl.12.pdf
2412,250562707,Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge,conclusion,Insight-tree,experimental results show that dehg significantly outperforms the state-of-the-art methods.,,https://aclanthology.org/2022.findings-naacl.12.pdf
2413,240288835,MetaICL: Learning to Learn In Context,conclusion,Insight-tree,"in this paper, we introduced metaicl, a new fewshot learning method where an lm is meta-trained to learn to in-context learn, i.e. condition on training examples to recover the task and make predictions.",,https://www.aclanthology.org/2022.naacl-main.201.pdf
2414,240288835,MetaICL: Learning to Learn In Context,conclusion,Insight-tree,"we experiment with a large, diverse collection of tasks, consisting of 142 unique tasks in total and 52 unique target tasks, using seven different settings.",,https://www.aclanthology.org/2022.naacl-main.201.pdf
2415,240288835,MetaICL: Learning to Learn In Context,conclusion,Insight-tree,"metaicl outperforms a range of strong baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer, and outperforms or matches 8x bigger models.",,https://www.aclanthology.org/2022.naacl-main.201.pdf
2416,240288835,MetaICL: Learning to Learn In Context,conclusion,Insight-tree,we identify ingredients for success of metaicl such as the number and diversity of meta-training tasks.,,https://www.aclanthology.org/2022.naacl-main.201.pdf
2417,240288835,MetaICL: Learning to Learn In Context,conclusion,Insight-tree,"we also demonstrate that, while metaicl is better than recent work using natural instructions, they are complementary and the best performance is achieved by integrating metaicl with instructions.",,https://www.aclanthology.org/2022.naacl-main.201.pdf
2418,211010520,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension,discussion and conclusions,Insight-tree,"we have in this work investigated an rc annotation paradigm which includes a model in the loop that has to be ""beaten"" by the annotator.",,https://arxiv.org/pdf/2002.00293v1.pdf
2419,211010520,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension,discussion and conclusions,Insight-tree,"applying this approach with a series of progressively stronger rc models in the annotation loop, we arrived at three separate rc datasets, graduated by the difficulty of the model adversary.",,https://arxiv.org/pdf/2002.00293v1.pdf
2420,211010520,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension,discussion and conclusions,Insight-tree,"based on this dataset series we investigated several questions surrounding the annotation paradigm, in particular whether such datasets grow outdated as stronger models emerge, and about their generalisation to standard (non-adversarially collected) questions.",,https://arxiv.org/pdf/2002.00293v1.pdf
2421,211010520,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension,discussion and conclusions,Insight-tree,"we found that stronger rc models can still learn from data collected with a weak adversary in the loop, and their generalisation improves even on datasets collected with a very strong adversary.",,https://arxiv.org/pdf/2002.00293v1.pdf
2422,211010520,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension,discussion and conclusions,Insight-tree,"models trained on data collected with a model in the loop furthermore generalise well towards nonadversarially collected data, both on squad and on naturalquestions, yet we observe a slow deterioration with progressively stronger adversaries.",,https://arxiv.org/pdf/2002.00293v1.pdf
2423,233297024,Constrained Language Models Yield Few-Shot Semantic Parsers,conclusion,Insight-tree,we wish to rapidly develop semantic parsers in new domains.,,https://arxiv.org/pdf/2104.08768v2.pdf
2424,233297024,Constrained Language Models Yield Few-Shot Semantic Parsers,conclusion,Insight-tree,"to this end, we have demonstrated that constrained decoding of powerful language models can enable the paraphrasing of user utterances into a controlled sublanguage, which may then be mapped to a task-specific representation.",,https://arxiv.org/pdf/2104.08768v2.pdf
2425,233297024,Constrained Language Models Yield Few-Shot Semantic Parsers,conclusion,Insight-tree,"with small hundreds of examples we are able to quickly bootstrap models for a variety of datasets, enabling future work that explores human in the loop interactions for iterative model refinement.",,https://arxiv.org/pdf/2104.08768v2.pdf
2426,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",conclusions,Insight-tree,we created a novel dataset socratiq to support research on automatic socratic question generation.,,https://www.aclanthology.org/2023.eacl-main.12.pdf
2427,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",conclusions,Insight-tree,"we applied latest research in prompt-based conditional text generation to fine-tune existing large language models from gpt, t5, and prophetnet to learn soqg.",,https://www.aclanthology.org/2023.eacl-main.12.pdf
2428,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",conclusions,Insight-tree,"through our study and the release of this novel dataset, we take a first step towards enabling future research on models for soqg as well as impactful applications in areas such as counseling and education (inkster et al., 2018;fitzpatrick et al., 2017).",,https://www.aclanthology.org/2023.eacl-main.12.pdf
2429,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,we note the following limitations in our work that also comprise our future research directions.,,https://www.aclanthology.org/2023.eacl-main.12.pdf
2430,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,"first, while a human socratic method practitioner will know what type of socratic question to ask based only on context, our prompt-based models assume the availability of question-type for generating a type-sensitive question.",,https://www.aclanthology.org/2023.eacl-main.12.pdf
2431,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,"in fact, when only contexts were used for qg (gpt, t5, prophetnet baselines in section 3), the generated questions matched the desired question-type (those of the available reference questions) in only 37-40% of the cases.",,https://www.aclanthology.org/2023.eacl-main.12.pdf
2432,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,"furthermore, the question-type identification of automated methods using context alone was very poor with overall accuracy comparable to that of random assignments (section 4.1).secondly, though we showcased the potential use of soqg in designing chatbots and dialog systems for applications such as counseling, we note that the current evaluation has only been at the singleturn level.",,https://www.aclanthology.org/2023.eacl-main.12.pdf
2433,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,we hope to extend socratiq to capture back and forth discussions on cmv to provide multiturn data and also deduce via forum votes and other indicators if the discussion indeed resulted in changed minds and enabled alternate perspectives.,,https://www.aclanthology.org/2023.eacl-main.12.pdf
2434,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,"furthermore, considering the special purpose of socratic questions in shaping perspectives and enabling introspection and reflection, a comprehensive evaluation would require measuring these aspects over the multi-turn sessions.finally, our dataset was created by re-purposing the cmv subreddit data available in english, a high-resource language for which large-scale pretrained language models (plms) are readily available.",,https://www.aclanthology.org/2023.eacl-main.12.pdf
2435,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,obtaining high classification and generation performances via fine-tuning of plms will be a challenge that needs addressing in low-resource languages.,,https://www.aclanthology.org/2023.eacl-main.12.pdf
2436,249062559,QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs,conclusion,Insight-tree,"we presented qampari, an odqa benchmark which focuses on the ability of models to handle questions that have many answers and thus require reading multiple text passages.",,https://arxiv.org/pdf/2205.12665v2.pdf
2437,249062559,QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs,conclusion,Insight-tree,"qampari is semi-automatically generated, where examples are generated from wikidata and wikipedia tables, and manual work is done only to prepare pseudolanguage templates, to validate examples, and to re-phrase questions.",,https://arxiv.org/pdf/2205.12665v2.pdf
2438,249062559,QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs,conclusion,Insight-tree,we evaluate strong baselines on qampari and show that the need to retrieve a large number of passages and generate long lists is challenging for state-of-the-art models from the retrieve-and-read family.,,https://arxiv.org/pdf/2205.12665v2.pdf
2439,249017531,From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,conclusion,Insight-tree,"we propose fe2h, a simple yet effective framework for multi-hop qa that divides both the document selection and question answering into two stages following an easy-to-hard manner.",,https://arxiv.org/pdf/2205.11729v1.pdf
2440,249017531,From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,conclusion,Insight-tree,"experimental results demonstrate that since we cannot feed all of the candidate documents to the plms at a time due to the input length limitation, taking the multi-hop reasoning nature into consideration at the document selection phase significantly improves the overall performance.",,https://arxiv.org/pdf/2205.11729v1.pdf
2441,249017531,From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,conclusion,Insight-tree,"as for the subsequent qa phase, thanks to the great natural language understanding ability of the plms, the performance of our simple two-stage reader is better than the stateof-the-art approaches without any graph structure and explicit reasoning chains.",,https://arxiv.org/pdf/2205.11729v1.pdf
2442,249017531,From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,conclusion,Insight-tree,we hope this work could facilitate more simple yet powerful multihop qa approaches with the help of the advanced plms.,,https://arxiv.org/pdf/2205.11729v1.pdf
2443,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"in this paper, we propose a pre-training framework eg 3 p for a structured explanation generation task.",,https://export.arxiv.org/pdf/2306.00652v1.pdf
2444,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"distinct from existing pre-training tasks based on natural language text, eg 3 p focuses more on training mapping between natural language and graphs.",,https://export.arxiv.org/pdf/2306.00652v1.pdf
2445,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"meanwhile, due to the high cost of manually tagging, we construct queries from the synthetic graph automatically to get a large-scale corpus to support the pre-training process.",,https://export.arxiv.org/pdf/2306.00652v1.pdf
2446,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"using explagraph as a main benchmark, experimental results show that eg 3 p could significantly improve the ability of the model to generate explanations.",,https://export.arxiv.org/pdf/2306.00652v1.pdf
2447,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"in addition, on the other dataset, the results of the model after pre-training also showed a considerable improvement.",,https://export.arxiv.org/pdf/2306.00652v1.pdf
2448,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,our approach offers a new possibility for addressing the challenges of limited labeled data in natural language processing tasks.,,https://export.arxiv.org/pdf/2306.00652v1.pdf
2449,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"in the future, the ability of the model to generate explanation graphs will benefit from more datasets released with labels and more and more objective evaluation indicators put forward.",,https://export.arxiv.org/pdf/2306.00652v1.pdf
2450,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"additionally, while our current approach processes graphs as strings, utilizing a model architecture that is more suitable for graph generation may further enhance the model's graph generation ability.",,https://export.arxiv.org/pdf/2306.00652v1.pdf
2451,258865847,Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering,conclusions,Insight-tree,"in this work, we present a novel pre-training scheme for multi-document tasks.",,https://export.arxiv.org/pdf/2305.15387v1.pdf
2452,258865847,Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering,conclusions,Insight-tree,"first, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task.",,https://export.arxiv.org/pdf/2305.15387v1.pdf
2453,258865847,Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering,conclusions,Insight-tree,"second, we generate high-quality large-scale qa pre-training data using a controlled generation approach, in which each qa pair originates from a salient sentence in one of the documents in the set.",,https://export.arxiv.org/pdf/2305.15387v1.pdf
2454,221448158,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,conclusion & future work,Insight-tree,"we introduced text modular networks, which provide a general-purpose framework that casts complex tasks as textual interaction between existing, simpler qa modules.",,https://www.aclweb.org/anthology/2021.naacl-main.99.pdf
2455,221448158,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,conclusion & future work,Insight-tree,"based on this conceptual framework, we built modularqa, an instantiation of tmns that can perform multi-hop and discrete numeric reasoning.",,https://www.aclweb.org/anthology/2021.naacl-main.99.pdf
2456,221448158,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,conclusion & future work,Insight-tree,"empirically, modu-larqa is on-par with other modular approaches (which are dataset-specific) and outperforms a stateof-the-art model in a limited data setting and on expert-generated perturbations.",,https://www.aclweb.org/anthology/2021.naacl-main.99.pdf
2457,221448158,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,conclusion & future work,Insight-tree,"importantly, mod-ularqa provides easy-to-interpret explanations of its reasoning.",,https://www.aclweb.org/anthology/2021.naacl-main.99.pdf
2458,221448158,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,conclusion & future work,Insight-tree,it is the first system that decomposes drop questions into textual sub-questions and can be applied to both drop and hotpotqa.,,https://www.aclweb.org/anthology/2021.naacl-main.99.pdf
2459,207853131,Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,conclusion,Insight-tree,we use neural representations of large-scale commonsense knowledge graphs (comet) to generate contextualized knowledge graphs on demand for zero-shot question answering.,,https://arxiv.org/pdf/1911.03876v1.pdf
2460,207853131,Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,conclusion,Insight-tree,our approach dynamically constructs a knowledge graph of commonsense inferences related to a presented context and conditions on it to evaluate answer options for a posed question.,,https://arxiv.org/pdf/1911.03876v1.pdf
2461,207853131,Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,conclusion,Insight-tree,we use probabilistic inference to reason over the constructed graph to select the most likely answer to a question.,,https://arxiv.org/pdf/1911.03876v1.pdf
2462,207853131,Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,conclusion,Insight-tree,our approach exceeds the performance of large-scale pretrained language models at the zero-shot setting by 8.5% on the socialiqa dataset.,,https://arxiv.org/pdf/1911.03876v1.pdf
2463,207853131,Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,conclusion,Insight-tree,"furthermore, on both the socialiqa and storycommonsense datasets, dynamically generating a contextualized commonsense knowledge graph performs better than using comet to directly answer questions.",,https://arxiv.org/pdf/1911.03876v1.pdf
2464,245144556,Large Dual Encoders Are Generalizable Retrievers,conclusion,Insight-tree,"this paper presents the generalizable t5 retriever (gtr), a scaled-up dual encoder model with a fixed-size dot-product bottleneck layer.",,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2465,245144556,Large Dual Encoders Are Generalizable Retrievers,conclusion,Insight-tree,"we show that scaling up the model size brings significant improvement on retrieval performance across the board on the beir zero-shot retrieval benchmark, especially for out-of-domain generalization.",,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2466,245144556,Large Dual Encoders Are Generalizable Retrievers,conclusion,Insight-tree,"the gtr-xxl model performs at the level of state-ofthe-art performance on beir, outperforming many models that use earlier interactions between queries and documents.",,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2467,245144556,Large Dual Encoders Are Generalizable Retrievers,conclusion,Insight-tree,this sheds light on the research direction to continue enhancing the single vector representation model through better backbone encoders.,,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2468,245144556,Large Dual Encoders Are Generalizable Retrievers,conclusion,Insight-tree,"moreover, our in-depth analysis reveals the impact of scaling up under the scenarios of different training stages, pre-training strategies, fine-tuning datasets, and bottleneck sizes, as well as how scaling up influences the retrieved document lengths.",,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2469,245144556,Large Dual Encoders Are Generalizable Retrievers,conclusion,Insight-tree,our findings can inform future work and is an integral part of the joint effort to improve dual encoder models.,,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2470,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,"in our work, we focus on standard dual encoder training and have not investigated other techniques such as distillation.",,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2471,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,"there have been shown distillation is a strong recipe to improve the dense retrieval models on out-of-domain performance (santhanam et al., 2021;formal et al., 2021).",,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2472,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,we hope to investigate whether the scaling effect could also benefit distillation if we scale up the student dual encoders.,,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2473,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,"in addition, we only focus on english-only corpus and we leave the exploration of scaling up dense retrievers for multi-lingual corpus to future work.",,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2474,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,table 7 shows the comparisons of gtr models and the baselines.,,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2475,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,note that the best rocketqa model used additional augmented data other than ms marco to improve the model performance while all others do not.,,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2476,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,our best gtr-xxl models outperforms rocketqa on both mrr and recall.,,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2477,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,a.2 recall on beir,,https://www.aclanthology.org/2022.emnlp-main.669.pdf
2478,215745470,From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap,conclusion,Insight-tree,task-oriented dialogue systems aim to help users to achieve a variety of tasks.,,https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf
2479,215745470,From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap,conclusion,Insight-tree,it is not unusual to have hundreds of different domains in modern taskoriented virtual assistants.,,https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf
2480,215745470,From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap,conclusion,Insight-tree,how can we ensure the dialogue system is robust enough to scale to different tasks given limited amount of data?,,https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf
2481,215745470,From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap,conclusion,Insight-tree,some approaches focus on domain expansion by training on several source domains and then adapting to the target domain.,,https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf
2482,215745470,From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap,conclusion,Insight-tree,"while such methods can be successful in certain cases, it is hard for them to generalize to other completely different out-of-domain tasks.",,https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf
2483,251953412,LEXMAE: LEXICON-BOTTLENECKED PRETRAINING FOR LARGE-SCALE RETRIEVAL,conclusion,Insight-tree,"in this work, we propose to improve the lexicon-weighing retrieval by pre-training a lexiconbottlenecked masked autoencoder (lexmae) which alleviates the objective mismatch between the masked language modeling encoders and relevance-oriented lexicon importance.",,https://export.arxiv.org/pdf/2208.14754v2.pdf
2484,251953412,LEXMAE: LEXICON-BOTTLENECKED PRETRAINING FOR LARGE-SCALE RETRIEVAL,conclusion,Insight-tree,"after pretraining lexmae on large-scale collections, we first observe great zero-shot performance.",,https://export.arxiv.org/pdf/2208.14754v2.pdf
2485,251953412,LEXMAE: LEXICON-BOTTLENECKED PRETRAINING FOR LARGE-SCALE RETRIEVAL,conclusion,Insight-tree,"then after fine-tuning the lexmae on the large-scale retrieval benchmark, we obtain state-of-the-art retrieval quality with very high efficiency and also deliver state-of-the-art zero-shot transfer performance on beir benchmark.",,https://export.arxiv.org/pdf/2208.14754v2.pdf
2486,251953412,LEXMAE: LEXICON-BOTTLENECKED PRETRAINING FOR LARGE-SCALE RETRIEVAL,conclusion,Insight-tree,further detailed analyses on the efficacy-efficiency trade-off in terms of retrieval latency and storage memory also verify the superiority of our fine-tuned lexmae.,,https://export.arxiv.org/pdf/2208.14754v2.pdf
2487,235097557,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,conclusion and future work,Insight-tree,"in this paper, we propose a novel method for accelerating bert inference, called tr-bert, which prunes bert at token-level granularity.",,https://www.aclweb.org/anthology/2021.naacl-main.463.pdf
2488,235097557,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,conclusion and future work,Insight-tree,"specifically, tr-bert utilizes reinforcement learning to learn a token selection policy, which is able to select general meaningful tokens in the bottom layers and select task-relevant tokens in the top layers.",,https://www.aclweb.org/anthology/2021.naacl-main.463.pdf
2489,235097557,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,conclusion and future work,Insight-tree,experiments on eleven nlp tasks demonstrate the effectiveness of tr-bert as it accelerates bert inference by 2-5 times for various performance demand.,,https://www.aclweb.org/anthology/2021.naacl-main.463.pdf
2490,235097557,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,conclusion and future work,Insight-tree,"besides, tr-bert achieves a better quality and speed trade-off on long-text tasks, which shows its potential to process large amounts of information in the real-world applications.",,https://www.aclweb.org/anthology/2021.naacl-main.463.pdf
2491,238744411,Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model,conclusion and future work,Insight-tree,we propose a novel encoder-decoder model incorporating soft templates and moe to address the problem of complexity-controllable question generation.,,https://arxiv.org/pdf/2110.06560v1.pdf
2492,238744411,Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model,conclusion and future work,Insight-tree,"as most domains do not have training data for ccqg models, we propose a simple and effective cross-domain estimator to predict the missing complexity levels of questions.",,https://arxiv.org/pdf/2110.06560v1.pdf
2493,238744411,Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model,conclusion and future work,Insight-tree,"in the extensive experiments of both ccqg and complexity assessment tasks, our models achieve superior performance over the competitive baselines across all experimental settings.",,https://arxiv.org/pdf/2110.06560v1.pdf
2494,238744411,Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model,conclusion and future work,Insight-tree,"in the future, we will consider anaphora resolution and numerical reasoning in complexity estimator, and explore the performance of our model in different applications, such as examination and assisting qa systems.",,https://arxiv.org/pdf/2110.06560v1.pdf
2495,230433978,Few-Shot Question Answering by Pretraining Span Selection,conclusion,Insight-tree,"we explore the few-shot setting of extractive question answering, and demonstrate that existing methods, based on fine-tuning large pretrained language models, fail in this setup.",,https://www.aclanthology.org/2021.acl-long.239.pdf
2496,230433978,Few-Shot Question Answering by Pretraining Span Selection,conclusion,Insight-tree,"we propose a new pretraining scheme and architecture for span selection that lead to dramatic improvements, reaching surprisingly good results even when only an order of a hundred examples are available.",,https://www.aclanthology.org/2021.acl-long.239.pdf
2497,230433978,Few-Shot Question Answering by Pretraining Span Selection,conclusion,Insight-tree,"our work shows that choices that are often deemed unimportant when enough data is available, again become crucial in the few-shot setting, opening the door to new methods that take advantage of prior knowledge on the downstream task during model development.",,https://www.aclanthology.org/2021.acl-long.239.pdf
2498,247292369,Feeding What You Need by Understanding What You Learned,conclusion,Insight-tree,"we design a competency assessment framework for mrc capabilities, which describes model skills in an explainable and multi-dimensional manner.",,https://www.aclanthology.org/2022.acl-long.403.pdf
2499,247292369,Feeding What You Need by Understanding What You Learned,conclusion,Insight-tree,"by leveraging the framework, we further uncover and disentangle the connections between various data properties and model performance on a specific task, as well as propose a capability boundary breakthrough curriculum (cbbc) strategy to maxi-mize the data value and improve training efficiency.",,https://www.aclanthology.org/2022.acl-long.403.pdf
2500,247292369,Feeding What You Need by Understanding What You Learned,conclusion,Insight-tree,the experiments performed on four benchmark datasets verified that our approach can significantly improve the performance of existing mrc models.,,https://www.aclanthology.org/2022.acl-long.403.pdf
2501,247292369,Feeding What You Need by Understanding What You Learned,conclusion,Insight-tree,our work shows a deep understanding of model capabilities and data properties helps monitor the model skills during training and improves learning efficiency.,,https://www.aclanthology.org/2022.acl-long.403.pdf
2502,247292369,Feeding What You Need by Understanding What You Learned,conclusion,Insight-tree,our framework and learning strategy are also generally applicable to other nlp tasks.,,https://www.aclanthology.org/2022.acl-long.403.pdf
2503,236429000,Thought Flow Nets: From Single Predictions to Trains of Model Thought,conclusion,Insight-tree,"in this paper, we introduced a task-agnostic self-correction formalism that turns a model's single output prediction into an evolving sequence of predictions-the thought flow.",,https://export.arxiv.org/pdf/2107.12220v2.pdf
2504,236429000,Thought Flow Nets: From Single Predictions to Trains of Model Thought,conclusion,Insight-tree,we take inspiration from hegel's dialectics and propose a correction module along with a gradient-based update rule that sequentially updates a model's output distributions in the direction of an increasing self-estimate of correctness.,,https://export.arxiv.org/pdf/2107.12220v2.pdf
2505,236429000,Thought Flow Nets: From Single Predictions to Trains of Model Thought,conclusion,Insight-tree,we apply our method to question answering models and conduct extensive experiments including human evaluation.,,https://export.arxiv.org/pdf/2107.12220v2.pdf
2506,236429000,Thought Flow Nets: From Single Predictions to Trains of Model Thought,conclusion,Insight-tree,"we find that thought flows (i) can increase f 1 -scores up to 9.3%, (ii) exhibit complex self-correction patterns and (iii) provide significant improvements in human interaction and system perception including task performance and perceived system correctness and naturalness.",,https://export.arxiv.org/pdf/2107.12220v2.pdf
2507,236429000,Thought Flow Nets: From Single Predictions to Trains of Model Thought,conclusion,Insight-tree,a potential next step to further improve performance is learning to stop.,,https://export.arxiv.org/pdf/2107.12220v2.pdf
2508,218486753,R 4 C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,conclusions,Insight-tree,"towards evaluating rc systems' internal reasoning, we have proposed r 4 c that requires systems not only to output answers but also to give their derivations.",,https://www.aclweb.org/anthology/2020.acl-main.602.pdf
2509,218486753,R 4 C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,conclusions,Insight-tree,"for scalability, we have carefully developed a crowdsourced framework for annotating existing rc datasets with derivations.",,https://www.aclweb.org/anthology/2020.acl-main.602.pdf
2510,218486753,R 4 C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,conclusions,Insight-tree,"our experiments have demonstrated that our framework produces high-quality derivations, and that automatic evaluation metrics using multiple reference derivations can reliably capture oracle derivations.",,https://www.aclweb.org/anthology/2020.acl-main.602.pdf
2511,218486753,R 4 C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,conclusions,Insight-tree,"the experiments using two simple baseline models highlight the nature of r 4 c, namely that the derivation generation task is not simply the sf detection task.",,https://www.aclweb.org/anthology/2020.acl-main.602.pdf
2512,218486753,R 4 C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,conclusions,Insight-tree,"we make the dataset, automatic evaluation script, and baseline systems publicly available at https://naoya-i.github.io/r4c/.",,https://www.aclweb.org/anthology/2020.acl-main.602.pdf
2513,3937062,Using Automatic Refactoring to Improve Energy Efficiency of Android Apps,conclusion,Insight-tree,our work shows the potential of using automatic refactoring tools to improve energy efficiency of mobile applications.,,https://arxiv.org/pdf/1803.05889v1.pdf
2514,3937062,Using Automatic Refactoring to Improve Energy Efficiency of Android Apps,conclusion,Insight-tree,we have analyzed 140 foss android apps and as an outcome we have fixed 222 energy related anti-patterns.,,https://arxiv.org/pdf/1803.05889v1.pdf
2515,3937062,Using Automatic Refactoring to Improve Energy Efficiency of Android Apps,conclusion,Insight-tree,"in total, we improved the energy footprint of 45 apps.",,https://arxiv.org/pdf/1803.05889v1.pdf
2516,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,"in this paper, we explored qa datasets through the lens of item response theory.",,https://www.aclanthology.org/2022.acl-short.15.pdf
2517,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,we have demonstrated a way to build regression models that can describe the difficulty and discrimination of a question.,,https://www.aclanthology.org/2022.acl-short.15.pdf
2518,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,"we note that our work is limited in two important ways: firstly, we only use the dfgn model in our artificial crowd, which may have introduced a bias in which some factors that make questions difficult/discriminatory are only applicable to this model.",,https://www.aclanthology.org/2022.acl-short.15.pdf
2519,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,"secondly, we only explore the hotpotqa dataset, which may further limit our analysis to only be applicable to hotpotqa or similar datasets.",,https://www.aclanthology.org/2022.acl-short.15.pdf
2520,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,future work could incorporate multiple models and datasets to explore a more easily generalizable difficulty/discrimination prediction pipeline.,,https://www.aclanthology.org/2022.acl-short.15.pdf
2521,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,we also note that our analysis here focused on qa.,,https://www.aclanthology.org/2022.acl-short.15.pdf
2522,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,"however, there are many nlp tasks in which the difficulty or discrimination of an item may be important.",,https://www.aclanthology.org/2022.acl-short.15.pdf
2523,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,our work here could naturally extend to these domains.,,https://www.aclanthology.org/2022.acl-short.15.pdf
2524,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,"finally, automatically predicting these traits without relying on user responses can engender a host of creative educational applications.",,https://www.aclanthology.org/2022.acl-short.15.pdf
2525,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,future work can also leverage such predictive models to explore more efficient strategies for learning and evaluation.,,https://www.aclanthology.org/2022.acl-short.15.pdf
2526,239009834,MixQG: Neural Question Generation with Mixed Answer Types,conclusion,Insight-tree,"in this paper, we present mixqg, a question generation model pre-trained on a collection of qa datasets with a mix of answer types.",,https://arxiv.org/pdf/2110.08175v2.pdf
2527,239009834,MixQG: Neural Question Generation with Mixed Answer Types,conclusion,Insight-tree,we show through experiments that the resulting model is a strong starting point for further fine-tuning which achieves state-of-the-art results on target datasets in commonly-used similarity metrics as well as our designed human evaluation.,,https://arxiv.org/pdf/2110.08175v2.pdf
2528,239009834,MixQG: Neural Question Generation with Mixed Answer Types,conclusion,Insight-tree,we release our code and the model checkpoints to facilitate qg research and downstream applications.,,https://arxiv.org/pdf/2110.08175v2.pdf
2529,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we proposed the japanese question answering dataset, jaquad.",,https://arxiv.org/pdf/2202.01764v1.pdf
2530,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,we collected the contexts from japanese wikipedia articles and 39k+ questions were manually annotated by fluent japanese speakers.,,https://arxiv.org/pdf/2202.01764v1.pdf
2531,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,"jaquad has the same format as squad, and the characteristics of the data are generally similar to korquad 1.0.",,https://arxiv.org/pdf/2202.01764v1.pdf
2532,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,"in the experiments, we fine-tuned a japanese pre-trained language model with jaquad as a baseline and achieved 78.92% for f1 score and 63.38% for em on test set.",,https://arxiv.org/pdf/2202.01764v1.pdf
2533,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,"the baseline reaches promising results, but there is plenty of room for improvement.",,https://arxiv.org/pdf/2202.01764v1.pdf
2534,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,"extension of the dataset, such as covering longer answers, is left for future work.",,https://arxiv.org/pdf/2202.01764v1.pdf
2535,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,the dataset and our experiments are available at https://github.com/skelterlabsinc/jaquad.,,https://arxiv.org/pdf/2202.01764v1.pdf
2536,237940293,Sorting through the noise: Testing robustness of information processing in pre-trained language models,conclusion,Insight-tree,"we have presented results manipulating inputs of pre-trained lms, to test the ability of such models to represent and retain information conveyed by input text.",,https://www.aclanthology.org/2021.emnlp-main.119.pdf
2537,237940293,Sorting through the noise: Testing robustness of information processing in pre-trained language models,conclusion,Insight-tree,"our results show that though models may appear to handle information correctly in simple settings, these correct predictions are easily broken by insertion of distracting material in the context.",,https://www.aclanthology.org/2021.emnlp-main.119.pdf
2538,237940293,Sorting through the noise: Testing robustness of information processing in pre-trained language models,conclusion,Insight-tree,systematic manipulation of the distracting content further indicates key roles for semantic similarity and relative word position in models' selection of relevant contextual cues for prediction.,,https://www.aclanthology.org/2021.emnlp-main.119.pdf
2539,237940293,Sorting through the noise: Testing robustness of information processing in pre-trained language models,conclusion,Insight-tree,"overall, the results suggest that lm predictions are driven more by coarse-grained superficial cues than by extraction of robust meaning information from context.",,https://www.aclanthology.org/2021.emnlp-main.119.pdf
2540,237940293,Sorting through the noise: Testing robustness of information processing in pre-trained language models,conclusion,Insight-tree,"the results serve as a reality check for considerations of the extent to which lms ""understand"" their input, and lay groundwork to understand the mechanisms that do drive predictions in these models.   ",,https://www.aclanthology.org/2021.emnlp-main.119.pdf
2541,221662105,CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management,conclusion,Insight-tree,"in this paper, we propose a general system, caire-covid, with open-domain qa and query focused multi-document summarization techniques for efficiently mining scientific literature given a query.",,
2542,221662105,CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management,conclusion,Insight-tree,"the system has shown its efficiency on the kaggle cord-19 challenge, which was evaluated by medical researchers, and a series of experimental results also proved the effectiveness of our proposed methods and the competency of each module.",,
2543,221662105,CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management,conclusion,Insight-tree,"the system is also easy to be generalized to general domain-agnostic literature information mining, especially for possible future pandemics.",,
2544,221662105,CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management,conclusion,Insight-tree,we have launched our website 2 for real-time interactions and released our code 3 for broader use.,,
2545,201668869,Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks,conclusion,Insight-tree,"in this paper, we propose a graph neural network that finds the sentences crucial for answering a question.",,https://www.aclweb.org/anthology/2020.lrec-1.664.pdf
2546,201668869,Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks,conclusion,Insight-tree,the experiments demonstrate that the model correctly classifies supporting sentences by iteratively propagating the necessary information through its novel architecture.,,https://www.aclweb.org/anthology/2020.lrec-1.664.pdf
2547,201668869,Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks,conclusion,Insight-tree,we believe that our approach will play an important role in building a qa pipeline in combination with other mrqa models trained in an end-to-end manner.,,https://www.aclweb.org/anthology/2020.lrec-1.664.pdf
2548,210859295,MANYMODALQA: Modality Disambiguation and QA over Diverse Inputs,conclusion,Insight-tree,"we presented a new challenge to the community, hoping to promote research in manymodal qa.",,https://arxiv.org/pdf/2001.08034v1.pdf
2549,210859295,MANYMODALQA: Modality Disambiguation and QA over Diverse Inputs,conclusion,Insight-tree,"we structured our challenge in a way that encourages research in other, more general areas, such as transfer learning and end-to-end modality disambiguation + multimodal qa.",,https://arxiv.org/pdf/2001.08034v1.pdf
2550,210859295,MANYMODALQA: Modality Disambiguation and QA over Diverse Inputs,conclusion,Insight-tree,we hope that this challenge will serve as a test bed for further work and that our model will inspire directions of subsequent research.,,https://arxiv.org/pdf/2001.08034v1.pdf
2551,210859295,MANYMODALQA: Modality Disambiguation and QA over Diverse Inputs,conclusion,Insight-tree,"we plan to continue our work by collecting data that exhibits a stronger form of multimodality, where the question can only be answered after combining multiple modalities, and by adding new modalities, such as video and audio.",,https://arxiv.org/pdf/2001.08034v1.pdf
2552,252846009,PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers,conclusion,Insight-tree,"in the half decade since (poliak et al., 2018) single sentence relation leakage bias has proven to remain a difficult issue.",,https://www.aclanthology.org/2023.eacl-main.223.pdf
2553,252846009,PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers,conclusion,Insight-tree,"efforts to debias nli have led to datasets that merely exhibit different kinds of bias than those shown before, or less saturated benchmarks that continue to exhibit cheating features.",,https://www.aclanthology.org/2023.eacl-main.223.pdf
2554,252846009,PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers,conclusion,Insight-tree,future work must prioritize reducing observable bias directly using a model-driven approach.,,https://www.aclanthology.org/2023.eacl-main.223.pdf
2555,214795146,R 3 : A Reading Comprehension Benchmark Requiring Reasoning Processes*,conclusion,Insight-tree,"in this work, we present r 3 , a large-scale reading comprehension dataset in which a qa system is required to give answers to questions over diverse natural language, but also needed to present the reasoning processes.",,
2556,214795146,R 3 : A Reading Comprehension Benchmark Requiring Reasoning Processes*,conclusion,Insight-tree,we hope this dataset can facilitating the development of explainable qa systems.,,
2557,15781784,Access to,conclusions,Insight-tree,"we found that the deletion of one gata factor, either gata4, gata5, or gata6, can drastically alter the gene expression profiles and lineage determination of es cells induced to differentiate by retinoic acid.",,
2558,15781784,Access to,conclusions,Insight-tree,"es cells lacking a single gata factor, either gata4, gata5, or gata6, exhibit a unique pattern of gene expression profile when differentiated.",,
2559,15781784,Access to,conclusions,Insight-tree,deletion of gata6 terminates the differentiation of es cells to endoderm but leads to mesoderm lineage differentiation.,,
2560,15781784,Access to,conclusions,Insight-tree,"normally, during in vitro differentiation, the majority of es cells differentiate into primitive endoderm cells [25].",,
2561,15781784,Access to,conclusions,Insight-tree,"thus, the deletion of gata6 allows the selection of lineage other than yolk sac endoderm.",,
2562,15781784,Access to,conclusions,Insight-tree,this study demonstrates a potential approach in redirecting the lineage determination of es cells in vitro by altering the expression of gata factors.,,
2563,251105205,REALTIME QA: What's the Answer Right Now? Kentaro Inui,conclusion and future work,Insight-tree,"we introduced realtime qa, a dynamic, open domain qa benchmark that asks questions at the present time.",,https://export.arxiv.org/pdf/2207.13332v1.pdf
2564,251105205,REALTIME QA: What's the Answer Right Now? Kentaro Inui,conclusion and future work,Insight-tree,the current version announces 30 questions every week and continually evaluates six real-time baselines.,,https://export.arxiv.org/pdf/2207.13332v1.pdf
2565,251105205,REALTIME QA: What's the Answer Right Now? Kentaro Inui,conclusion and future work,Insight-tree,"our experiments from the first six weeks suggest that accurate, up-to-date information retrieval is particularly important to serve speedy information needs.",,https://export.arxiv.org/pdf/2207.13332v1.pdf
2566,251105205,REALTIME QA: What's the Answer Right Now? Kentaro Inui,conclusion and future work,Insight-tree,"we hope that real-time qa encourages research efforts toward fast, accurate applications of natural language processing.",,https://export.arxiv.org/pdf/2207.13332v1.pdf
2567,221845203,ETC: Encoding Long and Structured Inputs in Transformers,conclusions,Insight-tree,"this paper introduced the extended transformer construction (etc), an architecture designed to (1) scale up the input length (linearly with input), and (2) encode structured inputs.",,https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf
2568,221845203,ETC: Encoding Long and Structured Inputs in Transformers,conclusions,Insight-tree,"etc allows lifting weights from existing bert models, improving results significantly.",,https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf
2569,221845203,ETC: Encoding Long and Structured Inputs in Transformers,conclusions,Insight-tree,"the key ideas are a new globallocal attention mechanism, coupled with relative position encodings and a cpc pre-training task.",,https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf
2570,216868500,Look at the First Sentence: Position Bias in Question Answering,conclusion,Insight-tree,most qa studies frequently utilize start and end positions of answers as training targets without much considerations.,,https://arxiv.org/pdf/2004.14602v1.pdf
2571,216868500,Look at the First Sentence: Position Bias in Question Answering,conclusion,Insight-tree,our study shows that most qa models fail to generalize over different positions when trained on datasets having answers in a specific position.,,https://arxiv.org/pdf/2004.14602v1.pdf
2572,216868500,Look at the First Sentence: Position Bias in Question Answering,conclusion,Insight-tree,"we introduce several de-biasing methods to make models to ignore the spurious positional cues, and find out that the sentence-level answer prior is very useful.",,https://arxiv.org/pdf/2004.14602v1.pdf
2573,216868500,Look at the First Sentence: Position Bias in Question Answering,conclusion,Insight-tree,our findings also generalize to different positions and different datasets.,,https://arxiv.org/pdf/2004.14602v1.pdf
2574,216868500,Look at the First Sentence: Position Bias in Question Answering,conclusion,Insight-tree,one limitation of our approach is that our method and analysis are based on a single paragraph setting which should be extended to a multiple paragraph setting to be more practically useful.,,https://arxiv.org/pdf/2004.14602v1.pdf
2575,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"this paper presents a novel mrc-sytle pre-training model, namely pmr.",,https://export.arxiv.org/pdf/2212.04755v1.pdf
2576,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,pmr can fully resolve the learning objective and data format gaps that frequently appear in fine-tuning existing plms.,,https://export.arxiv.org/pdf/2212.04755v1.pdf
2577,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"ex-perimental results over multiple dimensions, including effectiveness in solving few-shot tasks, outof-domain generalization, demonstrate its strong benefits in bridge the pretrain-finetune gaps.",,https://export.arxiv.org/pdf/2212.04755v1.pdf
2578,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,pmr also shows encouraging potential in explaining the sequence classification process in an extractiondriven mrc framework.,,https://export.arxiv.org/pdf/2212.04755v1.pdf
2579,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"(trischler et al., 2017), searchqa (dunn et al., 2017, hot-potqa (yang et al., 2018), bioasq (tsatsaronis et al., 2015), drop (dua et al., 2019), duorc (saha et al., 2018), race (lai et al., 2017), re-lationextraction (levy et al., 2017), textbookqa (kembhavi et al., 2017.",,https://export.arxiv.org/pdf/2212.04755v1.pdf
2580,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"eqa has always been treated as an mrc problem, where the question serves as the mrc query, and the passage containing the answers serves as the mrc context.",,https://export.arxiv.org/pdf/2212.04755v1.pdf
2581,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"for ner, we follow li et al.",,https://export.arxiv.org/pdf/2212.04755v1.pdf
2582,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"(2020) to formulate ner into the mrc paradigm, where the entity label together with its description serves as the mrc query, and the input text serves as the mrc context.",,https://export.arxiv.org/pdf/2212.04755v1.pdf
2583,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,the goal is to extract the corresponding entities as answers.,,https://export.arxiv.org/pdf/2212.04755v1.pdf
2584,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"we use the eq. 4 as the learning objective, where y ext i,j indicates that the input span x i:j is an answer/entity.",,https://export.arxiv.org/pdf/2212.04755v1.pdf
2585,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,"to improve ner with non-local information from the internet, we propose a transformer-based query generation method and a mention-aware re-ranker, mentionscore.",,https://export.arxiv.org/pdf/2303.02915v1.pdf
2586,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,"these can favor the recall of the retrieved results, select the non-local sentences specifically related to each mention in the local sentence and lead to a state-of-the-art performance of 61.56 micro-f1 score on wnut17 dataset is achieved.",,https://export.arxiv.org/pdf/2303.02915v1.pdf
2587,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,table 1.,,https://export.arxiv.org/pdf/2303.02915v1.pdf
2588,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,the experimental results of adopting different query strategies and reranking settings on wnut17 dataset with reference sentences retrieved by google search retrieval.,,https://export.arxiv.org/pdf/2303.02915v1.pdf
2589,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,all our experiments report the average performance of 10 models trained with different seeds.,,https://export.arxiv.org/pdf/2303.02915v1.pdf
2590,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,"the superscript "" * "" denotes the experiments reported in clner paper.",,https://export.arxiv.org/pdf/2303.02915v1.pdf
2591,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,clner w/ cl is the best setting of wang et al.,,https://export.arxiv.org/pdf/2303.02915v1.pdf
2592,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,"(2020), which utilized retrieved sentences and was trained with the technique cooperative learning, while clner w/o cl only used the retrieved sentences.",,https://export.arxiv.org/pdf/2303.02915v1.pdf
2593,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,baseline#2 implemented in our system is the counterpart to clner w/o cl.,,https://export.arxiv.org/pdf/2303.02915v1.pdf
2594,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,m&s+ms is the setting of using mention as query and mentionscore as re-ranker to form the external context and evaluate with the model baseline#1.,,https://export.arxiv.org/pdf/2303.02915v1.pdf
2595,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,m_g&s+ms uses gold mention as query.,,https://export.arxiv.org/pdf/2303.02915v1.pdf
2596,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,"compared to baseline#2, paired single-tail ttest for p-value < 0.05 is superscripted with  and p-value < 0.01 with  to denote the t-test is passed.",,https://export.arxiv.org/pdf/2303.02915v1.pdf
2597,235187342,Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering *,conclusion,Insight-tree,"we propose to dynamically construct amr-sg that can reflect the intrinsic relations of relevant facts leveraging amr, a graph annotation.",,https://arxiv.org/pdf/2105.11776v1.pdf
2598,235187342,Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering *,conclusion,Insight-tree,"amr-sg combines the advantages of rich textual corpus and graph structure, where we can select useful facts that completely form the reasoning chain and make fact-level modeling.",,https://arxiv.org/pdf/2105.11776v1.pdf
2599,235187342,Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering *,conclusion,Insight-tree,"experimental results show that amr-sg can maintain high explainability, and successfully couple with strong pretrained models to achieve significant improvement on openbookqa and arc-challenge over approaches leveraging additional kgs.",,https://arxiv.org/pdf/2105.11776v1.pdf
2600,258959550,Nonparametric Decoding for Generative Retrieval,limitations,Insight-tree,"np decoding uses k-means clustering to reduce the number of contextualized embeddings, the performance varies by how the contextualized embeddings are clustered.",,https://export.arxiv.org/pdf/2210.02068v3.pdf
2601,258959550,Nonparametric Decoding for Generative Retrieval,limitations,Insight-tree,"as the process is relatively inconsistent, reducing the number with other methods would make the model performance more consistent.",,https://export.arxiv.org/pdf/2210.02068v3.pdf
2602,258959550,Nonparametric Decoding for Generative Retrieval,limitations,Insight-tree,"also, as it is not trivial to add new contextualized token embeddings on top of preconstructed ce due to the clustering step, we did not perform on dynamic corpus setup where new items are added or updated.",,https://export.arxiv.org/pdf/2210.02068v3.pdf
2603,258959550,Nonparametric Decoding for Generative Retrieval,limitations,Insight-tree,"np decoding is applicable to all generative retrieval models including gmr or seal which needs all token embeddings, however, we focused on generative retrieval models with representative output as the retrieval target in this work.",,https://export.arxiv.org/pdf/2210.02068v3.pdf
2604,258959550,Nonparametric Decoding for Generative Retrieval,limitations,Insight-tree,"also, while it is a general approach applicable to all encoder-decoder models, we focused on applying the method to t5.",,https://export.arxiv.org/pdf/2210.02068v3.pdf
2605,235755349,Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training,conclusion,Insight-tree,"in this paper, we propose a new approach to train multi-hop qa models, not to take reasoning shortcuts of guessing right answers without sufficient evidences.",,https://www.aclanthology.org/2021.acl-long.476.pdf
2606,235755349,Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training,conclusion,Insight-tree,"we do not require annotations and generate pseudo-evidentiality instead, by regularizing qa model from being overconfident when evidences are insufficient.",,https://www.aclanthology.org/2021.acl-long.476.pdf
2607,235755349,Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training,conclusion,Insight-tree,our experimental results show that our method outperforms baselines on hotpotqa and has the effectiveness to distinguish between evidence-positive and negative set.,,https://www.aclanthology.org/2021.acl-long.476.pdf
2608,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,conclusion,Insight-tree,"in this paper, we propose a novel self-supervised framework called qada for qa domain adaptation.",,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2609,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,conclusion,Insight-tree,qada introduces: (1) hidden space augmentation tailored for qa data to enrich target training corpora; and (2) an attention-based contrastive adaptation to learn domain-invariant features that generalize across source and target domain.,,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2610,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,conclusion,Insight-tree,our experiments demonstrate the effectiveness of qada: it achieves a superior performance over state-ofthe-art baselines in qa domain adaptation.,,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2611,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,limitations,Insight-tree,"despite having introduced hidden space augmentation in qada, we have not discussed different choices of  values for multi-hop synonyms to exploit the potential benefits of the dirichlet distribution.",,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2612,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,limitations,Insight-tree,"for context cutoff, dropping multiple context spans in each qa example may bring additional benefits to improve context understanding and the answer extraction process of the qa model.",,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2613,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,limitations,Insight-tree,"combined with additional question value estimation in pseudo labeling, we plan to explore such directions in adaptive qa systems as our future work.",,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2614,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,limitations,Insight-tree,a combination of question augmentation ratio  and context cutoff ratio .,,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2615,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,limitations,Insight-tree,"specifically, we empirically search for the best combination in the range of [0.1, 0.2, 0.3, 0.4] for both  and .",,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2616,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,limitations,Insight-tree,"eventually, the best hyperparameter combination is selected.",,https://www.aclanthology.org/2022.emnlp-main.147.pdf
2617,72940739,Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring,conclusion,Insight-tree,we formulated a dialogue comprehension task motivated by the need in telehealth settings to extract key clinical information from spoken conver-sations between nurses and patients.,,https://arxiv.org/pdf/1903.03530v2.pdf
2618,72940739,Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring,conclusion,Insight-tree,"we analyzed linguistic characteristics of real-world humanhuman symptom checking dialogues, constructed a simulated dataset based on linguistically inspired and clinically validated templates, and prototyped a qa system.",,https://arxiv.org/pdf/1903.03530v2.pdf
2619,72940739,Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring,conclusion,Insight-tree,the model works effectively on a simulated test set using symptoms excluded during training and on real-world conversations between nurses and patients.,,https://arxiv.org/pdf/1903.03530v2.pdf
2620,72940739,Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring,conclusion,Insight-tree,we are currently improving the model's dialogue comprehension capability in complex reasoning and context understanding and also applying the qa model to summarization and virtual nurse applications.,,https://arxiv.org/pdf/1903.03530v2.pdf
2621,227231636,Towards building a Robust Industry-scale Question Answering System,conclusion,Insight-tree,"although large pre-trained language models have shown super-human performance on benchmark datasets like squad, we show that there is plenty of room to make improvements on top of bert qa .",,https://www.aclweb.org/anthology/2020.coling-industry.9.pdf
2622,227231636,Towards building a Robust Industry-scale Question Answering System,conclusion,Insight-tree,"specifically, we outline prior strategies that do not work on a real benchmark consisting of ""natural questions"" showing the difficulty of the dataset and need for better algorithms.",,https://www.aclweb.org/anthology/2020.coling-industry.9.pdf
2623,227231636,Towards building a Robust Industry-scale Question Answering System,conclusion,Insight-tree,we introduce gaama and outline several strategies that are broadly classified under attention and data augmentation and show how effective it can be to attain competitive performance on nq compared to other industry baselines.,,https://www.aclweb.org/anthology/2020.coling-industry.9.pdf
2624,227231636,Towards building a Robust Industry-scale Question Answering System,conclusion,Insight-tree,we also outline gaama's ootb zero-shot transfer on two unseen datsets and show optimistic performance.,,https://www.aclweb.org/anthology/2020.coling-industry.9.pdf
2625,227231636,Towards building a Robust Industry-scale Question Answering System,conclusion,Insight-tree,our future work will involve adding larger pre-trained language models like t5 and also exploring multi-lingual qa.,,https://www.aclweb.org/anthology/2020.coling-industry.9.pdf
2626,235313893,Knowing More About Questions Can Help: Improving Calibration in Question Answering,conclusion,Insight-tree,we introduce a richer feature space for question answering calibrators with question and context embeddings and paraphrase-augmented inputs.,,https://arxiv.org/pdf/2106.01494v1.pdf
2627,235313893,Knowing More About Questions Can Help: Improving Calibration in Question Answering,conclusion,Insight-tree,our work suggests deciding the correctness of a qa system depends on both the semantics of the questioncontext and the confidence of the model.,,https://arxiv.org/pdf/2106.01494v1.pdf
2628,235313893,Knowing More About Questions Can Help: Improving Calibration in Question Answering,conclusion,Insight-tree,"we thoroughly test our calibrator in domain shift, adversarial, and open domain qa settings.",,https://arxiv.org/pdf/2106.01494v1.pdf
2629,235313893,Knowing More About Questions Can Help: Improving Calibration in Question Answering,conclusion,Insight-tree,the experiments show noticeable gains in performance across all settings.,,https://arxiv.org/pdf/2106.01494v1.pdf
2630,235313893,Knowing More About Questions Can Help: Improving Calibration in Question Answering,conclusion,Insight-tree,we further demonstrate our calibrator's general applicability by using it as a reranker in extractive open domain qa.,,https://arxiv.org/pdf/2106.01494v1.pdf
2631,235313893,Knowing More About Questions Can Help: Improving Calibration in Question Answering,conclusion,Insight-tree,"to summarize, our calibrator is simple, effective and general, with potential to be incorporated into existing models or extended for other nlp tasks.",,https://arxiv.org/pdf/2106.01494v1.pdf
2632,254854373,Rethinking Label Smoothing on Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, we present c2fm, a simple architecture for the hotpotqa dataset, and systematically analyze the effect of label smoothing on various modules of multi-hop question answering (mhqa).",,https://export.arxiv.org/pdf/2212.09512v1.pdf
2633,254854373,Rethinking Label Smoothing on Multi-hop Question Answering,conclusion,Insight-tree,"we also propose f1 smoothing, a novel label smoothing technique specifically designed for machine reading comprehension (mrc) tasks.",,https://export.arxiv.org/pdf/2212.09512v1.pdf
2634,254854373,Rethinking Label Smoothing on Multi-hop Question Answering,conclusion,Insight-tree,"our experiments on the hotpotqa dataset demonstrate that c2fm with label smoothing outperforms several strong baselines, highlighting the effectiveness of label smoothing in mhqa.",,https://export.arxiv.org/pdf/2212.09512v1.pdf
2635,254854373,Rethinking Label Smoothing on Multi-hop Question Answering,conclusion,Insight-tree,"however, our results also show that the choice of smoothing strategy is critical for achieving optimal performance.",,https://export.arxiv.org/pdf/2212.09512v1.pdf
2636,245353743,AN INFERENCE APPROACH TO QUESTION ANSWERING OVER KNOWLEDGE GRAPHS A PREPRINT,conclusion & future work,Insight-tree,we presented a simple approach for converting question answering over knowledge graphs into an inference problem.,,https://arxiv.org/pdf/2112.11070v1.pdf
2637,245353743,AN INFERENCE APPROACH TO QUESTION ANSWERING OVER KNOWLEDGE GRAPHS A PREPRINT,conclusion & future work,Insight-tree,"leveraging existing models of natural language inference, as well as proposing a new model, we have shown state of the art results on metaqa dataset.",,https://arxiv.org/pdf/2112.11070v1.pdf
2638,245353743,AN INFERENCE APPROACH TO QUESTION ANSWERING OVER KNOWLEDGE GRAPHS A PREPRINT,conclusion & future work,Insight-tree,"our model is simple and amenable for domain adaptation, to solve the problem of qa over kgs from newer domains with lesser training data.",,https://arxiv.org/pdf/2112.11070v1.pdf
2639,245353743,AN INFERENCE APPROACH TO QUESTION ANSWERING OVER KNOWLEDGE GRAPHS A PREPRINT,conclusion & future work,Insight-tree,"to the best of our knowledge, this is the first attempt in treating qa over kg as an inferencing problem and the results are exciting.",,https://arxiv.org/pdf/2112.11070v1.pdf
2640,245353743,AN INFERENCE APPROACH TO QUESTION ANSWERING OVER KNOWLEDGE GRAPHS A PREPRINT,conclusion & future work,Insight-tree,the work is preliminary and provides a good starting point for discussion and further research.,,https://arxiv.org/pdf/2112.11070v1.pdf
2641,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,conclusion,Insight-tree,"as most pre-trained language models fail to attach enough importance to sentence-level representation learning, it usually leads to unsatisfactory performance in downstream tasks when good sentence representation is right indispensable.",,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2642,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,conclusion,Insight-tree,"based on investigating the intra-sentence relationship between components of sentences (important phrases) and the whole sentence representations, we propose a generative objective to align these phrases with their corresponding sentence representations.",,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2643,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,conclusion,Insight-tree,"this idea leads to paser, a phrase-aware sentence representation model.",,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2644,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,conclusion,Insight-tree,"as an effective alternative in sentence representation learning, our paser achieves comparable performance with strong contrastive learning baselines on sts tasks, and better performance on the downstream semantic retrieval and reranking tasks on datasets including qqp and askubuntu.",,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2645,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,limitations,Insight-tree,"we think our paser has the following limitations, and leave them for future work.",,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2646,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,limitations,Insight-tree,the combination of decoding signals is empirically designed.,,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2647,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,limitations,Insight-tree,hyperparameters m and n are selected by grid search and lack technical analysis.,,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2648,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,limitations,Insight-tree,table 8: ablation for finding best m and n in the unsupervised setting.,,https://www.aclanthology.org/2022.emnlp-main.221.pdf
2649,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,"for legal documents with clear structure and rigorous expression, it is helpful to improve human work efficiency to let machines understand and read legal documents.",,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2650,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,e purpose of reading comprehension in the legal field is to train the machine model through legal documents so that it can answer various questions according to the given case description.,,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2651,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,"an excellent reading and understanding system in the legal field can assist judges, lawyers, and other professionals in their work and also make it easy for people to understand the basic situation of each case.",,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2652,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,"it has a wide range of application prospects, such as crime prediction, evidence prediction, legal provisions recommendation, and intelligent court trial.",,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2653,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,is paper mainly studies the evidence prediction in the legal field.,,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2654,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,"taking the prediction of reading and understanding evidence in the legal field as the research task, this paper puts forward a prediction method of evidence based on sentence selection for legal documents.",,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2655,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,"a sentence selection module is designed to remove irrelevant sentences, and questions and answers are used to infer evidence, which has achieved good results.",,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2656,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,"rough experiments, it is found that the score of joint f1 proposed in this paper is 70.07%, which is more accurate than the mainstream model.",,https://downloads.hindawi.com/journals/am/2022/1926347.pdf
2657,248427125,Generative Retrieval for Long Sequences,conclusion,Insight-tree,"we show that generative retrieval, which has been originally proposed for retrieving short sequences such as entities, can also be considered for retrieving longer sequences.",,https://arxiv.org/pdf/2204.13596v1.pdf
2658,248427125,Generative Retrieval for Long Sequences,conclusion,Insight-tree,"we particularly find that generative retrieval can have an advantage over bi-encoder in certain situations, such as retrieving structured information (e.g., reasoning chains or graphs) and retrieving an arbitrary number of items.",,https://arxiv.org/pdf/2204.13596v1.pdf
2659,248427125,Generative Retrieval for Long Sequences,conclusion,Insight-tree,"given that generative retrieval inherently has gpu memory and speed benefits, it can be a practical alternative for general retrieval tasks in the future.",,https://arxiv.org/pdf/2204.13596v1.pdf
2660,143604734,Influence of In-Service Teacher Training on Their Opinions about IBSE,conclusions,Insight-tree,"the fact that, after training, polish teachers still have the opinion that curricula are not adjusted to ibse is significant, and informs us that the introduction of ibse into school practice will be a major challenge.",,https://web.archive.org/web/20200307032630/https:/ruj.uj.edu.pl/xmlui/bitstream/handle/item/18748/bernard_maciejowska_krzeczkowska_odrowaz_influence_of_in-service%20_2015.pdf?isAllowed=y&sequence=4
2661,143604734,Influence of In-Service Teacher Training on Their Opinions about IBSE,conclusions,Insight-tree,"on the other hand, the fact that the teachers, after training, reinforced their attitudes that existing curricula can be realized with the use of ibse methods and that carrying out experiments according to the ibse rules does not require more advanced equipment in school laboratories in comparison to the traditional methods, allows for an optimistic look at the future.",,https://web.archive.org/web/20200307032630/https:/ruj.uj.edu.pl/xmlui/bitstream/handle/item/18748/bernard_maciejowska_krzeczkowska_odrowaz_influence_of_in-service%20_2015.pdf?isAllowed=y&sequence=4
2662,143604734,Influence of In-Service Teacher Training on Their Opinions about IBSE,conclusions,Insight-tree,changes in attitudes are in this case less expensive than changes in external conditions.,,https://web.archive.org/web/20200307032630/https:/ruj.uj.edu.pl/xmlui/bitstream/handle/item/18748/bernard_maciejowska_krzeczkowska_odrowaz_influence_of_in-service%20_2015.pdf?isAllowed=y&sequence=4
2663,143604734,Influence of In-Service Teacher Training on Their Opinions about IBSE,conclusions,Insight-tree,the stronger agreement that ibse might have a positive influence on the examination results can be the result of a session on new forms of external examinations.,,https://web.archive.org/web/20200307032630/https:/ruj.uj.edu.pl/xmlui/bitstream/handle/item/18748/bernard_maciejowska_krzeczkowska_odrowaz_influence_of_in-service%20_2015.pdf?isAllowed=y&sequence=4
2664,143604734,Influence of In-Service Teacher Training on Their Opinions about IBSE,conclusions,Insight-tree,the increase in the number of statements saying that the school system and parents are in favor of the traditional/lecturing methods gives teachers some kind of excuse for such rare application of the use of strategy that they found valuable but at the same time very demanding.,,https://web.archive.org/web/20200307032630/https:/ruj.uj.edu.pl/xmlui/bitstream/handle/item/18748/bernard_maciejowska_krzeczkowska_odrowaz_influence_of_in-service%20_2015.pdf?isAllowed=y&sequence=4
2665,258959010,Answering Unanswered Questions through Semantic Reformulations in Spoken QA,conclusion,Insight-tree,"we tackled the problem of improving spoken qa, and analyzed questions from live data to identify key challenges that could be addressed with reformulation.",,https://export.arxiv.org/pdf/2305.17393v2.pdf
2666,258959010,Answering Unanswered Questions through Semantic Reformulations in Spoken QA,conclusion,Insight-tree,based on this we proposed surf with novel linguistically-motivated reformulation operators to solve the identified challenges.,,https://export.arxiv.org/pdf/2305.17393v2.pdf
2667,258959010,Answering Unanswered Questions through Semantic Reformulations in Spoken QA,conclusion,Insight-tree,"offline experiments show the effectiveness of our novel root transformation and generalization operations, with up to 24% of unanswered questions being answered via reformulations with high answer relevance.",,https://export.arxiv.org/pdf/2305.17393v2.pdf
2668,258959010,Answering Unanswered Questions through Semantic Reformulations in Spoken QA,conclusion,Insight-tree,live deployment in a leading voice assistant has positively impacted millions of requests.,,https://export.arxiv.org/pdf/2305.17393v2.pdf
2669,233033850,Discrete Reasoning Templates for Natural Language Understanding,conclusion,Insight-tree,we propose using reasoning templates for tackling reading comprehension tasks that involve reasoning over multiple paragraphs.,,https://www.aclweb.org/anthology/2021.eacl-srw.12.pdf
2670,233033850,Discrete Reasoning Templates for Natural Language Understanding,conclusion,Insight-tree,"we show that this approach is competitive with state of the art models on a subset of drop's subtraction questions, while requiring much less training data and providing better visibility of the model's decision making.",,https://www.aclweb.org/anthology/2021.eacl-srw.12.pdf
2671,233033850,Discrete Reasoning Templates for Natural Language Understanding,conclusion,Insight-tree,"in future work, we plan on extending to further templates and investigate how to learn templates instead of working from a predefined set.",,https://www.aclweb.org/anthology/2021.eacl-srw.12.pdf
2672,258557836,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,conclusion,Insight-tree,"in this work, we propose a simple strategy, random padding, to improve the performance of extractive qa models, especially when they are trained on short contexts but evaluated on longer contexts.",,https://export.arxiv.org/pdf/2305.04859v1.pdf
2673,258557836,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,conclusion,Insight-tree,"our method only re-organizes the input token sequences when fine-tuning, without any modifications to the architectures of plms.",,https://export.arxiv.org/pdf/2305.04859v1.pdf
2674,258557836,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,conclusion,Insight-tree,"experiments reveal that our simple method can effectively enhance qa models when predicting answers at the rear positions, where the position embeddings may not be sufficiently updated without random padding.",,https://export.arxiv.org/pdf/2305.04859v1.pdf
2675,258557836,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,conclusion,Insight-tree,we also show that our simple strategy can improve the performance of plm components in more benchmarks and tasks where accurate local context representations over longer context are necessary.,,https://export.arxiv.org/pdf/2305.04859v1.pdf
2676,238419458,TOWARDS CONTINUAL KNOWLEDGE LEARNING OF LANGUAGE MODELS,conclusion,Insight-tree,"in this paper, we propose continual knowledge learning (ckl), where we establish benchmark datasets and metrics, and explore methodologies towards continual knowledge learning of an ever-changing lm.",,https://arxiv.org/pdf/2110.03215v4.pdf
2677,238419458,TOWARDS CONTINUAL KNOWLEDGE LEARNING OF LANGUAGE MODELS,conclusion,Insight-tree,"we find that parameter-expansion methods show the most robust performance throughout all of the experimental settings, which nevertheless has severe memory inefficiency and that seeing the same data often is a critical cause of forgetting.",,https://arxiv.org/pdf/2110.03215v4.pdf
2678,238419458,TOWARDS CONTINUAL KNOWLEDGE LEARNING OF LANGUAGE MODELS,conclusion,Insight-tree,we also discuss several other interesting results of which we leave further exploration to future studies.,,https://arxiv.org/pdf/2110.03215v4.pdf
2679,238419458,TOWARDS CONTINUAL KNOWLEDGE LEARNING OF LANGUAGE MODELS,conclusion,Insight-tree,"to this end, we suggest the community to explore ckl for the better design of an ever-changing lm.",,https://arxiv.org/pdf/2110.03215v4.pdf
2680,202542881,Span Selection Pre-training for Question Answering,conclusion and future work,Insight-tree,"span selection pre-training is effective in improving reading comprehension across four diverse datasets, including both generated and natural questions, and with provided contexts of passages, documents and even passage sets.",,https://www.aclweb.org/anthology/2020.acl-main.247.pdf
2681,202542881,Span Selection Pre-training for Question Answering,conclusion and future work,Insight-tree,"this style of pretraining focuses the model on finding semantic connections between two sequences, and supports a style of cloze that can train deep semantic understanding without demanding memorization of specific knowledge in the model.",,https://www.aclweb.org/anthology/2020.acl-main.247.pdf
2682,202542881,Span Selection Pre-training for Question Answering,conclusion and future work,Insight-tree,"the span selection task is suitable for pre-training on any domain, since it makes no assumptions about document structure or availability of summary/article pairs.",,https://www.aclweb.org/anthology/2020.acl-main.247.pdf
2683,202542881,Span Selection Pre-training for Question Answering,conclusion and future work,Insight-tree,this allows pre-training of language understanding models in a very generalizable way.,,https://www.aclweb.org/anthology/2020.acl-main.247.pdf
2684,237502773,Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension,conclusions,Insight-tree,"we have proposed suqa, an rc system augmented with an abstractive explainer component.",,https://www.aclanthology.org/2021.emnlp-main.490.pdf
2685,237502773,Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension,conclusions,Insight-tree,"our experiments have demonstrated that the abstractive explainer can generate more concise explanations than an extractive explainer with limited supervison, while keeping explanations sufficient for qa.",,https://www.aclanthology.org/2021.emnlp-main.490.pdf
2686,245218668,Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering,conclusion,Insight-tree,"in this work, we propose an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts.",,https://arxiv.org/pdf/2112.08777v1.pdf
2687,245218668,Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering,conclusion,Insight-tree,we demonstrate consistent improvement when using our approach on three different models over two different benchmarks.,,https://arxiv.org/pdf/2112.08777v1.pdf
2688,245218668,Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering,conclusion,Insight-tree,"in future work, we would like to explore variations of our proposed supervised loss on other long-context tasks, such as long document summarization.",,https://arxiv.org/pdf/2112.08777v1.pdf
2689,257410167,MiR-1281 is involved in depression disorder and the antidepressant effects of Kai-Xin-San by targeting ADCY1 and DVL1,conclusions,Insight-tree,"collectively, based on the discovery of clinical-level mirna expression profiles, the biological function of mir-1281 was elucidated in vitro from the perspective of posttranscriptional regulation.",,
2690,257410167,MiR-1281 is involved in depression disorder and the antidepressant effects of Kai-Xin-San by targeting ADCY1 and DVL1,conclusions,Insight-tree,we found that the target genes of mir-1281 (adcy1 and dvl1) were closely related to the occurrence and development of depression.,,
2691,257410167,MiR-1281 is involved in depression disorder and the antidepressant effects of Kai-Xin-San by targeting ADCY1 and DVL1,conclusions,Insight-tree,"it was demonstrated in vitro that kxs may activate the camp/pka/ erk/creb and wnt/-catenin signal transduction pathways by downregulating mir-1281, which targets adcy1 and dvl1, to achieve its role in neuronal cell protection.",,
2692,257410167,MiR-1281 is involved in depression disorder and the antidepressant effects of Kai-Xin-San by targeting ADCY1 and DVL1,conclusions,Insight-tree,"these results provide a novel way of perceiving depression disorder, shedding light on the development of new therapeutic approaches.",,
2693,258714968,It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance,conclusion,Insight-tree,we develop a taxonomy of disagreement (based on measurement modeling) which distinguishes between how tasks are conceptualized and how measurements of model performance are operationalized.,,https://export.arxiv.org/pdf/2305.09022v1.pdf
2694,258714968,It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance,conclusion,Insight-tree,"to provide evidence for our taxonomy, we conduct a survey of practitioners and meta-analysis of relevant literature.",,https://export.arxiv.org/pdf/2305.09022v1.pdf
2695,258714968,It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance,conclusion,Insight-tree,"based on our taxonomy, we propose a framework for the creation of benchmarks and the documentation of their limitations.",,https://export.arxiv.org/pdf/2305.09022v1.pdf
2696,258714968,It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance,conclusion,Insight-tree,future work includes studying task conceptualization via benchmark inter-annotator disagreement.,,https://export.arxiv.org/pdf/2305.09022v1.pdf
2697,189898081,DocRED: A Large-Scale Document-Level Relation Extraction Dataset,conclusion,Insight-tree,"to promote re systems from sentence level to document level, we present docred, a large-scale document-level re dataset that features the data size, the requirement for reading and reasoning over multiple sentences, and the distantly supervised data offered for facilitating the development of weakly supervised document-level re. experiments show that human performance is significantly higher than re baseline models, which suggests ample opportunity for future improvement.",,https://www.aclweb.org/anthology/P19-1074.pdf
2698,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,"in this paper, we propose three solution program centric auxiliary pretraining tasks at both the whole program level and sub-program level.",,https://export.arxiv.org/pdf/2305.07475v1.pdf
2699,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,"at the wholeprogram level, we propose the variable integrity ranking pretraining task, which guides the model to distinguish required and irrelevant variables in the noisy input.",,https://export.arxiv.org/pdf/2305.07475v1.pdf
2700,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,"to further enhance the model's ability to learn the underlying reasoning process, we propose two additional pretraining tasks: variable operator prediction and variable keyphrase masking.",,https://export.arxiv.org/pdf/2305.07475v1.pdf
2701,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,these tasks help the model perform accurate sub-program construction.,,https://export.arxiv.org/pdf/2305.07475v1.pdf
2702,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,experimental results demonstrate the effectiveness of our method.,,https://export.arxiv.org/pdf/2305.07475v1.pdf
2703,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,variable integrity ranking achieves the most improvement on both the retriever and program solver.,,https://export.arxiv.org/pdf/2305.07475v1.pdf
2704,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,the sub-program level tasks substantially improve results on plms of different scales.,,https://export.arxiv.org/pdf/2305.07475v1.pdf
2705,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,our approach achieves 3.56% execution accuracy and 3.74% program accuracy improvement on the competitive roberta-large baseline.,,https://export.arxiv.org/pdf/2305.07475v1.pdf
2706,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,model capabilities we observed that current lms struggle to generate several categories of examples.,,https://export.arxiv.org/pdf/2212.09251v1.pdf
2707,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,lms struggled to generate examples related to concepts they do not understand well (e.g. cryptography and steganography).,,https://export.arxiv.org/pdf/2212.09251v1.pdf
2708,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"as discussed in 6, we also found that lms struggled to generate examples with many constraints, in particular, those in the bbq dataset (parrish et al., 2022).",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2709,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,we expect these limitations to wane as lms grow more capable with scale.,,https://export.arxiv.org/pdf/2212.09251v1.pdf
2710,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"lastly, many evaluations related to lm capabilities require the dataset creator to know how to solve the evaluation.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2711,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"we expect that lms will not be able to generate high-quality evaluations of this kind (e.g., to test for factual knowledge they do not yet know).",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2712,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"our approach is thus differentially useful for evaluating other properties of models aside from capabilities (e.g., safety-related behaviors).model biases lms learn biases from their training data (sheng et al., 2019;gehman et al., 2020;brown et al., 2020), impacting the generator p g and discriminator p d .",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2713,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"for example, generated evaluations may exhibit gender or racial biases and be lower quality for languages under-represented in the lm training data.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2714,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"lms will also be systematically worse at generating evaluations for tasks that are omitted from their training data (e.g., due to copyright, licensing, or privacy issues).example diversity we found limited example diversity for some kinds of evaluations ( 3.4) though not all ( 5).",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2715,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"diversity appears to depends on the kind of evaluation generated, the generation hyperparameters, and the prompt used, and thus sometimes requires e.g. hyperparameter tuning to get right.we found data visualizations to be powerful tools for understanding and debugging data diversity, such as those at evals.anthropic.com/model-written/. qualitatively, we also found that using p d to rank/filter examples limited the diversity, since p d sometimes selected for prototypical examples for testing some behavior (observed qualitatively by workers in 3.2).",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2716,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"we are excited for future work to explore other methods that achieve similar example quality with higher diversity than our method, such as generating many examples and subsampling for diversity.instructions may be misunderstood lms, similar to crowdworkers, may generate evaluations that are testing something different than intended, especially if the generation instructions are underspecified.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2717,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"for example, using the method in 3.1, we generated statements that a person who ""shares beliefs with derek parfit"" (the influential analytic philosopher) would agree or disagree with.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2718,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"the ""disagree"" statements were often ones that many people, not just derek parfit, would disagree with (""i support slavery"" or ""i believe evolution never happend"").",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2719,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"in this case, we should have provided more specific instructions to the lm, to have it generate examples that derek parfit would disagree with but that another philosopher would agree with.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2720,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"when feasible, we recommend briefly examining the generated data, to catch salient issues such as the above.sensitivity to instructions our approach allows the dataset developer fairly fine-grained control over the evaluation by using instructions to guide p g .",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2721,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"however, the quality of lm outputs is sensitive to text inputs in unintuitive ways (perez et al., 2021;lu et al., 2022), adding hard-to-predict variance to the quality of the resulting evaluation; see appendix a.4 for a possible example of this effect we found.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2722,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"we hope that lm advances such as instruction-tuning (wei et al., 2021;sanh et al., 2022;ouyang et al., 2022) mitigate this issue in the future.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2723,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"for now, it may be possible to use prompt sensitivity to generate more diverse datasets, by generating similar datasets with distinct prompts and combining the results, as we did in 3. where prompt sensitivity caused issues, we found it helpful to be able to view example generated outputs in seconds, to quickly iterate and catch salient failures.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2724,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"for the 133 datasets in 3, we found a general instruction template that worked well; we did not do dataset-specific tuning to obtain samples rated as high-quality by human evaluators.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
2725,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,this paper lays the foundation for building a general-purpose task-aware retriever that can follow natural language instructions.,,https://export.arxiv.org/pdf/2211.09260v2.pdf
2726,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,"we introduced a new problem, retrieval with instructions, to model users' intents explicitly.",,https://export.arxiv.org/pdf/2211.09260v2.pdf
2727,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,"we presented berri, the first large-scale retrieval dataset with expert-written annotations.",,https://export.arxiv.org/pdf/2211.09260v2.pdf
2728,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,"building upon berri, we trained the first instruction-following retrieval system by massive multi-task instruction-tuning, tart, adopting two widely used architectures.",,https://export.arxiv.org/pdf/2211.09260v2.pdf
2729,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,"tart advances the state of the art on the popular zeroshot retrieval benchmarks beir and lotte as well as on our newly introduced challenging evaluation setup, x 2 -retrieval.",,https://export.arxiv.org/pdf/2211.09260v2.pdf
2730,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,"our analysis shows that key factors to building a successful multi-task instruction-following retrieval system include informative instructions at training and test time, diversity in data and model scale, and carefully designed negative samples.",,https://export.arxiv.org/pdf/2211.09260v2.pdf
2731,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,"we conclude with two interesting open questions, which future work can explore.",,https://export.arxiv.org/pdf/2211.09260v2.pdf
2732,174801764,Compositional Questions Do Not Necessitate Multi-hop Reasoning,conclusions,Insight-tree,"in summary, we demonstrate that question compositionality is not a sufficient condition for multi-hop reasoning.",,https://arxiv.org/pdf/1906.02900v1.pdf
2733,174801764,Compositional Questions Do Not Necessitate Multi-hop Reasoning,conclusions,Insight-tree,"instead, future datasets must carefully consider what evidence they provide in order to ensure multi-hop reasoning is required.",,https://arxiv.org/pdf/1906.02900v1.pdf
2734,174801764,Compositional Questions Do Not Necessitate Multi-hop Reasoning,conclusions,Insight-tree,there are at least two different ways to achieve this.,,https://arxiv.org/pdf/1906.02900v1.pdf
2735,218486765,Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset,conclusion,Insight-tree,we study the clinical reading comprehension (clinirc) task with the recently created emrqa dataset.,,https://www.aclweb.org/anthology/2020.acl-main.410.pdf
2736,218486765,Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset,conclusion,Insight-tree,our qualitative and quantitative analysis as well as exploration of the two desired aspects of clinirc systems show that future clinical qa datasets should not only be large-scale but also less noisy and more diverse.,,https://www.aclweb.org/anthology/2020.acl-main.410.pdf
2737,218486765,Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset,conclusion,Insight-tree,"moreover, questions that involve complex relations and are across different domains should be included, and then more advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated.  ",,https://www.aclweb.org/anthology/2020.acl-main.410.pdf
2738,254564419,In Defense of Cross-Encoders for Zero-Shot Retrieval,conclusion,Insight-tree,in this work we study how parameter count influences the zero-shot effectiveness of neural retrievers.,,https://export.arxiv.org/pdf/2212.06121v1.pdf
2739,254564419,In Defense of Cross-Encoders for Zero-Shot Retrieval,conclusion,Insight-tree,"we begin by showing that in-domain effectiveness, i.e., when retrievers are fine-tuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al.",,https://export.arxiv.org/pdf/2212.06121v1.pdf
2740,102353837,BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis,conclusions,Insight-tree,we proposed a new task called review reading comprehension (rrc) and investigated the possibility of turning reviews as a valuable resource for answering user questions.,,https://www.aclweb.org/anthology/N19-1242.pdf
2741,102353837,BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis,conclusions,Insight-tree,we adopted bert as our base model and proposed a joint post-training approach to enhancing both the domain and task knowledge.,,https://www.aclweb.org/anthology/N19-1242.pdf
2742,102353837,BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis,conclusions,Insight-tree,we further explored the use of this approach in two other review-based tasks: aspect extraction and aspect sentiment classification.,,https://www.aclweb.org/anthology/N19-1242.pdf
2743,102353837,BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis,conclusions,Insight-tree,experimental results show that the post-training approach before fine-tuning is effective.,,https://www.aclweb.org/anthology/N19-1242.pdf
2744,207853300,Hierarchical Graph Network for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, we propose a new approach, hierarchical graph network (hgn), for multi-hop question answering.",,https://arxiv.org/pdf/1911.03631v1.pdf
2745,207853300,Hierarchical Graph Network for Multi-hop Question Answering,conclusion,Insight-tree,"to capture clues from different granularity levels, our hgn model weaves heterogeneous nodes into a single unified graph.",,https://arxiv.org/pdf/1911.03631v1.pdf
2746,207853300,Hierarchical Graph Network for Multi-hop Question Answering,conclusion,Insight-tree,"experiments with detailed analysis demonstrate the effectiveness of our proposed model, which achieves state-of-the-art performance on hotpotqa benchmark.",,https://arxiv.org/pdf/1911.03631v1.pdf
2747,207853300,Hierarchical Graph Network for Multi-hop Question Answering,conclusion,Insight-tree,"currently, in the fullwiki setting, an off-theshelf paragraph retriever is adopted for selecting relevant context from large corpus of text.",,https://arxiv.org/pdf/1911.03631v1.pdf
2748,207853300,Hierarchical Graph Network for Multi-hop Question Answering,conclusion,Insight-tree,future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.,,https://arxiv.org/pdf/1911.03631v1.pdf
2749,248798649,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,conclusion & future work,Insight-tree,"in this paper, we proposed a tag-level qa model called tie to better understand the topological information contained in the structured web pages.",,https://www.aclanthology.org/2022.naacl-main.132.pdf
2750,248798649,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,conclusion & future work,Insight-tree,"our model explicitly captures two of the most informative topological structures of the web pages, logical and spatial structures, by dom trees and npr graphs, respectively.",,https://www.aclanthology.org/2022.naacl-main.132.pdf
2751,248798649,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,conclusion & future work,Insight-tree,"with the proposed twostage pipeline, we conduct extensive experiments on the websrc dataset.",,https://www.aclanthology.org/2022.naacl-main.132.pdf
2752,248798649,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,conclusion & future work,Insight-tree,our tie successfully achieves sota performances and the contributions of its key components are validated.,,https://www.aclanthology.org/2022.naacl-main.132.pdf
2753,253581551,Data-Efficient Autoregressive Document Retrieval for Fact Verification,conclusions and future work,Insight-tree,we show that distant supervision and pre-training enables high precision autoregressive document retrieval with fewer annotated training data.,,https://export.arxiv.org/pdf/2211.09388v1.pdf
2754,253581551,Data-Efficient Autoregressive Document Retrieval for Fact Verification,conclusions and future work,Insight-tree,while previous work has studied the utility of pre-training   (2020)).,,https://export.arxiv.org/pdf/2211.09388v1.pdf
2755,248228026,StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,conclusion,Insight-tree,"in this paper, we proposed a new dataset named stepgame that requires a robust multi-hop spatial reasoning ability to be solved and mitigates the issues observed in the babi dataset.",,https://arxiv.org/pdf/2204.08292v1.pdf
2756,248228026,StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,conclusion,Insight-tree,"then, we introduced tp-mann, a tensor productbased memory-augmented neural network architecture that achieves state-of-the-art performance on both datasets.",,https://arxiv.org/pdf/2204.08292v1.pdf
2757,248228026,StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,conclusion,Insight-tree,further analysis also demonstrated the importance of a recurrent memory module for multi-hop reasoning.,,https://arxiv.org/pdf/2204.08292v1.pdf
2758,216641928,Conversations with Search Engines: SERP-based Conversational Response Generation,conclusion and future work,Insight-tree,"in this paper, we propose conversations with search engines as task for the community to consider and we contribute two types of result: first, we release a new test set, saac, which is more suitable and challenging for this research than existing resources.",,https://arxiv.org/pdf/2004.14162v2.pdf
2759,216641928,Conversations with Search Engines: SERP-based Conversational Response Generation,conclusion and future work,Insight-tree,"second, we propose an end-to-end neural model, case, to advance the state-of-the-art.",,https://arxiv.org/pdf/2004.14162v2.pdf
2760,216641928,Conversations with Search Engines: SERP-based Conversational Response Generation,conclusion and future work,Insight-tree,"we implement state-of-the-art methods from related tasks and conduct extensive experiments to show that: (1) the proposeed case model can achieve state-of-the-art performance; (2) the proposed sti and ppg modules can bring large improvements; and (3) saac is a more challenging dataset than previously introduced ones, leaving significant room for further improvements.",,https://arxiv.org/pdf/2004.14162v2.pdf
2761,252819457,Type-dependent prompt CycleQAG : Cycle consistency for Multi-hop Question Generation,conclusion,Insight-tree,"in this work, we propose type-dependent prompt cycleqag with cycle consistency.",,https://www.aclanthology.org/2022.coling-1.549.pdf
2762,252819457,Type-dependent prompt CycleQAG : Cycle consistency for Multi-hop Question Generation,conclusion,Insight-tree,"since multihop qg needs to know more diverse information because it needs to gather more scattered pieces of information for generating a question, we introduce the nce for the first time in the qg task.",,https://www.aclanthology.org/2022.coling-1.549.pdf
2763,252819457,Type-dependent prompt CycleQAG : Cycle consistency for Multi-hop Question Generation,conclusion,Insight-tree,"also, we demonstrate that the intermediate task is effective in the qg task.",,https://www.aclanthology.org/2022.coling-1.549.pdf
2764,252819457,Type-dependent prompt CycleQAG : Cycle consistency for Multi-hop Question Generation,conclusion,Insight-tree,"furthermore, we show a significant performance improvement by using prompt-style fine-tuning to make the most of the information obtained from the intermediate task.",,https://www.aclanthology.org/2022.coling-1.549.pdf
2765,252819457,Type-dependent prompt CycleQAG : Cycle consistency for Multi-hop Question Generation,conclusion,Insight-tree,the experiments show that the proposed model outperforms in all automatic evaluations comparing with the existing text-based multi-hop model and several qg models.,,https://www.aclanthology.org/2022.coling-1.549.pdf
2766,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,conclusion,Insight-tree,"this paper puts forward the view with empirical evidence for qa that contrary to popular belief, multi-source domain generalization (dg) is better modeled as a problem of addressing model underfitting than overfitting.",,https://arxiv.org/pdf/2205.07257v1.pdf
2767,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,conclusion,Insight-tree,"our experimental results show that by simply learning the training domains well, even when the number of such domains is relatively small, strong out-of-domain generalization can be achieved without the need for cross-domain regularization.",,https://arxiv.org/pdf/2205.07257v1.pdf
2768,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,conclusion,Insight-tree,we rely on knowledge distillation in our experiments for improved source domain learning over erm.,,https://arxiv.org/pdf/2205.07257v1.pdf
2769,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,conclusion,Insight-tree,"in light of these findings, we believe that focusing our efforts on adequately fitting the source domain patterns might be a more reasonable path forward for dg.",,https://arxiv.org/pdf/2205.07257v1.pdf
2770,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,conclusion,Insight-tree,"that said, further research is needed on the topic before a definitive conclusion can be reached; we hope that our work will inspire future explorations of this problem.",,https://arxiv.org/pdf/2205.07257v1.pdf
2771,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,limitations,Insight-tree,we explore the problem of multi-source domain generalization (dg) in qa with new and existing methods.,,https://arxiv.org/pdf/2205.07257v1.pdf
2772,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,limitations,Insight-tree,"we believe that our findings will generalize to more baselines and datasets, but here we only show proof of concept for a select set of existing baselines and a single dg benchmark (which consists of multiple datasets from various domains).",,https://arxiv.org/pdf/2205.07257v1.pdf
2773,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,in this paper we have discussed the theoretical foundations and practical implementations of bootstrap inference for self-exciting point process models.,,https://arxiv.org/pdf/2104.03122v2.pdf
2774,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,applications of the bootstrap in order to improve upon the poor quality of asymptotic approximations are scarce in the literature.,,https://arxiv.org/pdf/2104.03122v2.pdf
2775,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,"classic 'recursive intensity bootstrap' (rib) schemes have been proposed in the recent literature, although without proof of their first-order validity.",,https://arxiv.org/pdf/2104.03122v2.pdf
2776,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,"rib schemes can also be quite involved to implement in practice, as they generally require numerical integration for the recursive computation of the intensity for each bootstrap repetition.",,https://arxiv.org/pdf/2104.03122v2.pdf
2777,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,"to improve, we have introduced a new bootstrap scheme, the 'fixed intensity bootstrap' (fib), where the conditional intensity is kept fixed across bootstrap repetitions.",,https://arxiv.org/pdf/2104.03122v2.pdf
2778,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,"by doing so, conditionally on the original data the bootstrap data generating process follows a simple inhomogeneous point process with known intensity; therefore, it is very simple to implement and to use in practice.",,https://arxiv.org/pdf/2104.03122v2.pdf
2779,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,"for both bootstrap schemes, we have provided a new bootstrap (asymptotic) theory, which allows to assess bootstrap validity for both bootstraps.",,https://arxiv.org/pdf/2104.03122v2.pdf
2780,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,monte carlo evidence supports the idea that the bootstrap is a valid inference method when applied to point process models.,,https://arxiv.org/pdf/2104.03122v2.pdf
2781,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,"we examined the three main strategies for transfer learning in natural language processing: training on an intermediate supporting task to aid the target task (stilts), training on the target and supporting task simultaneously (mtl), or training on multiple supporting tasks alongside the target task (mtl all ).",,https://www.aclanthology.org/2022.acl-short.30.pdf
2782,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,we provide the first comprehensive comparison between these three methods using the glue dataset suite and show that there is a simple rule for when to use one of these techniques over the other.,,https://www.aclanthology.org/2022.acl-short.30.pdf
2783,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,"this simple heuristic, which holds true in more than 92% of applicable cases, states that multi-task learning is better than intermediate fine tuning when the target task is smaller than the supporting task and vice versa.",,https://www.aclanthology.org/2022.acl-short.30.pdf
2784,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,"additionally, we showed that these pairwise transfer learning techniques outperform the mtl all approach in almost every case.  ",,https://www.aclanthology.org/2022.acl-short.30.pdf
2785,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,pairwise oracle uses the best supplementary task for the given target task using the best pairwise method (stilts or mtl).,,https://www.aclanthology.org/2022.acl-short.30.pdf
2786,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,all scores are the average of 5 random seeds.,,https://www.aclanthology.org/2022.acl-short.30.pdf
2787,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,note that mtl all was run with three different sampling methods (top half).,,https://www.aclanthology.org/2022.acl-short.30.pdf
2788,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,"we find that on almost every task, pairwise approaches are better than mtl all .",,https://www.aclanthology.org/2022.acl-short.30.pdf
2789,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,bold scores indicate the best score in the column for the given section.,,https://www.aclanthology.org/2022.acl-short.30.pdf
2790,221978039,FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,conclusion,Insight-tree,forecasting is a difficult task that requires every possible advantage to do well.,,https://www.aclanthology.org/2021.acl-long.357.pdf
2791,221978039,FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,conclusion,Insight-tree,it would be wise to harness this pool of unstructured data for training automatic event forecasting agents.,,https://www.aclanthology.org/2021.acl-long.357.pdf
2792,221978039,FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,conclusion,Insight-tree,"to utilize this form of data for forecasting, we proposed a question-answering task that requires forecasting skills to solve forecastqa, and provided the accompanying dataset.",,https://www.aclanthology.org/2021.acl-long.357.pdf
2793,221978039,FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,conclusion,Insight-tree,"various baseline methods did not perform well, but this is not surprising given the inherent difficulty of forecasting.",,https://www.aclanthology.org/2021.acl-long.357.pdf
2794,221978039,FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,conclusion,Insight-tree,our benchmark dataset can benefit future research beyond natural language understanding and hope forecasting performance will be significantly improved.,,https://www.aclanthology.org/2021.acl-long.357.pdf
2795,226283753,COSATA: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences,conclusion,Insight-tree,"cosata is an open-source constraint satisfaction solver for easily expressing and evaluating multifact compositional patterns in semi-structured tables of text, paired with an interpreted language that allows expressing micro-models.",,https://www.aclweb.org/anthology/2020.emnlp-demos.10.pdf
2796,226283753,COSATA: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences,conclusion,Insight-tree,"the tool, source, examples, and documentation are available at http://www.github.com/clulab/cosata/ .",,https://www.aclweb.org/anthology/2020.emnlp-demos.10.pdf
2797,51940842,RETHINKING NUMERICAL REPRESENTATIONS FOR DEEP NEURAL NETWORKS,conclusion,Insight-tree,"in this work, we introduced the importance of carefully considering customized precision when realizing neural networks.",,https://arxiv.org/pdf/1808.02513v1.pdf
2798,51940842,RETHINKING NUMERICAL REPRESENTATIONS FOR DEEP NEURAL NETWORKS,conclusion,Insight-tree,we show that using the ieee 754 single precision floating point representation in hardware results in surrendering substantial performance.,,https://arxiv.org/pdf/1808.02513v1.pdf
2799,51940842,RETHINKING NUMERICAL REPRESENTATIONS FOR DEEP NEURAL NETWORKS,conclusion,Insight-tree,"on the other hand, picking a configuration that has lower precision than optimal will result in severe accuracy loss.",,https://arxiv.org/pdf/1808.02513v1.pdf
2800,51940842,RETHINKING NUMERICAL REPRESENTATIONS FOR DEEP NEURAL NETWORKS,conclusion,Insight-tree,"by reconsidering the representation from the ground up in designing custom precision hardware and using our search technique, we find an average speedup across deployable dnns, including googlenet and vgg, of 7.6 with less than 1% degradation in inference accuracy.",,https://arxiv.org/pdf/1808.02513v1.pdf
2801,247158054,Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities,conclusion,Insight-tree,"we introduce wes, an 11m example semantic entity similarity dataset for training question answering evaluation models.",,https://arxiv.org/pdf/2202.13581v1.pdf
2802,247158054,Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities,conclusion,Insight-tree,wes is generated by treating wikipedia link texts and target article titles as synonyms then filtering for quality.,,https://arxiv.org/pdf/2202.13581v1.pdf
2803,247158054,Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities,conclusion,Insight-tree,"wes is targeted to question answering evaluation, independent of human annotators, and consistent with human judgment.",,https://arxiv.org/pdf/2202.13581v1.pdf
2804,247158054,Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities,conclusion,Insight-tree,we hope that future questionanswering datasets will implement semantic evaluation metrics in their leaderboards to encourage the development of more free-form models.,,https://arxiv.org/pdf/2202.13581v1.pdf
2805,247158054,Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities,conclusion,Insight-tree,"in future works, link-mining similarity datasets like wes can be made more challenging by generating negative examples adversarially as described at the end of section 3, more consistent by unioning semantic clusters according to wikipedia's internal redirect pages, and more comprehensive by leveraging link-to-link pairwise synonymy within semantic clusters.",,https://arxiv.org/pdf/2202.13581v1.pdf
2806,258960507,GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking,conclusions,Insight-tree,"in this paper, we focus on narrowing the gap between retrieval and generation for retrieval-enhanced text generation methods.",,https://export.arxiv.org/pdf/2305.18144v1.pdf
2807,258960507,GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking,conclusions,Insight-tree,"we propose griprank, a novel approach to improve the passage ranking capability by distilling knowledge from a generative passage estimator to the passage ranker.",,https://export.arxiv.org/pdf/2305.18144v1.pdf
2808,258960507,GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking,conclusions,Insight-tree,"we evaluate our approach on diverse knowledge-intensive language tasks, including zero-shot slot filling, open-domain question answering, and knowledge-enhanced dialogue generation.",,https://export.arxiv.org/pdf/2305.18144v1.pdf
2809,258960507,GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking,conclusions,Insight-tree,experimental results show that the proposed griprank outperforms the previous state-of-the-art approaches by a significant margin.,,https://export.arxiv.org/pdf/2305.18144v1.pdf
2810,258960507,GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking,conclusions,Insight-tree,further analysis demonstrates the effectiveness of our proposed approach in narrowing the gap between passage retrieval and answer generation.,,https://export.arxiv.org/pdf/2305.18144v1.pdf
2811,238198206,Single-dataset Experts for Multi-dataset Question Answering,conclusion,Insight-tree,"made combines the benefits of single-and multidataset training, resulting in better in-domain accuracy and transfer performance than either multidataset models or ensembles of single-dataset models, especially in low resource settings.",,https://www.aclanthology.org/2021.emnlp-main.495.pdf
2812,238198206,Single-dataset Experts for Multi-dataset Question Answering,conclusion,Insight-tree,for future work we plan to explore ensembling methods for better zero-shot prediction and interpolating made weights for better transfer learning.,,https://www.aclanthology.org/2021.emnlp-main.495.pdf
2813,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,our work comes with some limitations.,,https://export.arxiv.org/pdf/2211.05598v2.pdf
2814,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,"it is uncertain whether our results in two specific settings, multiple choice and extractive qa, would extend to more general settings for nli, although the use of contradictions for factual consistency by laban et al.",,https://export.arxiv.org/pdf/2211.05598v2.pdf
2815,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,(2022) suggests that they could.,,https://export.arxiv.org/pdf/2211.05598v2.pdf
2816,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,"additionally, 3-class nli is not sufficient to capture all the natural language relations that might be needed to verify an answer.",,https://export.arxiv.org/pdf/2211.05598v2.pdf
2817,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,as such more challenging datasets in other settings and more granular nli settings should be attempted.another limitation involves answer ranking and the associated computational cost.,,https://export.arxiv.org/pdf/2211.05598v2.pdf
2818,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,"the main reason we did not test answer ranking in extractive qa is that we did not generate diverse outputs, but another reason is that such a procedure grows prohibitively expensive as the domain becomes more open.",,https://export.arxiv.org/pdf/2211.05598v2.pdf
2819,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,"in a fully open domain, ranking would require a quadratic evaluation for each context passage against each reformulated answer candidate (schuster et al., 2022).",,https://export.arxiv.org/pdf/2211.05598v2.pdf
2820,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,"future work should look at comparison approaches that amortize this cost, such as nli-based dense passage retrieval (reimers and gurevych, 2019).",,https://export.arxiv.org/pdf/2211.05598v2.pdf
2821,253447227,Using contradictions to improve QA systems,limitations,Insight-tree,"despite the results above, multiple choice qa and extractive qa with a provided context is a limited setting that doesn't indicate the results would extend to other popular settings where nli.",,https://export.arxiv.org/pdf/2211.05598v1.pdf
2822,253447227,Using contradictions to improve QA systems,limitations,Insight-tree,given that laban et al.,,https://export.arxiv.org/pdf/2211.05598v1.pdf
2823,253447227,Using contradictions to improve QA systems,limitations,Insight-tree,(2022) shows similar results that contradiction is an important signal in factual consistency we are hopeful that it would.,,https://export.arxiv.org/pdf/2211.05598v1.pdf
2824,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",conclusion,Insight-tree,"we introduced instructor, a single model that creates broadly-applicable text embeddings using natural language instructions.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2825,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",conclusion,Insight-tree,"we constructed medi, a collection of diverse datasets, to finetune instructor with instructions.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2826,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",conclusion,Insight-tree,"our extensive experiments showed that instructor achieves state-of-the-art performance on text embedding benchmarks, as well as prompt retrieval for fewshot in-context learning.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2827,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",conclusion,Insight-tree,we hope that researchers and practitioners will benefit from our embeddings or our datasets for tasks of their interest.,,https://export.arxiv.org/pdf/2212.09741v3.pdf
2828,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,although  table 5 in the appendix.,,https://export.arxiv.org/pdf/2212.09741v3.pdf
2829,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"at each step, we first randomly select a dataset and then construct a minibatch only using the examples from that dataset.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2830,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"in this way, we ensure that in-batch negatives are sampled from the same dataset, thereby preventing the model from using task differences to predict the negative label.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2831,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"we use the maximum batch size that fits the machine memory and run all our experiments on 40gb a100 gpus.training we initialize instructor with the gtr-large model (ni et al., 2021, 335m parameters) 4 and finetune it on medi using the adamw optimizer with learning rate 2  10 5 and warmup ratio 0.1.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2832,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"we use a softmax temperature of 0.01 and finetune instructor for 20k steps.baselines we use the official mteb benchmark for comparisons, but here we highlight several strong baselines with the following two types.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2833,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"the first class of baselines is embedding models specializing in information retrieval: contriever-  , 2022).",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2834,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"they are mainly trained on symmetric paraphrase datasets such as nli (williams et al., 2018) and the quora question pairs.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2835,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"5 all of these baselines are based on pretrained language models, achieving strong performance on the mteb leaderboard.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2836,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"in particular, sent-t5-xxl and gtr-xxl (both with 4.8b parameters) achieve the first and second best average performances.",,https://export.arxiv.org/pdf/2212.09741v3.pdf
2837,232290492,Controllable Generation from Pre-trained Language Models via Inverse Prompting,conclusion,Insight-tree,"in this paper, we present a new method, inverse prompting for text generation.",,https://arxiv.org/pdf/2103.10685v3.pdf
2838,232290492,Controllable Generation from Pre-trained Language Models via Inverse Prompting,conclusion,Insight-tree,inverse prompting offers a new option for controllable generation using language models by exploiting the inverse form of natural languages.,,https://arxiv.org/pdf/2103.10685v3.pdf
2839,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"we introduce halueval, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing hallucinations.",,https://export.arxiv.org/pdf/2305.11747v2.pdf
2840,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"to automatically generate large-scale samples, we propose a chatgpt-based two-step approach, i.e., sampling-then-filtering.",,https://export.arxiv.org/pdf/2305.11747v2.pdf
2841,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one.,,https://export.arxiv.org/pdf/2305.11747v2.pdf
2842,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.",,https://export.arxiv.org/pdf/2305.11747v2.pdf
2843,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"we find that, existing llms mostly fail to recog-nize the hallucinations in text and tend to generate hallucinated content.",,https://export.arxiv.org/pdf/2305.11747v2.pdf
2844,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"finally, we suggest several strategies to help llms mitigate the hallucinations and recognize them.",,https://export.arxiv.org/pdf/2305.11747v2.pdf
2845,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"our benchmark can facilitate research in understanding what types of content and to which extent llms tend to hallucinate, ultimately paving the way for building more effective and reliable llms in the future.",,https://export.arxiv.org/pdf/2305.11747v2.pdf
2846,252365096,VIMQA: A Vietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering,conclusion,Insight-tree,"in this work, we propose vimqa, a multi-hop vietnamese qa dataset.",,
2847,252365096,VIMQA: A Vietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering,conclusion,Insight-tree,it is highly necessary and important to facilitate the development of vietnamese qa models that can perform advanced reasoning and provide explainable answers with supporting facts.,,
2848,252365096,VIMQA: A Vietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering,conclusion,Insight-tree,"then, we also propose a pipeline for collecting multi-hop qa examples that can be generalized for all languages.",,
2849,252365096,VIMQA: A Vietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering,conclusion,Insight-tree,we also prove the efficiency of our pipeline via the detailed analysis in our vimqa dataset.,,
2850,252365096,VIMQA: A Vietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering,conclusion,Insight-tree,the experimental results indicate that vimqa is challenging for competitive approaches in both single and multiple hop qa.,,
2851,252365096,VIMQA: A Vietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering,conclusion,Insight-tree,"it reveals that our vimqa dataset is a good resource for vietnamese and cross-lingual qa models, especially in vietnamese multi-hop qa tasks for reasoning and explaining the comprehension and coherence of text understanding.",,
2852,248862979,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,conclusion,Insight-tree,"in this work, we hypothesize that (i) logic ability plays a key role in a wide scope of tasks requiring general reasoning; and (ii) plms' logic ability can be further improved beyond their original linguistic ability.",,https://export.arxiv.org/pdf/2205.08794v2.pdf
2853,248862979,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,conclusion,Insight-tree,"we correspondingly propose logigan, an unsupervised adversarial pre-training framework for logical reasoning enhancement.",,https://export.arxiv.org/pdf/2205.08794v2.pdf
2854,248862979,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,conclusion,Insight-tree,"logigan circumvents the non-differentiable challenge of sequential gan via a novel generator-verifier scoring consensus mechanism, and enables largescale pre-training with longer target length.",,https://export.arxiv.org/pdf/2205.08794v2.pdf
2855,248862979,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,conclusion,Insight-tree,"extensive experiments and ablation studies reveal the effectiveness and functional components of logigan, providing evidence to our major hypothesis.",,https://export.arxiv.org/pdf/2205.08794v2.pdf
2856,232478685,FeTaQA: Free-form Table Question Answering,conclusion,Insight-tree,"in this paper, we introduced the task of generative table question answering with fetaqa, a table qa dataset consisting of complex questions that require free-form, elaborate answers.",,https://www.aclanthology.org/2022.tacl-1.3.pdf
2857,232478685,FeTaQA: Free-form Table Question Answering,conclusion,Insight-tree,"we also proposed two modeling approaches: (1) a pipeline model that incorporates a table semantic parser and a data-to-text generator, and (2) an end-to-end model that integrates query comprehension, reasoning and text generation.",,https://www.aclanthology.org/2022.tacl-1.3.pdf
2858,232478685,FeTaQA: Free-form Table Question Answering,conclusion,Insight-tree,our experimental results indicate that the end-to-end model with a simple table encoding strategy achieves much higher scores than the pipeline model that requires table semantic parsing.,,https://www.aclanthology.org/2022.tacl-1.3.pdf
2859,232478685,FeTaQA: Free-form Table Question Answering,conclusion,Insight-tree,"furthermore, we show that fetaqa reveals the challenging nature of the table question answering task and calls for innovative model designs in the future.  ",,https://www.aclanthology.org/2022.tacl-1.3.pdf
2860,257913786,QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,conclusion,Insight-tree,"in this paper, we have described our study to scale qa-based on q/a db to open domain applications.",,https://export.arxiv.org/pdf/2304.01003v1.pdf
2861,257913786,QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,conclusion,Insight-tree,"this required to build a large db, which we built only using publicly available",,https://export.arxiv.org/pdf/2304.01003v1.pdf
2862,257913786,QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,conclusion,Insight-tree,"q/a pairs, reaching a significant size of  6.3m items.",,https://export.arxiv.org/pdf/2304.01003v1.pdf
2863,257913786,QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,conclusion,Insight-tree,"to enable retrieval from these large dbs, inspired by the latest neural ir technology, we modeled neural retrieval for q/a pairs.",,https://export.arxiv.org/pdf/2304.01003v1.pdf
2864,257913786,QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,conclusion,Insight-tree,"we proposed two different methods based on only questions, and on q/a pairs, where questions and answers can be seen as context.",,https://export.arxiv.org/pdf/2304.01003v1.pdf
2865,257913786,QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,conclusion,Insight-tree,"we analyzed the significant impact of using architectures with separate encoder versus dual encoders, in accuracy and efficiency.",,https://export.arxiv.org/pdf/2304.01003v1.pdf
2866,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,we present sled as a simple and effective method to extend the capabilities of pretrained short-text models to long-text tasks.,,https://export.arxiv.org/pdf/2208.00748v3.pdf
2867,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"despite its impressive empirical performance on scrolls, sled suffers from two disadvantages which may limit its applicability to some long-range tasks.long output to obtain linear complexity, sled assumes the output length k is constant.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2868,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"this is since the decoder uses quadratic selfattention over the output, on top of o(nk) crossattention between the output and input.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2869,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"while most current long-text tasks follow this assumption, future tasks, such as academic reports or script writing, may require long text generation.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2870,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,this limitation is not unique to sled and affects other long-range transformers including longt5 and led.,,https://export.arxiv.org/pdf/2208.00748v3.pdf
2871,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"aside from finetuning, this also affects pretraining models on long inputs with selfsupervised losses such as span-corruption (raffel et al., 2020b) or denoising (lewis et al., 2020), which require the decoder to process an output that is linear in the length of the input.co-reference resolution and fact retention an assumption at the heart of sled is the locality of information assumption.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2872,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"when the input text is long, this assumption may break if distant entity resolution or factual knowledge are required.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2873,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"for example, a chapter in a book may mention ""they were walking into the room"" when knowledge of what room or who walked is located a few chapters back.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2874,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"in such cases, the encoder used by sled will not be able to access this information, moving more responsibility to the decoder and reducing the effectiveness of the contextual encoding.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2875,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"similarly, in multi-hop questions (yang et al., 2018), attending to one part of the context is necessary in order to fully understand the question and encode a second piece of information correctly.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2876,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"as the encoder will not have access to the first context that leads to better question understanding, here as well more responsibility is delegated to the decoder.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2877,251224058,Efficient Long-Text Understanding with Short-Text Models,conclusions,Insight-tree,"in this work we present sled, a simple approach for modeling long texts which slides a pretrained short-range encoder over a long input document and then generates an output by attending to the encoded tokens.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2878,251224058,Efficient Long-Text Understanding with Short-Text Models,conclusions,Insight-tree,"we show sled can perform core operations that are important for long text understanding, such as finding relevant pieces of information and fusing them at decoding time, and demonstrate competitive performance on the scrolls benchmark compared to larger models and models that employ a dedicated and expensive pretraining step.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2879,251224058,Efficient Long-Text Understanding with Short-Text Models,conclusions,Insight-tree,one of sled's most attractive features is that it can be readily used with any short-range pretrained lm.,,https://export.arxiv.org/pdf/2208.00748v3.pdf
2880,251224058,Efficient Long-Text Understanding with Short-Text Models,conclusions,Insight-tree,"thus, any future encoder-decoder model can be flexibly plugged into it to achieve further gains in performance on scrolls, some of its tasks, or any other long-range task.",,https://export.arxiv.org/pdf/2208.00748v3.pdf
2881,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,spatial reasoning is an important problem in natural language understanding.,,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2882,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,"we propose the first human-created qa benchmark on spatial reasoning, and experiments show that state-of-the-art pretrained language models (lm) do not have the capability to solve this task given limited training data, while humans can solve those spatial reasoning questions reliably.",,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2883,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,"to improve lms' capability on this task, we propose to use hand-crafted grammar and spatial reasoning rules to automatically generate a large corpus of spatial descriptions and corresponding question-answer annotations; further pretraining lms on this distant supervision dataset significantly enhances their spatial language understanding and reasoning.",,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2884,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,we also show that a spatially-improved lm can have better results on two extrinsic datasets (babi and boolq).,,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2885,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,table 7 shows the templates used to create questions in spartqa-auto.,,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2886,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,"the ""<object>"" is a variable replaced by objects from the story (using choose-objects and describe-objects modules), and the ""<relation>"" variable can be replaced by the chosen relations between objects (using findall-relations module).",,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2887,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,the articles and the indefinite pronouns in each template play an essential role in understanding the question's objective.,,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2888,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,"for example, ""are all blue circles near to a triangle?"" is different from ""are there any blue circles near to a triangle?"", and ""are there any blue circles near to all triangles?"".",,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2889,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,"therefore, we check the uniqueness of the object definition, using ""a"" or ""the"" in proper places and randomly place the terms ""any"" or ""all"" in the yn questions to generate different questions.",,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2890,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,table 8 shows the percentage of correct labels in train and test sets.,,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2891,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,"in multi-choice q-types, more than one label can be true.",,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2892,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,table 10 shows some generated sentences in spartqa-auto with some specific features that challenge models to understand different forms of relation description in spatial language.,,https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
2893,256827305,Improving Out-of-Distribution Generalization of Neural Rerankers with Contextualized Late Interaction,conclusion,Insight-tree,"in this work, we presented our finding that adding late interaction to existing rerankers brings visible improvement to out-of-distribution capacity without any degradation on in-domain effectiveness, even though the reranker already processes the token interaction via the attention mechanism at previous layers.",,https://export.arxiv.org/pdf/2302.06589v1.pdf
2894,256827305,Improving Out-of-Distribution Generalization of Neural Rerankers with Contextualized Late Interaction,conclusion,Insight-tree,"extensive experiments on different model sizes and first-stage retrievers show that this improvement is consistent, and according to our analysis, the improvement is more prominent on longer queries.",,https://export.arxiv.org/pdf/2302.06589v1.pdf
2895,256827305,Improving Out-of-Distribution Generalization of Neural Rerankers with Contextualized Late Interaction,conclusion,Insight-tree,"our findings suggest that boiling all information into the [cls] token may not be the optimal choice for neural rerankers, and more studies are required to better explore its capacity.",,https://export.arxiv.org/pdf/2302.06589v1.pdf
2896,247595263,SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS,conclusion and discussion,Insight-tree,"we introduced a simple yet effective method called self-consistency, and observed that it significantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales.",,https://export.arxiv.org/pdf/2203.11171v4.pdf
2897,247595263,SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS,conclusion and discussion,Insight-tree,"beyond accuracy gains, self-consistency is also useful for collecting rationales when performing reasoning tasks with language models, and for providing uncertainty estimates and improved calibration of language model outputs.",,https://export.arxiv.org/pdf/2203.11171v4.pdf
2898,253098851,Iteratively Prompt Pre-trained Language Models for Chain of Thought,conclusion & future work,Insight-tree,"we explore an iterative prompting framework towards driving a ""chain of thought"" from plms for multi-step reasoning tasks.",,https://www.aclanthology.org/2022.emnlp-main.174.pdf
2899,253098851,Iteratively Prompt Pre-trained Language Models for Chain of Thought,conclusion & future work,Insight-tree,"we show the superiority of this iterative scheme, and also the effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme.",,https://www.aclanthology.org/2022.emnlp-main.174.pdf
2900,253098851,Iteratively Prompt Pre-trained Language Models for Chain of Thought,conclusion & future work,Insight-tree,"in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors.",,https://www.aclanthology.org/2022.emnlp-main.174.pdf
2901,253098851,Iteratively Prompt Pre-trained Language Models for Chain of Thought,conclusion & future work,Insight-tree,"in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities.",,https://www.aclanthology.org/2022.emnlp-main.174.pdf
2902,253098851,Iteratively Prompt Pre-trained Language Models for Chain of Thought,conclusion & future work,Insight-tree,"the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely, a human can track along the plm's chain of thought and make edits and corrections at different steps, similarly as in (mo et al., 2022a), which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process.",,https://www.aclanthology.org/2022.emnlp-main.174.pdf
2903,253098851,Iteratively Prompt Pre-trained Language Models for Chain of Thought,conclusion & future work,Insight-tree,we leave these investigations as future work.,,https://www.aclanthology.org/2022.emnlp-main.174.pdf
2904,234778323,Answering Any-hop Open-domain Questions with Iterative Document Reranking,conclusion,Insight-tree,"we present a qa framework that can answer any-hop open-domain questions, which iteratively retrieves, reranks and filters documents with a graph-based reranking model, and adaptively decides how many steps of retrieval and reranking are needed for a multi-hop question.",,https://export.arxiv.org/pdf/2009.07465v5.pdf
2905,234778323,Answering Any-hop Open-domain Questions with Iterative Document Reranking,conclusion,Insight-tree,our method consistently achieves promising performance on both single-and multi-hop open-domain qa datasets.,,https://export.arxiv.org/pdf/2009.07465v5.pdf
2906,202785879,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,conclusion,Insight-tree,"we introduced rocc, a simple unsupervised approach for selecting justification sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer.",,https://www.aclweb.org/anthology/D19-1260.pdf
2907,202785879,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,conclusion,Insight-tree,"we coupled this method with a state-of-the-art bert-based supervised question answering system, and achieved a new state-ofthe-art on the multirc and arc datasets among approaches that do not use external resources during training.",,https://www.aclweb.org/anthology/D19-1260.pdf
2908,202785879,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,conclusion,Insight-tree,"we showed that rocc-based qa approaches are more robust across domains, and generalize better to other related tasks like entailment.",,https://www.aclweb.org/anthology/D19-1260.pdf
2909,202785879,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,conclusion,Insight-tree,"in the future, we envision that rocc scores can be used as distant supervision signal to train supervised justification selection methods.",,https://www.aclweb.org/anthology/D19-1260.pdf
2910,253255229,MULTI-VECTOR RETRIEVAL AS SPARSE ALIGNMENT,conclusion,Insight-tree,"in this paper, we introduce aligner, a novel sparse alignment method for multi-vector document retrieval.",,https://export.arxiv.org/pdf/2211.01267v1.pdf
2911,253255229,MULTI-VECTOR RETRIEVAL AS SPARSE ALIGNMENT,conclusion,Insight-tree,we first formulate different retrieval models with token-level sparse alignments and propose aligner to tackle the limitations of existing models.,,https://export.arxiv.org/pdf/2211.01267v1.pdf
2912,253255229,MULTI-VECTOR RETRIEVAL AS SPARSE ALIGNMENT,conclusion,Insight-tree,"specifically, aligner uses pairwise alignments and unary saliences that allow us to adapt to different tasks and prune unimportant tokens, respectively.",,https://export.arxiv.org/pdf/2211.01267v1.pdf
2913,253255229,MULTI-VECTOR RETRIEVAL AS SPARSE ALIGNMENT,conclusion,Insight-tree,"as a result, we achieve strong performance on both zero-shot and few-shot document retrieval tasks while drastically improving the run-time and storage complexity of multi-vector retrieval.",,https://export.arxiv.org/pdf/2211.01267v1.pdf
2914,253255229,MULTI-VECTOR RETRIEVAL AS SPARSE ALIGNMENT,conclusion,Insight-tree,"with its interpretable alignments and better performance with large language models, we envision that our multi-vector retrieval model can serve as a strong standalone retriever in the future.",,https://export.arxiv.org/pdf/2211.01267v1.pdf
2915,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,"we presented a novel query generation method, qgek, that generates synthetic queries in a form more similar to human labeled queries by using external knowledge.",,https://www.aclanthology.org/2022.deelio-1.3.pdf
2916,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,"in order to use unprocessed external knowledge, we convert a query into a tripletbased template, which can include information of subjects and answers.",,https://www.aclanthology.org/2022.deelio-1.3.pdf
2917,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,"remarkably, when dense retrieval models are trained with the queries generated from qgek, the performance has improved much compared to using the queries without external knowledge.",,https://www.aclanthology.org/2022.deelio-1.3.pdf
2918,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,"also, we have shown that including external knowledge give rises to the distribution of the unique words similar to that of the human labeled queries.",,https://www.aclanthology.org/2022.deelio-1.3.pdf
2919,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,we believe that qgek can also be applied to the other generation methods by orthogonally adding some external knowledge processing modules.,,https://www.aclanthology.org/2022.deelio-1.3.pdf
2920,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,"for future work, we plan to generate queries both close to human labeled ones and optimized for ir tasks and to allow the template to accept more general logical forms for diverse highquality queries.",,https://www.aclanthology.org/2022.deelio-1.3.pdf
2921,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,the code and data will be made available for public access.,,https://www.aclanthology.org/2022.deelio-1.3.pdf
2922,226743717,Development of SW Interface between Healthcare Standards-DASTA and HL7,conclusions,Insight-tree,the paper described the interface created between dasta and hl7 standards using a visual simulation presentable via a web interface that very simply simulates the process of a doctor's prescribing a drug in the ward and its automated preparation in a hospital pharmacy.,,https://web.archive.org/web/20201103205833/https:/res.mdpi.com/d_attachment/sustainability/sustainability-12-07649/article_deploy/sustainability-12-07649-v2.pdf
2923,226743717,Development of SW Interface between Healthcare Standards-DASTA and HL7,conclusions,Insight-tree,"the newly-created interface between dasta and hl7 (which has a working name abbreviated as idh) allows the selection of medication in a very simple simulation of the registration of a patient's medical record and treatment plan in his, a statement of the data sentence generated in the dasta standard, a statement of the data sentence in the hl7 standard from the dasta message, and showing the patient's prepared medication in a simple graphical output, simulating a robotically prepared ""circuit"" of the patient's personalized drug therapy with accompanying information about the patient and his or her medications for the next 24 h.",,https://web.archive.org/web/20201103205833/https:/res.mdpi.com/d_attachment/sustainability/sustainability-12-07649/article_deploy/sustainability-12-07649-v2.pdf
2924,247748606,UKP-SQUARE: An Online Platform for Question Answering Research,conclusion and future work,Insight-tree,"we introduce the ukp-square platform that enables researchers and developers to study and compare qa pipelines, i.e., skills, that comprises a selection of datastores, retrieval mechanisms and reader models.",,https://www.aclanthology.org/2022.acl-demo.2.pdf
2925,247748606,UKP-SQUARE: An Online Platform for Question Answering Research,conclusion and future work,Insight-tree,"the platform enables querying ex-isting public skills, as well as implementing custom ones using ukp-square's microservices and utility functions that support a large collection of model types and datastores.",,https://www.aclanthology.org/2022.acl-demo.2.pdf
2926,247748606,UKP-SQUARE: An Online Platform for Question Answering Research,conclusion and future work,Insight-tree,"furthermore, users can simultaneously query multiple skills, and analyze them through integrated behavioural tests.",,https://www.aclanthology.org/2022.acl-demo.2.pdf
2927,247748606,UKP-SQUARE: An Online Platform for Question Answering Research,conclusion and future work,Insight-tree,our architecture is scalable and flexible to incorporate most of the latest developments in the qa domain.,,https://www.aclanthology.org/2022.acl-demo.2.pdf
2928,247748606,UKP-SQUARE: An Online Platform for Question Answering Research,conclusion and future work,Insight-tree,"future versions will include automated deployment of custom models and datastores, automated skill selection by incorporating previous works (puerto et al., 2021;geigle et al., 2021) and increasing the number of supported datastores (e.g., wikidata, vrandei and krtzsch, 2014).",,https://www.aclanthology.org/2022.acl-demo.2.pdf
2929,247748606,UKP-SQUARE: An Online Platform for Question Answering Research,conclusion and future work,Insight-tree,"we also plan to incorporate specialized models (e.g., using graph encoders, ribeiro et al., 2021), structured reasoning approaches (yasunaga et al., 2021) and interpretability techniques such as saliency maps .",,https://www.aclanthology.org/2022.acl-demo.2.pdf
2930,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,"in this paper, we study the compositional question answering where the answer is composed of discontiguous segments in the document.",,https://arxiv.org/pdf/2101.06400v1.pdf
2931,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,we present a large scale chinese comqa dataset containing more than 120k human-labeled questions.,,https://arxiv.org/pdf/2101.06400v1.pdf
2932,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,the data construction process has undergone rigid inspections to ensure high quality.,,https://arxiv.org/pdf/2101.06400v1.pdf
2933,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,"to solve the comqa problem, we propose a hierarchical graph neural networks that incorporate document graph structure to the model.",,https://arxiv.org/pdf/2101.06400v1.pdf
2934,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,"we also devise two novel tasks, i.e., question selection and node selection, to pre-train the model.",,https://arxiv.org/pdf/2101.06400v1.pdf
2935,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,the proposed methods achieve significant improvement over previous methods.,,https://arxiv.org/pdf/2101.06400v1.pdf
2936,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,we also conduct several ablation studies to demonstrate the superiority of the proposed pre-training tasks and the graph structure.,,https://arxiv.org/pdf/2101.06400v1.pdf
2937,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,"however, there is still a large gap between our model with human performance, suggesting that there is still room for improvement in comqa.",,https://arxiv.org/pdf/2101.06400v1.pdf
2938,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,figure 8: a snapshot of the annotation interface.,,https://arxiv.org/pdf/2101.06400v1.pdf
2939,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,each row is a single node that could be served as the final answer component.,,https://arxiv.org/pdf/2101.06400v1.pdf
2940,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,the question is the page title in the top.,,https://arxiv.org/pdf/2101.06400v1.pdf
2941,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,note that the image or table could also be selected.,,https://arxiv.org/pdf/2101.06400v1.pdf
2942,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,"benefiting from pre-trained bert models, the field of information retrieval has seen remarkable progress in neural ir models, as exemplified by the success of vanilla bert which has become a strong, yet simple, baseline for neural ir models.",,https://export.arxiv.org/pdf/2111.09852v3.pdf
2943,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,"to overcome the limitations of bert-based models regarding long documents, we have proposed to divide documents into blocks and to select only the most important key blocks.",,https://export.arxiv.org/pdf/2111.09852v3.pdf
2944,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,"this is reminiscent of the way humans assess the relevance of a document for a given query: one first identifies blocks relevant to the query, blocks which are then aggregated to obtain the overall assessment of the document.",,https://export.arxiv.org/pdf/2111.09852v3.pdf
2945,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,"in order to select blocks, we have investigated two approaches: the first one is straightforward and makes use of standard retrieval functions as tf-idf or bm25; the second one learns a single bert model used for both ranking blocks and documents.",,https://export.arxiv.org/pdf/2111.09852v3.pdf
2946,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,both approaches have been shown to improve over standard baselines and previous bert-based models.,,https://export.arxiv.org/pdf/2111.09852v3.pdf
2947,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,"we have followed the same approach on another highly competitive neural ir model, namely parade, here again with improved results.",,https://export.arxiv.org/pdf/2111.09852v3.pdf
2948,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,"all in all, selecting blocks is advantageous for the two models studied here, vanilla bert and parade.",,https://export.arxiv.org/pdf/2111.09852v3.pdf
2949,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,we conjecture that this selection is a way to remove passages in documents which are not relevant to the query and which are likely to bring noise when matching queries and documents.,,https://export.arxiv.org/pdf/2111.09852v3.pdf
2950,258947821,Fact-driven Logical Reasoning for Machine Reading Comprehension,conclusions,Insight-tree,"in this work, we propose extracting a general form called ""fact unit"" to cover both commonsense and temporary knowledge units for logical reasoning.",,https://export.arxiv.org/pdf/2105.10334v2.pdf
2951,258947821,Fact-driven Logical Reasoning for Machine Reading Comprehension,conclusions,Insight-tree,our proposed focal reasoner not only better uncovers the logical structures within the context but also better captures the logical interactions between context and options.,,https://export.arxiv.org/pdf/2105.10334v2.pdf
2952,258947821,Fact-driven Logical Reasoning for Machine Reading Comprehension,conclusions,Insight-tree,experimental results verify the effectiveness of our method.,,https://export.arxiv.org/pdf/2105.10334v2.pdf
2953,258865162,Why Does ChatGPT Fall Short in Providing Truthful Answers?,conclusion,Insight-tree,our paper investigates the common failures of chat-gpt in complex open-domain question answering.,,https://export.arxiv.org/pdf/2304.10513v2.pdf
2954,258865162,Why Does ChatGPT Fall Short in Providing Truthful Answers?,conclusion,Insight-tree,"we identify four types of errors: comprehension, factualness, specificity, and inference.",,https://export.arxiv.org/pdf/2304.10513v2.pdf
2955,258865162,Why Does ChatGPT Fall Short in Providing Truthful Answers?,conclusion,Insight-tree,"we also examine the key abilities knowledge memorization, knowledge recall, and knowledge reasoning, which are critical to these failures.",,https://export.arxiv.org/pdf/2304.10513v2.pdf
2956,258865162,Why Does ChatGPT Fall Short in Providing Truthful Answers?,conclusion,Insight-tree,"additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on recall, and the effect of decomposition on reasoning.",,https://export.arxiv.org/pdf/2304.10513v2.pdf
2957,258865162,Why Does ChatGPT Fall Short in Providing Truthful Answers?,conclusion,Insight-tree,"finally, we suggest several techniques to help users more effectively use chatgpt as a question-answering tool and enable system builders to develop better qa systems.",,https://export.arxiv.org/pdf/2304.10513v2.pdf
2958,258865162,Why Does ChatGPT Fall Short in Providing Truthful Answers?,conclusion,Insight-tree,"our research contributes to the understanding of what influencing the truthfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more reliable llms.",,https://export.arxiv.org/pdf/2304.10513v2.pdf
2959,245634501,OpenQA: Hybrid QA System Relying on Structured Knowledge Base as well as Non-structured Data,conclusion,Insight-tree,"we propose an intelligent question-answering system based on structured and unstructured data, that is, openqa, in which users can give query questions and the model can quickly give accurate answers back to users.",,https://arxiv.org/pdf/2112.15356v1.pdf
2960,245634501,OpenQA: Hybrid QA System Relying on Structured Knowledge Base as well as Non-structured Data,conclusion,Insight-tree,"we integrate the kbqa structured question answering based on semantic deep learning analysis and the two-stage unstructured question answering based on machine reading comprehension into the intelligent question answering system, and return the answer with the highest probability through the answer selection module based on transformer.",,https://arxiv.org/pdf/2112.15356v1.pdf
2961,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,"in this paper we (1) investigate the impact of debiasing methods on qa model generalization for both single and multi-domain training scenarios, and (2) propose a new framework for improving the in-domain and out-of-domain performances by concurrent modeling of multiple biases.",,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2962,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,our framework weights each training example according to multiple biases and based on the strength of each bias in the training data.,,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2963,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,it uses the resulting bias weights in the training objective to prevent the model from mainly focusing on learning biases.,,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2964,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,"we evaluate our framework using two different training objectives, i.e., multi-bias confidence regularization and multi-bias loss re-weighting, and show its effectiveness in both single and multidomain training scenarios.",,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2965,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,we further compare our framework with two state-of-the-art debiasing methods of utama et al. (2020) and mahabadi et al. (2020).,,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2966,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,"we show that knowledge distillation, modeling multiple biases at once, and weighting the impact of each bias based on its strength in the training data are all important factors in improving the in-domain and out-of-domain performances.",,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2967,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,"while recent literature on debiasing in nlp focuses on improving the performance on adversarial evaluation sets, this work opens new research directions on wider uses of debiasing methods.",,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2968,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,the main advantage of using our debiasing methods is that they improve the performance and generalization without requiring additional training data or larger models.,,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2969,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,future work could build upon our framework by applying it to a wide range of tasks beyond qa using task-specific bias models.,,https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
2970,174801080,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,conclusion,Insight-tree,"we proposed decomprc, a system for multihop rc that decomposes a multi-hop question into simpler, single-hop sub-questions.",,https://www.aclweb.org/anthology/P19-1613.pdf
2971,174801080,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,conclusion,Insight-tree,"we recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions.",,https://www.aclweb.org/anthology/P19-1613.pdf
2972,174801080,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,conclusion,Insight-tree,"moreover, decomprc achieved further gains from the decomposition scoring step.",,https://www.aclweb.org/anthology/P19-1613.pdf
2973,174801080,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,conclusion,Insight-tree,"decomprc achieved the state-of-the-art on hotpotqa distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines.",,https://www.aclweb.org/anthology/P19-1613.pdf
2974,174801080,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,conclusion,Insight-tree,"in this section, we describe span annotation collection procedure for bridging and intersection questions.",,https://www.aclweb.org/anthology/P19-1613.pdf
2975,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,foundation models come with the promise to be highly general and modular.,,https://export.arxiv.org/pdf/2301.10444v1.pdf
2976,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"it is believed that they contain a wide ""knowledge"" due to their pretraining on a large collection, which is then believed to be the source of their improved performance.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2977,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,we have examined how this pretraining collection influence the performance of ir models.,,https://export.arxiv.org/pdf/2301.10444v1.pdf
2978,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"our research question was to assess how much of this implicit knowledge, beneficial to the final performance, comes from pretraining on a large external collection.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2979,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"this is why we have experimented on a variety of collections, domains and languages to study how pretraining from scratch actually performed compared to their de facto approach of simple finetuning.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2980,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"while we were expecting the standard pretrained models to work better, we surprisingly revealed that pretraining from scratch works better for first-stage retrieval on msmarco, tripclick and several non-english languages on the mr.  tydi benchmark.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2981,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"in particular, the flops regularization played a critical role in those results, suggesting that regularization or better pretraining techniques could further improve the results.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2982,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"furthermore, pretrained models from scratch also behave well in the zero shot scenario for sparse models such as splade.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2983,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"nevertheless, pretraining from a large collection has a slight advantage when training rerankers.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2984,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"overall, these results, specific to ir, challenge the foundation model hypothesis for small models, ie that a more general model encapsulating the world knowledge would be better than a smaller one in a specific domain application.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2985,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"furthermore, our study makes a contribution to the debate between general purpose and specific purpose models.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2986,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"in a way, our experiments showed that less is more.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2987,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"in addition, pretrained language models come also with many challenges such as the societal bias in the data they have been trained on.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2988,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"we hope that our study could convince practitioners, both from industry and academia, to reconsider specific purpose models by pretraining from scratch.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2989,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"last but not least, doing so enable to better control efficiency, data bias and replicability, which are key research questions for the ir community.",,https://export.arxiv.org/pdf/2301.10444v1.pdf
2990,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,conclusion,Insight-tree,this work introduces mcr for meta-reasoning over multiple reasoning chains.,,https://export.arxiv.org/pdf/2304.13007v2.pdf
2991,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,conclusion,Insight-tree,we evaluate mcr on 7 datasets for multi-hop qa that require both implicit and explicit reasoning in an open-domain setting and show that it outperforms previous approaches on all evaluation benchmarks.,,https://export.arxiv.org/pdf/2304.13007v2.pdf
2992,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,limitations,Insight-tree,in this work we introduce a meta-reasoner model to reason over multiple reasoning chains.,,https://export.arxiv.org/pdf/2304.13007v2.pdf
2993,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,limitations,Insight-tree,"while we opt for a prompted llm as our meta-reasoner, we do not experiment with a fine-tuned meta-reasoning model.",,https://export.arxiv.org/pdf/2304.13007v2.pdf
2994,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,limitations,Insight-tree,"for the meta-reasoner context, we experiment with variants which include either generated qa pairs of retrieved evidence sentences.",,https://export.arxiv.org/pdf/2304.13007v2.pdf
2995,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,limitations,Insight-tree,we leave further improvements to the meta-reasoner context as future work.,,https://export.arxiv.org/pdf/2304.13007v2.pdf
2996,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,limitations,Insight-tree,"due to the inference costs of current state-of-the-art llms we evaluate on the code-davinci-002 model, similar to prior work (trivedi et al., 2022a;wang et al., 2023).",,https://export.arxiv.org/pdf/2304.13007v2.pdf
2997,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,limitations,Insight-tree,"to further improve the reproducibility of our work we added results with an open-sourced llm (chiang et al., 2023) and retriever (khattab and zaharia, 2020 we provide technical details, additional examples, and the exact prompts used below.",,https://export.arxiv.org/pdf/2304.13007v2.pdf
2998,258236218,Why Does ChatGPT Fall Short in Answering Questions Faithfully?,conclusion,Insight-tree,our paper investigates the common failures of chat-gpt in complex open-domain question answering.,,https://export.arxiv.org/pdf/2304.10513v1.pdf
2999,258236218,Why Does ChatGPT Fall Short in Answering Questions Faithfully?,conclusion,Insight-tree,"we identify four types of errors: comprehension, factualness, specificity, and inference.",,https://export.arxiv.org/pdf/2304.10513v1.pdf
3000,258236218,Why Does ChatGPT Fall Short in Answering Questions Faithfully?,conclusion,Insight-tree,"we also examine the key abilities knowledge memorization, knowledge association, and knowledge reasoning, which are critical to these failures.",,https://export.arxiv.org/pdf/2304.10513v1.pdf
3001,258236218,Why Does ChatGPT Fall Short in Answering Questions Faithfully?,conclusion,Insight-tree,"additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on association, and the effect of decomposition on reasoning.",,https://export.arxiv.org/pdf/2304.10513v1.pdf
3002,258236218,Why Does ChatGPT Fall Short in Answering Questions Faithfully?,conclusion,Insight-tree,"finally, we suggest several techniques to help users more effectively use chat-gpt as a question-answering tool and enable system builders to develop better qa systems.",,https://export.arxiv.org/pdf/2304.10513v1.pdf
3003,258236218,Why Does ChatGPT Fall Short in Answering Questions Faithfully?,conclusion,Insight-tree,"our research contributes to the understanding of what influencing the faithfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more efficient and reliable language models.",,https://export.arxiv.org/pdf/2304.10513v1.pdf
3004,259089062,Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training,conclusion,Insight-tree,"in this work, we propose recontriever to further explore the potential of contrastive pre-training to reduce the demand of human-annotated data for dense retrievers.",,https://export.arxiv.org/pdf/2306.03166v1.pdf
3005,259089062,Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training,conclusion,Insight-tree,"benefiting from multiple positives from the same document as well as relevance-aware contrastive loss, our model achieves remarkable performance under zero-shot cases.",,https://export.arxiv.org/pdf/2306.03166v1.pdf
3006,259089062,Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training,conclusion,Insight-tree,additional results on low data resources further verify its value under various practical scenarios.,,https://export.arxiv.org/pdf/2306.03166v1.pdf
3007,254564450,Prompting Is Programming: A Query Language for Large Language Models Prompting Is Programming: A Query Language for Large Language Models,conclusion,Insight-tree,"in this work, we introduce the concept of language model programming, a novel way to interact with (large) language models.",,https://export.arxiv.org/pdf/2212.06094v3.pdf
3008,254564450,Prompting Is Programming: A Query Language for Large Language Models Prompting Is Programming: A Query Language for Large Language Models,conclusion,Insight-tree,"we presented lmql, a high-level query language, offering a concise and intuitive syntax.",,https://export.arxiv.org/pdf/2212.06094v3.pdf
3009,254564450,Prompting Is Programming: A Query Language for Large Language Models Prompting Is Programming: A Query Language for Large Language Models,conclusion,Insight-tree,"lmql implements purpose-designed evaluation semantics, which enable efficient query execution.",,https://export.arxiv.org/pdf/2212.06094v3.pdf
3010,254564450,Prompting Is Programming: A Query Language for Large Language Models Prompting Is Programming: A Query Language for Large Language Models,conclusion,Insight-tree,"we have substantiated this claim in a series of case studies, where we demonstrate that complex, state-of-the-art prompting techniques can be implemented as intuitive, concise and efficient lmql programs that reduce (compute) costs by up to 80%.",,https://export.arxiv.org/pdf/2212.06094v3.pdf
3011,227230675,Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking,conclusion,Insight-tree,the lit architecture is a simple yet powerful adaptation of the transformer architecture to learn better cross-document interactions for multi-hop ranking.,,https://arxiv.org/pdf/2012.14164v1.pdf
3012,227230675,Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking,conclusion,Insight-tree,the structure can be easily integrated with any transformer language model to enable cross-referencing of knowledge statements and improved ranking performance.,,https://arxiv.org/pdf/2012.14164v1.pdf
3013,227230675,Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking,conclusion,Insight-tree,"for example, lit can be a drop-in encoder for other multi-hop question answering datasets such as hotpotqa (yang et al., 2018).",,https://arxiv.org/pdf/2012.14164v1.pdf
3014,227230675,Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking,conclusion,Insight-tree,"when applied to the challenging worldtree v2 dataset, lit achieves competitive performance with current state-of-the-art models despite a smaller footprint.",,https://arxiv.org/pdf/2012.14164v1.pdf
3015,227230675,Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking,conclusion,Insight-tree,we envision that this architecture can be beneficial to many nlp tasks which require multi-hop reasoning over documents.,,https://arxiv.org/pdf/2012.14164v1.pdf
3016,234335834,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,conclusion and future work,Insight-tree,"in this paper, we focus on the task of logical reasoning of text.",,https://www.aclanthology.org/2022.findings-acl.127.pdf
3017,234335834,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,conclusion and future work,Insight-tree,"following a three-step logical reasoning paradigm, we first propose a neuro-symbolic logicdriven context extension framework.",,https://www.aclanthology.org/2022.findings-acl.127.pdf
3018,234335834,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,conclusion and future work,Insight-tree,"it identifies logical expressions as elementary units of logical inference and symbolically deduces the implicitly mentioned expressions, and verbalizes them as an extended context into a pre-trained model to match the answer.",,https://www.aclanthology.org/2022.findings-acl.127.pdf
3019,234335834,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,conclusion and future work,Insight-tree,"we also introduce a logic-driven data augmentation algorithm, which augments literally similar but logically different instances and employs contrastive learning to help our model better capture logical information.",,https://www.aclanthology.org/2022.findings-acl.127.pdf
3020,234335834,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,conclusion and future work,Insight-tree,"experimental results confirm the general effectiveness of our lreasoner, and it even surpasses human performance on the reclor dataset.",,https://www.aclanthology.org/2022.findings-acl.127.pdf
3021,234335834,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,conclusion and future work,Insight-tree,"in the future, we will explore to model different logical reasoning types and directly incorporate symbolic logic into the model structure.",,https://www.aclanthology.org/2022.findings-acl.127.pdf
3022,249062748,Generating Natural Language Proofs with Verifier-Guided Search,conclusion,Insight-tree,we have introduced nlproofs for stepwise proof generation in natural language.,,https://www.aclanthology.org/2022.emnlp-main.7.pdf
3023,249062748,Generating Natural Language Proofs with Verifier-Guided Search,conclusion,Insight-tree,it learns to generate relevant proof steps conditioning on the hypothesis.,,https://www.aclanthology.org/2022.emnlp-main.7.pdf
3024,249062748,Generating Natural Language Proofs with Verifier-Guided Search,conclusion,Insight-tree,"to prevent hallucination, nlproofs searches for proofs that maximize a validity score judged by a verifier.",,https://www.aclanthology.org/2022.emnlp-main.7.pdf
3025,249062748,Generating Natural Language Proofs with Verifier-Guided Search,conclusion,Insight-tree,"our method has achieved state-of-the-art performance on en-tailmentbank and ruletaker, demonstrating the promise of stepwise proof generation for human-authored proofs.",,https://www.aclanthology.org/2022.emnlp-main.7.pdf
3026,249062748,Generating Natural Language Proofs with Verifier-Guided Search,conclusion,Insight-tree,"in the future, we hope to see increasing applications of verifiers and proof search in various reasoning tasks.",,https://www.aclanthology.org/2022.emnlp-main.7.pdf
3027,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"in this survey, we provide the first comprehensive overview of the hallucination problem in nlg, summarizing existing evaluation metrics, mitigation methods, and the remaining challenges for future research.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3028,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,hallucination is an artifact of neural-based nlg and is of concern because they appear fluent and can therefore be misleading to users.,,https://export.arxiv.org/pdf/2202.03629v5.pdf
3029,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"in some scenarios and tasks, hallucination can cause harm.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3030,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"we survey various contributors to hallucination, ranging from noisy data, erroneous parametric knowledge, incorrect attention mechanism, inappropriate training strategy, to inference exposure bias, etc.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3031,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"we show that there are two categories of hallucinations, namely intrinsic hallucination and extrinsic hallucination, and they need to be treated differently with diverse mitigation strategies.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3032,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,hallucination is relatively easy to detect in abstractive summarization and in nmt against the evidence in the source.,,https://export.arxiv.org/pdf/2202.03629v5.pdf
3033,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"for dialogue systems, it is important to balance diversity vs consistency in dialogue responses.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3034,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"hallucination in gqa and vl tasks is detrimental to the performance, but research on mitigation methods is still very preliminary in these areas.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3035,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"for datato-text generation, hallucination arises from the discrepancy between the input and output format.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3036,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,most methods to mitigate hallucinations in nmt either aim to reduce dataset noise or alleviate exposure bias.,,https://export.arxiv.org/pdf/2202.03629v5.pdf
3037,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"in the vl domain, models also generate unfaithful output given the visual scene, and recent works have mainly focused on the object hallucination problem.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3038,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"there remain many challenges ahead in identifying and mitigating hallucinations in nlg, and we hope research in this area can benefit from this survey.",,https://export.arxiv.org/pdf/2202.03629v5.pdf
3039,207847382,Ask to Learn: A Study on Curiosity-driven Question Generation,conclusions,Insight-tree,asking inquisitive questions allows humans to learn from each other and increase their knowledge.,,https://www.aclweb.org/anthology/2020.coling-main.202.pdf
3040,207847382,Ask to Learn: A Study on Curiosity-driven Question Generation,conclusions,Insight-tree,"we thus proposed a new task: curiosity-driven question generation, which attempts to address such a key component for several human-machine interaction scenarios.",,https://www.aclweb.org/anthology/2020.coling-main.202.pdf
3041,207847382,Ask to Learn: A Study on Curiosity-driven Question Generation,conclusions,Insight-tree,"in absence of data directly usable for this task, we proposed an automatic method to derive it from conversational qa datasets.",,https://www.aclweb.org/anthology/2020.coling-main.202.pdf
3042,207847382,Ask to Learn: A Study on Curiosity-driven Question Generation,conclusions,Insight-tree,"further, recognizing that the great majority of qa datasets are not conversational, we also extended the method to standard qa data.",,https://www.aclweb.org/anthology/2020.coling-main.202.pdf
3043,207847382,Ask to Learn: A Study on Curiosity-driven Question Generation,conclusions,Insight-tree,"our experiments, which include learning strategies such as pretraining and reinforcement, show promising results under both automatic and human evaluation.",,https://www.aclweb.org/anthology/2020.coling-main.202.pdf
3044,207847382,Ask to Learn: A Study on Curiosity-driven Question Generation,conclusions,Insight-tree,"in future works, we plan to extend the approach to conditional generation of curiosity-driven questions.",,https://www.aclweb.org/anthology/2020.coling-main.202.pdf
3045,214802013,Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,conclusions,Insight-tree,in this paper we propose the problem of recovering reasoning chains in multi-hop qa from weak supervision signals.,,https://arxiv.org/pdf/2004.02393v1.pdf
3046,214802013,Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,conclusions,Insight-tree,our model adopts an cooperative game approach where a ranker and a reasoner cooperate to select the most confident chains.,,https://arxiv.org/pdf/2004.02393v1.pdf
3047,214802013,Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,conclusions,Insight-tree,experiments on the hotpotqa and medhop benchmarks show the effectiveness of the proposed approach.,,https://arxiv.org/pdf/2004.02393v1.pdf
3048,256461326,Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation,conclusions,Insight-tree,automatic question generation is an important task in the improvement of artificial intelligent systems.,,https://www.aclanthology.org/2022.emnlp-main.4.pdf
3049,256461326,Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation,conclusions,Insight-tree,"in this work, we propose a novel primal-dual approach for question generation.",,https://www.aclanthology.org/2022.emnlp-main.4.pdf
3050,256461326,Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation,conclusions,Insight-tree,it integrates question generation with its dual problem question answering into a unified framework.,,https://www.aclanthology.org/2022.emnlp-main.4.pdf
3051,256461326,Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation,conclusions,Insight-tree,a knowledge distillation module is introduced into the framework to improve model generalization on uncommon word generation.,,https://www.aclanthology.org/2022.emnlp-main.4.pdf
3052,256461326,Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation,conclusions,Insight-tree,experimental results on two benchmarks demonstrate the effectiveness of the primal-dual modeling.,,https://www.aclanthology.org/2022.emnlp-main.4.pdf
3053,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,"in this paper, we borrowed the formalization of the notion of diversity from the literature in ecology.",,
3054,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,"we focused on two out of the three main aspects of diversity, namely variety and balance.",,
3055,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,"our contribution is to apply these measures to assess intralinguistic diversity, focusing on the particular phenomenon of multiword expressions.",,
3056,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,we not only formalize variety and balance measures in this context but we also put forward methods for selecting those variants of these measure which fit the nature of the mwe phenomenon.,,
3057,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,this validation methodology is based on corpus sampling with variable sample size.,,
3058,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,"as a result, we retain richness and the e 2,1 evenness as the optimal variety and balance measures for mwes (among those studied by us).",,
3059,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,we apply these measures to the corpora and system results in the parseme shared task on automatic identification of mwes.,,
3060,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,the results show that richness of the correct annotations produced by the systems is roughly consistent with their f-measure performances.,,
3061,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,"however, their balance is much less correlated with more traditional measures.",,
3062,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,"we also display the limits of the richness and balance measures, when calculated on automatically annotated data, due to incorrect approximation of types under improper lemmatization in a morphologically rich language.",,
3063,246441887,Active Learning Over Multiple Domains in Natural Language Tasks,conclusion,Insight-tree,"we examine a challenging variant of active learning where target data is scarce, and multiple shifted domains operate as the source set of unlabeled data.",,https://arxiv.org/pdf/2202.00254v2.pdf
3064,246441887,Active Learning Over Multiple Domains in Natural Language Tasks,conclusion,Insight-tree,"for practitioners facing multi-domain active learning, we benchmark 18 acquisition functions, demonstrating the h-divergence family of methods and our proposed variant dal-e achieve the best results.",,https://arxiv.org/pdf/2202.00254v2.pdf
3065,246441887,Active Learning Over Multiple Domains in Natural Language Tasks,conclusion,Insight-tree,"our analysis shows the importance of example selection in existing methods, and also the surprising potential of domain budget allocation strategies.",,https://arxiv.org/pdf/2202.00254v2.pdf
3066,250334200,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,conclusion,Insight-tree,we proposed a simple yet effective approach for multi-task training of the fid retrieval-augmented generation model on the kilt benchmark.,,https://arxiv.org/pdf/2207.03030v1.pdf
3067,250334200,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,conclusion,Insight-tree,we cleaned (and downsampled were necessary) the training set by removing query-answer pairs with low relevance confidence.,,https://arxiv.org/pdf/2207.03030v1.pdf
3068,250334200,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,conclusion,Insight-tree,"we demonstrated that this approach substantially improves two imbalanced tasks, and has a smaller benefit on two of the remaining five tasks.",,https://arxiv.org/pdf/2207.03030v1.pdf
3069,250334200,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,conclusion,Insight-tree,by scaling the model capacity we achieve state-of-the-art results on five kilt tasks evaluated by the leaderboard.,,https://arxiv.org/pdf/2207.03030v1.pdf
3070,252873630,Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning,conclusion,Insight-tree,"in this work, we proposed a novel counterfactual reasoning approach to reduce the disconnected reasoning in multi-hop qa.",,https://export.arxiv.org/pdf/2210.07138v1.pdf
3071,252873630,Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning,conclusion,Insight-tree,"we used the causal graph to explain the existing multi-hop qa approaches' behaviors, which consists of the shortcut impacts and reasoning impacts.",,https://export.arxiv.org/pdf/2210.07138v1.pdf
3072,252873630,Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning,conclusion,Insight-tree,the shortcut impacts capture the disconnected reasoning.,,https://export.arxiv.org/pdf/2210.07138v1.pdf
3073,252873630,Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning,conclusion,Insight-tree,thus the disconnected reasoning is formulated as natural direct causal effect and we construct the counterfactual examples during the training phase to estimate the both natural direct effects of question and context on answer prediction as well as supporting facts identification.,,https://export.arxiv.org/pdf/2210.07138v1.pdf
3074,252873630,Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning,conclusion,Insight-tree,the reasoning impact represents the multi-hop reasoning and is estimated via introducing learnable parameters.,,https://export.arxiv.org/pdf/2210.07138v1.pdf
3075,139103297,Understanding Dataset Design Choices for Multi-hop Reasoning,discussion and conclusion,Insight-tree,"there exist several other multi-hop reasoning datasets including worldtree , openbookqa (mihaylov et al., 2018), and mul-tirc (khashabi et al., 2018).",,https://arxiv.org/pdf/1904.12106v1.pdf
3076,139103297,Understanding Dataset Design Choices for Multi-hop Reasoning,discussion and conclusion,Insight-tree,these datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content.,,https://arxiv.org/pdf/1904.12106v1.pdf
3077,139103297,Understanding Dataset Design Choices for Multi-hop Reasoning,discussion and conclusion,Insight-tree,we leave a detailed investigation of these for future work.,,https://arxiv.org/pdf/1904.12106v1.pdf
3078,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,conclusions,Insight-tree,this work aims to propose a sensible assessment framework for multiple choice question generation in order to encourage the question generation community to consider more appropriate methods of assessing and benchmarking developed systems to reflect the qualities of interest rather than arbitrary n-gram based approaches.,,https://export.arxiv.org/pdf/2209.11830v1.pdf
3079,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,conclusions,Insight-tree,"here, the first fully automated end-to-end multiple-choice question generation system is proposed for generating a question, the correct answer and distractor options for an input context without relying on explicit phrase extraction based techniques.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3080,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"here, the limitations of the current approaches are discussed.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3081,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"first, both measures of unanswerability and question complexity are model and corpusspecific.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3082,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"hence, it is not clear about the applicability of these metrics beyond the race++ dataset.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3083,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"specifically, the unanswerability measure runs the risk of conflating answerability with the failure of a reading comprehension question.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3084,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"the validity of such metrics is centred on in-domain data, which may be diminishingly effective at discriminating the performance of generative models on shifted data (e.g the nature of questions in reclor is more logical-based and requires higher inference than questions in race++, which might make measures trained on race++ struggle at assessment on questions generated in the reclor style).",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3085,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,no human evaluation is performed of whether the assessment metrics correlate explicitly with human notions of answerability and complexity.,,https://export.arxiv.org/pdf/2209.11830v1.pdf
3086,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"hence, further work should invest resources to comprehensively establish the validity of the proposed measures.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3087,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"finally, the question complexity system is trained specifically on the meaning of complexity as described for race++.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3088,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"however, question complexity has several definitions and hence further work should establish whether this interpretation of complexity may align with other views of complexity. .",,https://export.arxiv.org/pdf/2209.11830v1.pdf
3089,252819353,Question Generation Based on Grammar Knowledge and Fine-grained Classification,conclusion,Insight-tree,"to solve the problem of mismatch between question types and answers in question generation, this paper constructs a question type classifier and a question generator.",,https://www.aclanthology.org/2022.coling-1.562.pdf
3090,252819353,Question Generation Based on Grammar Knowledge and Fine-grained Classification,conclusion,Insight-tree,"we classify questions into finegrained classification and integrate grammar knowledge into question type classifier to improve the accuracy of question types, then, the prediction results of the classifier are fused into the question generator to improve the performance of question generation.",,https://www.aclanthology.org/2022.coling-1.562.pdf
3091,252819353,Question Generation Based on Grammar Knowledge and Fine-grained Classification,conclusion,Insight-tree,"to verify the effectiveness of our model, we perform an upper bound analysis on it, we integrate grammar knowledge into the question generator to provide accurate question types to guide the model to generate questions.",,https://www.aclanthology.org/2022.coling-1.562.pdf
3092,252819353,Question Generation Based on Grammar Knowledge and Fine-grained Classification,conclusion,Insight-tree,"the final experimental results show that the method proposed in this paper not only improves the accuracy of question words in the generated question, but also improves the quality of the generated question.",,https://www.aclanthology.org/2022.coling-1.562.pdf
3093,258987734,BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language,conclusions,Insight-tree,information retrieval in the polish language is still developing and is an intensive research area.,,https://export.arxiv.org/pdf/2305.19840v1.pdf
3094,258987734,BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language,conclusions,Insight-tree,"therefore, there is a great need for resources to enable further training and more accurate evaluation of existing and new deep neural ir models.",,https://export.arxiv.org/pdf/2305.19840v1.pdf
3095,258987734,BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language,conclusions,Insight-tree,"in this work, we introduced the translated beir-pl benchmark and showed the results of a broad family of ir baseline models.",,https://export.arxiv.org/pdf/2305.19840v1.pdf
3096,258987734,BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language,conclusions,Insight-tree,we would like to encourage other researchers to participate in further development of polish and multilingual ir models using our new resource.,,https://export.arxiv.org/pdf/2305.19840v1.pdf
3097,258865346,On Degrees of Freedom in Defining and Testing Natural Language Understanding,conclusions,Insight-tree,the prevalence of exaggerated claims about the achievements of foundation models motivates us to reconsider how we define and evaluate nlu.,,https://export.arxiv.org/pdf/2305.15130v1.pdf
3098,258865346,On Degrees of Freedom in Defining and Testing Natural Language Understanding,conclusions,Insight-tree,"our formulation of nlu using the response-dependent interpretation mitigates the issues of the turing and octopus tests; it stipulates that observers and target conditions, including tasks and abilities, must be specified.",,https://export.arxiv.org/pdf/2305.15130v1.pdf
3099,258865346,On Degrees of Freedom in Defining and Testing Natural Language Understanding,conclusions,Insight-tree,"however, current practices for creating nlu datasets are yet to be aligned, which provides researchers with the freedom to choose convenient strategies.",,https://export.arxiv.org/pdf/2305.15130v1.pdf
3100,258865346,On Degrees of Freedom in Defining and Testing Natural Language Understanding,conclusions,Insight-tree,"to organize essential practices using a standard guideline, we introduce the validity ar-gument, which guides stakeholders to collect and interpret evidence for validating that the test subject executes its intended behavior.",,https://export.arxiv.org/pdf/2305.15130v1.pdf
3101,258865346,On Degrees of Freedom in Defining and Testing Natural Language Understanding,conclusions,Insight-tree,"our proposed checklist helps researchers find relevant practices for benchmarking nlu, but we continually revise it by investigating potential refutation to promote more credible nlu studies.",,https://export.arxiv.org/pdf/2305.15130v1.pdf
3102,231709861,Benchmarking Machine Reading Comprehension: A Psychological Perspective,conclusion,Insight-tree,"in this paper, we outlined current issues and future directions for benchmarking machine reading comprehension.",,https://www.aclweb.org/anthology/2021.eacl-main.137.pdf
3103,231709861,Benchmarking Machine Reading Comprehension: A Psychological Perspective,conclusion,Insight-tree,we visited the psychology study to analyze what we should ask of reading comprehension and the construct validity in psychometrics to analyze how we should correctly evaluate it.,,https://www.aclweb.org/anthology/2021.eacl-main.137.pdf
3104,231709861,Benchmarking Machine Reading Comprehension: A Psychological Perspective,conclusion,Insight-tree,we deduced that future datasets should evaluate the capability of the situation model for understanding context-dependent situations and for grounding to non-textual information and ensure the substantive validity by creating shortcut-proof questions and designing an explanatory task formulation.,,https://www.aclweb.org/anthology/2021.eacl-main.137.pdf
3105,255372929,Inflected Forms Are Redundant in Question Generation Models,conclusion,Insight-tree,"in this paper, we discover two major issues in the existing neural qg models.",,https://export.arxiv.org/pdf/2301.00397v1.pdf
3106,255372929,Inflected Forms Are Redundant in Question Generation Models,conclusion,Insight-tree,"to tackle the two issues, we propose this enhancing approach for qg and apply the approach to two typical sequenceto-sequence models, i.e., the pointer generator network and unilm.",,https://export.arxiv.org/pdf/2301.00397v1.pdf
3107,255372929,Inflected Forms Are Redundant in Question Generation Models,conclusion,Insight-tree,we further conduct extensive experiments using squad and marco datasets.,,https://export.arxiv.org/pdf/2301.00397v1.pdf
3108,255372929,Inflected Forms Are Redundant in Question Generation Models,conclusion,Insight-tree,the experimental results show that improved versions of models can significantly enhance the quality of qg and speed up the decoding.,,https://export.arxiv.org/pdf/2301.00397v1.pdf
3109,256105432,Information Retrieval: Recent Advances and Beyond,conclusions,Insight-tree,"this survey provided a comprehensive overview of the state-of-the-art for semantic retrieval models, in the context of information retrieval.",,https://export.arxiv.org/pdf/2301.08801v1.pdf
3110,256105432,Information Retrieval: Recent Advances and Beyond,conclusions,Insight-tree,"we covered a wide range of topics, from the early semantic retrieval methods, to the most recent neural semantic retrieval methods, discussing the connections between them.",,https://export.arxiv.org/pdf/2301.08801v1.pdf
3111,256105432,Information Retrieval: Recent Advances and Beyond,conclusions,Insight-tree,"in terms of structure, our focus was on the key ir topics: first-stage and second-stage retrieval, neural semantic retrieval model learning.",,https://export.arxiv.org/pdf/2301.08801v1.pdf
3112,256105432,Information Retrieval: Recent Advances and Beyond,conclusions,Insight-tree,"additionally, the survey highlights the major difficulties and challenges in the field, and points for promising directions for future research.",,https://export.arxiv.org/pdf/2301.08801v1.pdf
3113,256105432,Information Retrieval: Recent Advances and Beyond,conclusions,Insight-tree,"overall, this survey is expected to be useful for researchers interested in this challenging topic, providing inspiration for new ideas and further developments.",,https://export.arxiv.org/pdf/2301.08801v1.pdf
3114,249926985,Questions Are All You Need to Train a Dense Passage Retriever,conclusions and future work,Insight-tree,"we introduced art, a novel approach to train a dense passage retriever using only questions.",,https://export.arxiv.org/pdf/2206.10658v4.pdf
3115,249926985,Questions Are All You Need to Train a Dense Passage Retriever,conclusions and future work,Insight-tree,art does not require question-passage pairs or hard-negative examples for training and yet achieves state-of-the-art results.,,https://export.arxiv.org/pdf/2206.10658v4.pdf
3116,249926985,Questions Are All You Need to Train a Dense Passage Retriever,conclusions and future work,Insight-tree,"the key to making art work is to optimize the retriever to select relevant passages such that conditioning on them, the question generation likelihood computed using a large pre-trained language model iteratively improves.",,https://export.arxiv.org/pdf/2206.10658v4.pdf
3117,249926985,Questions Are All You Need to Train a Dense Passage Retriever,conclusions and future work,Insight-tree,"despite requiring much less supervision, art substantially outperforms dpr when evaluated on multiple qa datasets and also generalizes better on out-of-distribution questions.",,https://export.arxiv.org/pdf/2206.10658v4.pdf
3118,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,"in this paper, we propose an effective method called berm to improve the generalization ability of dense retrieval without target domain data and additional modules.",,https://export.arxiv.org/pdf/2305.11052v1.pdf
3119,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,"the basic idea of berm is learning the domain-invariant feature, that is, matching signal.",,https://export.arxiv.org/pdf/2305.11052v1.pdf
3120,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,"to achieve it, we introduce a novel concept of dense retrieval to represent the matching information between two texts, the matching representation.",,https://export.arxiv.org/pdf/2305.11052v1.pdf
3121,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,"further, we propose two requirements for matching and text representations as the constraint in the training of dense retrieval to enhance the ability to extract essential matching information from the passage according to different queries under the premise of balanced expression of the text.",,https://export.arxiv.org/pdf/2305.11052v1.pdf
3122,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,the two requirements unlock the ability of dense retrieval to capture matching signal without additional interaction.,,https://export.arxiv.org/pdf/2305.11052v1.pdf
3123,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,experimental results show that berm is a flexible method that can be combined with different dense retrieval training methods without inference overhead to improve the out-of-domain generalization ability.,,https://export.arxiv.org/pdf/2305.11052v1.pdf
3124,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,"in domain adaptation setting, our method is also effective and performs better than baselines.",,https://export.arxiv.org/pdf/2305.11052v1.pdf
3125,231698419,WebSRC: A Dataset for Web-Based Structural Reading Comprehension,conclusion,Insight-tree,"in this paper, we introduce websrc, a multi-modal dataset for web-based structural reading comprehension with both html documents and screenshots.",,https://www.aclanthology.org/2021.emnlp-main.343.pdf
3126,231698419,WebSRC: A Dataset for Web-Based Structural Reading Comprehension,conclusion,Insight-tree,the task is to answer questions about the web pages.,,https://www.aclanthology.org/2021.emnlp-main.343.pdf
3127,231698419,WebSRC: A Dataset for Web-Based Structural Reading Comprehension,conclusion,Insight-tree,"we evaluate several baselines on our dataset, and the results showed that incorporating layout features with textual contents is crucial to web understanding, but how to utilize such structural information requires further investigation.",,https://www.aclanthology.org/2021.emnlp-main.343.pdf
3128,231698419,WebSRC: A Dataset for Web-Based Structural Reading Comprehension,conclusion,Insight-tree,we hope this work can push the research on web-based structural reading comprehension forward.,,https://www.aclanthology.org/2021.emnlp-main.343.pdf
3129,231698419,WebSRC: A Dataset for Web-Based Structural Reading Comprehension,conclusion,Insight-tree,"in the future, we will go beyond web pages to explore more structural reading comprehension tasks.",,https://www.aclanthology.org/2021.emnlp-main.343.pdf
3130,253098398,Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning,conclusion,Insight-tree,"in this work, we explore the hypothesis that the adaptations of different delta tuning methods could all be re-parameterized as low-dimensional optimizations in a unified optimization subspace.",,https://export.arxiv.org/pdf/2210.13311v1.pdf
3131,253098398,Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning,conclusion,Insight-tree,the empirical results provide strong evidence for our hypothesis.,,https://export.arxiv.org/pdf/2210.13311v1.pdf
3132,253098398,Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning,conclusion,Insight-tree,we also extend our analysis to find the connection between fine-tuning and delta tuning.,,https://export.arxiv.org/pdf/2210.13311v1.pdf
3133,258822888,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,conclusion,Insight-tree,"in this paper, we explore methods for augmenting large language models (llms) with knowledge, enabling them to accurately respond to user inquiries in industrial domain-specific scenarios.",,https://export.arxiv.org/pdf/2305.11541v2.pdf
3134,258822888,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,conclusion,Insight-tree,"due to the limited availability of relevant benchmarks, we first construct a cloud domain questionanswering dataset, namely msqa.",,https://export.arxiv.org/pdf/2305.11541v2.pdf
3135,258822888,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,conclusion,Insight-tree,"subsequently, we propose a novel model interaction paradigm that empowers the llm with generated domainspecific knowledge.",,https://export.arxiv.org/pdf/2305.11541v2.pdf
3136,258822888,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,conclusion,Insight-tree,"to validate the effectiveness of our approach, we introduce new metrics for evaluating long-length answers in domain-specific qa tasks.",,https://export.arxiv.org/pdf/2305.11541v2.pdf
3137,258822888,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,conclusion,Insight-tree,"extensive experiments demonstrate that msqa dataset presents a significant challenge for existing large models, and our proposed model interaction paradigm is effective.",,https://export.arxiv.org/pdf/2305.11541v2.pdf
3138,233296924,Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation,discussion and conclusion,Insight-tree,"in this work, we develop a synthetic adversarial data generation pipeline for qa, identify the best components, and evaluate on a variety of robustness measures.",,https://www.aclanthology.org/2021.emnlp-main.696.pdf
3139,233296924,Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation,discussion and conclusion,Insight-tree,"we propose novel approaches for answer candidate selection, adversarial question generation, and synthetic example filtering and relabelling, demonstrating improvements over existing methods.",,https://www.aclanthology.org/2021.emnlp-main.696.pdf
3140,233296924,Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation,discussion and conclusion,Insight-tree,"furthermore, we evaluate the final models on three existing robustness measures and achieve state-of-the-art results on adversarialqa, improved learnability of various comprehension skills for checklist, and improved domain generalisation for the suite of mrqa tasks.",,https://www.aclanthology.org/2021.emnlp-main.696.pdf
3141,235097509,Breadth First Reasoning Graph for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, we proposed a novel gnn model of bfr-graph.",,https://www.aclweb.org/anthology/2021.naacl-main.464.pdf
3142,235097509,Breadth First Reasoning Graph for Multi-hop Question Answering,conclusion,Insight-tree,"specifically, the reasoning message starts from the question node and passes to the next sentences node hop by hop until all the edges have been passed.",,https://www.aclweb.org/anthology/2021.naacl-main.464.pdf
3143,235097509,Breadth First Reasoning Graph for Multi-hop Question Answering,conclusion,Insight-tree,we also construct the reasoning graph as a weighted graph and present a more interpretable way to aggregate scores of different levels from gnn.,,https://www.aclweb.org/anthology/2021.naacl-main.464.pdf
3144,235097509,Breadth First Reasoning Graph for Multi-hop Question Answering,conclusion,Insight-tree,"on hotpotqa leaderboard, bfr-graph achieved state-of-the-art on answer span prediction.",,https://www.aclweb.org/anthology/2021.naacl-main.464.pdf
3145,258564753,DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation,conclusion,Insight-tree,"in this paper we proposed a novel qa domain adaptation framework called domaininv, an unsupervised algorithm which does not require the use of labeled target domain, neither it depends on the synthetic data or pseudo labeled target domain.",,https://export.arxiv.org/pdf/2305.05589v1.pdf
3146,258564753,DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation,conclusion,Insight-tree,domaininv uses 1) domain invariant fine tuning which fine tunes the qa model using the target style on the source domain and 2) adversarial label correction which identifies the target distributions which are still far apart from source domain and optimize the feature generator to bring them closer near to source support class wisely.,,https://export.arxiv.org/pdf/2305.05589v1.pdf
3147,258564753,DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation,conclusion,Insight-tree,evaluation of domaininv showed that it outperforms all the baselines and achieves the superior performance establishing the new benchmark on qa domain adaptation.,,https://export.arxiv.org/pdf/2305.05589v1.pdf
3148,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,question answering (qa) software has been widely used in our daily life.,,
3149,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,"in this paper, we propose a novel recursive metamorphic testing method qaasker",,
3150,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,#NAME?,,
3151,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,qaasker + tests qa software by checking its behaviors on multiple recursively asked questions that are relevant to the same or some further enriched knowledge.,,
3152,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,"it cuts off the reliance on the preannotated labels of test cases, thus enables both the flexible just-in-time test during usage and the extensible test with massive unlabeled data for qa software, which cannot be supported by the current reference-based test paradigm.",,
3153,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,"we evaluate the effectiveness of qaasker + by using it to test four representative state-of-the-art qa software that covers two mainstream types of qa software, as well as a popular reallife qa application, the google search service.",,
3154,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,comprehensive results demonstrate that qaasker + can reveal quantities of valid violations that depict diverse answering issues for various kinds of mainstream qa software.,,
3155,257768705,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,"besides, we also found that our recursive mrs have a better fault detection effectiveness than two representative non-recursive mrs and can even help to fix the revealed issues.",,
3156,236478213,Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations,conclusion,Insight-tree,we conducted user studies to understand whether explanations from a state-of-the-art open-domain qa system help improve error-detectability for endusers.,,https://www.aclanthology.org/2021.findings-acl.95.pdf
3157,236478213,Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations,conclusion,Insight-tree,"our study showed that for odqa, simple explanations based on evidence snippets can significantly improve error-detectability and beat strong baselines such as communicating model's confidence.",,https://www.aclanthology.org/2021.findings-acl.95.pdf
3158,236478213,Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations,conclusion,Insight-tree,we observed this for multiple modalities of interaction: spoken and visual modalities.,,https://www.aclanthology.org/2021.findings-acl.95.pdf
3159,236478213,Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations,conclusion,Insight-tree,"however, results also indicated that not every explanation type is guaranteed to improve performance over confidence and the best explanation strategy may change with the modality, e.g., due to differences in users' cognitive abilities across modalities.",,https://www.aclanthology.org/2021.findings-acl.95.pdf
3160,236478213,Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations,conclusion,Insight-tree,"thus, developers and researchers of explainable odqa systems should not take the effectiveness of explanations for granted and should evaluate and tune them on the tasks and modalities where these models will be eventually deployed.",,https://www.aclanthology.org/2021.findings-acl.95.pdf
3161,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,"in this paper, we deal with the multi-choice question answering task which requires background knowledge or commonsense.",,https://www.aclweb.org/anthology/2020.coling-main.232.pdf
3162,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,"we propose a novel question-answering method by exploring how to efficiently integrate multiple knowledge sources, i.e. conceptnet, wikipedia and the cambridge dictionary.",,https://www.aclweb.org/anthology/2020.coling-main.232.pdf
3163,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,"firstly, we propose a novel graph-based iterative knowledge retrieval module to iteratively retrieve concepts and entities related to a given question and its choices.",,https://www.aclweb.org/anthology/2020.coling-main.232.pdf
3164,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,"in addition, we propose an answer choice-aware attention mechanism to fuse all hidden representations encoded by a pre-trained language model.",,https://www.aclweb.org/anthology/2020.coling-main.232.pdf
3165,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,we conducted experiments on the commonsenseqa dataset and the experimental results show that our method significantly outperforms other competitive methods in accuracy.,,https://www.aclweb.org/anthology/2020.coling-main.232.pdf
3166,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,further ablation studies show the effectiveness of graph-based iterative knowledge retrieval module and answer choice-aware attention module in retrieving and synthesizing background knowledge from multiple knowledge sources.,,https://www.aclweb.org/anthology/2020.coling-main.232.pdf
3167,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,"in the future, we will extend our method to deal with the open-domain question answering tasks that require the external background knowledge.",,https://www.aclweb.org/anthology/2020.coling-main.232.pdf
3168,252907685,MTEB: Massive Text Embedding Benchmark,conclusion,Insight-tree,"in this work, we presented the massive text embedding benchmark (mteb).",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3169,252907685,MTEB: Massive Text Embedding Benchmark,conclusion,Insight-tree,"consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, mteb aims to provide reliable embedding performance estimates.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3170,252907685,MTEB: Massive Text Embedding Benchmark,conclusion,Insight-tree,"by opensourcing mteb alongside a leaderboard, we provide a foundation for further pushing the state-ofthe-art of available text embeddings.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3171,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"while mteb aims to be a diverse benchmark to provide holistic performance reviews, the benchmark has its limitations.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3172,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,we list them here:1.,,https://www.aclanthology.org/2023.eacl-main.148.pdf
3173,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"long document datasets mteb covers multiple text lengths (s2s, p2p, s2p), but very long documents are still missing.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3174,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"the longest datasets in mteb have a few hundred words, and longer text sizes could be relevant for use cases like retrieval.2. task imbalance tasks in mteb have a different amount of datasets with summarization consisting of only a single dataset.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3175,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"this means mteb average scores, which are computed over all datasets, are biased towards tasks with many datasets, notably retrieval, classification and clustering.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3176,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"as mteb grows, we hope to add more datasets to currently underrepresented tasks like summarization or pair classification.3.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3177,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"multinguality mteb contains multilingual classification, sts and bitext mining datasets.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3178,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"however, retrieval and clustering are english-only.",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3179,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,sgpt-bloom-7b1-msmarco is geared towards multilingual retrieval datasets and due to the lack thereof cannot be comprehensively benchmarked in mteb.,,https://www.aclanthology.org/2023.eacl-main.148.pdf
3180,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"further, mteb does not contain any code datasets that could be used to benchmark code models (neelakantan et al., 2022;allal et al., 2023).",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3181,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"it should be easy to extend mteb with datasets, such as codesearchnet",,https://www.aclanthology.org/2023.eacl-main.148.pdf
3182,244909409,Hybrid Autoregressive Inference for Scalable Multi-hop Explanation Regeneration,conclusion,Insight-tree,"this work presented scar, a hybrid autoregressive architecture for scalable explanation regeneration.",,https://arxiv.org/pdf/2107.11879v2.pdf
3183,244909409,Hybrid Autoregressive Inference for Scalable Multi-hop Explanation Regeneration,conclusion,Insight-tree,"an extensive evaluation demonstrated that scar achieves performance comparable with that of state-of-the-art crossencoders while being  50 times faster and intrinsically scalable, and confirmed the impact of the hybridisation on semantic drift and question answering.",,https://arxiv.org/pdf/2107.11879v2.pdf
3184,244909409,Hybrid Autoregressive Inference for Scalable Multi-hop Explanation Regeneration,conclusion,Insight-tree,"this work demonstrated the effectiveness of hybrid architectures for explainable inference at scale, opening the way for future research at the intersection of latent and explicit models.",,https://arxiv.org/pdf/2107.11879v2.pdf
3185,244909409,Hybrid Autoregressive Inference for Scalable Multi-hop Explanation Regeneration,conclusion,Insight-tree,"as a future work, we plan to investigate the integration of relevance and explanatory power in an end-to-end differentiable architecture, and explore the applicability of the hybrid framework on additional natural language and scientific reasoning tasks, with a focus on real-world scientific inference problems.",,https://arxiv.org/pdf/2107.11879v2.pdf
3186,259096157,"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",conclusion,Insight-tree,"we revisit ood robustness research in nlp, identifying deficiencies in benchmarks and evaluation.",,https://export.arxiv.org/pdf/2306.04618v1.pdf
3187,259096157,"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",conclusion,Insight-tree,"correspondingly, a benchmark construction protocol and an ood robustness evaluation suite are proposed to facilitate future research.",,https://export.arxiv.org/pdf/2306.04618v1.pdf
3188,259096157,"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",conclusion,Insight-tree,"the correlation between ood and id performance, the effectiveness of existing methods, and the challenges faced by llms are investigated.",,https://export.arxiv.org/pdf/2306.04618v1.pdf
3189,259096157,"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",conclusion,Insight-tree,"in the future, we will focus on motivating effective approaches to improve the performance of llms in downstream tasks, particularly in narrowing the gap with fine-tuned domain-specific models on id datasets.",,https://export.arxiv.org/pdf/2306.04618v1.pdf
3190,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"in this work, we present an empirical investigation on the relationships between (1) a task's format, (2) the number of labeled examples available for said task, and (3) the number of parameters the model tackling the task has.",,https://arxiv.org/pdf/2110.04374v1.pdf
3191,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"through our extensive experiments, we determine that task format greatly affects the relative performance improvement that can be expected from increased training set size and parameter count.",,https://arxiv.org/pdf/2110.04374v1.pdf
3192,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,for tasks that do not require the recollection of specific external information -i.e.,,https://arxiv.org/pdf/2110.04374v1.pdf
3193,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"classification, multiple choice, and extractive qa -we find that more labeled data and larger models both reliably improve performance.",,https://arxiv.org/pdf/2110.04374v1.pdf
3194,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"in fact, for some of these tasks, adding a few hundred labeled examples is more beneficial than scaling up the model size by billions of parameters.",,https://arxiv.org/pdf/2110.04374v1.pdf
3195,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"it seems then, from a practitioner's perspective, that for many tasks where data is very sparse, the tried-and-true strategy of simply collecting more training data will often be a more effective strategy than attempting to scale to larger, more computationally-demanding models.",,https://arxiv.org/pdf/2110.04374v1.pdf
3196,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"however, the picture is very different for open qa tasks; for such tasks, we find that increasing the size of the training data barely improves performance, leaving parameter inflation as the only reliable approach to improve accuracy.",,https://arxiv.org/pdf/2110.04374v1.pdf
3197,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"finally, we provide a hypothesis to explain these results and conclude with a practical corollary -when possible, changing the format from open qa into a more ""self-contained"" one will allow labeled data to bridge performance gaps between moderately-sized models and much larger ones.  ",,https://arxiv.org/pdf/2110.04374v1.pdf
3198,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"table 2 provides the results from our main experiment (section 3, figure 2) in tabular form.",,https://arxiv.org/pdf/2110.04374v1.pdf
3199,221507798,KILT: a Benchmark for Knowledge Intensive Language Tasks,conclusion,Insight-tree,"we introduce kilt, a benchmark for assessing models that need to condition on specific knowledge in a defined snapshot of wikipedia to solve tasks spanning five domains.",,https://www.aclweb.org/anthology/2021.naacl-main.200.pdf
3200,221507798,KILT: a Benchmark for Knowledge Intensive Language Tasks,conclusion,Insight-tree,the goal is to catalyze and facilitate research towards general and explainable models equipped with task-agnostic representations of knowledge.,,https://www.aclweb.org/anthology/2021.naacl-main.200.pdf
3201,221507798,KILT: a Benchmark for Knowledge Intensive Language Tasks,conclusion,Insight-tree,"our experiments show promising results for a general solution combining dense retrieval and seq2seq generations, although there is large room for improvements.",,https://www.aclweb.org/anthology/2021.naacl-main.200.pdf
3202,221507798,KILT: a Benchmark for Knowledge Intensive Language Tasks,conclusion,Insight-tree,"in particular, we find that provenance of current models is generally low.",,https://www.aclweb.org/anthology/2021.naacl-main.200.pdf
3203,233189637,NLQuAD: A Non-Factoid Long Question Answering Data Set,conclusion,Insight-tree,"we introduce nlquad, a non-factoid long question answering data set from bbc news articles.",,https://www.aclweb.org/anthology/2021.eacl-main.106.pdf
3204,233189637,NLQuAD: A Non-Factoid Long Question Answering Data Set,conclusion,Insight-tree,"nlquad's question types and the long lengths of its context documents as well as answers, make it a challenging real-world task.",,https://www.aclweb.org/anthology/2021.eacl-main.106.pdf
3205,233189637,NLQuAD: A Non-Factoid Long Question Answering Data Set,conclusion,Insight-tree,we propose to use intersection over union (iou) as an evaluation metric for long question answering.,,https://www.aclweb.org/anthology/2021.eacl-main.106.pdf
3206,233189637,NLQuAD: A Non-Factoid Long Question Answering Data Set,conclusion,Insight-tree,"to establish a baseline performance, we experimented with the bert, roberta, and longformer question answering models.",,https://www.aclweb.org/anthology/2021.eacl-main.106.pdf
3207,233189637,NLQuAD: A Non-Factoid Long Question Answering Data Set,conclusion,Insight-tree,"longformer outperforms the other methods with an iou of 73.57%, but the results show that the performance of state-of-the-art question answering systems is far from perfect.",,https://www.aclweb.org/anthology/2021.eacl-main.106.pdf
3208,233189637,NLQuAD: A Non-Factoid Long Question Answering Data Set,conclusion,Insight-tree,we hope nlquad will inspire more research in the area of document-level language understanding and question answering.,,https://www.aclweb.org/anthology/2021.eacl-main.106.pdf
3209,254564418,Momentum Contrastive Pre-training for Question Answering,conclusion,Insight-tree,this paper presents a novel pre-training method mcross for extractive qa which contains two tasks: 1) contrastive learning and 2) answer term prediction.,,https://www.aclanthology.org/2022.emnlp-main.291.pdf
3210,254564418,Momentum Contrastive Pre-training for Question Answering,conclusion,Insight-tree,"specifically, mcross adapts moco frameworks to maintain consistency in answering cloze-like and natural questions, enabling pretrained models to have a more comprehensive understanding of supporting passages.",,https://www.aclanthology.org/2022.emnlp-main.291.pdf
3211,254564418,Momentum Contrastive Pre-training for Question Answering,conclusion,Insight-tree,the empirical experiments on three public datasets demonstrate that our approach can obtain noticeable improvements in extractive qa tasks in supervised and zero-shot scenarios.,,https://www.aclanthology.org/2022.emnlp-main.291.pdf
3212,254564418,Momentum Contrastive Pre-training for Question Answering,limitations,Insight-tree,"although mcross can already obtain satisfactory qa performance, due to limited time and computational resources, we only use 5 million cloze-like samples for pre-training, which is one-twentieth of the scale of original sspt experiments.",,https://www.aclanthology.org/2022.emnlp-main.291.pdf
3213,233219849,"MULTIMODALQA: COMPLEX QUESTION ANSWERING OVER TEXT, TABLES AND IMAGES",conclusion,Insight-tree,"we present mmqa, a new qa dataset that contains 29,918 examples, 35.7% of which require crossmodality reasoning.",,https://arxiv.org/pdf/2104.06039v1.pdf
3214,233219849,"MULTIMODALQA: COMPLEX QUESTION ANSWERING OVER TEXT, TABLES AND IMAGES",conclusion,Insight-tree,"we describe a novel framework for generating complex multimodal questions at scale, and showcase the diversity and multimodal properties of the resulting dataset.",,https://arxiv.org/pdf/2104.06039v1.pdf
3215,233219849,"MULTIMODALQA: COMPLEX QUESTION ANSWERING OVER TEXT, TABLES AND IMAGES",conclusion,Insight-tree,"we evaluate mmqa using a variety of models, and confirm that the best model exploits the multimodality of the dataset and takes into account multi-hop reasoning via implicit decomposition.",,https://arxiv.org/pdf/2104.06039v1.pdf
3216,233219849,"MULTIMODALQA: COMPLEX QUESTION ANSWERING OVER TEXT, TABLES AND IMAGES",conclusion,Insight-tree,"however, human performance substantially exceeds the best model, establishing the need for further research involving multiple modalities in question answering systems, which we hope that our work will drive.",,https://arxiv.org/pdf/2104.06039v1.pdf
3217,247447562,Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering,conclusion,Insight-tree,"this paper proposes hyperlink-induced pretraining (hlp), a pre-training method for openqa passage retrieval by leveraging the online textual relevance induced by hyperlink-based topology.",,https://www.aclanthology.org/2022.acl-long.493.pdf
3218,247447562,Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering,conclusion,Insight-tree,"our experiments show that hlp gains significant improvements across multiple qa datasets under different scenarios, consistently outperforming other pre-training methods.",,https://www.aclanthology.org/2022.acl-long.493.pdf
3219,247447562,Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering,conclusion,Insight-tree,our method provides insights into openqa passage retrieval by analyzing the underlying bi-text relevance.,,https://www.aclanthology.org/2022.acl-long.493.pdf
3220,247447562,Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering,conclusion,Insight-tree,future work involves addressing tasks like ms marco where the granularity of the information-seeking target is at the passage level.,,https://www.aclanthology.org/2022.acl-long.493.pdf
3221,256358906,KN O WDA: ALL-IN-ONE KNOWLEDGE MIXTURE MODEL FOR DATA AUGMENTATION IN LOW- RESOURCE NLP TASKS,conclusion and future work,Insight-tree,this paper explores multi-task learning paradigms at a massive scale for data augmentation in low-resource nlp tasks for the first time.,,https://export.arxiv.org/pdf/2206.10265v2.pdf
3222,256358906,KN O WDA: ALL-IN-ONE KNOWLEDGE MIXTURE MODEL FOR DATA AUGMENTATION IN LOW- RESOURCE NLP TASKS,conclusion and future work,Insight-tree,"we demonstrate that the proposed knowledge mixture training enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences).",,https://export.arxiv.org/pdf/2206.10265v2.pdf
3223,256358906,KN O WDA: ALL-IN-ONE KNOWLEDGE MIXTURE MODEL FOR DATA AUGMENTATION IN LOW- RESOURCE NLP TASKS,conclusion and future work,Insight-tree,"experiments verified the effectiveness of our knowda, and knowda outperforms state-of-the-art data augmentation approaches on the popular benchmarks fewglue, conll'03, and wikiann.",,https://export.arxiv.org/pdf/2206.10265v2.pdf
3224,256358906,KN O WDA: ALL-IN-ONE KNOWLEDGE MIXTURE MODEL FOR DATA AUGMENTATION IN LOW- RESOURCE NLP TASKS,conclusion and future work,Insight-tree,we also perform ablation studies indicating the importance of including demonstrations and the impact of different keys.,,https://export.arxiv.org/pdf/2206.10265v2.pdf
3225,256358906,KN O WDA: ALL-IN-ONE KNOWLEDGE MIXTURE MODEL FOR DATA AUGMENTATION IN LOW- RESOURCE NLP TASKS,conclusion and future work,Insight-tree,"moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration.",,https://export.arxiv.org/pdf/2206.10265v2.pdf
3226,248562985,KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering,conclusion,Insight-tree,"to bridge the gap between the pre-training and finetuning objectives, kecp views eqa as an answer generation task.",,https://www.aclanthology.org/2022.emnlp-main.206.pdf
3227,248562985,KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering,conclusion,Insight-tree,"in kecp, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query.",,https://www.aclanthology.org/2022.emnlp-main.206.pdf
3228,248562985,KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering,conclusion,Insight-tree,the span-level contrastive learning objective is proposed to improve the performance of eqa.,,https://www.aclanthology.org/2022.emnlp-main.206.pdf
3229,248562985,KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering,conclusion,Insight-tree,experiments on multiple benchmarks in both instance-level and task-level few-shot scenarios show that our framework consistently outperforms the state-of-the-art methods.,,https://www.aclanthology.org/2022.emnlp-main.206.pdf
3230,248512731,Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI,conclusion,Insight-tree,"in order to provide an epistemologically grounded characterisation of natural language explanations, this paper attempted to bridge the gap in the notion of scientific explanation (salmon, 2006;salmon, 1984), studying it as both a formal object and as a linguistic expression.",,https://arxiv.org/pdf/2205.01809v2.pdf
3231,248512731,Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI,conclusion,Insight-tree,"the combination of a systematic survey with a corpus analysis on natural language explanations (jansen et al., 2014;jansen et al., 2018),",,https://arxiv.org/pdf/2205.01809v2.pdf
3232,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,the main centres of interactions in the adducts forming in the systems of glu-triamine are the oxygen atoms from carboxyl groups and the nitrogen atom from the amine group of glu and the nitrogen atoms from the amine groups of triamine.,,
3233,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"moreover, these centres are also potential sites of metal ions coordination.",,
3234,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"at lower ph in the metal-free systems, the terminal amine groups of the two triamines studied, the oxygen atoms from -c (5) ooand the amine group from glu are not engaged in the weak noncovalent interactions between ligands, in contrast to the situation in the system asp-triamine [43]).",,
3235,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"thus, not only the length of the polyamine carbon chain [42][43][44] but also the length of the amino acid carbon chain influences the interactions between the bioligands.",,
3236,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"in the adducts forming above ph 7, all available active centres of the ligands are involved in the interaction and the inversion effect is observed, similarly as in the system of asp-triamine [43]).",,
3237,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"the amine groups of the polyamine could act either as positive or negative centres of interaction, depending on ph (the character of interaction depends on the degree of protonation).",,
3238,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"in the system cu(ii)-glu-triamine the following species are formed: mlhl', mll' and mll'oh (where l = glu, l' = triamine).",,
3239,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"in contrast to the situation in the systems with a shorter chain asp [42,43], no formation of ml  l' type molecular complexes was observed, with polyamine in the outer coordination sphere engaged in noncovalent interactions with the anchoring binary complex ml.",,
3240,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"in the monoprotonated species, the coordination of 3,3-tri and spd to copper(ii) ions is of the same character, but in disparity to the 3,3-tri species, in the complex cu(glu)h(spd) the oxygen atoms from -c (5) ooof the amino acid are not engaged in the metallation.",,
3241,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,"in this work, we proposed an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts.",,https://www.aclanthology.org/2022.naacl-main.207.pdf
3242,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,we demonstrate consistent improvement when using our approach on three different models over two different benchmarks.,,https://www.aclanthology.org/2022.naacl-main.207.pdf
3243,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,"for future work, we propose exploring variations of our proposed supervised loss on other long-context tasks, such as longdocument and multi-document summarization, and integrating our method into information retrieval re-ranker models.",,https://www.aclanthology.org/2022.naacl-main.207.pdf
3244,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,depends on the size of the model).,,https://www.aclanthology.org/2022.naacl-main.207.pdf
3245,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,"in order to determine the temperature hyperparameter  , we searched over {0.2, 0.4, 0.6, 0.8, 1.0} per question type (if applicable).",,https://www.aclanthology.org/2022.naacl-main.207.pdf
3246,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,"we also applied dropout with a rate of p = 0.1 over the linear projections, which consistently improved the results over all the benchmarks.",,https://www.aclanthology.org/2022.naacl-main.207.pdf
3247,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,"finally, we searched for the best performing  hyperparameter over the values of {0.2, 0.4, 0.6, 0.8, 1.0}.",,https://www.aclanthology.org/2022.naacl-main.207.pdf
3248,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we introduce ynat, the first korean topic classification benchmark.",,https://arxiv.org/pdf/2105.09680v4.pdf
3249,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"the benchmark includes 63,892 news headlines classified to a single hand-labeled topic among 7 categories.",,https://arxiv.org/pdf/2105.09680v4.pdf
3250,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we assume each headline has only a single topic, but it could be formulated as multi-label classification.",,https://arxiv.org/pdf/2105.09680v4.pdf
3251,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we thus open the second and third relevant topic annotations.,,https://arxiv.org/pdf/2105.09680v4.pdf
3252,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"also, urls for each headlines are accompanied for future work if metadata is needed.",,https://arxiv.org/pdf/2105.09680v4.pdf
3253,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"if some of them requires permission to use, one should contact to the agency.",,https://arxiv.org/pdf/2105.09680v4.pdf
3254,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we expect ynat to serve as a simple and basic nlu task compared to others in klue.,,https://arxiv.org/pdf/2105.09680v4.pdf
3255,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we create the first human-annotated korean sts benchmark, klue-sts, that covers multiple domains and styles with free accessibility to everyone.",,https://arxiv.org/pdf/2105.09680v4.pdf
3256,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,the similarity score annotation process is specially designed to capture the characteristics of the korean language.,,https://arxiv.org/pdf/2105.09680v4.pdf
3257,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"covering the expressions from various domains, our benchmark is expected to be a useful resource for further research, beyond serving as a benchmark.",,https://arxiv.org/pdf/2105.09680v4.pdf
3258,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"our benchmark helps to develop numerous models established on sts resources, such as sentencebert [115].",,https://arxiv.org/pdf/2105.09680v4.pdf
3259,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"our new dataset, klue-nli, is the first resource constructed upon naturally occurring korean sentences.",,https://arxiv.org/pdf/2105.09680v4.pdf
3260,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"klue-nli represents diverse linguistic phenomena, writing style, degree of formality and contents that are most natural and suitable for korean.",,https://arxiv.org/pdf/2105.09680v4.pdf
3261,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"the premise sentences of our dataset come from six korean corpora, and the hypothesis sentences are written by well-trained workers.",,https://arxiv.org/pdf/2105.09680v4.pdf
3262,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we construct a new korean ner benchmark that covers broad domains and styles, which is freely accessible to anyone.",,https://arxiv.org/pdf/2105.09680v4.pdf
3263,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,the entity types are annotated so that a model has to use both morphological and contextual cues.,,https://arxiv.org/pdf/2105.09680v4.pdf
3264,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,the character-level entity tagging and evaluation method reflects the characteristics of korean morphology.,,https://arxiv.org/pdf/2105.09680v4.pdf
3265,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"since klue-ner dataset covers both formal news articles and informal user-generated web texts, we hope that our benchmark helps develop ner models that can be used in a wide a range of domains, and serve as a resource for developing advanced models for information extraction.",,https://arxiv.org/pdf/2105.09680v4.pdf
3266,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we propose klue-re, a large-scale human-annotated re benchmark for korean.",,https://arxiv.org/pdf/2105.09680v4.pdf
3267,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"to overcome the lack of large-scale and up-to-date korean kbs, we design an efficient candidate collection method, coupled with an effective annotation scheme.",,https://arxiv.org/pdf/2105.09680v4.pdf
3268,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,klue-re can not only be used for online information extraction but also contribute to building a large-scale knowledge graph from unstructured texts.,,https://arxiv.org/pdf/2105.09680v4.pdf
3269,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we therefore expect klue-re to be a starting point for building a large-scale, ever-growing public kb in korean, as well as a valuable korean nlu benchmark.",,https://arxiv.org/pdf/2105.09680v4.pdf
3270,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we build a korean dp benchmark klue-dp consisting of formal news and informal user-generated web data.,,https://arxiv.org/pdf/2105.09680v4.pdf
3271,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,klue-dp is helpful for developing a dp model that can be used in multiple domains.,,https://arxiv.org/pdf/2105.09680v4.pdf
3272,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"pos tagging is performed together to improve dp performance, and the tagset and guideline for dp and pos tagging are applied by revising the existing tta dataset.",,https://arxiv.org/pdf/2105.09680v4.pdf
3273,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"this guideline is customized to reflect the characteristics of korean (agglutinative, free word order, etc.), and it also tackles omission of predicates in web data or errors in spacing.",,https://arxiv.org/pdf/2105.09680v4.pdf
3274,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we hope that our benchmarks will help in the development of korean dp models and other natural language processing.,,https://arxiv.org/pdf/2105.09680v4.pdf
3275,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,40 https://universaldependencies.org,,https://arxiv.org/pdf/2105.09680v4.pdf
3276,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we create a new challenging korean mrc benchmark named (klue-mrc).,,https://arxiv.org/pdf/2105.09680v4.pdf
3277,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"in order to evaluate different aspects of mrc capabilities, klue-mrc includes multi-domain passages and three types of questions: paraphrase, multisentence reasoning, and unanswerable.",,https://arxiv.org/pdf/2105.09680v4.pdf
3278,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"klue-mrc shows improvements in question type diversity, difficulty, and lexical overlap compared to existing korean mrc datasets.",,https://arxiv.org/pdf/2105.09680v4.pdf
3279,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we introduce wizard-of-seoul (wos), the first large-scale korean multi-domain task-oriented dialogue dataset that simulates conversations between seoul tourists and travel agents.",,https://arxiv.org/pdf/2105.09680v4.pdf
3280,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we adapt 'self-dialog' for efficiently scaling up of dialogue collection scheme.,,https://arxiv.org/pdf/2105.09680v4.pdf
3281,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"in addition, consideration on annotation interfaces (drop-down menu and turn-switching) mitigates erroneous cases and diverse goal instructions including counterfactual ones promote each conversation to be more natural and challenging.",,https://arxiv.org/pdf/2105.09680v4.pdf
3282,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we hope that wos sparks various future dialogue research in korean and also offers valuable insights to pushing forward end-to-end dialogue modeling.,,https://arxiv.org/pdf/2105.09680v4.pdf
3283,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we present klue, a suite of korean nlu benchmarks that includes diverse tasks.",,https://arxiv.org/pdf/2105.09680v4.pdf
3284,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we open klue to everyone, and we also provide korean language models trained to outperform multilingual models and other existing open-sourced korean language models.",,https://arxiv.org/pdf/2105.09680v4.pdf
3285,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we set high standards from the outset, as we built the benchmark and trained the models from scratch.",,https://arxiv.org/pdf/2105.09680v4.pdf
3286,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we designed the benchmark datasets and trained the annotators rigorously to consider potential ethical issues including private information and hate speech.,,https://arxiv.org/pdf/2105.09680v4.pdf
3287,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we documented in detail all of the benchmark construction and testing processes.,,https://arxiv.org/pdf/2105.09680v4.pdf
3288,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,we also discussed broader impacts and limitations of klue and our models.,,https://arxiv.org/pdf/2105.09680v4.pdf
3289,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"despite the limitations, klue and the accompanying language models will facilitate future korean nlp research by setting a valuable precedent describing how datasets and language models should be created and spread to a wider community.",,https://arxiv.org/pdf/2105.09680v4.pdf
3290,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"the large n c limit is inherent in all holographic qcd methods, and this poses a problem for the aspects of nuclear physics that are different between the small n c and the large n c regimes.",,https://arxiv.org/pdf/1003.2621v2.pdf
3291,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in particular, the bulk nuclear matter at zero temperature and pressure (but finite density) forms a quantum liquid for small n c -such as real-life n c = 3 -but becomes a crystalline solid for large n c",,https://arxiv.org/pdf/1003.2621v2.pdf
3292,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,".to see how this works, consider a condensed matter analogy -some atoms which attract to each other at long or medium distances but have repulsive hard cores.",,https://arxiv.org/pdf/1003.2621v2.pdf
3293,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"semi-classically, at zero temperature and pressure such atoms always form some kind of a crystal; it takes strong quantum effects to put the atoms into some other phase such as liquid or super-solid.of particular importance is the kinetic energy of the zero-point quantum motion of atoms confined to narrow potential wells,k   2 2 2m atom (well diameter) 2 ,(2.1)or rather its ratio k/u to the potential binding energy u per atom.",,https://arxiv.org/pdf/1003.2621v2.pdf
3294,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"according to newton bernardes [18], this ratio is related to the de bour parameter  b of the inter-atomic potential ask u  11 2 b ,  b = r c  2m ,",,https://arxiv.org/pdf/1003.2621v2.pdf
3295,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,(2.2)where r c is the radius of the atomic hard core and  is the maximal depth of the potential.,,https://arxiv.org/pdf/1003.2621v2.pdf
3296,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"for small de bour parameters, the quantum corrections to the semi-classical approximation are weak and the crystal remains stable at zero pressure.",,https://arxiv.org/pdf/1003.2621v2.pdf
3297,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"for larger  b , the quantum corrections due to kinetic energy become important, and when  b exceeds a critical value somewhere between 0.2 and 0.3",,https://arxiv.org/pdf/1003.2621v2.pdf
3298,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"[19], the crystal melts into a quantum liquid.",,https://arxiv.org/pdf/1003.2621v2.pdf
3299,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"4 for example, helium atoms have  b = 0.306 and hence k/u  1 while neon atoms have  b = 0.063 and hence k/u  0.05; consequently, at zero temperature and zero pressure helium is a quantum liquid while neon is a crystalline solid.to see how the k/u ratio of the nuclear matter depends on the number of colors, we note that in the large n c limit, the leading nuclear forces are proportional to n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3300,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"specifically, according to kaplan and manohar [20],v ( r, i 1 , i 2 , j 2 , j 2 ; n c ) = n c  a c (r) + n c ",,https://arxiv.org/pdf/1003.2621v2.pdf
3301,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,a s (r)(i 1 i 2 )(j 1 j 2 ) + n c  a t (r)(i 1 i 2 ) 3(nj 1 )(nj 2 )  (j 1 j 2 ) (2.3) + o(1/n c ),,https://arxiv.org/pdf/1003.2621v2.pdf
3302,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,".for the same n c -independent radial profiles a c , a s , a t of the central, spin-spin, and tensor potentials.",,https://arxiv.org/pdf/1003.2621v2.pdf
3303,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"classically, such potentials would like to arrange a many-nucleon system in some kind of a crystal with n c -independent nearest-neighbor distance  1 fm, while the binding energy of a nucleon in such a crystal would be proportional to the n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3304,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"indeed, all models of nuclear matter based on semi-classical models of nucleons form such crystals, for example skyrmion crystals of ref.",,https://arxiv.org/pdf/1003.2621v2.pdf
3305,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,[6].,,https://arxiv.org/pdf/1003.2621v2.pdf
3306,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in the quantum theory, nucleons in such a lattice have zeropoint kinetic energies (2.1) where the well diameter is independent on n c while the nucleon's mass m  n c , hence k  1/n c andk u  n 1 c",,https://arxiv.org/pdf/1003.2621v2.pdf
3307,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,n +1 c,,https://arxiv.org/pdf/1003.2621v2.pdf
3308,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,#ERROR!,,https://arxiv.org/pdf/1003.2621v2.pdf
3309,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,(2.4)we may estimate the coefficient of this proportionality using the de bour parameter  b .,,https://arxiv.org/pdf/1003.2621v2.pdf
3310,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"the maximal depth of the central potential between two nucleons is about 100 mev for n c = 3, so we take it to be ",,https://arxiv.org/pdf/1003.2621v2.pdf
3311,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree, n c  30 mev for large n c .,,https://arxiv.org/pdf/1003.2621v2.pdf
3312,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"likewise, we take the nucleon mass to be m n  n c  300 mev and hard-core radius r c  0.7 fm regardless of n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3313,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"consequently, b = r c  2m  2 n c = k u  45 n 2 c (2.5)and hence liquid nuclear matter for n c 8 and solid nuclear matter for n c 8.the numerical coefficient in eq.",,https://arxiv.org/pdf/1003.2621v2.pdf
3314,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,(2.5) and hence our estimate n crit c  8 for the dividing line between liquid and solid bulk nuclear matter (at low pressures and temperatures) should be taken with a large grain of salt.,,https://arxiv.org/pdf/1003.2621v2.pdf
3315,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"also, the transition between liquid nuclear matter for n c = 3 and crystalline nuclear matter for large n c may go through some exotic phases at intermediate values of n c , perhaps something like a quantum supersolid, perhaps something more exotic without known condensed-matter analogues.",,https://arxiv.org/pdf/1003.2621v2.pdf
3316,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"but regardless of the details of this transition, in the large n c limit the potential energy of interacting near-static nucleons becomes much larger than the nucleons' kinetic energies, and the bulk nuclear matter at t = 0, p = 0 conditions becomes a conventional semi-classical crystal.",,https://arxiv.org/pdf/1003.2621v2.pdf
3317,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,the structure of such crystals can be modeled holographically -and indeed there is active research in this direction (for instance [7]),,https://arxiv.org/pdf/1003.2621v2.pdf
3318,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"-but we have no experimental data to compare to the models because real-life nuclei with n c = 3 are liquid rather than solid.meanwhile, instead of trying do build holographic models of complete nuclei we focus on holographic models of the nuclear forces.",,https://arxiv.org/pdf/1003.2621v2.pdf
3319,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"but even at the level of the two-body forces, the large n c limit maybe different from the real-life case of just 3 colors.",,https://arxiv.org/pdf/1003.2621v2.pdf
3320,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,of particular concern is the isoscalar attractive force due to exchanges of the (600) scalar mesons between the nucleons.,,https://arxiv.org/pdf/1003.2621v2.pdf
3321,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in real life, this is a major component of the net attractive force -especially at the medium-long distances between the nucleons -but",,https://arxiv.org/pdf/1003.2621v2.pdf
3322,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,in the large n c limit this component may weaken or disappear because the (600) meson itself may become heavier or even disappear from the scalar meson spectrum.the (600) (also known as f 0,,https://arxiv.org/pdf/1003.2621v2.pdf
3323,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,(600)) is the lightest isoscalar true-scalar meson.,,https://arxiv.org/pdf/1003.2621v2.pdf
3324,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in real life, it appears as a very broad resonance of two pions -so broad that its central mass is somewhat controversial and different experimentalists locate it anywhere between 400 mev and 700 mev, and sometimes even higher, cf. references in the particle data group's listing",,https://arxiv.org/pdf/1003.2621v2.pdf
3325,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,[21].,,https://arxiv.org/pdf/1003.2621v2.pdf
3326,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,but the real controversy about the (600) resonance is its physical origin.,,https://arxiv.org/pdf/1003.2621v2.pdf
3327,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,unlike the heavier i g = 0,,https://arxiv.org/pdf/1003.2621v2.pdf
3328,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"+ , j cp = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3329,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,+,,https://arxiv.org/pdf/1003.2621v2.pdf
3330,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,+ mesons f 0,,https://arxiv.org/pdf/1003.2621v2.pdf
3331,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"(980), f 0 (1370), etc., the (600) meson does not exist in the non-relativistic quark model 5 so for many years r. l. jaffe and others",,https://arxiv.org/pdf/1003.2621v2.pdf
3332,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"[22,23,24,25] were claiming that the (600) is not a true qq meson but a qqqq tetraquark.",,https://arxiv.org/pdf/1003.2621v2.pdf
3333,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"specifically, it's a molecule-like bound state of two pions which exists because the -meson exchanges in the t-channel induce an attractive s-channel force between the pions.",,https://arxiv.org/pdf/1003.2621v2.pdf
3334,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"if this claim is true, then the  resonance goes away in the large n c limit because the forces between pions become weak as 1/n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3335,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,but many other authors (see [26] for a sample) identify the (600) with the  field of the linear sigma model of the chiral symmetry breaking.,,https://arxiv.org/pdf/1003.2621v2.pdf
3336,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"or rather, the massive (x) field parametrizing fluctuations of magnitude of the symmetry-breaking vev  gives rise to primordial sigma quanta, while the real sigma mesons (600) are quantum mixtures of those primordial quanta with the | states (and to lesser extent with the other",,https://arxiv.org/pdf/1003.2621v2.pdf
3337,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,i g,,https://arxiv.org/pdf/1003.2621v2.pdf
3338,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,0,,https://arxiv.org/pdf/1003.2621v2.pdf
3339,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"+ , j p c = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3340,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,+,,https://arxiv.org/pdf/1003.2621v2.pdf
3341,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,+ mesons),,https://arxiv.org/pdf/1003.2621v2.pdf
3342,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,.,,https://arxiv.org/pdf/1003.2621v2.pdf
3343,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"from this point of view, the non-relativistic quark model is irrelevant because the quarks do not become non-relativistic until after the chiral symmetry has already been broken.",,https://arxiv.org/pdf/1003.2621v2.pdf
3344,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"indeed, the nrqm does not see that the pions are (pseudo) goldstone bosons, so the fact that it does not see the sigma meson at all is simply another limitation of the nrqm as far as the chiral symmetry breaking is concerned.",,https://arxiv.org/pdf/1003.2621v2.pdf
3345,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"if this point of view is right, then the sigma meson exits for all n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3346,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"for large n c limit, this meson is mostly a quantum of the (x) field -its mixing with | and other states becomes weak",,https://arxiv.org/pdf/1003.2621v2.pdf
3347,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"-and it's a narrow resonance rather than a broad hump we have for n c = 3, but it remains a dominant resonance in the i g = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3348,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"+ , j p c = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3349,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"++  channel, and its mass should not be too different from the real-life 600 mev.the other mesons -scalar or vector, isoscalar or isovector -are unlikely to be disturbed by the large n c limit, so their contributions to the nuclear forces would be sim",,https://arxiv.org/pdf/1003.2621v2.pdf
3350,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"if the (600) meson remains in the spectrum in the large n c limit and if its mass remains similar to the real-life 600 mev, then the entire nuclear potential (2.3) for n c   would be similar to what it is in real life, except for the overall factor n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3351,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in particular, the net central potential v c (r) would be repulsive at short distances (the hard core) but 5 in the non-relativistic quark model, all 0 ++ mesons have s = 1 and l = 1.",,https://arxiv.org/pdf/1003.2621v2.pdf
3352,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"consequently, the lightest 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3353,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,++ meson should be heavier than the lightest 1  mesons (770) or (787) that have s = 1 but l = 0.,,https://arxiv.org/pdf/1003.2621v2.pdf
3354,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"depending on the assumptions one makes about the forces between the quark and the antiquark, this argument identifies the lightest true qq meson with i g = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3355,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,+,,https://arxiv.org/pdf/1003.2621v2.pdf
3356,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,and j p c = 0,,https://arxiv.org/pdf/1003.2621v2.pdf
3357,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,++ as either f 0 (980) or f 0 (1370).,,https://arxiv.org/pdf/1003.2621v2.pdf
3358,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in any case, the (600) resonance is way too light to be a p-wave qq state, so it has to be something else.attractive at medium and long distances:r v c (r) n c   limit with a light  meson (2.6)on the other hand, if the (600) meson disappears from the spectrum for large n c , or if it becomes heavier than the lightest vector meson, then the dominant attractive force would become shorter-ranged than the repulsive force, and the net force at medium and long distances would be repulsive rather than attractive:r v c (r) n c   limit without the  meson (2.7)in this scenario, at large n c the nuclear force is repulsive at all distances, and there are no bound nuclei at all, liquid or crystalline.so what really happens to the sigma-meson and to the nuclear forces at large n c ?",,https://arxiv.org/pdf/1003.2621v2.pdf
3359,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,the best way to settle this controversy would be to find the  resonance and its mass in a lattice qcd calculation for several values of n c .,,https://arxiv.org/pdf/1003.2621v2.pdf
3360,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"such a calculation would require a realistic pion mass (unlike most present-day lattice calculations extrapolating from m   350 mev) andrather large lattices to distinguish the sigma resonance from the two-pion continuum, so it may be too hard for the present-day computers.",,https://arxiv.org/pdf/1003.2621v2.pdf
3361,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"but thanks to the moore's law, finding the  resonance on a lattice should become possible in a not-too-distant future.alternatively, we may try to resolve the issue using holography.",,https://arxiv.org/pdf/1003.2621v2.pdf
3362,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"although a holographic model of real qcd -or rather, of qcd with large n c -is yet to be constructed, several known models seem to be qualitatively similar, so we can compare their predictions for the meson spectra in general, and for the lightest true scalar meson in particular.",,https://arxiv.org/pdf/1003.2621v2.pdf
3363,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"however, the models that seem qualitatively similar to qcd may not be similar enough, and their predictions could be widely off target.",,https://arxiv.org/pdf/1003.2621v2.pdf
3364,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"indeed, the predictions of different models have turned out to be quite different from each other.",,https://arxiv.org/pdf/1003.2621v2.pdf
3365,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"for example, in the sakai-sugimoto model which we use in this article, the lightest true scalar meson is more than twice as heavy as the lightest vector meson.",,https://arxiv.org/pdf/1003.2621v2.pdf
3366,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,consequently -as we shall see in painful detail in section 5 -the net nuclear force is everywhere repulsive and looks like (2.7) rather than like (2.6).,,https://arxiv.org/pdf/1003.2621v2.pdf
3367,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"on the other hand, in the highly-non-antipodal version of the dymarsky-kuperstein-sonnenschein model [27], the lightest j cp = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3368,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,#NAME?,,https://arxiv.org/pdf/1003.2621v2.pdf
3369,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,[29].,,https://arxiv.org/pdf/1003.2621v2.pdf
3370,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"however, this lightest scalar is a pseudo-goldstone boson of the approximate conformal symmetry of the flavor sector, so it is not clear how much attractive force it can mediate.",,https://arxiv.org/pdf/1003.2621v2.pdf
3371,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"as of this writing, it is not clear if the net nuclear potential in this model looks like the real-life potential (2.6) or like the everywhere-repulsive potential (2.7) we calculate in this paper for the sakai-sugimoto model.",,https://arxiv.org/pdf/1003.2621v2.pdf
3372,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"but suppose tomorrow somebody discovers a holographic model of the real qcd andmiracle of miracles -it has a realistic spectrum of mesons, including the (600) resonance, and even the realistic yukawa couplings of those mesons to the baryons.",,https://arxiv.org/pdf/1003.2621v2.pdf
3373,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"even for such a model, the two-body nuclear forces would not be quite as in the real world because the semi-classical holography limits n c  ,    suppress the multiple meson exchanges between baryons.",,https://arxiv.org/pdf/1003.2621v2.pdf
3374,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"although in this case, the culprit is not the large number of colors but the large 't hooft coupling  = n c g 2 ym .",,https://arxiv.org/pdf/1003.2621v2.pdf
3375,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"indeed, from the hadronic point of view, nuclear forces arise from the mucleons exchanging one, two, or more mesons, and in real life the double-meson exchanges are just as important as the single-meson exchanges.",,https://arxiv.org/pdf/1003.2621v2.pdf
3376,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in particular, since the lightest mesonic state with i g = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3377,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"+ , j cp = 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3378,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,#NAME?,,https://arxiv.org/pdf/1003.2621v2.pdf
3379,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in holography, the single-meson exchanges happen at the tree level of the string theory while the multiple meson exchanges involve string loops (k  1 loops for k mesons), and the loop amplitudes are suppressed by the powers of 1/ relative to the tree amplitudes.naively, one would expect the loop amplitudes to carry additional factors of 1/n c (which is dual to the string coupling) rather than 1/, or maybe both 1/ and 1/n c factors, but the naive power-of-n c counting does not work for loop amplitudes involving baryons made of n c quarks.",,https://arxiv.org/pdf/1003.2621v2.pdf
3380,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"6 indeed, in honest qcd with a large number of colors, the multi-meson-exchange contributions to the non-relativistic effective potential for the baryons are not suppressed by powers of 1/n c",,https://arxiv.org/pdf/1003.2621v2.pdf
3381,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,[27].,,https://arxiv.org/pdf/1003.2621v2.pdf
3382,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"however, the extra powers of n c due to n c quarks in a baryon are not accompanied by the extra powers of , so in holography, the contributions of the multiple meson exchanges are suppressed, albeit by powers of 1/ rather than 1/n c",,https://arxiv.org/pdf/1003.2621v2.pdf
3383,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,".to see how this works in a general holographic model of qcd with n c  n f , note that such a model starts with a string-theoretic construction where the colors and the flavors live on separate branes.",,https://arxiv.org/pdf/1003.2621v2.pdf
3384,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"for large n c and large , the color branes become black branes producing curvature and fluxes through the bulk, which provide a non-trivial background for degrees of freedom living in the bulk itself as well as on the flavor branes.",,https://arxiv.org/pdf/1003.2621v2.pdf
3385,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"the bulk degrees of freedom are dual to the pure-color sector of qcd (glueballs, etc.), while the vector and scalar fields living on the flavor branes are dual to the qq mesons.",,https://arxiv.org/pdf/1003.2621v2.pdf
3386,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"the flavor fields have rather weak couplings to each other: in 5d terms,g 5d,flavor   r kk  n c (2.8)so the 4d mesons -which are modes of the 5d vector and scalar fields with wave functions  r 1/2kk -have couplings to each other of the orderg m m m  1  n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3387,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"(2.9)a holographic baryon is made from some brane spanning only the compact dimensions that is connected to the flavor branes by n c strings, although this construction is often equivalent to an instanton of the 5d flavor gauge fields.",,https://arxiv.org/pdf/1003.2621v2.pdf
3388,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"consequently, the baryon-meson coupling is enhanced by an extra factor of n c ,g m bb  n c  1  n c",,https://arxiv.org/pdf/1003.2621v2.pdf
3389,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,#ERROR!,,https://arxiv.org/pdf/1003.2621v2.pdf
3390,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"(2.10)at the tree level of the baryon-meson theory, scattering of two baryons proceeds through a single-meson exchange, which produces a o(n c /) amplitude,a tree  g 2 m bb  n c",,https://arxiv.org/pdf/1003.2621v2.pdf
3391,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree, .,,https://arxiv.org/pdf/1003.2621v2.pdf
3392,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"(2.11)at the one-loop level, there are two types of diagrams, the triangle diagrams such asa   g 3 m bb ",,https://arxiv.org/pdf/1003.2621v2.pdf
3393,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,g m m m  n c  2 (2.12)and the box and crossed-box diagrams (2.13) with amplitudesa  g 4 m bb  n 2 c,,https://arxiv.org/pdf/1003.2621v2.pdf
3394,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree, 2 .,,https://arxiv.org/pdf/1003.2621v2.pdf
3395,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,(2.14)that carry an extra power of n c .,,https://arxiv.org/pdf/1003.2621v2.pdf
3396,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"however, banerjee et al showed [27] that for non-relativistic baryons, the box and the crossed-box diagrams almost cancel each other from the effective potential between the baryons, with the un-canceled part having a lower power of the n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3397,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"banerjee et al did not pay any attention to the powers of , but clearly the un-canceled sub-leading terms in the box and crossed-box diagrams cannot carry higher powers of the 't hooft coupling than the leading terms (2.14), thusa uncanceled  n c  2  a   1  a tree .",,https://arxiv.org/pdf/1003.2621v2.pdf
3398,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"(2.15)in other words, the contribution of the double-meson exchange and other one-loop processes to the 2-body nuclear potential carries the same power of n c but is suppressed by a factor 1/ compared to the tree-level singe-meson exchange.to be precise, the large  limit suppresses exchanges of the un-bound meson pairs but not of the meson-meson resonances -which become narrow (because of weak g m m m ) and act as single mesons exchanged between the two baryons.",,https://arxiv.org/pdf/1003.2621v2.pdf
3399,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in particular, in the isoscalar 0 ++ channel that gives rise to the dominant attractive force between nucleons, the    limit suppresses the contribution of the unbound two-pion continuum, but it replaces it with a discrete set of f 0 resonances.",,https://arxiv.org/pdf/1003.2621v2.pdf
3400,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in a good holographic model of qcd (which alas has not been found yet), the overall strength of the 0 ++ channel should be similar to the real qcd, so it would produce a similar isoscalar attractive force at short distances.",,https://arxiv.org/pdf/1003.2621v2.pdf
3401,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"however, the range of this attractive force would be significantly shorter: instead of decaying with distance like exp(2m  r) as in real life, the holographic attractive force decays as exp(m 0 r) where m 0 is the mass of the lightest isoscalar 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3402,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"++ meson, presumably (600 mev).in principle, the isoscalar 1  channel that gives rise to the dominant repulsive force suffers from similar corrections in the    limit.",,https://arxiv.org/pdf/1003.2621v2.pdf
3403,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"but in practice, the strongest and the longest-range contribution to this channel comes from exchanges of a single (787) meson, so suppressing the multi-meson exchanges in this channel would not make a qualitative difference.",,https://arxiv.org/pdf/1003.2621v2.pdf
3404,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"thus altogether, the net effect of large 't hooft coupling on the central nuclear potential -besides the overall 1/ factor -is the shortening of the attractive tail at long distances:r   v c (r)blue: real qcd,   1 red: best possibility for holographic qcd,   1 (2.16) however, this optimistic picture presumes a holographic model of qcd that correctly reproduces (a) the overall strength of the isoscalar 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3405,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"++ and 1  channels, and (b) the mass spectra of vector and scalar mesons, especially the masses of the lightest 0",,https://arxiv.org/pdf/1003.2621v2.pdf
3406,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,++ and 1  mesons (600) and (787).,,https://arxiv.org/pdf/1003.2621v2.pdf
3407,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"but thus far, no known model satisfies these requirements, not even approximately, so the nuclear forces they produce could be much more different from the real life than (2.16).",,https://arxiv.org/pdf/1003.2621v2.pdf
3408,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in particular, the nuclear force we calculate in this paper for the sakai-sugimoto model turns out to be everywhere repulsive:r v c (r)/n c blue: real qcd red: sakai-sugimoto model (2.17)now let's go back to the large n c limit -in holography or in honest qcd -and consider yet another general problem with baryons made from many quarks: how to separate the nucleons with i = j = 1 2 from the other kinds of baryons such as  with i = j = 3 2 ?",,https://arxiv.org/pdf/1003.2621v2.pdf
3409,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in real life, there is a large mass gap between the nucleons and the  baryons -almost 300 mev -but for large n c this gap shrinks as 1/n c .",,https://arxiv.org/pdf/1003.2621v2.pdf
3410,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"at the same time, the two-baryon potential grows like n c , so for large n c it becomes stronger than the gap.",,https://arxiv.org/pdf/1003.2621v2.pdf
3411,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"consequently, two interacting nucleons may ""forget"" their individual spins and isospins and mix up with other baryonic species such as . in fact, for large n c there is a whole lot of baryonic species with i = j ranging from 1 2 (for odd n c ) or 0 (for even n c ) all the way up to n c /2, and a strongly-interacting nucleon might mix up with all of them.",,https://arxiv.org/pdf/1003.2621v2.pdf
3412,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"while such mixing would not affect the isoscalar spin-blind central force between two baryons, it might significantly enhance the isovector spin-spin and tensor forces.therefore, comparing the two-baryon forces in the large n c limit to the real-life twonucleon forces is rather tricky.",,https://arxiv.org/pdf/1003.2621v2.pdf
3413,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"one has to carefully keep track of the spin and isospin degrees of freedom of the two baryons, expand the interaction hamiltonian into central, spinspin, and tensor forces as in eq.",,https://arxiv.org/pdf/1003.2621v2.pdf
3414,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"(2.3), and then compare the radial profiles a c (r), a s (r), and a t (r).",,https://arxiv.org/pdf/1003.2621v2.pdf
3415,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"moreover, the spin and isospin degrees of freedom require quantum mechanical treatment because semi-classically, we do not get definite spins or isospins even for standalone single baryons.",,https://arxiv.org/pdf/1003.2621v2.pdf
3416,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"instead, we get skyrmions, or instantons, or some other kind of solitons with a definite orientation of the su(2) isospin relative to the su(2) spin ; in quantum terms, they become superpositions of baryons with all possible",,https://arxiv.org/pdf/1003.2621v2.pdf
3417,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"i = j = 1 2 , 3 2 , . . .",,https://arxiv.org/pdf/1003.2621v2.pdf
3418,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,", . consequently, a force between two such semiclassical baryons is not a force between two nucleons but rather a superposition of forces between different baryonic species.this problem affected the first holographic calculation of the nuclear forces by k. y. kim and i. zahed",,https://arxiv.org/pdf/1003.2621v2.pdf
3419,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,[30].,,https://arxiv.org/pdf/1003.2621v2.pdf
3420,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"their baryons were semiclassical instantons in the sakai-sugimoto model, so instead of definite |i, i z , j, j z they had a definite direction n in s 3 = su(2) isospin  su(2) spin /su(2) common .",,https://arxiv.org/pdf/1003.2621v2.pdf
3421,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"consequently, kim and zahed [30] found that the force between two baryons depends on the angle between n 1 and n 2 -it was attractive for some angles and repulsive for other",,https://arxiv.org/pdf/1003.2621v2.pdf
3422,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,#NAME?,,https://arxiv.org/pdf/1003.2621v2.pdf
3423,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"by comparison, hashimoto, sakai, and sugimoto [16] made a similar calculation using properly quantized collective coordinates for each instanton.consequently, they obtained the force between two nucleons rather than some mixed-up baryons, and they could see how this force depends on each nucleon's i z and j z .",,https://arxiv.org/pdf/1003.2621v2.pdf
3424,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in particular, they saw that at medium-short distances, the net force between two nucleons is always repulsive.",,https://arxiv.org/pdf/1003.2621v2.pdf
3425,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"evidently, the attraction kim and zahed saw for some relative orientations of semiclassical baryons happens only for high spins and isospins, but not for nucleons withi = j = 1 2 .on",,https://arxiv.org/pdf/1003.2621v2.pdf
3426,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"the other hand, the analysis of hashimoto et al was limited to the first-order perturbation theory for nucleons that are far enough from each other to avoid the strong mixing of spins and isospins.",,https://arxiv.org/pdf/1003.2621v2.pdf
3427,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,this approach will not work for the hard-core region at very short distances where the interactions are much stronger than the gaps between states of the individual baryons.,,https://arxiv.org/pdf/1003.2621v2.pdf
3428,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in the hard core, the semiclassical analysis of kim and zahed might work better than the perturbative expansion of hashimoto et al, although comparing the semi-classical large-n c results to the real-life nuclear forces might be problematic.to summarize, the large n c limit of nuclear physics suffers from three major problems.",,https://arxiv.org/pdf/1003.2621v2.pdf
3429,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"the third problem of baryon mixing is only technical, and it can be solved -at least for the medium and long distances between the nucleons",,https://arxiv.org/pdf/1003.2621v2.pdf
3430,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,#NAME?,,https://arxiv.org/pdf/1003.2621v2.pdf
3431,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,following hashimoto et al rather than kim and zahed.,,https://arxiv.org/pdf/1003.2621v2.pdf
3432,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"but there are no ways around the first problem of different phase structures of nuclear matter with n c = 3 and with n c  . even at high pressures and densities, there is a difference: for n c = 3, squeezing nucleons together makes them merge into a quark liquid, while for n c   the nucleons always retain their individual identities and a would-be quark liquid suffers from the ""chiral density wave"" instability [4].",,https://arxiv.org/pdf/1003.2621v2.pdf
3433,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,it is possible that at some intermediate pressures and densities the n c = 3 nucleons form a crystal -just like helium solidifies at high pressures -before merging into a quark liquid.,,https://arxiv.org/pdf/1003.2621v2.pdf
3434,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"if this intermediate-pressure phase of real nuclear matter is ever observed in a lab, or can be reliably shown to exist in some exotic but observable places like inferiors of neutron stars, it would be very interesting to compare its properties to the holographic models.",,https://arxiv.org/pdf/1003.2621v2.pdf
3435,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"until then, we can only speculate.finally, the second problem -concerning the fate of the (600) resonance in the large n c limit and its effect on the attractive nuclear force -is solvable in principle, but it has not been solved yet.",,https://arxiv.org/pdf/1003.2621v2.pdf
3436,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"in holography, this problem is aggravated by using qcd-like models in lieu of the presently unknown holographic dual of the real qcd.",,https://arxiv.org/pdf/1003.2621v2.pdf
3437,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"the meson spectra of such models are not quite realistic; for example, in the sakai-sugimoto model (both antipodal and non-antipodal versions) there is no  resonance and the lightest scalar meson has more than twice the mass of the lightest vector meson.",,https://arxiv.org/pdf/1003.2621v2.pdf
3438,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"consequently, we shall see in section 5 that in this model, the attractive force is both weaker and shorter-ranged than the repulsive force, so the net nuclear force is always repulsive.",,https://arxiv.org/pdf/1003.2621v2.pdf
3439,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"this could be a peculiar failing of the sakai-sugimoto model, or it could be the general problem of holography or even of the large n c limit.",,https://arxiv.org/pdf/1003.2621v2.pdf
3440,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c   limit and holography,Insight-tree,"hopefully, future research will resolve this issue.",,https://arxiv.org/pdf/1003.2621v2.pdf
3441,237563234,Slot Filling for Biomedical Information Extraction,conclusions and future work,Insight-tree,in this work we formulated the task of biomedical information extraction as a slot filling problem.,,https://www.aclanthology.org/2022.bionlp-1.7.pdf
3442,237563234,Slot Filling for Biomedical Information Extraction,conclusions and future work,Insight-tree,"this approach aims to forgo the need for entity and relation type specific training data, which is scarce and costly to annotate in the biomedical domain.",,https://www.aclanthology.org/2022.bionlp-1.7.pdf
3443,237563234,Slot Filling for Biomedical Information Extraction,conclusions and future work,Insight-tree,"additionally, this formulation allows to deal with the addition of new relation types, without needing to re-train the relevant models.",,https://www.aclanthology.org/2022.bionlp-1.7.pdf
3444,237563234,Slot Filling for Biomedical Information Extraction,conclusions and future work,Insight-tree,"additionally, we have introduced a new biomedical slot filling benchmark and used it to train a biomedical dpr model, a dual bert-based encoder for retrieval, as well as a biomedical slot filling reader based on biobert.",,https://www.aclanthology.org/2022.bionlp-1.7.pdf
3445,237563234,Slot Filling for Biomedical Information Extraction,conclusions and future work,Insight-tree,in a series of experiments our approach outperforms significantly a number of general domain baselines as well as the simpler bm25 retriever.,,https://www.aclanthology.org/2022.bionlp-1.7.pdf
3446,237563234,Slot Filling for Biomedical Information Extraction,conclusions and future work,Insight-tree,"furthermore, our results illustrate the importance of in-domain, taskspecific training data, in line with findings from recent works (glass et al., 2021;maillard et al., 2021).",,https://www.aclanthology.org/2022.bionlp-1.7.pdf
3447,232135266,Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,conclusion,Insight-tree,"in this work, we proposed rissanen data analysis (rda), a method for examining the characteristics of a dataset.",,https://arxiv.org/pdf/2103.03872v1.pdf
3448,232135266,Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,conclusion,Insight-tree,"we began by viewing the labels of a dataset as being generated by a program over the inputs, then positing that a capability is helpful if it reduces the length of the shortest label-generating program.",,https://arxiv.org/pdf/2103.03872v1.pdf
3449,232135266,Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,conclusion,Insight-tree,"instead of evaluating minimum program length directly, we use blockwise prequential coding to upper bound minimum description length (mdl).",,https://arxiv.org/pdf/2103.03872v1.pdf
3450,232135266,Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,conclusion,Insight-tree,"while the choice of learning algorithm a influences absolute mdl values, we only interpret mdl relative to other mdl values estimated with the same a. in particular, we conduct rda by comparing mdl with or without access to a subroutine with a certain capability, and we say that a capability is useful when invoking the subroutine reduces mdl.",,https://arxiv.org/pdf/2103.03872v1.pdf
3451,222208820,Cross-Thought for Sentence Encoder Pre-training,conclusion,Insight-tree,"we propose a novel approach, cross-thought, to pre-train sentence encoder.",,https://arxiv.org/pdf/2010.03652v1.pdf
3452,222208820,Cross-Thought for Sentence Encoder Pre-training,conclusion,Insight-tree,experiments demonstrate that using cross-thought trained with short sequences can effectively improve sentence embedding.,,https://arxiv.org/pdf/2010.03652v1.pdf
3453,222208820,Cross-Thought for Sentence Encoder Pre-training,conclusion,Insight-tree,our pre-trained sentence encoder with further finetuning can beat several strong baselines on many nlp tasks.,,https://arxiv.org/pdf/2010.03652v1.pdf
3454,233219449,What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models,conclusions and discussion,Insight-tree,"we show that training multiple heads on top of a pre-trained language model creates a steering effect, where the target head influences the behaviour of another head, steering it towards capabilities beyond its training objective.",,https://www.aclanthology.org/2021.emnlp-main.646.pdf
3455,233219449,What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models,conclusions and discussion,Insight-tree,"in three multi-task settings, we find that without any dedicated training, the steered head often outputs explanations for the model predictions.",,https://www.aclanthology.org/2021.emnlp-main.646.pdf
3456,233219449,What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models,conclusions and discussion,Insight-tree,"moreover, modifying the input representation based on the outputs of the steered head can lead to predictable changes in the target head predictions.",,https://www.aclanthology.org/2021.emnlp-main.646.pdf
3457,258236093,CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval,conclusions,Insight-tree,this paper proposes to pre-train with the mixtureof-textual-experts to counter the imbalanced discrimination issue in existing dual encoders.,,https://export.arxiv.org/pdf/2304.10195v1.pdf
3458,258236093,CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval,conclusions,Insight-tree,textual-specific experts are introduced for individual modeling of the distinct traits of queries and passages.,,https://export.arxiv.org/pdf/2304.10195v1.pdf
3459,258236093,CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval,conclusions,Insight-tree,results on large-scale web bench-marks show steady improvement in retrieval performances.,,https://export.arxiv.org/pdf/2304.10195v1.pdf
3460,258236093,CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval,conclusions,Insight-tree,quantitive analysis shows a more balanced distribution of query-passage embeddings for dual-encoders.,,https://export.arxiv.org/pdf/2304.10195v1.pdf
3461,234776197,If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,we introduced a simple unsupervised approach for retrieving candidate evidence chains that after reranking achieves state-of-the-art evidence retrieval performance on two multi-hop qa datasets: qasc and multirc.,,https://www.aclweb.org/anthology/2021.naacl-main.363.pdf
3462,234776197,If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,we highlight the importance of generating and feeding candidate evidence chains by showing several benefits over the widely followed approach that retrieves evidence sentences individually.,,https://www.aclweb.org/anthology/2021.naacl-main.363.pdf
3463,234776197,If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"further, we introduced few attention and embedding analyses demonstrating that jointly retrieving and reranking chains assist in learning compositional information, which is also beneficial to the downstream qa task.",,https://www.aclweb.org/anthology/2021.naacl-main.363.pdf
3464,234776197,If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"overall, our work highlights the strengths and potential of joint retrieval+reranking approaches for future works.",,https://www.aclweb.org/anthology/2021.naacl-main.363.pdf
3465,52822214,"HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",conclusions,Insight-tree,"we present hotpotqa, a large-scale question answering dataset aimed at facilitating the development of qa systems capable of performing explainable, multi-hop reasoning over diverse natural language.",,https://www.aclweb.org/anthology/D18-1259.pdf
3466,52822214,"HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",conclusions,Insight-tree,we also offer a new type of factoid comparison questions to test systems' ability to extract and compare various entity properties in text.,,https://www.aclweb.org/anthology/D18-1259.pdf
3467,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,this paper studies the paradigm of tool learning with foundation models.,,https://export.arxiv.org/pdf/2304.08354v1.pdf
3468,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,we first recapitulate the cognitive origins of tool use in human history and categorize tools from the perspective of the user interface.,,https://export.arxiv.org/pdf/2304.08354v1.pdf
3469,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,"then we review the ai paradigm shift brought about by foundation models and discuss the complementary roles of tools and foundation models, followed by a comprehensive literature review for existing exploration in tool learning.",,https://export.arxiv.org/pdf/2304.08354v1.pdf
3470,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,"we formulate a general tool learning framework and highlight core research problems such as bridging user intents with appropriate tools, better planning by leveraging the reasoning abilities of foundation models, training strategies for tool learning, and how to facilitate generalization for tool learning.",,https://export.arxiv.org/pdf/2304.08354v1.pdf
3471,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,we experiment with 17 representative tools and demonstrate that current foundation models showcase dexterity and finesse in tool use.,,https://export.arxiv.org/pdf/2304.08354v1.pdf
3472,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,"finally, we discuss important research topics, including safe and trustworthy tool learning, ai tool creation, personalized tool learning, embodied tool learning, knowledge conflict issue in tool learning, etc. in general, this paper serves as a systematic investigation of tool learning.",,https://export.arxiv.org/pdf/2304.08354v1.pdf
3473,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,we hope this paper could facilitate research in integrating tools with foundation models in the future.,,https://export.arxiv.org/pdf/2304.08354v1.pdf
3474,67855846,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,conclusion,Insight-tree,"we have presented drop, a dataset of complex reading comprehension questions that require discrete reasoning over paragraphs.",,https://arxiv.org/pdf/1903.00161v1.pdf
3475,67855846,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,conclusion,Insight-tree,"this dataset is substantially more challenging than existing datasets, with the best baseline achieving only 32.7% f1, while humans achieve 96%.",,https://arxiv.org/pdf/1903.00161v1.pdf
3476,67855846,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,conclusion,Insight-tree,"we hope this dataset will spur research into more compre-hensive analysis of paragraphs, and into methods that combine distributed representations with symbolic reasoning.",,https://arxiv.org/pdf/1903.00161v1.pdf
3477,67855846,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,conclusion,Insight-tree,"we have additionally presented initial work in this direction, with a model that augments qanet with limited numerical reasoning capability, achieving 47% f1 on drop.",,https://arxiv.org/pdf/1903.00161v1.pdf
3478,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,conclusion,Insight-tree,"we present quest, a new benchmark of queries which contain implicit set operations with corresponding sets of relevant entity documents.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3479,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,conclusion,Insight-tree,our experiments indicate that such queries present a challenge for modern retrieval systems.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3480,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,conclusion,Insight-tree,"future work could consider approaches that have better inductive biases for handling set operations in natural language expressions (for example, vilnis et al. (2018)).",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3481,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,conclusion,Insight-tree,the attributions in quest can be leveraged for building systems that can provide finegrained attributions at inference time.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3482,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,conclusion,Insight-tree,"the potential of pretrained generative lms and multi-evidence aggregation methods to answer set-seeking selective queries, while providing attribution to sources, can also be investigated.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3483,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,naturalness.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3484,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"since our dataset relies on the wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3485,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3486,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"however, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on quest should correlate with improvements on at least a fraction of natural search engine queries with set operations.recall.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3487,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"we also note that because wikipedia categories have imperfect recall of all relevant entities (that contain sufficient evidence in their documents), systems may be incorrectly penalised for predicted relevant entities assessed as false positive.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3488,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,we quantify this in section 5.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3489,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,we have also limited the trusted source for an entity to its wikipedia document but entities with insufficient textual evidence in their documents may still be relevant.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3490,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3491,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"romqa (zhong et al., 2022) takes a step in this latter direction although the evidence attribution is not manually verified.answer set sizes.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3492,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"to ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3493,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"however, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3494,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"on one hand, this is realistic for a search scenario because users may only be interested in a limited set of results.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3495,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"on the other hand, our dataset does not model a scenario where the answer set sizes are much larger.all models were fine-tuned starting from t5 1.1 checkpoints 6 .",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3496,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,we fine-tune t5 models on 32 cloud tpu v3 cores 7 .,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3497,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,fine-tuning takes less than 8 hours for all models.dual encoder.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3498,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,we used the t5x_retrieval library 8 for implementing dual encoder models.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3499,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,we tuned some parameters based on results on the validation set.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3500,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,relevant hyperparameters for training the dual encoder are: for the t5 input we concatenated the query and truncated document text.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3501,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"the t5 output is the string ""relevant"" or ""not relevant"".",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3502,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"to classify document relevance at inference time, we applied a threshold to the probability assigned to the ""relevant"" label, which we tuned on the validation set.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3503,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"when classifying bm25 candidates we used a threshold of 0.9 and when classifying the dual encoder candidates we used a threshold of 0.95.other relevant hyperparameters for training the classifier are: we quantify the degree of missing entities by denoting recall as r a and r b , such that || = r a * |a| and |b| = r b * |b|.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3504,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"we quantify the fraction of elements in a that are also in b as r  , such that |a  b| = r  * |a|.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3505,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"for simplicity, we also assume that the overlap between a andb is such that |  b| = r a * |a  b| and | b| = r a * r b * |a  b|.derivation",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3506,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"what is the recall (r) and precision (p) of \b relative to a \ b as a function of r a , r b , and r  ?",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3507,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,we note some useful properties of pairs of sets x and y :x \,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3508,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"y = x  y c , |x \ y | = |x|  |x  y |, if x  y then x  y = x, and",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3509,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,if x  y then y c  x c .,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3510,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,p = |( \ b)| |( \b)| p = ||  |  b| ||  | b| p = r a * |a|  r,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3511,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,a * r  * |a| r a * |a|  r a * r b * r  * |a| p = r a * (1  r  ) *,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3512,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,|a| r a * (1  r b * r  ) *,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3513,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"|a| p = (1  r  ) (1  r b * r  )discussion while recall is simply equal to r a , precision is a more complicated function of r b and r  , and can be very low for large values of r  .",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3514,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"intuitively, if subtractingb from removes most of, then the precision of the resulting set will be dominated by the relevant entities missing fromb.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3515,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,this motivates limiting the intersection of the two sets used to construct queries involving set intersection.,,https://export.arxiv.org/pdf/2305.11694v2.pdf
3516,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"for example, if r b = 0.95, then with r  < 0.8, we can ensure p > 0.83.",,https://export.arxiv.org/pdf/2305.11694v2.pdf
3517,237513496,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,conclusion and future work,Insight-tree,"in this paper, we have presented a novel paradigm of training a question filter to capture the semantics of a qa system's answering capability by distilling the knowledge of the answer scores from it.",,https://www.aclanthology.org/2021.emnlp-main.583.pdf
3518,237513496,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,conclusion and future work,Insight-tree,"our experiments on three academic and one industrial qa benchmark show that the trained question models can estimate the pr/re curves of the qa system well, and can be used to effectively filter questions while only incurring a small drop in recall.",,https://www.aclanthology.org/2021.emnlp-main.583.pdf
3519,237513496,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,conclusion and future work,Insight-tree,"an interesting future work direction is to analyze the impact/behavior of the question filters in a cross-domain setting, where the training and testing corpora are from different domains.",,https://www.aclanthology.org/2021.emnlp-main.583.pdf
3520,237513496,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,conclusion and future work,Insight-tree,this would allow examining the transferability of the semantics learned by the question filters.,,https://www.aclanthology.org/2021.emnlp-main.583.pdf
3521,237513496,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,conclusion and future work,Insight-tree,"a complementary future work direction could be knowledge distillation from a sophisticated answer verification module like (rodriguez et al., 2019;kamath et al., 2020;zhang et al., 2021).",,https://www.aclanthology.org/2021.emnlp-main.583.pdf
3522,219573577,Multi-hop Reading Comprehension across Documents with Path-based Graph Convolutional Network,conclusion,Insight-tree,"in this paper, we propose a novel approach for multi-hop reading comprehension across documents.",,https://arxiv.org/pdf/2006.06478v2.pdf
3523,219573577,Multi-hop Reading Comprehension across Documents with Path-based Graph Convolutional Network,conclusion,Insight-tree,"our approach extends the entity graph by introducing reasoning entities, which can form the reasoning path from question to candidates.",,https://arxiv.org/pdf/2006.06478v2.pdf
3524,219573577,Multi-hop Reading Comprehension across Documents with Path-based Graph Convolutional Network,conclusion,Insight-tree,"in addition, our approach incorporates the question in the multi-hop reasoning through a new gate mechanism to regulate how much useful information propagating from neighbors to the node.",,https://arxiv.org/pdf/2006.06478v2.pdf
3525,219573577,Multi-hop Reading Comprehension across Documents with Path-based Graph Convolutional Network,conclusion,Insight-tree,experiments show that our approach achieves state-of-the-art accuracy both for single and ensemble models.,,https://arxiv.org/pdf/2006.06478v2.pdf
3526,246904646,A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models,conclusions,Insight-tree,"we comprehensively survey existing works about knowledgeintensive nlp with pre-trained language models and summarize the current progress in terms of the three critical components in plmkes: knowledge sources, knowledge-intensive nlp tasks, and knowledge fusion methods.",,https://arxiv.org/pdf/2202.08772v1.pdf
3527,246904646,A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models,conclusions,Insight-tree,"based on the discussion about the three components, we further pose several challenges that would be influential in the practical usage and propose the related future directions in response to the challenges.",,https://arxiv.org/pdf/2202.08772v1.pdf
3528,246904646,A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models,conclusions,Insight-tree,we hope that this paper could provide nlp practitioners with a clear picture on the topic and boost the development of the current knowledge-intensive nlp technologies.,,https://arxiv.org/pdf/2202.08772v1.pdf
3529,220445474,CopyBERT: A Unified Approach to Question Generation with Self-Attention,conclusion,Insight-tree,"we showed that having a unified encoder-decoder transformer model initialized with contextualized word embeddings and further extended with copy mechanism can already give state-of-the-art, without additional pre-training on generation tasks (dong et al., 2019).",,https://www.aclweb.org/anthology/2020.nlp4convai-1.3.pdf
3530,220445474,CopyBERT: A Unified Approach to Question Generation with Self-Attention,conclusion,Insight-tree,we also sped up the training of qg models that use bert by choosing predictions on output embeddings that are offset by one position ( 3.3).,,https://www.aclweb.org/anthology/2020.nlp4convai-1.3.pdf
3531,220445474,CopyBERT: A Unified Approach to Question Generation with Self-Attention,conclusion,Insight-tree,this work shows the significance of explicitly using self-attentions of bert like models.,,https://www.aclweb.org/anthology/2020.nlp4convai-1.3.pdf
3532,220445474,CopyBERT: A Unified Approach to Question Generation with Self-Attention,conclusion,Insight-tree,these models can further be used in other tasks such as abstractive summarization and machine translation to see qualitative improvements.,,https://www.aclweb.org/anthology/2020.nlp4convai-1.3.pdf
3533,258762903,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader,conclusions,Insight-tree,this work presents a novel mrc-style pre-training model called pmr.,,https://export.arxiv.org/pdf/2212.04755v2.pdf
3534,258762903,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader,conclusions,Insight-tree,pmr can fully resolve the learning objective and model architecture gaps that frequently appear in fine-tuning existing mlms.,,https://export.arxiv.org/pdf/2212.04755v2.pdf
3535,258762903,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader,conclusions,Insight-tree,"experimental results from multiple dimensions, including effectiveness in solving few-shot tasks and ood generalization, show the benefits of bridging the gap between pre-training and fine-tuning for span extraction tasks.",,https://export.arxiv.org/pdf/2212.04755v2.pdf
3536,258762903,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader,conclusions,Insight-tree,pmr also shows promising potential in explaining the sequence classification process and unifying nlu tasks.,,https://export.arxiv.org/pdf/2212.04755v2.pdf
3537,250562913,Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation,conclusion,Insight-tree,"in this paper, an incremental learning mrc model with uncertainty-aware fixed memory and adversarial domain adaptation, ma-mrc, is proposed for continual mrc and alleviating catastrophically forgetting.",,https://export.arxiv.org/pdf/2208.05217v1.pdf
3538,250562913,Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation,conclusion,Insight-tree,"inspired by the human learning process, there are two main ideas of ma-mrc: a memory that stores a small number of samples in previous seen domain data and always focuses on what the model most needs to replay; and adversarial learning the domain adaptation in a two-player game to learn better transfer representations between previous and current domain data.",,https://export.arxiv.org/pdf/2208.05217v1.pdf
3539,250562913,Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation,conclusion,Insight-tree,experimental results show that the proposed ma-mrc can achieve a good continuous learning performance without catastrophically forgetting under cda-c and cda-q settings.,,https://export.arxiv.org/pdf/2208.05217v1.pdf
3540,204823992,MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension,conclusions,Insight-tree,"we have presented the mrqa 2019 shared task, which focused on testing whether reading comprehension systems can generalize to examples outside of their training domain.",,https://www.aclweb.org/anthology/D19-5801.pdf
3541,204823992,MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension,conclusions,Insight-tree,"many submissions improved significantly over our baseline, and investigated a wide range of techniques.",,https://www.aclweb.org/anthology/D19-5801.pdf
3542,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"in this survey, we outlined the key aspects of trustworthiness that we think are essential to ai systems.",,https://arxiv.org/pdf/2110.01167v2.pdf
3543,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"we introduced how ai systems can be evaluated and assessed on each of these aspects, and reviewed current efforts in this direction in the industry.",,https://arxiv.org/pdf/2110.01167v2.pdf
3544,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"we further proposed a systematic approach to consider these aspects of trustworthiness in the entire lifecycle of real-world ai systems, which offers recommendations for every step of the development and use of these systems.",,https://arxiv.org/pdf/2110.01167v2.pdf
3545,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,we recognize that fully adopting this systematic approach to build trustworthy ai systems requires that practitioners embrace the concepts underlying the key aspects that we have identified.,,https://arxiv.org/pdf/2110.01167v2.pdf
3546,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"more importantly, it requires a shift of focus from performance-driven ai to trust-driven ai.",,https://arxiv.org/pdf/2110.01167v2.pdf
3547,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"in the short run, this shift will inevitably involve side-effects, such as longer learning time, slowed development, and/or increased cost to build ai systems.",,https://arxiv.org/pdf/2110.01167v2.pdf
3548,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"however, we encourage practitioners to focus on the long-term benefits of gaining the trust of all stakeholders for the sustained use and development of these systems.",,https://arxiv.org/pdf/2110.01167v2.pdf
3549,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"in this section, we conclude by discussing some of the open challenges and potential opportunities in the future development of trustworthy ai.",,https://arxiv.org/pdf/2110.01167v2.pdf
3550,238259667,Trustworthy AI: From Principles to Practices,limitations in current evaluations of trustworthiness.,Insight-tree,repeatable and quantitative measurements are the cornerstone of scientific and engineering progress.,,https://arxiv.org/pdf/2110.01167v2.pdf
3551,238259667,Trustworthy AI: From Principles to Practices,limitations in current evaluations of trustworthiness.,Insight-tree,"however, despite increasing research interest and efforts, the quantification of many aspects of ai trustworthiness remains elusive.",,https://arxiv.org/pdf/2110.01167v2.pdf
3552,238259667,Trustworthy AI: From Principles to Practices,limitations in current evaluations of trustworthiness.,Insight-tree,"of the various aspects that we have discussed in this paper, the explainability, transparency, and accountability of ai systems are still seldom evaluated quantitatively, which makes it difficult to accurately compare systems.",,https://arxiv.org/pdf/2110.01167v2.pdf
3553,238259667,Trustworthy AI: From Principles to Practices,limitations in current evaluations of trustworthiness.,Insight-tree,"developing good methods of quantitative evaluation for these desiderata, we believe, will be an important first step in research on these aspects of ai trustworthiness as a scientific endeavor, rather than a purely philosophical one.",,https://arxiv.org/pdf/2110.01167v2.pdf
3554,251224044,Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation,conclusion,Insight-tree,"in this paper, a unified framework is presented for the pre-training of sentence representation.",,https://export.arxiv.org/pdf/2208.00231v1.pdf
3555,251224044,Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation,conclusion,Insight-tree,"on top of the consecutive mae style pre-training on generic and domain-specific data, and with proper collaboration with contrastive learning, the generated model may support a wide variety of sentence representation tasks, including zero-shot retrieval, in-domain retrieval, and sentence embeddings.",,https://export.arxiv.org/pdf/2208.00231v1.pdf
3556,251224044,Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation,conclusion,Insight-tree,"the experiment studies demonstrate that the proposed framework helps to achieve strong performances on benchmarks, like beir, ms marco, natural questions, sts and transfer tasks in senteval.",,https://export.arxiv.org/pdf/2208.00231v1.pdf
3557,251224044,Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation,conclusion,Insight-tree,"for future works, we'll make extensions by scaling up the size of the encoding network and having it pre-trained on more unsupervised data, which will push to the empirical limit of the current method.",,https://export.arxiv.org/pdf/2208.00231v1.pdf
3558,251224044,Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation,conclusion,Insight-tree,"we'll also explore its effectiveness in other languages other than english, and its impact to more applications beyond dense retrieval and nli.",,https://export.arxiv.org/pdf/2208.00231v1.pdf
3559,216562224,Semantics-Aware Inferential Network for Natural Language Understanding,conclusion,Insight-tree,"this work focuses on two typical nlu tasks, machine reading comprehension and natural language inference by refining the use of semantic clues and inferential model.",,https://arxiv.org/pdf/2004.13338v1.pdf
3560,216562224,Semantics-Aware Inferential Network for Natural Language Understanding,conclusion,Insight-tree,the proposed semantics-aware inferential network (sain) is capable of taking multiple semantic structures as input of an inferential network by closely integrating semantics and reasoning steps in a creative way.,,https://arxiv.org/pdf/2004.13338v1.pdf
3561,216562224,Semantics-Aware Inferential Network for Natural Language Understanding,conclusion,Insight-tree,"experiment results on 11 benchmarks, including 4 nli tasks and 7 mrc tasks, show that our model outperforms all previous strong baselines, which consistently indicate the general effectiveness of our model 6 .",,https://arxiv.org/pdf/2004.13338v1.pdf
3562,218469981,On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections,conclusion,Insight-tree,this work is the first attempt to develop a formal framework for understanding the behavior of complex natural language reasoning in the presence of linguistic noise.,,https://arxiv.org/pdf/1901.02522v3.pdf
3563,218469981,On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections,conclusion,Insight-tree,the importance of this work is two-fold.,,https://arxiv.org/pdf/1901.02522v3.pdf
3564,218469981,On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections,conclusion,Insight-tree,"first, it proposes a novel graph-theoretic paradigm for studying reasoning, inspired by the symbol-meaning problem in the presence of redundancy, ambiguity, incompleteness, and inaccuracy of language.",,https://arxiv.org/pdf/1901.02522v3.pdf
3565,218469981,On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections,conclusion,Insight-tree,"second, it shows how to use this framework to analyze a class of reasoning algorithms.",,https://arxiv.org/pdf/1901.02522v3.pdf
3566,218469981,On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections,conclusion,Insight-tree,"we expect our findings, as well as those from future extensions to other classes of reasoning algorithms, to have important implications on how we study problems in language comprehension.",,https://arxiv.org/pdf/1901.02522v3.pdf
3567,164284,A Rewriting Logic Approach for Automatic Composition of Web Services,conclusions,Insight-tree,"in this work, we showed how the formalism of graphs can be used to improve the composition of web services and make it automatic.",,https://arxiv.org/pdf/1411.5153v1.pdf
3568,164284,A Rewriting Logic Approach for Automatic Composition of Web Services,conclusions,Insight-tree,"more precisely, we have proposed rewriting logic and its maude language as a support for a graph-based approach for automatic composition of web services.",,https://arxiv.org/pdf/1411.5153v1.pdf
3569,164284,A Rewriting Logic Approach for Automatic Composition of Web Services,conclusions,Insight-tree,the proposed model has made possible the exploration of different composition schemas as well as the formal analysis of service compositions.,,https://arxiv.org/pdf/1411.5153v1.pdf
3570,164284,A Rewriting Logic Approach for Automatic Composition of Web Services,conclusions,Insight-tree,our contribution has broadly followed two main steps:,,https://arxiv.org/pdf/1411.5153v1.pdf
3571,220483148,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,conclusion,Insight-tree,"we have presented logiqa, a large-scale logical reasoning reading comprehension dataset.",,https://arxiv.org/pdf/2007.08124v1.pdf
3572,220483148,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,conclusion,Insight-tree,"in addition to testing reasoning capacities of machine reading, our dataset can also serve as a benchmark for re-examining the long pursued research of logical ai in the deep learning nlp era.",,https://arxiv.org/pdf/2007.08124v1.pdf
3573,220483148,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,conclusion,Insight-tree,"results show that the state-of-the-art machine readers still fall far behind human performance, making our dataset one of the most challenging test for reading comprehension.",,https://arxiv.org/pdf/2007.08124v1.pdf
3574,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,"in this work, we presented k2r: a modular approach for knowledge-based dialogue models.",,https://arxiv.org/pdf/2111.05204v1.pdf
3575,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,"we showed that by decomposing the knowledge step and response generation into explicit sequence-tosequence subtasks, we could improve dialogue systems by incorporating knowledge or turning short qa model answers into an appropriate conversational form.",,https://arxiv.org/pdf/2111.05204v1.pdf
3576,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,"in detailed experiments, we showed that this modular system helps with hallucination in knowledge-grounded dialogue, is rated by humans as more knowledgeable and engaging when answering questions, and improves generation metrics on open-domain dialogue.",,https://arxiv.org/pdf/2111.05204v1.pdf
3577,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,"furthermore, it allows for more interpretable results and supports knowledge injection.",,https://arxiv.org/pdf/2111.05204v1.pdf
3578,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,future work should continue to investigate methods with modular reasoning steps to help in difficult language tasks.,,https://arxiv.org/pdf/2111.05204v1.pdf
3579,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,"a genius is a person who displays exceptional intellectual ability, creative productivity, universality in genres or originality, typically to a degree that is associated with the achievement of new advances in a domain of knowledge.",,https://arxiv.org/pdf/2111.05204v1.pdf
3580,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,gold response universality in genres or originality typically to a degree that is associated with achievements bart a genius has a high iq.,,https://arxiv.org/pdf/2111.05204v1.pdf
3581,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,we systematically compare the extractive and generative readers for qa tasks.,,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3582,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,two sets of experiments are designed to control the effects of different prlms and the size of models.,,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3583,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"by conducting experiments on 12 qa datasets, our findings provide guidelines on how to choose extractive or generative readers given their strength and weakness.",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3584,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"while current work investigates the pros and cons of extractive and generative models systematically, there are some hyperparameters that might affect the model performance.",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3585,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"for example, it is known that different prompts in the input effect generative model performance (mishra et al., 2021b,a).",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3586,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"also, it is worth studying the ood performance of models deeply.",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3587,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,gokhale et al.,,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3588,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"(2022) compares multiple ways to improve the ood performance of an extractive model on qa task, and how these methods affect generative models have not been well-studied yet.",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3589,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"meanwhile, most of the work including this work evaluate ood performance by averaging the performance across multiple dataset, but as mentioned in (mishra et al., 2020), the evaluation should be more carefully designed.",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3590,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"also, diagnosing the performance on each ood dataset can provide more insights.",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3591,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"for example, why models perform better on bioasq dataset than most other datasets (see table 4",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
3592,247187611,"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",conclusion,Insight-tree,selective prediction ability is crucial for nlp systems to be reliably deployed in real-world applications and we presented the most systematic study of existing selective prediction approaches.,,https://www.aclanthology.org/2022.findings-acl.158.pdf
3593,247187611,"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",conclusion,Insight-tree,"our study involved experiments in iid, ood, and adv settings with 17 datasets across several nlp tasks.",,https://www.aclanthology.org/2022.findings-acl.158.pdf
3594,247187611,"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",conclusion,Insight-tree,"we showed that despite leveraging additional resources (held-out data/computation), existing approaches fail to consistently and considerably outperform the simplest baseline (maxprob",,https://www.aclanthology.org/2022.findings-acl.158.pdf
3595,247292088,Question-Answer Sentence Graph for Joint Modeling Answer Selection,conclusions,Insight-tree,"to our knowledge, our model is the first graphbased approach for jointly modeling sentence-level semantics of question-answer pairs for as2 as an offline processing application, such as those required by community qa, forums, etc.",,https://www.aclanthology.org/2023.eacl-main.68.pdf
3596,247292088,Question-Answer Sentence Graph for Joint Modeling Answer Selection,conclusions,Insight-tree,"this is different from previous methods using graphs, e.g., multihop or graph-based qa, which mainly model semantics via entities.",,https://www.aclanthology.org/2023.eacl-main.68.pdf
3597,247292088,Question-Answer Sentence Graph for Joint Modeling Answer Selection,conclusions,Insight-tree,"our approach builds query-specific small-scale training graphs for offline learning, through (q, a) pairs as nodes, and edges encoding relations between members of pairs to capture both supporting question-question, and answer-answer dependencies.",,https://www.aclanthology.org/2023.eacl-main.68.pdf
3598,247292088,Question-Answer Sentence Graph for Joint Modeling Answer Selection,conclusions,Insight-tree,"further, we demonstrate that our approach achieves significant performance gains over existing sota models on as2 for metrics of p@1, map, and mrr.",,https://www.aclanthology.org/2023.eacl-main.68.pdf
3599,203610361,Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks,conclusion,Insight-tree,"in this paper, we investigated the role played by interlinked sentence representation for complex, multi-hop question answering under the focus of supporting facts identification, i.e. retrieving the minimum set of facts required to answer a given question.",,https://www.aclweb.org/anthology/D19-5306.pdf
3600,203610361,Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks,conclusion,Insight-tree,"we emphasise that this problem is worth pursuing, showing that the performance of stateof-the-art models substantially deteriorates as the size of the accompanying context increases.",,https://www.aclweb.org/anthology/D19-5306.pdf
3601,254097925,Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering,conclusion,Insight-tree,open-domain and multi-hop qa is an important problem for both humans and computers.,,https://export.arxiv.org/pdf/2212.03296v1.pdf
3602,254097925,Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering,conclusion,Insight-tree,"to compare how humans and computers search and answer complex questions, our interface collects human question answering data as agents search with traditional and neural search engines alongside question answering models that suggest queries and answers.",,https://export.arxiv.org/pdf/2212.03296v1.pdf
3603,254097925,Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering,conclusion,Insight-tree,"humans often use shorter queries, apply dynamic search chains, and use world knowledge.",,https://export.arxiv.org/pdf/2212.03296v1.pdf
3604,254097925,Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering,conclusion,Insight-tree,"future qa models should have the ability to generate novel queries, ""discard"" irrelevant results, and explicitly check answers.",,https://export.arxiv.org/pdf/2212.03296v1.pdf
3605,254097925,Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering,conclusion,Insight-tree,"moreover, computer agents for qa should also be able to use diverse retrievers to find evidence to answer questions, learning from the insights found in human data.",,https://export.arxiv.org/pdf/2212.03296v1.pdf
3606,254097925,Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering,conclusion,Insight-tree,"with an agent trained on our data, we could have the ""best of both worlds"" to combine the ingenuity and tacit knowledge of humans with an indefatigable agent with access to all the world's information.",,https://export.arxiv.org/pdf/2212.03296v1.pdf
3607,259095666,Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks,conclusion,Insight-tree,"we show that composition of memorized world knowledge can be triggered in lms with up to 11b parameters (t5-xxl) to a desirable extent by leveraging training signal from random walks over structured knowledge using approaches based on prompt-tuning (lester et al., 2021).",,https://export.arxiv.org/pdf/2306.04009v1.pdf
3608,259095666,Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks,conclusion,Insight-tree,"doing so leads to substantial improvements in the lms' ability to answer 2-hop questions, even beyond standard, full model fine-tuning.",,https://export.arxiv.org/pdf/2306.04009v1.pdf
3609,245131402,GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval,conclusion,Insight-tree,"in this work we propose gpl, a novel unsupervised domain adaptation method for dense retrieval models.",,https://www.aclanthology.org/2022.naacl-main.168.pdf
3610,245131402,GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval,conclusion,Insight-tree,it generates queries for a target corpus and pseudo labels these with a cross-encoders.,,https://www.aclanthology.org/2022.naacl-main.168.pdf
3611,245131402,GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval,conclusion,Insight-tree,pseudolabeling overcomes two important short-comings of previous methods: not all generated queries are of high quality and pseudo-labels efficiently detects those.,,https://www.aclanthology.org/2022.naacl-main.168.pdf
3612,245131402,GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval,conclusion,Insight-tree,"further, training with mined hard negatives is possible as the pseudo labels performs efficient denoising.",,https://www.aclanthology.org/2022.naacl-main.168.pdf
3613,237593105,NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset,conclusion,Insight-tree,"in this work, we present a new qa datasets with complex numerical questions and interpretable reasoning graph.",,https://arxiv.org/pdf/2109.10604v2.pdf
3614,237593105,NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset,conclusion,Insight-tree,we also introduce an automatic evaluation metric for the generated reasoning process.,,https://arxiv.org/pdf/2109.10604v2.pdf
3615,237593105,NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset,conclusion,Insight-tree,we finally present an initial model producing the reasoning process while answering questions.,,https://arxiv.org/pdf/2109.10604v2.pdf
3616,237593105,NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset,conclusion,Insight-tree,the experiments show that noahqa is challenging and will become an interesting direction in both numerical qa and explainable qa.,,https://arxiv.org/pdf/2109.10604v2.pdf
3617,234742165,FEW-NERD: A Few-shot Named Entity Recognition Dataset,conclusion and future work,Insight-tree,"we propose few-nerd, a large-scale few-shot ner dataset with fine-grained entity types.",,https://www.aclanthology.org/2021.acl-long.248.pdf
3618,234742165,FEW-NERD: A Few-shot Named Entity Recognition Dataset,conclusion and future work,Insight-tree,this is the first few-shot ner dataset and also one of the largest human-annotated ner dataset.,,https://www.aclanthology.org/2021.acl-long.248.pdf
3619,234742165,FEW-NERD: A Few-shot Named Entity Recognition Dataset,conclusion and future work,Insight-tree,few-nerd provides three unified benchmarks to assess approaches of few-shot ner and could facilitate future research in this area.,,https://www.aclanthology.org/2021.acl-long.248.pdf
3620,234742165,FEW-NERD: A Few-shot Named Entity Recognition Dataset,conclusion and future work,Insight-tree,"by implementing state-of-the-art methods, we carry out a series of experiments on few-nerd, demonstrating that few-shot ner remains a challenging problem and worth exploring.",,https://www.aclanthology.org/2021.acl-long.248.pdf
3621,234742165,FEW-NERD: A Few-shot Named Entity Recognition Dataset,conclusion and future work,Insight-tree,"in the future, we will extend few-nerd by adding cross-domain annotations, distant annotations, and finer-grained entity types.",,https://www.aclanthology.org/2021.acl-long.248.pdf
3622,234742165,FEW-NERD: A Few-shot Named Entity Recognition Dataset,conclusion and future work,Insight-tree,few-nerd also has the potential to advance the construction of continual knowledge graphs.,,https://www.aclanthology.org/2021.acl-long.248.pdf
3623,211146419,Conditional Self-Attention for Query-based Summarization,conclusion,Insight-tree,this paper introduces conditional self-attention (csa) and its variants as versatile and plugin mod-ules for conditional contextual dependency modeling.,,https://arxiv.org/pdf/2002.07338v1.pdf
3624,211146419,Conditional Self-Attention for Query-based Summarization,conclusion,Insight-tree,we develop an attention-only neural network built from csa and transformer for querybased summarization.,,https://arxiv.org/pdf/2002.07338v1.pdf
3625,211146419,Conditional Self-Attention for Query-based Summarization,conclusion,Insight-tree,it consistently outperforms vanilla transformer and other baselines for abstractive and extractive qsumm tasks on debatepedia and hotpotqa datasets.,,https://arxiv.org/pdf/2002.07338v1.pdf
3626,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,"in this work, we propose test-time learning (ttl) as a new framework for unsupervised extractive question answering (eqa).",,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3627,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,we present four variants of ttl with a simple but effective context expansion method.,,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3628,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,"we utilize four question-answer pair generation methods for eqa and propose using qa-srl as an additional source of qa pairs, to supplement prior methods.",,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3629,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,"we show ttl enables ""understanding"" of contexts at test-time, without human-authored annotations, and significantly improves eqa, including low parameter models.",,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3630,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,we envision ttl as a framework that can direct work in reading comprehension to be viewed as a problem of ever-evolving datasets instead of a static corpus.,,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3631,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,"natural language itself undergoes continuous evolution (gentner and france, 1988;traugott and dasher, 2001;hamilton et al., 2016) via changes in preference for syntactical structures; creation of new words and phrases; and changing usage frequencies and semantics for existing words.",,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3632,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,ttl can potentially be applied to such scenarios with semantic drift or domain shift.,,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3633,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,"further improvements w.r.t. selection of similar contexts for k-neighbor ttl could be explored by leveraging hard sample selection, hard negative mining, bootstrapping, and contrastive learning, along with improved currculum strategies.",,https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
3634,246823260,QA4QG: USING QUESTION ANSWERING TO CONSTRAIN MULTI-HOP QUESTION GENERATION,conclusion,Insight-tree,"in this paper, we propose a novel framework, qa4qg, a qaaugmented bart-based framework for mqg.",,https://arxiv.org/pdf/2202.06538v1.pdf
3635,246823260,QA4QG: USING QUESTION ANSWERING TO CONSTRAIN MULTI-HOP QUESTION GENERATION,conclusion,Insight-tree,it is the first work to explore large pre-trained language models for mqg and takes advantage of an additional multi-hop qa module to further constrain the question generation.,,https://arxiv.org/pdf/2202.06538v1.pdf
3636,246823260,QA4QG: USING QUESTION ANSWERING TO CONSTRAIN MULTI-HOP QUESTION GENERATION,conclusion,Insight-tree,"our results on the hotpotqa dataset show that qa4qg outperforms all state-of-the-art models, with an increase of 8 bleu-4 and 8 rouge points compared to the best results previously reported.",,https://arxiv.org/pdf/2202.06538v1.pdf
3637,246823260,QA4QG: USING QUESTION ANSWERING TO CONSTRAIN MULTI-HOP QUESTION GENERATION,conclusion,Insight-tree,our work suggests the advantage of introducing pre-trained language models and qa modules for the mqg task.,,https://arxiv.org/pdf/2202.06538v1.pdf
3638,233219869,QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering,conclusion,Insight-tree,"we presented qa-gnn, an end-to-end question answering model that leverages lms and kgs.",,https://www.aclweb.org/anthology/2021.naacl-main.45.pdf
3639,247476141,Hyperdecoders: Instance-specific decoders for multi-task NLP,conclusion,Insight-tree,we propose a novel method for generating adapters conditioned on a model's input and show that this improves performance in multi-task settings across a variety of tasks.,,https://export.arxiv.org/pdf/2203.08304v3.pdf
3640,247476141,Hyperdecoders: Instance-specific decoders for multi-task NLP,conclusion,Insight-tree,"we explore the effectiveness of our approach for sequence classification, qa, and summarisation tasks, and find that it often outperforms strong parameter-efficient baselines.",,https://export.arxiv.org/pdf/2203.08304v3.pdf
3641,247476141,Hyperdecoders: Instance-specific decoders for multi-task NLP,conclusion,Insight-tree,future work could examine applying our approach to other architectures (e.g. decoder-only models) or explore the tradeoffs between shared and generated parameters across different layers.,,https://export.arxiv.org/pdf/2203.08304v3.pdf
3642,247476141,Hyperdecoders: Instance-specific decoders for multi-task NLP,conclusion,Insight-tree,"an analysis of our approach suggests the primary benefits come from improved control of the encoder over the decoder, enhancing the effects of positive transfer from the shared encoder.",,https://export.arxiv.org/pdf/2203.08304v3.pdf
3643,247476141,Hyperdecoders: Instance-specific decoders for multi-task NLP,conclusion,Insight-tree,this allows our approach to efficiently adapt a pretrained language model to multiple tasks unseen during pretraining while still benefiting strongly from positive transfer.,,https://export.arxiv.org/pdf/2203.08304v3.pdf
3644,219124082,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,conclusion,Insight-tree,"we present a structured survey of methods that reveal heuristics and spurious correlations in datasets, methods which show that neural models inherit those correlations or assess their capabilities otherwise, and methods that mitigate this by adversarial training, data augmentation and model architecture or training procedure improvements.",,
3645,219124082,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,conclusion,Insight-tree,"various nli datasets are reported to contain spurious correlations between input and expected output, might be unsuitable to evaluate some task modality due to dataset design or suffer from quality issues.",,
3646,219124082,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,conclusion,Insight-tree,rte is a popular target task for these data-centred investigations with more than half of the surveyed papers focusing on it.,,
3647,219124082,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,conclusion,Insight-tree,"nli models, in turn, are shown to exploit those correlations and to rely on superficial lexical cues.",,
3648,219124082,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,conclusion,Insight-tree,"furthermore, they lack generalisation beyond the evaluation set resulting in poor performance on out-of-distribution evaluation sets, generated adversarially or targeted at a specific capability.",,
3649,219124082,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,conclusion,Insight-tree,"efforts to achieve robustness include augmenting the training data with adversarial examples, making use of external resources and modifying the neural network architecture or training objective.",,
3650,254877586,When Do Decompositions Help for Machine Reading?,conclusion,Insight-tree,our work explored when decompositions are helpful for machine reading.,,https://export.arxiv.org/pdf/2212.10019v1.pdf
3651,254877586,When Do Decompositions Help for Machine Reading?,conclusion,Insight-tree,"we showed that decompositions are helpful when there is limited data available, or when parameters cannot be tuned.",,https://export.arxiv.org/pdf/2212.10019v1.pdf
3652,254877586,When Do Decompositions Help for Machine Reading?,conclusion,Insight-tree,"however, when enough data exists (empirically around a few hundred instances) and parameters can be fine-tuned, it is best to let the model learn the decompositions implicitly through end-to-end training.",,https://export.arxiv.org/pdf/2212.10019v1.pdf
3653,254877586,When Do Decompositions Help for Machine Reading?,conclusion,Insight-tree,we hope that our work will help to inform readers as they create new datasets and select methods to use for complex question answering.,,https://export.arxiv.org/pdf/2212.10019v1.pdf
3654,252531182,Evaluation of Question Answering Systems: Complexity of judging a natural language,conclusion,Insight-tree,the question-answering task is among the oldest challenges in artificial intelligence and it is still one of the most important tasks in natural language processing to this day as it enables humans to interact with a machine in a natural way.,,https://export.arxiv.org/pdf/2209.12617v1.pdf
3655,252531182,Evaluation of Question Answering Systems: Complexity of judging a natural language,conclusion,Insight-tree,"for dealing with domain-specific information and different formats of data which could be either structured or unstructured data there are three main paradigms for constructing qa systems: (1) information retrieval-based question answering, (2) knowledge base question answering, and (3) generative question answering.",,https://export.arxiv.org/pdf/2209.12617v1.pdf
3656,247187956,Semantic Sentence Composition Reasoning for Multi-Hop Question Answering,conclusion,Insight-tree,"to improve the retrieval results and multi-hop reasoning in question answering, we present a multistage semantic matching module and a factual sentences composition module, respectively.",,https://arxiv.org/pdf/2203.00160v1.pdf
3657,247187956,Semantic Sentence Composition Reasoning for Multi-Hop Question Answering,conclusion,Insight-tree,experimental results fully demonstrate our modules outperform the standard ir system and two-step ir method.,,https://arxiv.org/pdf/2203.00160v1.pdf
3658,247187956,Semantic Sentence Composition Reasoning for Multi-Hop Question Answering,conclusion,Insight-tree,"we first combine the semantic sentence composition module with multi-stage semantic matching module, and it is suitable for any open-domain qa architecture.",,https://arxiv.org/pdf/2203.00160v1.pdf
3659,247187956,Semantic Sentence Composition Reasoning for Multi-Hop Question Answering,conclusion,Insight-tree,further exploration will be done on the other question answering datasets with our proposed modules.,,https://arxiv.org/pdf/2203.00160v1.pdf
3660,14758721,Computational Design of a DNA-and Fc-Binding Fusion Protein,conclusion,Insight-tree,we have applied multi-objective optimization guided by directed evolution to combine the myod dna-binding motif into the z domain conserving the scaffolds structure.,,
3661,14758721,Computational Design of a DNA-and Fc-Binding Fusion Protein,conclusion,Insight-tree,"simulations showed that the optimization of the sequences based on hydrophobicity, molecular weight, and secondary structure predictions improved structural stability while maintaining protein functionality.",,
3662,14758721,Computational Design of a DNA-and Fc-Binding Fusion Protein,conclusion,Insight-tree,"the use of simple fitness functions reduces the optimization complexity, and thus allows to optimize more individuals over more generations resulting in a better sampling of the sequence space.",,
3663,240354203,Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge,conclusion,Insight-tree,"in this paper, we proposed an unsupervised mcqa method, which exploits the pseudo labels generated by some basic rules or external non-mcqa datasets.",,https://www.aclanthology.org/2021.mrqa-1.12.pdf
3664,240354203,Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge,conclusion,Insight-tree,the proposed method significantly outperforms the baseline approaches on race and is even comparable with the supervised learning performance on mc500.,,https://www.aclanthology.org/2021.mrqa-1.12.pdf
3665,240354203,Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge,conclusion,Insight-tree,we hope this paper sheds light on unsupervised learning in nlp tasks.,,https://www.aclanthology.org/2021.mrqa-1.12.pdf
3666,252222436,Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?,conclusion and future work,Insight-tree,"in this work, we proposed a progressive pre-training strategy for adhoc retrieval, which consists of three hyperlink-based pre-training tasks, namely hyperlink prediction, symmetric hyperlink prediction, and most relevant document selection.",,https://export.arxiv.org/pdf/2209.06583v1.pdf
3667,252222436,Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?,conclusion and future work,Insight-tree,"these pre-training tasks can simulate different stages of a retrieval process, and gradually enhance the model's performance on relevance modeling.",,https://export.arxiv.org/pdf/2209.06583v1.pdf
3668,252222436,Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?,conclusion and future work,Insight-tree,"compared with existing methods, our strategy can make better use of utilize the hyperlink, which reveal the great potential of hyperlink in learning relevance modeling.",,https://export.arxiv.org/pdf/2209.06583v1.pdf
3669,252222436,Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?,conclusion and future work,Insight-tree,the experimental results demonstrated the effectiveness of our model.,,https://export.arxiv.org/pdf/2209.06583v1.pdf
3670,252222436,Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?,conclusion and future work,Insight-tree,"in the future, we plan to integrate our method with other pre-training objectives (such as rop), which may bring further improvement.",,https://export.arxiv.org/pdf/2209.06583v1.pdf
3671,203836061,Multi-hop Question Answering via Reasoning Chains,discussion and conclusion,Insight-tree,"in this work, we learn to extract reasoning chains to answer multi-hop reasoning questions.",,https://arxiv.org/pdf/1910.02610v2.pdf
3672,203836061,Multi-hop Question Answering via Reasoning Chains,discussion and conclusion,Insight-tree,"experimental results show that the chains are as effective as human annotations, and achieve strong performance on two large datasets.",,https://arxiv.org/pdf/1910.02610v2.pdf
3673,203836061,Multi-hop Question Answering via Reasoning Chains,discussion and conclusion,Insight-tree,"however, as remarked in past work (chen and durrett, 2019;min et al., 2019a), there are several aspects of hot-potqa and wikihop which make them require multi-hop reasoning less strongly than they otherwise might.",,https://arxiv.org/pdf/1910.02610v2.pdf
3674,203836061,Multi-hop Question Answering via Reasoning Chains,discussion and conclusion,Insight-tree,"as more challenging qa datasets are built based on lessons learned from these, we feel that reasoning in a more explicit way and properties of chain-like representations will be critical.",,https://arxiv.org/pdf/1910.02610v2.pdf
3675,203836061,Multi-hop Question Answering via Reasoning Chains,discussion and conclusion,Insight-tree,this work represents a first step towards this goal of improving qa systems in such settings.,,https://arxiv.org/pdf/1910.02610v2.pdf
3676,257687445,Modular Retrieval for Generalization and Interpretation,conclusion,Insight-tree,"this paper formally defines a new information retrieval paradigm, modular retrieval, which aims to bring the benefits of modular learning to information retrieval tasks and increase the interpretability of the retrieval process.",,https://export.arxiv.org/pdf/2303.13419v1.pdf
3677,257687445,Modular Retrieval for Generalization and Interpretation,conclusion,Insight-tree,"based on the proposed retrieval paradigm, we propose a novel modular retrieval method remop, which utilizes three modular operations to flexibly combine retrieval modules to perform zero-shot retrieval tasks, with high interpretability and generalization ability.",,https://export.arxiv.org/pdf/2303.13419v1.pdf
3678,257687445,Modular Retrieval for Generalization and Interpretation,conclusion,Insight-tree,"we explore and verify the effectiveness of the proposed module arithmetic, and experiments on a zero-shot retrieval benchmark show that remop has comparable performance to the fine-tuned model in zero-shot retrieval.",,https://export.arxiv.org/pdf/2303.13419v1.pdf
3679,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,"we present hover, a fact extraction and verification dataset requiring evidence retrieval from as many as four wikipedia articles that form reasoning graphs of diverse shapes.",,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3680,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,"we show that the performance of existing state-of-the-art models degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of robust many-hop reasoning in achieving strong results.",,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3681,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,we hope that hover will encourage the development of models capable of performing complex many-hop reasoning in the tasks of information retrieval and verification.,,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3682,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,"document retrieval, sentence selection, and claim verification.",,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3683,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,the fine-tuning is done with a batch size of 16 and the default learning rate of 5e-5 without warmup.,,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3684,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,"we set k r = 20, k p = 5,  p = 0.5, and  s = 0.3 based on the memory limit and the dev set performance.",,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3685,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,we select our system with the best dev-set verification accuracy and report its scores on the hidden test set.,,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3686,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,the entire pipeline is visualized in fig.,,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3687,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,"2. for document retrieval and sentence selection tasks, we fine-tune the bert on 4 nvidia v100 gpus for 3 epochs.",,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3688,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,the training of both tasks takes around 1 hour.,,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3689,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,"for claim verification task, we fine-tune the bert on a single nvidia v100 for 3 epochs.",,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3690,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,the training finishes in 30 minutes.,,https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
3691,252283929,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",conclusion,Insight-tree,making progress towards trustworthy nlp models requires specific definitions for the behavior expected of these models in different situations.,,https://www.aclanthology.org/2022.coling-1.8.pdf
3692,252283929,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",conclusion,Insight-tree,we propose a framework for rc model analysis that involves: (a) the definition of the expected 'reasoning' steps; (b) analysis of model behavior.,,https://www.aclanthology.org/2022.coling-1.8.pdf
3693,252283929,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",conclusion,Insight-tree,"we contribute such definitions for two linguistic 'skills' (comparison and coreference resolution), and use parallel explainability techniques to investigate whether rc models based on bert family encoders answer such questions correctly for the right reasons.",,https://www.aclanthology.org/2022.coling-1.8.pdf
3694,252283929,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",conclusion,Insight-tree,"we find that to be the case for comparison, but not for coreference.",,https://www.aclanthology.org/2022.coling-1.8.pdf
3695,252283929,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",conclusion,Insight-tree,"moreover, we find that, even for comparison, the models 'break' when encountering out-of-distribution counterfactual perturbations, suggesting that they memorize specific lexical patterns rather than learn more general reasoning 'skills'.",,https://www.aclanthology.org/2022.coling-1.8.pdf
3696,252283929,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",conclusion,Insight-tree,"as such, more research is needed on developing definitions and tests for specific 'skills' expected of nlu models, as well as on more faithful interpretability techniques.",,https://www.aclanthology.org/2022.coling-1.8.pdf
3697,253018998,Inferring Implicit Relations in Complex Questions with Language Models,conclusion,Insight-tree,"we propose the task of implicit relation inference, which decouples inference of reasoning steps from their execution.",,https://export.arxiv.org/pdf/2204.13778v2.pdf
3698,253018998,Inferring Implicit Relations in Complex Questions with Language Models,conclusion,Insight-tree,"we introduce implicitrela-tions, a benchmark that includes more than 2,000 annotated implicit relations.",,https://export.arxiv.org/pdf/2204.13778v2.pdf
3699,253018998,Inferring Implicit Relations in Complex Questions with Language Models,conclusion,Insight-tree,"we show large lms can infer implicit relations across multiple types of questions and reasoning skills, but this success does not translate to an improvement in answering implicit reasoning questions.",,https://export.arxiv.org/pdf/2204.13778v2.pdf
3700,253018998,Inferring Implicit Relations in Complex Questions with Language Models,conclusion,Insight-tree,"our work sheds light on capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for improving the ability of models to infer implicit relations.",,https://export.arxiv.org/pdf/2204.13778v2.pdf
3701,252383151,Activity report analysis with automatic single or multi span answer extraction,conclusion,Insight-tree,"we present a simple multispan architecture, mseqa, for multi type question answering by classifying each sentence as a probable answer candidate.",,https://export.arxiv.org/pdf/2209.09316v1.pdf
3702,252383151,Activity report analysis with automatic single or multi span answer extraction,conclusion,Insight-tree,"we show that when a combination of single/multi-span classifier with multispan tagging is used, the model provides robust answers for multi-span tasks without degrading its performance on single-span questions.",,https://export.arxiv.org/pdf/2209.09316v1.pdf
3703,252383151,Activity report analysis with automatic single or multi span answer extraction,conclusion,Insight-tree,"as future work, we would like to further process selected sentences from the multi-span tagger and consolidate them into one fluent answer.",,https://export.arxiv.org/pdf/2209.09316v1.pdf
3704,252383151,Activity report analysis with automatic single or multi span answer extraction,conclusion,Insight-tree,we also plan to explore ways to put this question answering capability onto edge devices for various applications.,,https://export.arxiv.org/pdf/2209.09316v1.pdf
3705,202539031,Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases,conclusion,Insight-tree,our key contribution is a method of using human knowledge about what methods will not generalize well to improve model robustness to domain-shift.,,https://www.aclweb.org/anthology/D19-1418.pdf
3706,202539031,Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases,conclusion,Insight-tree,"our approach is to train a robust model in an ensemble with a pre-trained naive model, and then use the robust model alone at test time.",,https://www.aclweb.org/anthology/D19-1418.pdf
3707,202539031,Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases,conclusion,Insight-tree,"extensive experiments show that our method works well on two adversarial datasets, and two changing-prior datasets, including a 12 point gain on vqa-cp. future work includes learning to automatically detect dataset bias, which would allow our method to be applicable with less specific prior knowledge.",,https://www.aclweb.org/anthology/D19-1418.pdf
3708,249062996,LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA,conclusion,Insight-tree,"this work introduces lepus, a method to perform unsupervised re-ranking of multi-document paths for question answering based on large language models.",,https://arxiv.org/pdf/2205.12650v1.pdf
3709,249062996,LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA,conclusion,Insight-tree,"given a question, the document path is encoded into a prompt and the document path is scored as the probability of generating the question given the prompt.",,https://arxiv.org/pdf/2205.12650v1.pdf
3710,249062996,LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA,conclusion,Insight-tree,"experiments on a standard multihop qa benchmark show the strong performance of lepus in the zero-shot setting, displaying comparable performance to fully-supervised retrievers.",,https://arxiv.org/pdf/2205.12650v1.pdf
3711,249062996,LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA,conclusion,Insight-tree,we also analyze our approach showing the utility of using multi-hop prompts as opposed to singlehop ones.,,https://arxiv.org/pdf/2205.12650v1.pdf
3712,249062996,LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA,conclusion,Insight-tree,"lastly, our work shows that language models can indeed function as strong unsupervised re-rankers for multi-hop question answering.",,https://arxiv.org/pdf/2205.12650v1.pdf
3713,258865498,Pre-training Language Models for Comparative Reasoning,conclusion,Insight-tree,"in this paper, we presented a novel framework for pre-training language models for comparative rea-d 1 : airsoft is a team game in which participants eliminate opposing players by tagging them out of play with spherical plastic projectiles shot with mock air weapons called airsoft guns.",,https://export.arxiv.org/pdf/2305.14457v1.pdf
3714,258865498,Pre-training Language Models for Comparative Reasoning,conclusion,Insight-tree,...,,https://export.arxiv.org/pdf/2305.14457v1.pdf
3715,258865498,Pre-training Language Models for Comparative Reasoning,conclusion,Insight-tree,(446 words left) d 2 : paintball is a competitive team shooting sport in which players eliminate opponents from play by hitting them with spherical dye-filled gelatin capsules called paintballs that break upon impact.,,https://export.arxiv.org/pdf/2305.14457v1.pdf
3716,258865498,Pre-training Language Models for Comparative Reasoning,conclusion,Insight-tree,...,,https://export.arxiv.org/pdf/2305.14457v1.pdf
3717,258865498,Pre-training Language Models for Comparative Reasoning,conclusion,Insight-tree,(472 words left) gold: airsoft is a popular combat simulation game where participants are eliminated when hit by pellets launched from guns that resemble real firearms.,,https://export.arxiv.org/pdf/2305.14457v1.pdf
3718,258865498,Pre-training Language Models for Comparative Reasoning,conclusion,Insight-tree,in paintball participants try to hit each other with paintballs launched from a special paintball marker/gun.,,https://export.arxiv.org/pdf/2305.14457v1.pdf
3719,258865498,Pre-training Language Models for Comparative Reasoning,conclusion,Insight-tree,"while airsoft is cheaper and provides a more realistic warfare experience, paintball is more popular, more organized and has larger events.",,https://export.arxiv.org/pdf/2305.14457v1.pdf
3720,258833025,Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases,conclusions,Insight-tree,"in this paper, we introduce chain of knowledge (cok), a novel groundbreaking framework designed to enhance the factual correctness of large language models (llms) through the integration of structured knowledge bases (kbs).",,https://export.arxiv.org/pdf/2305.13269v1.pdf
3721,258833025,Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases,conclusions,Insight-tree,we address the significant challenge of improving the accuracy of knowledge-base queries by proposing a query generator model with contrastive instruction tuning.,,https://export.arxiv.org/pdf/2305.13269v1.pdf
3722,258833025,Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases,conclusions,Insight-tree,"our experimental results on knowledgeintensive tasks demonstrate the substantial improvements achieved by cok, including enhanced factual correctness of llms and reduced hallucination.",,https://export.arxiv.org/pdf/2305.13269v1.pdf
3723,258833025,Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases,conclusions,Insight-tree,the results obtained through the implementation of cok underscore the significance of leveraging structured knowledge bases in language models.,,https://export.arxiv.org/pdf/2305.13269v1.pdf
3724,258833025,Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases,conclusions,Insight-tree,"as future research progresses in this direction, we anticipate further advancements in factual correctness and information reliability, thus paving the way for more trustworthy and accurate language models that can effectively cater to the needs of diverse users and applications.",,https://export.arxiv.org/pdf/2305.13269v1.pdf
3725,237048095,A Dataset for Answering Time-Sensitive Questions,conclusion,Insight-tree,"though time-sensitive facts are pervasive in our daily text corpus, there has been little prior work exploring this direction.",,https://arxiv.org/pdf/2108.06314v5.pdf
3726,237048095,A Dataset for Answering Time-Sensitive Questions,conclusion,Insight-tree,"in this paper, we build the first dataset to investigate whether existing models can understand time-sensitive facts.",,https://arxiv.org/pdf/2108.06314v5.pdf
3727,237048095,A Dataset for Answering Time-Sensitive Questions,conclusion,Insight-tree,our experiments show that the sota models are still lagged behind humans in temporal reasoning.,,https://arxiv.org/pdf/2108.06314v5.pdf
3728,237048095,A Dataset for Answering Time-Sensitive Questions,conclusion,Insight-tree,"in order to empower the future nlp models to understand temporal information, different temporal-aware models need to be proposed.",,https://arxiv.org/pdf/2108.06314v5.pdf
3729,237048095,A Dataset for Answering Time-Sensitive Questions,conclusion,Insight-tree,"finally, this paper opens up new research directions for better modeling temporal information in text representations.",,https://arxiv.org/pdf/2108.06314v5.pdf
3730,252762395,REAC T : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS,conclusion,Insight-tree,we have proposed react -a simple yet effective method for synergizing reasoning and acting in large language models.,,https://export.arxiv.org/pdf/2210.03629v3.pdf
3731,252762395,REAC T : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS,conclusion,Insight-tree,"through a diverse set of experiments on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that react leads to superior performance with interpretable decision traces.",,https://export.arxiv.org/pdf/2210.03629v3.pdf
3732,252762395,REAC T : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS,conclusion,Insight-tree,"despite the simplicity of our method, complex tasks with large action spaces require more demonstrations to learn well, which unfortunately can easily go beyond the input length limit of in-context learning.",,https://export.arxiv.org/pdf/2210.03629v3.pdf
3733,252762395,REAC T : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS,conclusion,Insight-tree,"we explore the fine-tuning approach on hotpotqa with initial promising results, but learning from more high-quality human annotations will be the desiderata to further improve the performance.",,https://export.arxiv.org/pdf/2210.03629v3.pdf
3734,252762395,REAC T : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS,conclusion,Insight-tree,scaling up react with multi-task training and combining it with complementary paradigms like reinforcement learning could result in stronger agents that further unlock the potential of llms for more applications.,,https://export.arxiv.org/pdf/2210.03629v3.pdf
3735,258461053,Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA,conclusion and discussion,Insight-tree,"in this work, we formulated 'post-abstention', a task that allows re-attempting the abstained instances of the given selective prediction system with the aim of increasing its coverage without significantly sacrificing the accuracy.",,https://export.arxiv.org/pdf/2305.01812v1.pdf
3736,258461053,Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA,conclusion and discussion,Insight-tree,we also explored several baseline methods for this task.,,https://export.arxiv.org/pdf/2305.01812v1.pdf
3737,258461053,Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA,conclusion and discussion,Insight-tree,"through comprehensive experiments on 11 qa datasets, we showed that these methods lead to considerable performance improvements in both in-domain and out-of-domain settings.",,https://export.arxiv.org/pdf/2305.01812v1.pdf
3738,258461053,Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA,conclusion and discussion,Insight-tree,we further performed a thorough analysis that resulted in several interesting findings.,,https://export.arxiv.org/pdf/2305.01812v1.pdf
3739,255546131,ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,conclusion,Insight-tree,"in this work, we propose a simple method to improve task-agnostic distillation generalization by leveraging mtl.",,https://export.arxiv.org/pdf/2301.03416v1.pdf
3740,255546131,ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,conclusion,Insight-tree,the teacher is first augmented by mtl and then distilled.,,https://export.arxiv.org/pdf/2301.03416v1.pdf
3741,255546131,ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,conclusion,Insight-tree,"empirical results show that our method outperforms several baselines on in-domain, out-domain and low-resource tasks.",,https://export.arxiv.org/pdf/2301.03416v1.pdf
3742,255546131,ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,limitation,Insight-tree,"compared with vanilla task-agnostic distillation, mitkd has an additional multi-task finetuning stage which may require additional computation resources.    ",,https://export.arxiv.org/pdf/2301.03416v1.pdf
3743,225066758,Unsupervised Multi-hop Question Answering by Question Generation,conclusion and future works,Insight-tree,"in this work, we study unsupervised multi-hop qa and propose a novel framework mqa-qg to generate multi-hop questions via composing reasoning graphs built upon basic operators.",,https://www.aclweb.org/anthology/2021.naacl-main.469.pdf
3744,225066758,Unsupervised Multi-hop Question Answering by Question Generation,conclusion and future works,Insight-tree,the experiments show that our model can generate human-like questions that help to train a well-performing multi-hop qa model in both the unsupervised and the fewshot learning setting.,,https://www.aclweb.org/anthology/2021.naacl-main.469.pdf
3745,225066758,Unsupervised Multi-hop Question Answering by Question Generation,conclusion and future works,Insight-tree,further work is required to include more flexible paraphrasing at the fusion stage.,,https://www.aclweb.org/anthology/2021.naacl-main.469.pdf
3746,225066758,Unsupervised Multi-hop Question Answering by Question Generation,conclusion and future works,Insight-tree,we can also design more reasoning graphs and operators to generate more complex questions and support more input modalities.,,https://www.aclweb.org/anthology/2021.naacl-main.469.pdf
3747,257019916,"Complex QA & language models hybrid architectures, Survey",conclusion,Insight-tree,"in this paper, we present a comprehensive survey of language model hybrid architectures for answering complex questions.",,https://export.arxiv.org/pdf/2302.09051v4.pdf
3748,257019916,"Complex QA & language models hybrid architectures, Survey",conclusion,Insight-tree,"we review the various skills required and typical approach, datasets and metrics that are used, the current limits of large language models for complex qa, the potential of hybrid architectures, better training and prompting strategies for this goal.",,https://export.arxiv.org/pdf/2302.09051v4.pdf
3749,257019916,"Complex QA & language models hybrid architectures, Survey",conclusion,Insight-tree,we also identify the main challenges and research avenues for solving more complex questions including knowledge capitalization.,,https://export.arxiv.org/pdf/2302.09051v4.pdf
3750,257019916,"Complex QA & language models hybrid architectures, Survey",conclusion,Insight-tree,we identify the need to address multi-sensitivity data in language models architectures and potential approaches.,,https://export.arxiv.org/pdf/2302.09051v4.pdf
3751,257019916,"Complex QA & language models hybrid architectures, Survey",conclusion,Insight-tree,"finally, we outline research topics and highlight the potential of exploration in this field.",,https://export.arxiv.org/pdf/2302.09051v4.pdf
3752,257019916,"Complex QA & language models hybrid architectures, Survey",conclusion,Insight-tree,this paper aims to provide a comprehensive and useful resource for readers interested in the development of complex non-factoid question answering.,,https://export.arxiv.org/pdf/2302.09051v4.pdf
3753,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,one potential threat to the validity in user studies like ours lies in the reliability of the study participants.,,https://arxiv.org/pdf/2109.02311v1.pdf
3754,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"given that we applied more than one quality-assurance measure-participant selection, an attention check, manual inspection-we are confident that our results are reliable.",,https://arxiv.org/pdf/2109.02311v1.pdf
3755,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"given also that most participants are fluent in english and regular movie watchers, we believe that the participants are representative at least for a subset of potential users of an online crs.another potential limitation is that we so far only analyzed two language generation systems.",,https://arxiv.org/pdf/2109.02311v1.pdf
3756,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,the question therefore remains to what extent the findings of our study would generalize to other approaches.,,https://arxiv.org/pdf/2109.02311v1.pdf
3757,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"since both analyzed systems (kbrd and kgsf) were published in the last two years, and since they were published at top-ranked scientific conferences, we believe that they are good representatives of the state-of-the-art in neural generation-based systems.",,https://arxiv.org/pdf/2109.02311v1.pdf
3758,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"moreover, an earlier analysis of the deepcrs system in [16] indicates that similar phenomena might be found also for other approaches.",,https://arxiv.org/pdf/2109.02311v1.pdf
3759,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"note also that our study can be easily extended to include alternative or even newer approaches, as long as the respective authors share the needed artifacts for reproducibility.",,https://arxiv.org/pdf/2109.02311v1.pdf
3760,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"as of now, we did not find any work that demonstrate superior quality than the kgsf system, and that the source code of such a system is available.",,https://arxiv.org/pdf/2109.02311v1.pdf
3761,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"to ensure replicability of our own work, we also share all the code and data used for our analyses online.so far, we analyzed our approach only with the help of the redial dataset.",,https://arxiv.org/pdf/2109.02311v1.pdf
3762,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"this choice was necessary to ensure a fair comparison with two recent works (kbrd and kgsf), which also relied on this dataset.",,https://arxiv.org/pdf/2109.02311v1.pdf
3763,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"recently, a number of alternative dataset was proposed, e.g., [5,14,65].",,https://arxiv.org/pdf/2109.02311v1.pdf
3764,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"an evaluation of all compared methods on other datasets is however beyond the scope of our present work, which aimed to assess the relative performance of generation-based and retrievalbased system based on the dataset for which they were originally designed and tuned.regarding the general nature of the proposed crb-crs system, it is a retrieval-based system.",,https://arxiv.org/pdf/2109.02311v1.pdf
3765,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"however, as discussed in section 3, we also rely on a small set of heuristics in a few processing phases, which are, for example, implemented using keyword lists and string matching.",,https://arxiv.org/pdf/2109.02311v1.pdf
3766,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"these heuristics, which are not yet learned automatically, are documented in the provided source code of our system.",,https://arxiv.org/pdf/2109.02311v1.pdf
3767,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"automating the construction of these rules and keyword lists, e.g., based on movielens metadata, is a part of our future work to avoid any knowledge-engineering bottlenecks.",,https://arxiv.org/pdf/2109.02311v1.pdf
3768,237420723,Towards Retrieval-based Conversational Recommendation,conclusion,Insight-tree,conversational recommender systems (crs) that interact with users in natural language obtained increased attention in the past few years.,,https://arxiv.org/pdf/2109.02311v1.pdf
3769,237420723,Towards Retrieval-based Conversational Recommendation,conclusion,Insight-tree,"in this paper, we have proposed a retrieval-based approach to conversational recommendations, and we conducted a study involving humans to understand how our system performs compared to recent language generation approaches.",,https://arxiv.org/pdf/2109.02311v1.pdf
3770,237420723,Towards Retrieval-based Conversational Recommendation,conclusion,Insight-tree,"our study led to promising results, and we hope that our study design can be used as a blueprint for user-centric studies of crs in the future.",,https://arxiv.org/pdf/2109.02311v1.pdf
3771,237420723,Towards Retrieval-based Conversational Recommendation,conclusion,Insight-tree,"overall, the main conclusion of our studies is retrieval-based approaches to crs can be a promising alternative or complement to language generation approaches.",,https://arxiv.org/pdf/2109.02311v1.pdf
3772,254017497,Solving math word problems with process- and outcome-based feedback,limitations to generalizability of our results,Insight-tree,we generally expect process-based and outcome-based feedback to align more closely for math compared to other domains.,,https://export.arxiv.org/pdf/2211.14275v1.pdf
3773,254017497,Solving math word problems with process- and outcome-based feedback,limitations to generalizability of our results,Insight-tree,"for math problems, incorrect traces are typically harmful for reaching correct final answers.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3774,254017497,Solving math word problems with process- and outcome-based feedback,limitations to generalizability of our results,Insight-tree,this matches our earlier finding that outcome-supervised rms approximate process-based feedback.,,https://export.arxiv.org/pdf/2211.14275v1.pdf
3775,254017497,Solving math word problems with process- and outcome-based feedback,limitations to generalizability of our results,Insight-tree,"in contrast, in other domains, undesirable behaviors may be helpful for highly-rated outcomes, e.g., manipulation may increase reported user satisfaction.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3776,254017497,Solving math word problems with process- and outcome-based feedback,limitations to generalizability of our results,Insight-tree,"as a result, we believe optimizing for outcomes (final-answer correctness) for math problems has a stronger effect on inducing a correct process than it would in other domains.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3777,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"in this work, we run the first comprehensive comparison between process-and outcome-based supervision on a natural language task.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3778,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"we find that both types of supervision lead to similar finalanswer error rates, with our best models improving the state-of-the-art final-answer error on gsm8k from 16.8% to 13.8% when using outcome-based supervision and to 12.9% when using processbased supervision.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3779,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"in contrast, we find that obtaining low trace error requires either process-based supervision, or a reward model that emulates it.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3780,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"a purely process-based approach of sft with prm reranking reduces the state-of-the-art trace error rate from 14.0% to 3.4%, while its outcome-based analogue achieves 12.7% trace error.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3781,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"however, somewhat surprisingly, we find that reward models trained with outcome-based labels result in predictions that agree more closely with the process-based labels than they do with the outcome-based labels themselves.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3782,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"by using this reward model during rl training, we close most of this gap, reducing trace error from 12.7% to 5.5%.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3783,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"while some of these conclusions may be specific to our setting of math word problems, we hope that future work explores the extent to which they generalize to other domains.",,https://export.arxiv.org/pdf/2211.14275v1.pdf
3784,53116244,Re Co CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,conclusion,Insight-tree,"we introduced record, a large-scale reading comprehension dataset requiring commonsense reasoning.",,https://arxiv.org/pdf/1810.12885v1.pdf
3785,53116244,Re Co CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,conclusion,Insight-tree,"unlike existing machine reading comprehension (mrc) datasets, record contains a large portion of queries that require commonsense reasoning to be answered.",,https://arxiv.org/pdf/1810.12885v1.pdf
3786,53116244,Re Co CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,conclusion,Insight-tree,"our baselines, including top performers on existing mrc datasets, are no match for human competence on record.",,https://arxiv.org/pdf/1810.12885v1.pdf
3787,53116244,Re Co CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,conclusion,Insight-tree,we hope that record will spur more research in mrc with commonsense reasoning.,,https://arxiv.org/pdf/1810.12885v1.pdf
3788,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,"this paper presents ibr, a proof generation model via iterative backward reasoning for rule-based qa tasks.",,https://www.aclanthology.org/2022.naacl-main.216.pdf
3789,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,"we equip the reasoning procedure with detailed hidden state tracking by predicting nodes and edges in the proof path iteratively backward from the question, and allow the model to reason on the elaborate representations of nodes and his- tory paths.",,https://www.aclanthology.org/2022.naacl-main.216.pdf
3790,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,"our model is more interpretable than previous at-once models, and is also more effective and efficient than former iterative models.",,https://www.aclanthology.org/2022.naacl-main.216.pdf
3791,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,experiments also demonstrate the superiority of ibr to various baselines on proof generation under various settings.,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3792,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,we implement our model based on pytorch along with huggingface-transformers toolkit 6 .,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3793,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,we use roberta large model 7 as our backbone encoder to generate token-level representations.,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3794,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,"table 10 shows the implementation details of ibr, including learning rates for different modules.",,https://www.aclanthology.org/2022.naacl-main.216.pdf
3795,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,all linear layers used in our model have one layer.,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3796,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,the model trained after 8 epochs will be used in the evaluation.,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3797,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,"we remove functional words without lexical meaning like ""a"" and ""the"" from facts, rules, and questions to shorten the input length, so each training epoch takes about 2 hours.",,https://www.aclanthology.org/2022.naacl-main.216.pdf
3798,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,we select these hyper-parameters according to tuning them empirically based on the performance.,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3799,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,all experiments are run on nvidia tesla-v100 gpus.,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3800,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,the main experiment performance of ibr fluctuates by one point.,,https://www.aclanthology.org/2022.naacl-main.216.pdf
3801,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,we tackle the problem of question generation targeted for human information seeking using automatic question answering technology.,,https://www.aclanthology.org/2021.dialdoc-1.11.pdf
3802,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,we focus on generating questions for news articles that can be answered by longer passages rather than short text spans as suggested questions.,,https://www.aclanthology.org/2021.dialdoc-1.11.pdf
3803,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,"we build a bert-based pointer-generator network as the qg model, trained with the natural questions dataset.",,https://www.aclanthology.org/2021.dialdoc-1.11.pdf
3804,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,"our method shows state-of-the-art performance in terms of bleu, meteor, and rouge l scores on our nq question generation dataset.",,https://www.aclanthology.org/2021.dialdoc-1.11.pdf
3805,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,we then apply our model to the out-of-domain news articles without further training.,,https://www.aclanthology.org/2021.dialdoc-1.11.pdf
3806,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,we use a qa system to evaluate our qg models as there are no gold questions for comparison.,,https://www.aclanthology.org/2021.dialdoc-1.11.pdf
3807,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,we also conduct a human evaluation to confirm the qa evaluation results.,,https://www.aclanthology.org/2021.dialdoc-1.11.pdf
3808,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,"textual novelty detection has an array of use-cases starting from search and retrieval on the web, nlp tasks like plagiarism detection, paraphrase detection, summarization, modeling interestingness, fake news detection, and so forth.",,
3809,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,"however, less attention is paid to the document-level variant of the problem in comparison to sentence-level novelty detection.",,
3810,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,"in this work, we present a comprehensive account of our experiments so far on document-level novelty detection.",,
3811,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,we study existing literature on textual novelty detection as well as our earlier explorations on the topic.,,
3812,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,here we assert that we would need to perform information assimilation from multiple premises to identify the novelty of a given text.,,
3813,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,our current approach performs better than our earlier approaches.,,
3814,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,"also, we show that our method could be suitably applied to allied tasks like plagiarism detection and paraphrase detection.",,
3815,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,"we point out some limitations of our approach, which we aim to explore next.",,
3816,249922213,Discover Oncology Circ_0000705 facilitates proline metabolism of esophageal squamous cell carcinoma cells by targeting miR-621/PYCR1 axis,conclusion,Insight-tree,"these findings indicated that circ_0000705 could promote proline metabolism and escc progression by targeting mir-621/pycr1 axis, suggest a key role for circ_0000705 in proline metabolism in escc, and imply that circ_0000705 may be a potential biomarker or/and possible therapeutic target for escc.",,
3817,249922213,Discover Oncology Circ_0000705 facilitates proline metabolism of esophageal squamous cell carcinoma cells by targeting miR-621/PYCR1 axis,conclusion,Insight-tree,"however, whether circ_0000705 is also highly expressed and has a similar mechanism of regulating proline metabolism in other types of tumors deserve further research in the future.",,
3818,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,conclusion,Insight-tree,"we propose a new parameterization framework, taser, for improving the efficiency and robustness of dense retrieval for odqa.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3819,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,conclusion,Insight-tree,it interleaves shared encoder blocks with specialized ones in a single encoder where some sub-networks are task-specific.,,https://export.arxiv.org/pdf/2210.05156v2.pdf
3820,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,conclusion,Insight-tree,"as the specialized sub-networks are sparsely activated, taser can provide better parameter efficiency with almost no additional computation cost.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3821,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,conclusion,Insight-tree,experiments show that taser substantially outperforms existing fully supervised biencoder dense retrievers on both in-domain and out-of-domain generalization.,,https://export.arxiv.org/pdf/2210.05156v2.pdf
3822,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"in this section, we point out several limitations in this work.first, our in-domain evaluation experiments focus on passage retrieval for odqa.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3823,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"while the dense retriever is mostly successful in odqa, it can be also used in other types of retrieval tasks which may have different input and output format.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3824,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"for example, the kilt benchmark (petroni et al., 2021) provides several knowledge-intensive tasks other than odqa.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3825,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"the performance of taser models trained on such retrieval tasks remain unknown.second, compared with traditional sparse vector models like tf-idf and bm25, the cost of training is an inherent issue of dense retrievers.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3826,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"although taser significantly reduce the number of model parameters, the training cost is still high.third, in our experiments, we show that the learned routing does not outperform the deterministic routing.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3827,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"this may suggest a better architecture and/or training algorithms for learned routing is needed to fully unleash the power of moe.last, as observed in 4.2, there is still a gap between taser and bm25 in out-of-domain evaluation.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3828,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"therefore, how to close this gap will remain a critical topic for future work on dense retrievers.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3829,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"in the paper, only input-dependent routing is considered.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3830,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"here, we provide a more comprehensive study of routing mechanisms.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3831,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"in particular, we introduce three routing mechanisms: the deterministic routing (det-r) which is used in our main experiments, the sequence-based routing (seq-r), and the token-based routing (tok-r).",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3832,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,both seq-r and tok-r are learned jointly with the task-specific objective.,,https://export.arxiv.org/pdf/2210.05156v2.pdf
3833,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"specifically, det-r is the input-dependent routing studied in the main paper where two expert ffn sub-layers are needed for odqa retrieval, one for questions and one for passages.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3834,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"in this case, the router determines the expert ffn sub-layer based on whether the input is a question or a passage.",,https://export.arxiv.org/pdf/2210.05156v2.pdf
3835,229923177,ERNIE-DOC: A Retrospective Long-Document Modeling Transformer,conclusion,Insight-tree,"in this paper, we proposed ernie-doc, a document-level language pretraining model based on the recurrence transformers paradigm.",,https://www.aclanthology.org/2021.acl-long.227.pdf
3836,229923177,ERNIE-DOC: A Retrospective Long-Document Modeling Transformer,conclusion,Insight-tree,"two well-designed mechanisms, namely the retrospective feed mechanism and the enhanced recurrent mechanism, enable ernie-doc, which theoretically has the longest possible dependency, to model bidirectional contextual information of a complete document.",,https://www.aclanthology.org/2021.acl-long.227.pdf
3837,229923177,ERNIE-DOC: A Retrospective Long-Document Modeling Transformer,conclusion,Insight-tree,"additionally, ernie-doc is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context.",,https://www.aclanthology.org/2021.acl-long.227.pdf
3838,229923177,ERNIE-DOC: A Retrospective Long-Document Modeling Transformer,conclusion,Insight-tree,"experiments on various downstream tasks demonstrate that ernie-doc outperforms existing strong pretraining models such as roberta, longformer, and bigbird and achieves sota results on several language modeling and language understanding benchmarks.",,https://www.aclanthology.org/2021.acl-long.227.pdf
3839,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,conclusion,Insight-tree,"we propose critic, a novel plug-and-play framework that empowers frozen llms to self-verify and self-correct by interacting with the external environment.",,https://export.arxiv.org/pdf/2305.11738v1.pdf
3840,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,conclusion,Insight-tree,"leveraging the intuition of critical thinking with external feedback, critic enables llms to validate their knowledge and improve their answers through introspection without requiring further training.",,https://export.arxiv.org/pdf/2305.11738v1.pdf
3841,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,conclusion,Insight-tree,"experiments on diverse tasks and datasets have consistently shown the effectiveness, generality, and interoperability of critic.",,https://export.arxiv.org/pdf/2305.11738v1.pdf
3842,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,conclusion,Insight-tree,"moreover, we shed light on the unreliability of llms in self-verification, highlighting the potential of external tool interaction to solve this problem.",,https://export.arxiv.org/pdf/2305.11738v1.pdf
3843,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,conclusion,Insight-tree,"we hope our findings will inspire further exploration into the truthfulness of language models, ultimately leading to more trustworthy ai systems.",,https://export.arxiv.org/pdf/2305.11738v1.pdf
3844,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,limitations & future work,Insight-tree,latency accessing external tools and engaging in iterative verification and correction processes can introduce additional overhead in terms of inference time.,,https://export.arxiv.org/pdf/2305.11738v1.pdf
3845,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,limitations & future work,Insight-tree,"in practice, as shown in figures 3,4, and 5, we can effectively utilize critic for a relatively small number of iterations (even just one), while still reaping significant benefits.",,https://export.arxiv.org/pdf/2305.11738v1.pdf
3846,258823123,CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,limitations & future work,Insight-tree,"furthermore, the proposed critic can be employed to automatically construct high-quality corpora, which is essential for internalizing such gains for data-centric ai.",,https://export.arxiv.org/pdf/2305.11738v1.pdf
3847,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,"we present crepe, a benchmark for causal reasoning about events and entities in procedural texts.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3848,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,"we show that mainstream llms such as gpt-3 perform close to chance on crepe, while using code-like event representation as a prompt to code language model codex greatly improves the performance.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3849,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,"further, we experiment with various ways to encode entity information into this representation and find that eliciting chain-of-thought reasoning from codex further improves performance while existing cot approaches with gpt-3 are ineffective.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3850,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,we clearly show that llms benefit from lower-level entity information when making predictions about higher-level events.,,https://export.arxiv.org/pdf/2301.10896v3.pdf
3851,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,"future work should explore related tasks such as next-event prediction, event temporal ordering, etc., by injecting relevant information about entities into our representation.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3852,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,our code-representation of events allows more powerful expressions than simply entailment and negation considered in this work.,,https://export.arxiv.org/pdf/2301.10896v3.pdf
3853,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,future work may explore other forms of code chain-of-thought such as first-order logic.,,https://export.arxiv.org/pdf/2301.10896v3.pdf
3854,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,"these expressions generated by llms can be computed objectively, thus ameliorating llms' hallucinations and improving the interpretability and faithfulness of predictions.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3855,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"despite our best efforts, our crepe dataset has inherent limitations.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3856,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"first, the choice of studying procedure texts, despite many discussed advantages, limits the domain, writing style, and other semantic features of the texts.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3857,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"as a result, porting our methods and findings to other text styles such as stories or news might require domain adaptation.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3858,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"second, we prioritize quality over quantity when creating this benchmark, which suffers from small size and contains biases from the annotators, even though we address the latter by having different annotators label a test set.when annotating the hypothetical events, our intention is that they represent a wild variety that doers of the procedures, humans or machines, would care about.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3859,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"however, we also have to ensure these events are unambiguously bound to some entities in order to challenge models for their causal reasoning ability.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3860,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"while we do our utmost to balance these two conflicting objectives, the issue might still persist.in crepe, each event likelihood change is caused by exactly one entity state change.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3861,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,this is an over-simplification made to facilitate evaluation.,,https://export.arxiv.org/pdf/2301.10896v3.pdf
3862,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"in real life, many complex events require many entity states to be reasoned about, which in turn may have complex logical relations among them.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3863,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"we leave this for future work.while we intend our representation of events and entities to be a general and effective one, we have only shown that it works well empirically using codex, which is one of the only code language models at present.",,https://export.arxiv.org/pdf/2301.10896v3.pdf
3864,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,whether the idea of our structured representation applies to other models remains to be explored.,,https://export.arxiv.org/pdf/2301.10896v3.pdf
3865,237415234,CDLM: Cross-Document Language Modeling,conclusion,Insight-tree,"we presented a novel pretraining strategy and technique for cross-document language modeling, providing better encoding for cross-document (cd) downstream tasks.",,https://arxiv.org/pdf/2101.00406v2.pdf
3866,237415234,CDLM: Cross-Document Language Modeling,conclusion,Insight-tree,"our contributions include the idea of leveraging clusters of related documents for pretraining, via cross-document masking, along with a new long-range attention pattern, together driving the model to learn to encode cd relationships.",,https://arxiv.org/pdf/2101.00406v2.pdf
3867,237415234,CDLM: Cross-Document Language Modeling,conclusion,Insight-tree,"this was achieved by extending the global attention mechanism of the longformer model to apply already in pretraining, creating encodings that attend to long-range information across and within documents.",,https://arxiv.org/pdf/2101.00406v2.pdf
3868,237415234,CDLM: Cross-Document Language Modeling,conclusion,Insight-tree,"our experiments assess that our crossdocument language model yields new state-of-theart results over several cd benchmarks, while, in fact, employing substantially smaller models.",,https://arxiv.org/pdf/2101.00406v2.pdf
3869,237415234,CDLM: Cross-Document Language Modeling,conclusion,Insight-tree,our analysis showed that cdlm implicitly learns to recover long-distance cd relations via the attention mechanism.,,https://arxiv.org/pdf/2101.00406v2.pdf
3870,237415234,CDLM: Cross-Document Language Modeling,conclusion,Insight-tree,"we propose future research to extend this framework to train larger models, and to develop cross-document sequence-to-sequence models, which would support cd tasks that involve a generation phase.",,https://arxiv.org/pdf/2101.00406v2.pdf
3871,253761592,Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,conclusions,Insight-tree,"inspired by how humans accumulate skills from past experience and re-use them to solve new tasks, in this paper, we develop and conduct extensive experiments with transformer-based task-level mixture-of-expert (moe) models, in hope to provide new insights on multi-task learning and crosstask generalization in nlp.",,https://export.arxiv.org/pdf/2205.12701v2.pdf
3872,253761592,Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,conclusions,Insight-tree,"firstly, we empirically investigate importance design choices and quantify their influence on final model.",,https://export.arxiv.org/pdf/2205.12701v2.pdf
3873,253761592,Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,conclusions,Insight-tree,"secondly, in both few-shot and zero-shot settings, we demonstrate that task-level mixture-of-expert models are better at generalizing to new tasks.",,https://export.arxiv.org/pdf/2205.12701v2.pdf
3874,253761592,Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,conclusions,Insight-tree,"finally, by conducting a detailed analysis on the routing decisions, we find they have strong correlations with human-defined task characteristics, even when the decisions are learned spontaneously without no prior knowledge such as pre-computed task representations.",,https://export.arxiv.org/pdf/2205.12701v2.pdf
3875,253761592,Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,conclusions,Insight-tree,we hope our work provide useful advice on training and interpreting multi-task models in nlp and we hope it will inspire future work in improving multitask learning and cross-task generalization in nlp.,,https://export.arxiv.org/pdf/2205.12701v2.pdf
3876,258332119,Exploring the Curious Case of Code Prompts,conclusion,Insight-tree,in this work we investigate whether or not there exists a systematic performance difference between prompting plms with code or with text.,,https://export.arxiv.org/pdf/2304.13250v1.pdf
3877,258332119,Exploring the Curious Case of Code Prompts,conclusion,Insight-tree,we confirm that there are indeed tasks for which code prompting is significantly more effective than text prompting and that this finding holds across different types of models.,,https://export.arxiv.org/pdf/2304.13250v1.pdf
3878,258332119,Exploring the Curious Case of Code Prompts,conclusion,Insight-tree,"however, for most tasks, we find that text prompting is still the best method for eliciting few-shot generalization from plms.",,https://export.arxiv.org/pdf/2304.13250v1.pdf
3879,237433880,Exploiting Reasoning Chains for Multi-hop Science Question Answering *,conclusion,Insight-tree,we propose a novel chain guided retriever-reader framework for multi-hop qa.,,https://arxiv.org/pdf/2109.02905v1.pdf
3880,237433880,Exploiting Reasoning Chains for Multi-hop Science Question Answering *,conclusion,Insight-tree,our modeling for the reasoning chains is effective to find both direct and indirect facts and is less likely to introduce noise.,,https://arxiv.org/pdf/2109.02905v1.pdf
3881,237433880,Exploiting Reasoning Chains for Multi-hop Science Question Answering *,conclusion,Insight-tree,"moreover, our framework is corpus-independent and is capable of handling the setting without any ground-truth annotations.",,https://arxiv.org/pdf/2109.02905v1.pdf
3882,237433880,Exploiting Reasoning Chains for Multi-hop Science Question Answering *,conclusion,Insight-tree,further analysis and discussions also elucidate some of the inner workings of our framework while maintaining the explainability at the same time.,,https://arxiv.org/pdf/2109.02905v1.pdf
3883,237385768,WebQA: Multihop and Multimodal QA,conclusion,Insight-tree,"webqa is a new multi-hop, multi-modal question answering challenge for our community.",,https://arxiv.org/pdf/2109.00590v4.pdf
3884,237385768,WebQA: Multihop and Multimodal QA,conclusion,Insight-tree,"designed to simulate the heterogeneous information landscape one might expect during a web search, webqa covers a series of opendomain general visual queries while also forcing models to still reason about text.",,https://arxiv.org/pdf/2109.00590v4.pdf
3885,237385768,WebQA: Multihop and Multimodal QA,conclusion,Insight-tree,"our task requires a system to determine relevant sources, perform aggregation and reasoning.",,https://arxiv.org/pdf/2109.00590v4.pdf
3886,237385768,WebQA: Multihop and Multimodal QA,conclusion,Insight-tree,we also propose a novel general recipe for evaluation on webqa which measures both fluency and accuracy.,,https://arxiv.org/pdf/2109.00590v4.pdf
3887,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"we notice that for certain tasks where little context about the environment is available, fully relying on foreseeable reasoning becomes impractical.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3888,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,consider the following task from alfworld [27]:you are in the middle of a room.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3889,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"looking quickly around you, you see a drawer 2, a shelf 5, a drawer 1, a shelf 4, a sidetable 1, a drawer 5, a shelf 6, a shelf 1, a shelf 9, a cabinet 2, a sofa 1, a cabinet 1, a shelf 3, a cabinet 3, a drawer 3, a shelf 11, a shelf 2, a shelf 10, a dresser 1, a shelf 12, a garbagecan 1, a armchair 1, a cabinet 4, a shelf 7, a shelf 8, a safe 1, and a drawer 4.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3890,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"your task is to: put some vase in safe.since a planner has no prior knowledge about the environment, it has to enumerate all possible plans that can potentially lead to some vase.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3891,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"the number of reasoning steps of planner in such tasks is equivalent to the worst-case complexity of observation-dependent reasoning.the example above implies that a robust alm system should not be built on a singleton -it looks promising to wire different nodes of llms, tools, and sub-models into a directed acyclic graph (dag) so that each node functions for its predesignated tasks organically.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3892,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,4 directions to further improve the efficiency and performance of such alm systems include (1) offloading specialized abilities from foundation llms into smaller models.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3893,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,section 3.3 demonstrate the possibility for small lms specializing [14] in general foreseeable reasoning.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3894,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"we expect that with a greater number of open domain instructions, foreseeable reasoning can be even more holistically offloaded.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3895,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"other parametric nodes in the dag, such as a solver, can be fine-tuned alike.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3896,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,(2) tool representation learning.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3897,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"in many cases from hotpotqa, wikipedia and google can both lead to the correct answer, indicating a certain level of similarity between those tools.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3898,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,we can set up a model to minimize the energy among similar-functioning workers.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3899,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,tool representations allow us to parametrize the whole alm system and therefore enabling end-to-end training.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3900,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,(3) graph optimization.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3901,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"furthermore, we should be able to optimize dag execution through multiple graph and concurrency algorithms.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3902,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,"we present rewoo , a modular alm framework to solve multi-step reasoning tasks efficiently by decoupling reasoning from tool feedback and observations.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3903,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,theoretical decomposition of prompt tokens establishes that rewoo is able to substantially reduce prompting redundancy in prevailing thought-action-observation alm systems.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3904,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,comprehensive experiments on both public nlp benchmarks and curated tasks reveal superior performance of rewoo in achieving boosted performance with much less token consumption.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3905,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,a side study also shows that rewoo has relatively robust performance under tool-failure cases.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3906,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,our study further unveils the potential for generic reasoning offloading via instruction tuning and specialization.,,https://export.arxiv.org/pdf/2305.18323v1.pdf
3907,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,"future improvements beyond rewoo based alm systems involve modular llm fine-tuning, tool representation learning, and system graph learning and optimization.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3908,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,"we demonstrate that our work lays a solid foundation for these advancements, inching us closer to truly scalable agi.",,https://export.arxiv.org/pdf/2305.18323v1.pdf
3909,238856979,Representation Decoupling for Open-Domain Passage Retrieval,conclusion,Insight-tree,"in this paper, we analyze the contrastive conflicts problem in current contrastive learning framework adopted in the open-domain passage retrieval area.",,https://arxiv.org/pdf/2110.07524v1.pdf
3910,238856979,Representation Decoupling for Open-Domain Passage Retrieval,conclusion,Insight-tree,"to solve such problem, we propose dense contextual sentence representation (dscr).",,https://arxiv.org/pdf/2110.07524v1.pdf
3911,238856979,Representation Decoupling for Open-Domain Passage Retrieval,conclusion,Insight-tree,"concretely, we first decouple the original passage representations into contextual sentence-level ones, and then refine the original contrastive learning framework by creating sentence-aware positive and negative samples.",,https://arxiv.org/pdf/2110.07524v1.pdf
3912,238856979,Representation Decoupling for Open-Domain Passage Retrieval,conclusion,Insight-tree,"our dcsr achieves significant performance gain compared to original dpr baseline, especially on datasets with severe conflicting problem.",,https://arxiv.org/pdf/2110.07524v1.pdf
3913,238856979,Representation Decoupling for Open-Domain Passage Retrieval,conclusion,Insight-tree,"extensive experiments shows that our dcsr also enjoys better transferability, indicating that dcsr well captures the universality in different datasets.",,https://arxiv.org/pdf/2110.07524v1.pdf
3914,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,"we introduced dynabench, a research platform for dynamic benchmarking.",,https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
3915,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,"dynabench opens up exciting new research directions, such as investigating the effects of ensembles in the loop, distributional shift characterisation, exploring annotator efficiency, investigating the effects of annotator expertise, and improving model robustness to targeted adversarial attacks in an interactive setting.",,https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
3916,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,"it also facilitates further study in dynamic data collection, and more general cross-task analyses of humanand-machine interaction.",,https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
3917,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,the current iteration of the platform is only just the beginning of a longer journey.,,https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
3918,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,"in the immediate future, we aim to achieve the following goals: anyone can run a task.",,https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
3919,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,"having created a tool that allows for human-in-the-loop model evaluation and data collection, we aim to make it possible for anyone to run their own task.",,https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
3920,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,"to get started, only three things are needed: a target model, a (set of) context(s), and a pool of annotators.",,https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
3921,213474484,Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks,conclusion,Insight-tree,"we presented quail 5 , the first multi-domain text comprehension challenge that is balanced and annotated for 9 types of verbal reasoning.",,https://ojs.aaai.org/index.php/AAAI/article/download/6398/6254
3922,213474484,Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks,conclusion,Insight-tree,"quail aims to show the extent to which current models can generalize over different domains and reasoning strategies and handle questions that can be answered with the information in a given text, unanswerable questions and questions that require extra world knowledge.",,https://ojs.aaai.org/index.php/AAAI/article/download/6398/6254
3923,213474484,Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks,conclusion,Insight-tree,"we hope that quail will stimulate efforts to develop generalist systems tackling different kinds of verbal reasoning, and that it will be useful in diagnostics and qualitative analysis for new qa systems.",,https://ojs.aaai.org/index.php/AAAI/article/download/6398/6254
3924,258865495,Few-shot Unified Question Answering: Tuning Models or Prompts?,conclusion,Insight-tree,"in this work, we explore the viability of prompttuning as a solution to unified qa and conduct a thorough analysis of its promise, effectiveness, and trade-offs compared with the model-tuning paradigm on a set of 16 qa datasets, focusing particularly on several few-shot scenarios.",,https://export.arxiv.org/pdf/2305.14569v1.pdf
3925,258865495,Few-shot Unified Question Answering: Tuning Models or Prompts?,conclusion,Insight-tree,"as a result, we obtain several key findings and insights that hopefully will inform which paradigm to prefer under which scenarios.",,https://export.arxiv.org/pdf/2305.14569v1.pdf
3926,258865495,Few-shot Unified Question Answering: Tuning Models or Prompts?,conclusion,Insight-tree,"prompt tuning is quite competitive with model-tuning in the lower extreme of the few-shot scenarios, given a good initialization.",,https://export.arxiv.org/pdf/2305.14569v1.pdf
3927,258865495,Few-shot Unified Question Answering: Tuning Models or Prompts?,conclusion,Insight-tree,"while parameter-sharing leads to superior performance in the few-shot setting, the trends flip in the full-shot setting, a simple knowledge transfer approach (i.e., an average of relevant prompts) is as effective as complex methods without introducing additional parameters.",,https://export.arxiv.org/pdf/2305.14569v1.pdf
3928,258865495,Few-shot Unified Question Answering: Tuning Models or Prompts?,conclusion,Insight-tree,pre-training the backbone model on the source tasks significantly benefits prompt tuning.,,https://export.arxiv.org/pdf/2305.14569v1.pdf
3929,258865495,Few-shot Unified Question Answering: Tuning Models or Prompts?,conclusion,Insight-tree,"while initializing from a strong prior is very helpful for prompt tuning, its benefit is not as substantial when using a larger backbone model, especially when the number of training examples exceeds a certain threshold.",,https://export.arxiv.org/pdf/2305.14569v1.pdf
3930,247158553,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",conclusion,Insight-tree,"this paper introduces kmir benchmark for evaluating knowledge memorization, identification, and reasoning abilities.",,https://arxiv.org/pdf/2202.13529v1.pdf
3931,247158553,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",conclusion,Insight-tree,"kmir includes a systematic evaluation syllabus to summarize the knowledgerelated abilities of plms, and has 184,348 questions involving 4 types of questions covering 3 types of knowledge.",,https://arxiv.org/pdf/2202.13529v1.pdf
3932,247158553,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",conclusion,Insight-tree,we also find many interesting phenomena through extensive experiments: 1) the memorization ability of plms depends more on the number of parameters than training schemes.,,https://arxiv.org/pdf/2202.13529v1.pdf
3933,247158553,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",conclusion,Insight-tree,2) current plms are struggling to robustly remember the facts.,,https://arxiv.org/pdf/2202.13529v1.pdf
3934,247158553,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",conclusion,Insight-tree,"3) model compression technology retains the amount of knowledge well, but hurts the identification and reasoning ability, etc.",,https://arxiv.org/pdf/2202.13529v1.pdf
3935,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,"the 2021 edition of the shared task on multi-hop inference for explanation regeneration was a success, with 4 participating teams each substantially improving performance over the baseline model.",,
3936,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,"the best performing team, deepblueai, produced a system that improves absolute performance by 32%, up to 0.820 ndcg, bringing overall state-ofthe-art performance at this relevancy ranking aspect of multi-hop inference to a moderate level.",,
3937,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,we hope that future systems for many-hop multi-hop inference that aim to build large detailed explanations for question answering will be able to leverage these results to build strong relevancy retrieval subcomponents to augment their compositional inference algorithms.,,
3938,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,"award #1815948, ""explainable natural language inference"").",,
3939,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,"this edition of the shared task would not have been possible without the hard work of a number of relevance annotators, and their generous offer to anonymously use their data while their work is under review.",,
3940,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,a special thanks to andr freitas for the helpful discussions.,,
3941,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,"additionally, we would like to thank the computational shared facility of the university of manchester for providing the infrastructure to run our experiments.",,
3942,233296709,CROSSFIT : A Few-shot Learning Challenge for Cross-task Generalization in NLP,conclusion and future work,Insight-tree,"in this paper, we study the problem of building better few-shot learners via acquiring cross-task generalization ability from diverse nlp tasks.",,https://www.aclanthology.org/2021.emnlp-main.572.pdf
3943,233296709,CROSSFIT : A Few-shot Learning Challenge for Cross-task Generalization in NLP,conclusion and future work,Insight-tree,"towards our goal, we introduce the crossfit challenge, an task setup that standardizes the training pipeline, data access and evaluation protocol.",,https://www.aclanthology.org/2021.emnlp-main.572.pdf
3944,233296709,CROSSFIT : A Few-shot Learning Challenge for Cross-task Generalization in NLP,conclusion and future work,Insight-tree,"we also present the nlp few-shot gym, a repository of 160 diverse few-shot nlp tasks, to support crossfit learning in different scenarios.",,https://www.aclanthology.org/2021.emnlp-main.572.pdf
3945,233296709,CROSSFIT : A Few-shot Learning Challenge for Cross-task Generalization in NLP,conclusion and future work,Insight-tree,we empirically demonstrated that cross-task generalization can be acquired via multi-task learning and meta-learning; confirmed that the selection of seen tasks would influence the few-shot performance on unseen tasks.,,https://www.aclanthology.org/2021.emnlp-main.572.pdf
3946,232335785,Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2,conclusion and future work,Insight-tree,"in this paper, we introduce chainruler, a dataset for multi-hop deductive argumentation, and assess gpt-2's zero-shot ability both to solve the inference tasks and to generate effective problem elaborations, i.e., texts which -once added to the context -improve performance.",,https://arxiv.org/pdf/2103.13033v1.pdf
3947,232335785,Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2,conclusion and future work,Insight-tree,our main findings are:,,https://arxiv.org/pdf/2103.13033v1.pdf
3948,248665596,From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective,conclusion,Insight-tree,"in this paper, we have built on the splade model, and studied to which extent it is able to take advantage of training improvements like distillation and hard negative mining, as well as better suited plm initialization: combined altogether, the resulting model reaches state-of-the-art performance on both in-domain and zeroshot evaluation.",,https://arxiv.org/pdf/2205.04733v2.pdf
3949,248665596,From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective,conclusion,Insight-tree,we also investigated the link between effectiveness and efficiency -induced by the degree of regularization -highlighting that more expressive models are better at generalization.,,https://arxiv.org/pdf/2205.04733v2.pdf
3950,257767698,Augmenting ebooks with Recommended Questions using Contrastive fine-tuned T5,conclusion and future scope,Insight-tree,the use of ai is constantly evolving in diverse applications.,,https://www.aclanthology.org/2022.icon-main.15.pdf
3951,257767698,Augmenting ebooks with Recommended Questions using Contrastive fine-tuned T5,conclusion and future scope,Insight-tree,this study investigates the potential advantages of a natural language processing approach for education.,,https://www.aclanthology.org/2022.icon-main.15.pdf
3952,257767698,Augmenting ebooks with Recommended Questions using Contrastive fine-tuned T5,conclusion and future scope,Insight-tree,this research presented an education question generating (eqg) approach that augments the ebook content with generated edu-questions to provide students with an effective learning platform.,,https://www.aclanthology.org/2022.icon-main.15.pdf
3953,257767698,Augmenting ebooks with Recommended Questions using Contrastive fine-tuned T5,conclusion and future scope,Insight-tree,"through experiments, we assessed the model's performance on a question generation task both before and after contrastive training.",,https://www.aclanthology.org/2022.icon-main.15.pdf
3954,257767698,Augmenting ebooks with Recommended Questions using Contrastive fine-tuned T5,conclusion and future scope,Insight-tree,we discovered that a contrastive trained model can produce more pertinent questions on the input text and can comprehend key concepts more effectively.,,https://www.aclanthology.org/2022.icon-main.15.pdf
3955,257767698,Augmenting ebooks with Recommended Questions using Contrastive fine-tuned T5,conclusion and future scope,Insight-tree,"experiments on qa dataset, prml (bishop, 2006) and ncert 2 ebook shows that our model succeeds to produces complex questions at scale.",,https://www.aclanthology.org/2022.icon-main.15.pdf
3956,235606327,On the Diversity and Limits of Human Explanations,conclusion,Insight-tree,"explanations represent a fascinating phenomenon and are actively studied in psychology, cognitive science, and other social sciences.",,https://www.aclanthology.org/2022.naacl-main.158.pdf
3957,235606327,On the Diversity and Limits of Human Explanations,conclusion,Insight-tree,"while the growing interest in explanations from the nlp community is exciting, we encourage the community to view this as an opportunity to understand how humans approach explanations and contribute to understanding and exploring the explanation processes.",,https://www.aclanthology.org/2022.naacl-main.158.pdf
3958,235606327,On the Diversity and Limits of Human Explanations,conclusion,Insight-tree,this will in turn inform how to collect and use human explanations in nlp.,,https://www.aclanthology.org/2022.naacl-main.158.pdf
3959,235606327,On the Diversity and Limits of Human Explanations,conclusion,Insight-tree,a modest proposal is that it is useful to examine and characterize human explanations before assuming that all explanations are equal and chasing a leaderboard.,,https://www.aclanthology.org/2022.naacl-main.158.pdf
3960,254125751,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,conclusion,Insight-tree,"we present a new parameter-effluent tuning method attempt, which learns to produce instance-wise prompts by interpolating multiple reusable soft prompts trained on source tasks and a new taskspecific prompt, while keeping the original lm frozen.",,https://www.aclanthology.org/2022.emnlp-main.446.pdf
3961,254125751,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,conclusion,Insight-tree,"our large-scale experiments demonstrate that attempt achieves a great trade-off between task performance and efficiency, introducing an interpretable and modular task transfer.",,https://www.aclanthology.org/2022.emnlp-main.446.pdf
3962,256459315,The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models,conclusion,Insight-tree,"in this work, we investigate the effects of training mrc models with unanswerable questions on their robustness against adversarial attacks.",,https://www.aclanthology.org/2023.eacl-main.113.pdf
3963,256459315,The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models,conclusion,Insight-tree,we construct adversarial samples from answerable and unanswerable questions in squad 2.0 and evaluate three mrc models fine-tuned on either squad 1.1 (v1 models) or squad 2.0 (v2 models) independently.,,https://www.aclanthology.org/2023.eacl-main.113.pdf
3964,256459315,The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models,conclusion,Insight-tree,"adversarial attacks on answerable questions reveal that v2 models initially show little improved robustness over v1 models yet possess a latent ability to deal with these attacks that v1 models do not; the correct responses are often hidden as second-best answers, an indicator of the ""hidden robustness"" of v2 models resulting from additional training on unanswerable questions.",,https://www.aclanthology.org/2023.eacl-main.113.pdf
3965,256459315,The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models,conclusion,Insight-tree,"by eliminating the ""unanswerable"" option and forcing v2 models to output an answer to any answer-  able questions, we leverage this hidden robustness to improve the performance of mrc models to attacks on answerable questions.",,https://www.aclanthology.org/2023.eacl-main.113.pdf
3966,256459315,The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models,conclusion,Insight-tree,"furthermore, we also show that this robustness translates well to out-of-domain test sets.",,https://www.aclanthology.org/2023.eacl-main.113.pdf
3967,118548248,He II 4686 IN  CARINAE: COLLAPSE OF THE WIND-WIND COLLISION REGION DURING PERIASTRON PASSAGE,conclusions,Insight-tree,"in this paper, we present data from the last 4 low excitation events in  car in the light of the he ii 4686 emission line.",,https://arxiv.org/pdf/1104.2276v3.pdf
3968,118548248,He II 4686 IN  CARINAE: COLLAPSE OF THE WIND-WIND COLLISION REGION DURING PERIASTRON PASSAGE,conclusions,Insight-tree,we summarize our results and conclusions below.,,https://arxiv.org/pdf/1104.2276v3.pdf
3969,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,"in this paper, we propose a novel continual learning formulation named continual model refinement (cmr).",,https://www.aclanthology.org/2022.acl-long.223.pdf
3970,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,the cmr problem aims to efficiently fix prediction errors when learning in outof-distribution data streams without catastrophically forgetting the acquired knowledge.,,https://www.aclanthology.org/2022.acl-long.223.pdf
3971,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,"for studying such a realistic and complex problem, we presented a dedicated evaluation protocol with a general method to create non-stationary, diverse ood data streams for analysis.",,https://www.aclanthology.org/2022.acl-long.223.pdf
3972,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,"also, we design multiple evaluation metrics to deliver a comprehensive yet concise measurement of cmr methods.",,https://www.aclanthology.org/2022.acl-long.223.pdf
3973,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,the proposed cmr problem with our comprehensive analysis opens up a range of new opportunities for studying continual learning problems that are closer to real-world applications for the nlp community and beyond.,,https://www.aclanthology.org/2022.acl-long.223.pdf
3974,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,"for example, based on our results and analysis about (q3) and (q6), we find that it is promising to study how we can integrate both regularization methods and replay methods for mitigating the forgetting issue while improving the generalization ability.",,https://www.aclanthology.org/2022.acl-long.223.pdf
3975,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,"the analysis about (q5) suggests that developing more stable ranking criteria is also important to conditional replay methods (e.g., our simple extension maxloss can outperform mir under specific settings).",,https://www.aclanthology.org/2022.acl-long.223.pdf
3976,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,developing cmr methods of which the configurations can generalize to diverse types of streams is also an important challenge.,,https://www.aclanthology.org/2022.acl-long.223.pdf
3977,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,we release our codebase and processed datasets for supporting the reproducibility of our experiments and future research.,,https://www.aclanthology.org/2022.acl-long.223.pdf
3978,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,"we have demonstrated use of question decomposition based on the amr semantic representation for multi-hop qa, using an intrinsically interpretable framework to incorporate interpretability directly into the system structure.",,https://export.arxiv.org/pdf/2206.08486v1.pdf
3979,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,the complex task of multi-hop question interpretation is delegated to amr parsers.,,https://export.arxiv.org/pdf/2206.08486v1.pdf
3980,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,"these parsers produce amr graphs to which two segmentation methods are applied, i.e., unknownbased and path-based graph segmentation, to achieve question decomposition.",,https://export.arxiv.org/pdf/2206.08486v1.pdf
3981,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,"to generate a well-formed sub-question, we perform both amr parsing and amr-to-text generation with the same architecture, which uses a fully graphisomorphic linearization technique to complete the transformation from graph to a sequence of symbols without losing adjacency information.",,https://export.arxiv.org/pdf/2206.08486v1.pdf
3982,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,"experimental results demonstrate that our qdamr system outperforms baseline question decomposition methods, both in performance of multi-hop qa and in the quality of generated sub-questions.",,https://export.arxiv.org/pdf/2206.08486v1.pdf
3983,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,"since our proposed graph segmentation methods are based on predicateargument relations and parallel conditions/entities respectively, they could in principle be generalized to an unknown number of hops by identifying multiple predicate nodes or capturing multiple parallel conditions/entities.",,https://export.arxiv.org/pdf/2206.08486v1.pdf
3984,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,"while, as noted, an aim of the qdamr design is to provide inherent interpretability, the effectiveness with which its outputs serve as explanations for human users remains to be evaluated in future work.",,https://export.arxiv.org/pdf/2206.08486v1.pdf
3985,247450729,Efficient Long Sequence Encoding via Synchronization,conclusion,Insight-tree,"in this work, we propose transync framework with flexible synchronization mechanisms for encoding long sequences.",,https://arxiv.org/pdf/2203.07644v1.pdf
3986,247450729,Efficient Long Sequence Encoding via Synchronization,conclusion,Insight-tree,"we demonstrate the feasibility of our method in reasoning tasks with long context, and also show its high adaptability to different scenarios.",,https://arxiv.org/pdf/2203.07644v1.pdf
3987,247450729,Efficient Long Sequence Encoding via Synchronization,conclusion,Insight-tree,"we consider our work to be valuable as an easy solution to address the long context issue in qa, and to be potentially applicable to other long sequence modeling tasks.",,https://arxiv.org/pdf/2203.07644v1.pdf
3988,243865561,Neural Natural Logic Inference for Interpretable Question Answering,conclusion,Insight-tree,"in this work, we explore the feasibility of combining natural logic with neural networks for interpretable question answering.",,https://www.aclanthology.org/2021.emnlp-main.298.pdf
3989,243865561,Neural Natural Logic Inference for Interpretable Question Answering,conclusion,Insight-tree,"we present an end-toend differentiable method for learning the parameters as well as the structure of natural logical rules, which is capable of considering the contextual information while conducting natural logic-based reasoning.",,https://www.aclanthology.org/2021.emnlp-main.298.pdf
3990,243865561,Neural Natural Logic Inference for Interpretable Question Answering,conclusion,Insight-tree,experimental results on the regents science exam of the aristo dataset show that our proposed model could bring improvements over baseline methods.,,https://www.aclanthology.org/2021.emnlp-main.298.pdf
3991,119162516,Adaptive PBDW approach to state estimation: noisy observations; user-defined update spaces,conclusions,Insight-tree,"in this paper, we presented a number of extensions to the pbdw formulation for state estimation.",,https://arxiv.org/pdf/1712.09594v1.pdf
3992,119162516,Adaptive PBDW approach to state estimation: noisy observations; user-defined update spaces,conclusions,Insight-tree,"first, we proposed a tikhonov regularization of the original pbdw statement for general linear functionals, which relies on holdout validation, to systematically deal with noisy measurements.",,https://arxiv.org/pdf/1712.09594v1.pdf
3993,119162516,Adaptive PBDW approach to state estimation: noisy observations; user-defined update spaces,conclusions,Insight-tree,"second, we proposed user-defined update spaces, which guarantee rapid convergence with respect to the number of measurements m and also might not require the solution to m riesz problems.",,https://arxiv.org/pdf/1712.09594v1.pdf
3994,119162516,Adaptive PBDW approach to state estimation: noisy observations; user-defined update spaces,conclusions,Insight-tree,"third, we presented an a priori error analysis that provides insights into the role of the regularization hyper-parameter  associated with the penalized formulation.",,https://arxiv.org/pdf/1712.09594v1.pdf
3995,219965751,"A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets",conclusions,Insight-tree,"we conducted a comprehensive survey of recent efforts on the tasks, evaluation metrics and benchmark datasets of machine reading comprehension (mrc).",,https://arxiv.org/pdf/2006.11880v1.pdf
3996,219965751,"A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets",conclusions,Insight-tree,"we discussed the definition and taxonomy of mrc tasks, and proposed a new classification method for mrc tasks.",,https://arxiv.org/pdf/2006.11880v1.pdf
3997,219965751,"A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets",conclusions,Insight-tree,the computing methods of different mrc evaluation metrics have been introduced with their usage in each type of mrc tasks also analyzed.,,https://arxiv.org/pdf/2006.11880v1.pdf
3998,219965751,"A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets",conclusions,Insight-tree,"we also introduced attributes and characteristics of mrc datasets, with 47 mrc datasets described in detail.",,https://arxiv.org/pdf/2006.11880v1.pdf
3999,219965751,"A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets",conclusions,Insight-tree,"finally, we discussed the open issues for future research of mrc and we argued that high-quality multi-modal mrc datasets and the research findings of cognitive neuroscience may help us find better ways to construct more challenging datasets and develop related mrc algorithms to achieve the ultimate goal of human-level machine reading comprehension.",,https://arxiv.org/pdf/2006.11880v1.pdf
4000,248987123,Unifier: A Unified Retriever for Large-Scale Retrieval,conclusion,Insight-tree,"we present a brand-new learning framework, dubbed unifier, to unify dense-vector and lexicon-based representing paradigms for large-scale retrieval.",,https://export.arxiv.org/pdf/2205.11194v2.pdf
4001,248987123,Unifier: A Unified Retriever for Large-Scale Retrieval,conclusion,Insight-tree,it improves the two paradigms by a carefully designed neural encoder to fully exploit the representing capability of pre-trained language models.,,https://export.arxiv.org/pdf/2205.11194v2.pdf
4002,248987123,Unifier: A Unified Retriever for Large-Scale Retrieval,conclusion,Insight-tree,its capability is further strengthened by our proposed dual-consistency learning with selfadversarial and -regularization.,,https://export.arxiv.org/pdf/2205.11194v2.pdf
4003,248987123,Unifier: A Unified Retriever for Large-Scale Retrieval,conclusion,Insight-tree,"moreover, the uni-retrieval scheme and the advanced architectures upon our encoder are presented to achieve more.",,https://export.arxiv.org/pdf/2205.11194v2.pdf
4004,248987123,Unifier: A Unified Retriever for Large-Scale Retrieval,conclusion,Insight-tree,experiments on several benchmarks verify the effectiveness and versatility of our framework.,,https://export.arxiv.org/pdf/2205.11194v2.pdf
4005,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"to enable the evaluation of long document models, we introduce muld: a benchmark of varied nlp tasks where each document consists of more than 10,000 tokens.",,https://arxiv.org/pdf/2202.07362v1.pdf
4006,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"the six tasks in our benchmark are created by filtering, extending, or modifying existing nlp tasks and are designed to require a long context for high performance.",,https://arxiv.org/pdf/2202.07362v1.pdf
4007,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"we evaluate simple chunking-based baselines, and find that the longformer model is able to outperform the t5 model suggesting our benchmark is a good test for the ability of models to make use of longer contexts.",,https://arxiv.org/pdf/2202.07362v1.pdf
4008,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"we believe that the technique explored in this work of augmenting and extending existing 'short document' datasets, can be applied to many other nlp tasks.",,https://arxiv.org/pdf/2202.07362v1.pdf
4009,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"as the performance of efficient transformers improves, we anticipate the need to update this benchmark with more challenging tasks.",,https://arxiv.org/pdf/2202.07362v1.pdf
4010,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"while we are focused on creating a benchmark which tests a model's ability to solve real-world long document tasks, we also expect improvements in the efficiencies of the models themselves which may make datasets with more than 100,000 tokens necessary which may require a fundamentally different approach to creating long document datasets.",,https://arxiv.org/pdf/2202.07362v1.pdf
4011,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"we leave for future work both the development of improved chunking methods, and more efficient transformers which make such methods unnecessary.",,https://arxiv.org/pdf/2202.07362v1.pdf
4012,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,we hope that the muld benchmark will encourage this further research into efficient models for long document nlp.,,https://arxiv.org/pdf/2202.07362v1.pdf
4013,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"to this end we provide the data, baseline models, and other code at www.github.com/ ghomashudson/muld.",,https://arxiv.org/pdf/2202.07362v1.pdf
4014,257771900,Explicit Planning Helps Language Models in Logical Reasoning,conclusion and future work,Insight-tree,"in this paper, we presented an lm-based logical reasoning system that integrates explicit planning into the inference method.",,https://export.arxiv.org/pdf/2303.15714v2.pdf
4015,257771900,Explicit Planning Helps Language Models in Logical Reasoning,conclusion and future work,Insight-tree,"additionally, we proposed a method that learns to prevent the explicit planning from being misguided.",,https://export.arxiv.org/pdf/2303.15714v2.pdf
4016,257771900,Explicit Planning Helps Language Models in Logical Reasoning,conclusion and future work,Insight-tree,"in our experiments, our planning-based systems outperform strong baseline methods including the selection-inference method and chain-of-thought prompting.",,https://export.arxiv.org/pdf/2303.15714v2.pdf
4017,257771900,Explicit Planning Helps Language Models in Logical Reasoning,conclusion and future work,Insight-tree,this has inspired us to explore several exciting avenues for further improvements.,,https://export.arxiv.org/pdf/2303.15714v2.pdf
4018,236478211,GCRC: A New MRC Dataset from Gaokao Chinese for Explainable Evaluation,conclusions,Insight-tree,"in this paper, we present a new challenging machine reading comprehension dataset (gcrc), collected from gaokao chinese, consisting of 8,719 high-level comprehensive multiple-choice questions.",,https://www.aclanthology.org/2021.findings-acl.113.pdf
4019,236478211,GCRC: A New MRC Dataset from Gaokao Chinese for Explainable Evaluation,conclusions,Insight-tree,"to the best of our knowledge, this is currently the most comprehensive, challenging, and high-quality dataset in mrc domain.",,https://www.aclanthology.org/2021.findings-acl.113.pdf
4020,236478211,GCRC: A New MRC Dataset from Gaokao Chinese for Explainable Evaluation,conclusions,Insight-tree,"in addition, we spend considerable effort to label three types of information, including sentence-level sfs, ers of a distractor, and reasoning skills required for qa, aiming to comprehensively evaluate systems in an explainable way.",,https://www.aclanthology.org/2021.findings-acl.113.pdf
4021,236478211,GCRC: A New MRC Dataset from Gaokao Chinese for Explainable Evaluation,conclusions,Insight-tree,"through experiments, we observe gcrc is very challenging data set for existing models, and we hope it can inspire innovative machine learning and reasoning approach to tackle the challenging problem and make mrc as an enabling technology for many real-world applications.",,https://www.aclanthology.org/2021.findings-acl.113.pdf
4022,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,"despite the simplicity and ubiquity of lexical matching as an evaluation metric in open-domain qa, it is unnecessarily rigid because plausible candidate answers are likely not to appear in the list of gold answers.",,https://export.arxiv.org/pdf/2305.06984v2.pdf
4023,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,"this flaw has been long known, but the efforts to circumvent it have been mostly artisanal.",,https://export.arxiv.org/pdf/2305.06984v2.pdf
4024,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,"in this paper, we report a systematic study of lexical matching by manually judging answers generated by several prominent open-domain qa models.",,https://export.arxiv.org/pdf/2305.06984v2.pdf
4025,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,we found that llms achieve stateof-the-art on nq-open.,,https://export.arxiv.org/pdf/2305.06984v2.pdf
4026,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,"the accuracy of models is severely underestimated, with most em failure cases stemming from syntactical variations of answers.",,https://export.arxiv.org/pdf/2305.06984v2.pdf
4027,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,"moreover, a zero-shot prompting method can be a reasonable substitute for human evaluation although it cannot detect unattributability in long-form answers.",,https://export.arxiv.org/pdf/2305.06984v2.pdf
4028,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,our insights and analysis in this paper will hopefully underpin the development of solid evaluation techniques in open-domain qa.,,https://export.arxiv.org/pdf/2305.06984v2.pdf
4029,235186933,Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting,conclusion,Insight-tree,"we explored the task of difficulty-controllable question generation, with question difficulty redefined as the inference steps required to answer it.",,https://www.aclanthology.org/2021.acl-long.465.pdf
4030,235186933,Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting,conclusion,Insight-tree,"a step-by-step generation framework was proposed to accomplish this objective, with an input sampler to extract the reasoning chain, a question generator to produce a simple question, and a question rewriter to further adapt it into a more complex one.",,https://www.aclanthology.org/2021.acl-long.465.pdf
4031,235186933,Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting,conclusion,Insight-tree,a dataset was automatically constructed based on hotpotqa to facilitate the research.,,https://www.aclanthology.org/2021.acl-long.465.pdf
4032,235186933,Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting,conclusion,Insight-tree,"extensive evaluations demonstrated that our method can effectively control difficulty of the generated questions, and keep high question quality at the same time.",,https://www.aclanthology.org/2021.acl-long.465.pdf
4033,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"to our knowledge, this is the largest publicly documented lm pretraining experiment, spanning 28 1.5b parameter models-larger in experiment scope than chinchilla (hoffmann et al., 2022) and also model scale than minibertas (warstadt et al., 2020), multiberts (sellam et al., 2022), pythia (biderman et al., 2023).",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4034,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"it is important to acknowledge each of these pretrainings, with their corresponding finetuning and evaluations is computationally and environmentally costly.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4035,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"with this in mind, we made the careful decision on what experiments to pursue-narrowing our list to: age of the corpora, quality filters, toxicity filters, and the choice of source domains.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4036,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"we carefully curated the choice of experiments in advance, without the luxury of multiple rounds of reflection and repetition, common in many nlp experimental settings.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4037,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"as a result, we struck a balance as best we could between the computational costs, and reproducible validity.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4038,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,we hope to justify the merits of our selection and also point out the surprises that motivate future work or a deeper look into the results.blackbox apis an additional limitation is our use of perspective's api for evaluating the toxicity of generations.,,https://export.arxiv.org/pdf/2305.13169v1.pdf
4039,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"while most of our toxicity filters and evaluations were in a compressed time period, pozzobon et al. (2023) have since demonstrated the irreproducibility of black-box apis, which may have shifting implementations over time.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4040,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"we also believe that while this is the standard procedure for popular toxic generation benchmarks like realtoxicityprompts, the reliance on apis and narrow evaluation setting can have limited implications for toxic generation in real applications.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4041,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",limitations compute expense & single shot experiments,Insight-tree,"for the time being, these are the best proxies we have.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4042,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",conclusion,Insight-tree,"the relative age of documents, content filters, and data sources each have significant effects on downstream model behaviour.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4043,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",conclusion,Insight-tree,"these effects can be reduced, but not eliminated, by finetuning.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4044,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",conclusion,Insight-tree,"we recommend that model developers and users pay close attention to these details in designing/selecting the model most relevant to their needs, as each decision has a specific, quantifiable trade-off profile.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4045,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",conclusion,Insight-tree,"for instance, it may be important to decide between improving toxicity identification or reducing toxic generation, performance on brand new or older data sources, and biomedical or books text domains.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4046,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",conclusion,Insight-tree,these countless choices are inherent in curating any pretraining dataset.,,https://export.arxiv.org/pdf/2305.13169v1.pdf
4047,258832491,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",conclusion,Insight-tree,"while we are only able to evaluate a small fraction of these, we are able to show which choices matter and by how much, and we hope to inspire further work evaluating dataset composition and predicting behaviors of models given pretraining datasets.",,https://export.arxiv.org/pdf/2305.13169v1.pdf
4048,218487733,Exploring and Predicting Transferability across NLP Tasks,conclusion,Insight-tree,"in this work, we conduct a large-scale empirical study of the transferability between 33 nlp tasks across three broad classes of problems, encompassing classification, question answering, and sequence labeling.",,https://arxiv.org/pdf/2005.00770v1.pdf
4049,218487733,Exploring and Predicting Transferability across NLP Tasks,conclusion,Insight-tree,"we show that the benefits of transfer learning are more pronounced than previously thought, especially when target training data is limited, and we develop methods that learn vector representations of tasks that can be used to reason about the relationships between them.",,https://arxiv.org/pdf/2005.00770v1.pdf
4050,218487733,Exploring and Predicting Transferability across NLP Tasks,conclusion,Insight-tree,these task embeddings allow us to predict source tasks that will positively transfer to a given target task.,,https://arxiv.org/pdf/2005.00770v1.pdf
4051,218487733,Exploring and Predicting Transferability across NLP Tasks,conclusion,Insight-tree,"our analysis suggests that data size, the similarity between the source and target tasks and domains, and task complexity are crucial for effective transfer, particularly in data-constrained regimes.",,https://arxiv.org/pdf/2005.00770v1.pdf
4052,128344862,Repurposing Entailment for Multi-Hop Question Answering Tasks,conclusions,Insight-tree,using entailment for question answering has seen limited success.,,https://arxiv.org/pdf/1904.09380v1.pdf
4053,128344862,Repurposing Entailment for Multi-Hop Question Answering Tasks,conclusions,Insight-tree,"neural entailment models are designed and trained on tasks defined over sentence pairs, whereas qa often requires reasoning over longer texts spanning multiple sentences.",,https://arxiv.org/pdf/1904.09380v1.pdf
4054,128344862,Repurposing Entailment for Multi-Hop Question Answering Tasks,conclusions,Insight-tree,"we propose multee, a novel qa model that addresses this mismatch.",,https://arxiv.org/pdf/1904.09380v1.pdf
4055,128344862,Repurposing Entailment for Multi-Hop Question Answering Tasks,conclusions,Insight-tree,it uses an existing entailment model to both focus on relevant sentences and aggregate information from these sentences.,,https://arxiv.org/pdf/1904.09380v1.pdf
4056,128344862,Repurposing Entailment for Multi-Hop Question Answering Tasks,conclusions,Insight-tree,"results on two challenging qa datasets, as well as our ablation study, indicate that entailment based qa can achieve state-of-the-art performance and is a promising direction for further research.",,https://arxiv.org/pdf/1904.09380v1.pdf
4057,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,this paper investigates calibration in the realistic application of odqa where users need to decide whether to trust the model prediction based on the confidence scores.,,https://export.arxiv.org/pdf/2205.12507v2.pdf
4058,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,"although confidence scores produced by existing calibration methods improves the popular ece metric, these confidence scores do not help distinguish correct and wrong predictions.",,https://export.arxiv.org/pdf/2205.12507v2.pdf
4059,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,"we propose to use the macroce metric to remedy the flaws, and existing calibration methods fail on our macroce metric.",,https://export.arxiv.org/pdf/2205.12507v2.pdf
4060,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,we further propose a simple and effective calibration method conscal that leverages training consistency.,,https://export.arxiv.org/pdf/2205.12507v2.pdf
4061,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,our human study confirms both the effectiveness of conscal as well as the alignment between macroce and human preference.,,https://export.arxiv.org/pdf/2205.12507v2.pdf
4062,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,"our work advocates and paves the path for user-centric calibration, and our con-scal method is a promising direction for better calibration.",,https://export.arxiv.org/pdf/2205.12507v2.pdf
4063,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,future work can adapt our calibration metric and method to more diverse tasks (such as generative tasks) and explore other ways to further improve user-centric calibration.,,https://export.arxiv.org/pdf/2205.12507v2.pdf
4064,256900985,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,conclusion,Insight-tree,"in this paper, we have introduced meta prompt tuning (mpt), which learns to initialize the prompt embeddings for adapting to a target task.",,https://export.arxiv.org/pdf/2302.08143v2.pdf
4065,256900985,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,conclusion,Insight-tree,we have identified key research questions and systematically studied where and how meta learning can improve cross-task generalization in prompt tuning.,,https://export.arxiv.org/pdf/2302.08143v2.pdf
4066,256900985,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,conclusion,Insight-tree,"we have empirically analyzed a representative set of meta learning methods in a variety of adaptation settings on a large, diverse collection of few-shot tasks.",,https://export.arxiv.org/pdf/2302.08143v2.pdf
4067,256900985,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,conclusion,Insight-tree,extensive experimental results and analysis verify the effectiveness of mpt.,,https://export.arxiv.org/pdf/2302.08143v2.pdf
4068,256900985,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,conclusion,Insight-tree,"given the findings, in the future, we would like to explore more advanced meta learning algorithms which can consistently outperform multi-task learning.",,https://export.arxiv.org/pdf/2302.08143v2.pdf
4069,220525901,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,conclusion and future work,Insight-tree,in this paper we have briefly described the multi-task approach for joint prediction of rumor stance and veracity for data obtained from various social media platforms (in our case twitter and reddit).,,https://arxiv.org/pdf/2007.07803v1.pdf
4070,220525901,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,conclusion and future work,Insight-tree,"we presented ensemble of deep learning models having the same architecture, but varying the parameters.",,https://arxiv.org/pdf/2007.07803v1.pdf
4071,220525901,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,conclusion and future work,Insight-tree,our approach outperforms the previous approaches by sufficient margin and able to generalize across different social media.,,https://arxiv.org/pdf/2007.07803v1.pdf
4072,220525901,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,conclusion and future work,Insight-tree,"in future, we can extend our model for multilingual setting [51] (ibereval is the counterpart of rumoreval 2019 for other languages [13]).",,https://arxiv.org/pdf/2007.07803v1.pdf
4073,220525901,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,conclusion and future work,Insight-tree,"moreover, we can leverage more sophisticated resources like pre-trained model which was trained specifically to handle data from different social media platforms.",,https://arxiv.org/pdf/2007.07803v1.pdf
4074,220525901,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,conclusion and future work,Insight-tree,"further, we can explore other methods like diffusion process of rumors [48] to make informed changes to model architecture.",,https://arxiv.org/pdf/2007.07803v1.pdf
4075,215785913,HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data,conclusion,Insight-tree,"we present hybridqa, which is collected as the first hybrid question answering dataset over both tabular and textual data.",,https://arxiv.org/pdf/2004.07347v3.pdf
4076,215785913,HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data,conclusion,Insight-tree,we release the data to facilitate the current research on using heterogeneous information to answer real-world questions.,,https://arxiv.org/pdf/2004.07347v3.pdf
4077,215785913,HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data,conclusion,Insight-tree,we design hybrider as a strong baseline and offer interesting insights about the model.,,https://arxiv.org/pdf/2004.07347v3.pdf
4078,215785913,HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data,conclusion,Insight-tree,we believe hybridqa is an interesting yet challenging nextproblem for the community to solve.,,https://arxiv.org/pdf/2004.07347v3.pdf
4079,236477336,Latent Reasoning for Low-Resource Question Generation,conclusions,Insight-tree,we proposed a jointly optimized two-phase model named plar for low-resource question generation.,,https://www.aclanthology.org/2021.findings-acl.265.pdf
4080,236477336,Latent Reasoning for Low-Resource Question Generation,conclusions,Insight-tree,plar effectively utilizes non-parallel singlehop and multi-hop question answering data to perform optimization.,,https://www.aclanthology.org/2021.findings-acl.265.pdf
4081,236477336,Latent Reasoning for Low-Resource Question Generation,conclusions,Insight-tree,we further designed a planning mechanism to guide the generation process of subquestions so that the generation results are valid to compose a multi-hop question.,,https://www.aclanthology.org/2021.findings-acl.265.pdf
4082,236477336,Latent Reasoning for Low-Resource Question Generation,conclusions,Insight-tree,"experimental results confirm that plar achieves better performance compared with the state-of-the-art under various metrics, especially in a question answering based evaluation.",,https://www.aclanthology.org/2021.findings-acl.265.pdf
4083,236477336,Latent Reasoning for Low-Resource Question Generation,conclusions,Insight-tree,"for future work, we will explore the heterogeneous multi-hop qg task that requires reasoning beyond plain texts, e.g., tables.",,https://www.aclanthology.org/2021.findings-acl.265.pdf
4084,257833654,Quantifying the Academic Quality of Children's Videos using Machine Comprehension,conclusion and future work,Insight-tree,"in this research, we demonstrate the ability of an rc model to assess academic quality by introducing a new dataset consisting of questions and answers from children's videos.",,https://export.arxiv.org/pdf/2303.17201v1.pdf
4085,257833654,Quantifying the Academic Quality of Children's Videos using Machine Comprehension,conclusion and future work,Insight-tree,we then determine the academic value of the top channels by measuring the number of textbook questions answered correctly by the model.,,https://export.arxiv.org/pdf/2303.17201v1.pdf
4086,257833654,Quantifying the Academic Quality of Children's Videos using Machine Comprehension,conclusion and future work,Insight-tree,"our analysis of over 80,000 videos posted on the top 100 channels provides a comprehensive evaluation of the academic quality of content on ytk and utilizes a large dataset of middle school textbook questions on various topics.",,https://export.arxiv.org/pdf/2303.17201v1.pdf
4087,257833654,Quantifying the Academic Quality of Children's Videos using Machine Comprehension,conclusion and future work,Insight-tree,"our findings reveal the academic topics covered in these children's videos, and we compare the quality of the channels.",,https://export.arxiv.org/pdf/2303.17201v1.pdf
4088,235364000,Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study,conclusion,Insight-tree,"we conduct a comprehensive analysis on the book qa task, taking the representative narrativeqa dataset as an example.",,https://arxiv.org/pdf/2106.03826v1.pdf
4089,235364000,Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study,conclusion,Insight-tree,"firstly, we design the book qa techniques by borrowing the wisdom from the cutting-edge open-domain qa research and demonstrate through extensive experiments that (1) evidence retrieval in book qa is difficult even with the state-of-the-art pre-trained lms, due to the factors of rich writing style, recurrent book plots and characters, and the requirement of high-level story understanding; (2) our proposed approaches that adapt pre-trained lms to books, especially the prereading technique for the reader training, are consistently helpful.",,https://arxiv.org/pdf/2106.03826v1.pdf
4090,234093776,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,conclusion,Insight-tree,"we presented qasper, an information-seeking qa dataset over nlp research papers.",,https://www.aclweb.org/anthology/2021.naacl-main.365.pdf
4091,234093776,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,conclusion,Insight-tree,"with natural questions asked as follow-up to titles and abstracts, the task presented by qasper requires evidence from multiple paragraphs and/or figures and tables within the full text of the papers.",,https://www.aclweb.org/anthology/2021.naacl-main.365.pdf
4092,234093776,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,conclusion,Insight-tree,our empirical quire workers to write questions grounded in those snippets.,,https://www.aclweb.org/anthology/2021.naacl-main.365.pdf
4093,234093776,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,conclusion,Insight-tree,"table 5: error analysis of our best model (led from row 5 from table 2) on 55 test examples with low f 1 score (excluding those with ""yes,"" ""no,"" or ""unanswerable"" gold answers).",,https://www.aclweb.org/anthology/2021.naacl-main.365.pdf
4094,234093776,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,conclusion,Insight-tree,"""quotations"" denote extractive gold answers.",,https://www.aclweb.org/anthology/2021.naacl-main.365.pdf
4095,234093776,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,conclusion,Insight-tree,we note lacks domain knowledge errors are not always solved by better entity type resolution (see ).,,https://www.aclweb.org/anthology/2021.naacl-main.365.pdf
4096,240526380,Real-time polyp detection model using convolutional neural networks,conclusions,Insight-tree,"this work has described the development of a dl model for real-time polyp detection, which could be integrated, in the future, into a cad system.",,
4097,240526380,Real-time polyp detection model using convolutional neural networks,conclusions,Insight-tree,yolov3 was selected as the base architecture for the development of this model due to its balance between performance and prediction time and complemented with an object-tracking filtering step able to reduce false positives.,,
4098,208089867,TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration *,conclusion,Insight-tree,the textgraphs 2019 shared task on multi-hop inference for explanation regeneration received four team submissions that exceeded the performance of the baseline system.,,https://www.aclweb.org/anthology/D19-5309.pdf
4099,208089867,TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration *,conclusion,Insight-tree,the systems used a variety of methods from additional knowledge resources (such as conceptnet or framenet) to directly training language models to perform multihop inference by predicting chains of facts.,,https://www.aclweb.org/anthology/D19-5309.pdf
4100,208089867,TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration *,conclusion,Insight-tree,"the top-performing system increased baseline performance by nearly a factor of two on this task, achieving a new state-of-the-art.",,https://www.aclweb.org/anthology/D19-5309.pdf
4101,250390665,CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course,conclusion,Insight-tree,"in this paper, we present cs1qa, a dataset for codebased question answering in introductory programming course.",,https://www.aclanthology.org/2022.naacl-main.148.pdf
4102,250390665,CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course,conclusion,Insight-tree,cs1qa's crowdsourced data from a programming course provide rich information that code understanding models need to consider to correctly answer the given questions.,,https://www.aclanthology.org/2022.naacl-main.148.pdf
4103,250390665,CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course,conclusion,Insight-tree,"we introduce three tasks for cs1qa, whose output can help students debug and reduce workloads for the teaching staff.",,https://www.aclanthology.org/2022.naacl-main.148.pdf
4104,250390665,CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course,conclusion,Insight-tree,results from the baseline models indicate that tasks for cs1qa are challenging for current language understanding models.,,https://www.aclanthology.org/2022.naacl-main.148.pdf
4105,250390665,CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course,conclusion,Insight-tree,cs1qa promotes further research to better represent and understand source code for code-based question answering.,,https://www.aclanthology.org/2022.naacl-main.148.pdf
4106,232269988,Evaluating Document Coherence Modelling,conclusion,Insight-tree,"we propose the new task of detecting whether there is an intruder sentence in a document, generated by replacing an original sentence with a similar sentence from a second document.",,https://arxiv.org/pdf/2103.10133v1.pdf
4107,232269988,Evaluating Document Coherence Modelling,conclusion,Insight-tree,"to benchmark model performance over this task, we construct a large-scale dataset consisting of documents from english wikipedia and cnn news articles.",,https://arxiv.org/pdf/2103.10133v1.pdf
4108,232269988,Evaluating Document Coherence Modelling,conclusion,Insight-tree,"experimental results show that pretrained lms which incorporate larger document contexts in pretraining perform remarkably well in-domain, but experience a substantial drop cross-domain.",,https://arxiv.org/pdf/2103.10133v1.pdf
4109,232269988,Evaluating Document Coherence Modelling,conclusion,Insight-tree,"in follow-up analysis based on human annotations, substantial divergences from human intuitions were observed, pointing to limitations in their ability to model document coherence.",,https://arxiv.org/pdf/2103.10133v1.pdf
4110,232269988,Evaluating Document Coherence Modelling,conclusion,Insight-tree,"further results over a linguistic probe dataset show that pretrained models fail to identify some linguistic characteristics that affect document coherence, suggesting room to improve for them to truly capture document coherence, and motivating the construction of a dataset with intruder text at the intra-sentential level.",,https://arxiv.org/pdf/2103.10133v1.pdf
4111,254246977,Global memory transformer for processing long documents,conclusion and future work,Insight-tree,this paper presented a study of our proposed model on two new tasks; masked language modeling as pretraining task and question answering task using hot-potqa as finetuning task.,,https://export.arxiv.org/pdf/2212.01650v1.pdf
4112,254246977,Global memory transformer for processing long documents,conclusion and future work,Insight-tree,experimental results showed good performance of the model on masked language modeling using linear learning rate.,,https://export.arxiv.org/pdf/2212.01650v1.pdf
4113,254246977,Global memory transformer for processing long documents,conclusion and future work,Insight-tree,the prposed model with chunked input outperformed t5 as baseline.,,https://export.arxiv.org/pdf/2212.01650v1.pdf
4114,254246977,Global memory transformer for processing long documents,conclusion and future work,Insight-tree,this approves previous results on translation task where the proposed model overcomes the base line.,,https://export.arxiv.org/pdf/2212.01650v1.pdf
4115,15017837,A DNA Network as an Information Processing System,conclusions,Insight-tree,"we have investigated the implementation, in a synthetic dna reaction network, of computing paradigms abstracted from two different cellular biochemical reaction networks, the yeast galactose network [34] and the type-1 incoherent feed-forward loop network motif [27].",,
4116,15017837,A DNA Network as an Information Processing System,conclusions,Insight-tree,the results of chemical kinetics simulations show that the proposed dna network can be programmed to implement transient pulse generation with a steady-state output that can be made robust to changes in network dosage.,,
4117,15017837,A DNA Network as an Information Processing System,conclusions,Insight-tree,this network has another interesting property: it can be configured such that the steady-state output is proportional to the initial dose of one of the inputs but insensitive to subsequent additions which generate only transient output pulses.,,