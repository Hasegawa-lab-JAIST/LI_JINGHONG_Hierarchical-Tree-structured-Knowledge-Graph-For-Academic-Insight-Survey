,corpusid,paper-title,section-title,Target,Content,ref_paper_id,pdfurl
0,263908862,Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation,limitation and discussion,Insight-tree,"tasks beyond image generation. idea2img explores the emergent ability of multimodal self-refinement in lmmbased systems, through the image design and generation task. specifically, idea2img views the t2i model to use as an unknown multimodal environment to explore, and iteratively refines t2i prompts to find its optimal usage. this concept mirrors the intrinsic human approach of iterative problem-solving when faced with unknown environments or complex challenges. we leave its extension to other intriguing tasks [48], e.g., gui navigation, embodied agents, and complicated visual reasoning, for future exploration. from a single image generation model to multiple tools. idea2img explores the usage of a single image generation model, including a text-to-image model [34] or a textconditioned image-to-image model [24]. when needed, other specialized generative models like controlnet [56], inpainting [4], region-controlled t2i generation [21,51], customized generation [8,35], and video generation [42,53] can be seamlessly switched as replacements. that is, idea2img could broadly boost different image generation models with diverse specialties by exploring their optimal text description or instruction prompts.beyond a single generation model, idea2img can also be used to allocate multiple tools as in multimodal agent studies [45,49]. in this case, idea2img isn't limited to optimizing the use of individual tools but also investigates their effective collaboration when used together, such as generator selection and multi-step visual generation. consolidating explored knowledge. we have shown the effectiveness of lmm iterative self-refinement in automatic image design and generation. going beyond merely preserving the probed t2i model's properties in memory, idea2img can also help to consolidate or distill the explored knowledge into t2i model parameters, such that no inference-time iterative refinement is needed when encountering similar generation scenarios. for example, one could collect a dataset using idea2img for a scenario of interest, and fine-tune a t2i model with the explored selfrefinement trajectory. storing the probed knowledge as sample-agnostic prompt for each image generation model is another promising direction [14,32,57].",{},https://export.arxiv.org/pdf/2310.08541v1.pdf
1,263908862,Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation,conclusion,Insight-tree,"we have presented idea2img, a multimodal iterative selfrefinement framework that leverages gpt-4v(ision) for image design and generation. idea2img delves into the emergent capabilities of iterative self-refinement in lmm-based systems, showcasing its effectiveness in improving, assessing, and verifying the generated multimodal content. the user preference study demonstrates idea2img's capability in assisting humans to find the optimal usage of a t2i model for automatic image design and generation.",{},https://export.arxiv.org/pdf/2310.08541v1.pdf
2,248665596,From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective,conclusion,Insight-tree,"in this paper, we have built on the splade model, and studied to which extent it is able to take advantage of training improvements like distillation and hard negative mining, as well as better suited plm initialization: combined altogether, the resulting model reaches state-of-the-art performance on both in-domain and zeroshot evaluation. we also investigated the link between effectiveness and efficiency -induced by the degree of regularization -highlighting that more expressive models are better at generalization. ",{220302524: '[30]'},https://arxiv.org/pdf/2205.04733v2.pdf
3,264490922,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems,discussion and conclusion,Insight-tree,"large general-purpose language models, such as the gpt family of models (brown et al., 2020;openai, 2023), lamda (thoppilan et al., 2022), palm (chowdhery et al., 2022;anil et al., 2023), and others, have propagated into informationseeking workflows of a general audience.a vast host of existing and ongoing work in nlp examines the deficiencies of these language models, ranging from hallucinated generations (bang et al., 2023;ji et al., 2023), lack of transparency (weld and bansal, 2019;lipton, 2018), reasoning gaps (bang et al., 2023;press et al., 2022), and more.however, close examination of the other piece of the puzzle-the user-is fairly sparse.literature in explainable nlp considers how nlp systems, and their underlying reasoning process, can be made available to the user to allow for better, more informed use of these systems and their predictions (ribeiro et al., 2016;wang and yin, 2021).extending this view of explainability, we considered the question of aiding users in their decision-making process, not by explaining the model prediction but by providing them with relevant pieces of information to assess the prediction.as argued by the contemporary work by fok and weld (2023), explanations in the form of the model's internal reasoning are rarely useful for supporting human decision-making (bansal et al., 2021;buÃ§inca et al., 2021;zhang et al., 2020).instead, explanations should aim to help the users assess the model prediction.our study takes a step in this direction by analyzing users' trust and reliance on qa models by providing information external to the model's input to enable verification of model predictions.","{123758373: 'Chen and Durrett, 2019;', 246652372: 'Bang et al., 2023;', 174801764: 'Min et al., 2019;', 221749191: 'Trivedi et al., 2020', 248227734: 'Xie et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.14331v2.pdf
4,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,conclusion & future work,Insight-tree,"in this work, we proposed cft as an improvement upon end-to-end learning. to enable research on this topic, we developed a new schema for generating recommendation datasets, which we instantiated in two domains. we showed that cft indeed consistently outperforms end-to-end learning, as much as 32% for local dining. furthermore, we found evidence suggesting that more component tasks can be beneficial for cft. finally, instantiating chain of thought prompting in our dataset and cft in sports understanding, we found cft to be as good or better with lms only 7.4% of the size.","{204915921: 'Khot et al., 2020;', 248666080: 'Khot et al. 2022', 174801080: 'Min et al., 2019;', 211258645: 'Perez et al., 2020', 219573621: 'Talmor et al., 2020', 211003735: 'Wolfson et al., 2020;', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2210.12607v1.pdf
5,253098647,Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models,limitations,Insight-tree,"this work focuses on testing if cft outperforms end-to-end learning and chain of thought prompting in two very different domains. despite the positive evidence, it remains to be seen: (i) if task decomposition can be fully automated, and (ii) if different decompositions-in the case of tasks that allow for multiple decompositions-yield similar results. both are second-order research questions that can be pursued once compositionality has been confirmed to improve performance. importantly, both questions have been left open in the initial chain of thought work as well. we hope that our results will add to theirs in attracting more attention to these questions in the future.another limitation of this work is that cft is not applicable to several decomposition datasets that have been proposed. for example, a dataset focused on compositional generalization may include many different types of questions, each requiring different types of intermediate steps. cft is not designed for intermediate steps that carry out very heterogeneous logic. nonetheless, as shown in the recommendation tasks, cft is still relevant for a substantial family of tasks with real-world applicability.lastly, this work is limited by its focus on the english language, and by the use of gpt-3 for its unique range of model sizes. for example, when we discuss that cft on a 13b parameter model (curie) is a much cheaper alternative to chain of thought prompting on a 175b parameter model (davinci), the finding is limited to this setting. it is important to replicate this work on other languages and models, which we plan to do as these become available.comprises two attributes. for world cities, we have a = {temperature, population} where average city temperatures are obtained from wikipedia 4 and city populations from sim-plemaps 2019. 5 after merging items from both sources, we end with 347 well-known cities (>50k inhabitans) from around the globe, such that d c = (i f ull c , {temperature, population}) and |i f ull c | = 347. for local restaurants, we randomly sample 240 restaurants from the city with most restaurants in the yelp dataset 6 , toronto. we have a = {price, distance} where restaurant prices are obtained from yelp and distances to a hypothetical location are randomly generated, thus limiting the lm's access to prior knowledge in this scenario. with that, we have d r = (i f ull r , {price, distance}) and |i f ull r | = 240.in terms of component tasks, we have 694 factual statements for the cities domain and 480 for restaurants, covering two attributes per item. whenever factual statements are provided in cft, they always cover i f ull entirely in order to give the lm full knowledge of the attribute values.","{204915921: 'Khot et al., 2020;', 248666080: 'Khot et al. 2022', 174801080: 'Min et al., 2019;', 211258645: 'Perez et al., 2020', 219573621: 'Talmor et al., 2020', 211003735: 'Wolfson et al., 2020;', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2210.12607v1.pdf
6,2945602,Proteome Science Detection and identification of NAP-2 as a biomarker in hepatitis B-related hepatocellular carcinoma by proteomic approach,conclusion,Insight-tree,"in summary, we have identified a set of protein peaks that could discriminate hcc from healthy controls. from the protein peaks specific to hcc disease, we identified and characterized neutrophil-activating peptide-2 as a potential proteomic biomarker of hcc. further studies with larger sample sizes will be needed to verify this specific protein marker and to address its efficacy, especially with regard to discriminating histologic types of hcc and disease stages. nevertheless, our study demonstrates a rational approach for identifying hcc biomarkers that could be used for detection and monitoring hcc by proteomic techniques.",{},NaN
7,208000835,Generalizing Question Answering System with Pre-trained Language Model Fine-tuning,conclusion,Insight-tree,"in this paper, we propose a multi-task framework to improve the generalization ability of question answering systems by leveraging large pre-trained language models. experimental results indicate the effectiveness of our methods on broader qa tasks, with an average exact match score of 56.59 and an average f1 score of 68.98, which are significantly higher than the bert-large baseline results by 8.39 and 7.22, respectively.","{67855846: 'Dua et al., 2019', 173188058: 'Talmor and Berant 2019'}",https://www.aclweb.org/anthology/D19-5827.pdf
8,263609132,LARGE LANGUAGE MODELS CANNOT SELF-CORRECT REASONING YET,conclusion,Insight-tree,"our research shows that llms are not yet capable of self-correcting their reasoning.this implies that expecting these models to inherently recognize and rectify their inaccuracies might be overly optimistic, at least with the current state of technology.more broadly, this underscores the need for ongoing improvement and a multi-faceted approach when deploying llms in real-world applications.in light of these findings, it is imperative for researchers and practitioners to approach the concept of self-correction with a discerning perspective, acknowledging its potential and recognizing its boundaries.by doing so, we can better equip this technique to address the limitations of llms, steering their evolution towards enhanced accuracy and reliability.","{258833055: 'Shinn et al. 2023', 247595263: 'Wang et al., 2022;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.01798v1.pdf
9,263609132,LARGE LANGUAGE MODELS CANNOT SELF-CORRECT REASONING YET,limitations and broader impact,Insight-tree,"although our exploration spans a variety of self-correction strategies, prompts, and tasks, as we mentioned earlier, it is plausible that there exist specific prompts or strategies that could enhance the reasoning performance of models for particular benchmarks.however, searching such prompts or strategies may inadvertently rely on external feedback, either from human insights or training data.additionally, such search efforts can be equally applied to the pre-hoc prompts (zhou et al., 2022;yang et al., 2023).a potential outcome of our study might be that it discourages research into self-correction for certain applications.nonetheless, we are optimistic that our findings will steer future research towards more practical applications of self-correction, especially those where tangible benefits can be derived, potentially by leveraging external feedback.furthermore, several related works have already presented findings consistent with our observation: that self-correction, when devoid of external feedback, serves as a relatively weak baseline (gou et al., 2023;zhou et al., 2023a).despite this, we have identified a prevailing ambiguity in the wider community, with even domain experts being uncertain about the intricacies of when and how selfcorrection operates.some existing literature may inadvertently contribute to this confusion, either by relegating crucial details about label usage to less prominent sections or by failing to clarify that their designed self-correction strategies actually incorporate external feedback.our intention in this paper is to amplify these concerns and offer a comprehensive overview of the state of ""selfcorrection"" in llms.the title, ""large language models cannot self-correct reasoning yet"", is not an outright dismissal of self-correction techniques.instead, it serves as a call to action, urging researchers to approach this domain with a discerning and critical perspective.furthermore, it encourages future research to explore approaches that can genuinely enhance reasoning.for the sake of clarity, we recommend omitting the term ""self"" from concepts like self-correction or selfcritique when the context does not strictly adhere to an intrinsic self-correction setting and adopting less confusing terms like ""correction with external tools/feedback"".finally, in light of our findings that llms struggle to self-correct reasoning based purely on their inherent capabilities, we urge a more circumspect view on any unwarranted optimism (or fear) regarding the autonomous evolution of llms (and ai systems) through self-improvement.","{258833055: 'Shinn et al. 2023', 247595263: 'Wang et al., 2022;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.01798v1.pdf
10,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we introduce ynat, the first korean topic classification benchmark. the benchmark includes 63,892 news headlines classified to a single hand-labeled topic among 7 categories. we assume each headline has only a single topic, but it could be formulated as multi-label classification. we thus open the second and third relevant topic annotations. also, urls for each headlines are accompanied for future work if metadata is needed. if some of them requires permission to use, one should contact to the agency. we expect ynat to serve as a simple and basic nlu task compared to others in klue.","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
11,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we create the first human-annotated korean sts benchmark, klue-sts, that covers multiple domains and styles with free accessibility to everyone. the similarity score annotation process is specially designed to capture the characteristics of the korean language. covering the expressions from various domains, our benchmark is expected to be a useful resource for further research, beyond serving as a benchmark. our benchmark helps to develop numerous models established on sts resources, such as sentencebert [115].","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
12,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"our new dataset, klue-nli, is the first resource constructed upon naturally occurring korean sentences. klue-nli represents diverse linguistic phenomena, writing style, degree of formality and contents that are most natural and suitable for korean. the premise sentences of our dataset come from six korean corpora, and the hypothesis sentences are written by well-trained workers.","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
13,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we construct a new korean ner benchmark that covers broad domains and styles, which is freely accessible to anyone. the entity types are annotated so that a model has to use both morphological and contextual cues. the character-level entity tagging and evaluation method reflects the characteristics of korean morphology. since klue-ner dataset covers both formal news articles and informal user-generated web texts, we hope that our benchmark helps develop ner models that can be used in a wide a range of domains, and serve as a resource for developing advanced models for information extraction.","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
14,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we propose klue-re, a large-scale human-annotated re benchmark for korean. to overcome the lack of large-scale and up-to-date korean kbs, we design an efficient candidate collection method, coupled with an effective annotation scheme. klue-re can not only be used for online information extraction but also contribute to building a large-scale knowledge graph from unstructured texts. we therefore expect klue-re to be a starting point for building a large-scale, ever-growing public kb in korean, as well as a valuable korean nlu benchmark.","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
15,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we build a korean dp benchmark klue-dp consisting of formal news and informal user-generated web data. klue-dp is helpful for developing a dp model that can be used in multiple domains. pos tagging is performed together to improve dp performance, and the tagset and guideline for dp and pos tagging are applied by revising the existing tta dataset. this guideline is customized to reflect the characteristics of korean (agglutinative, free word order, etc.), and it also tackles omission of predicates in web data or errors in spacing. we hope that our benchmarks will help in the development of korean dp models and other natural language processing. 40 https://universaldependencies.org","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
16,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we create a new challenging korean mrc benchmark named (klue-mrc). in order to evaluate different aspects of mrc capabilities, klue-mrc includes multi-domain passages and three types of questions: paraphrase, multisentence reasoning, and unanswerable. klue-mrc shows improvements in question type diversity, difficulty, and lexical overlap compared to existing korean mrc datasets.","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
17,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we introduce wizard-of-seoul (wos), the first large-scale korean multi-domain task-oriented dialogue dataset that simulates conversations between seoul tourists and travel agents. we adapt 'self-dialog' for efficiently scaling up of dialogue collection scheme. in addition, consideration on annotation interfaces (drop-down menu and turn-switching) mitigates erroneous cases and diverse goal instructions including counterfactual ones promote each conversation to be more natural and challenging. we hope that wos sparks various future dialogue research in korean and also offers valuable insights to pushing forward end-to-end dialogue modeling.","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
18,234790338,KLUE: Korean Language Understanding Evaluation Seungwon Do,conclusion,Insight-tree,"we present klue, a suite of korean nlu benchmarks that includes diverse tasks. we open klue to everyone, and we also provide korean language models trained to outperform multilingual models and other existing open-sourced korean language models. we set high standards from the outset, as we built the benchmark and trained the models from scratch. we designed the benchmark datasets and trained the annotators rigorously to consider potential ethical issues including private information and hate speech. we documented in detail all of the benchmark construction and testing processes. we also discussed broader impacts and limitations of klue and our models. despite the limitations, klue and the accompanying language models will facilitate future korean nlp research by setting a valuable precedent describing how datasets and language models should be created and spread to a wider community.","{165163607: '[21]', 196170479: '38]', 216868500: '66', 86611921: '69]', 174801764: '[90]', 52822214: '145,', 189898081: '147,'}",https://arxiv.org/pdf/2105.09680v4.pdf
19,3322899,The Intelligent Healthcare Data Management System Using Nanosensors,conclusions and future works,Insight-tree,the proposed intelligent healthcare data management system dynamically assembles the nanosensor information of the sick person or patient and operates it on the smart phone.this system also transforms raw data of the user from nanosensors to hl7 format and transfers the encrypted hl7 data to a remote healthcare server.,{},http://downloads.hindawi.com/journals/js/2017/7483075.pdf
20,252407569,Dynamic Relevance Graph Network for Knowledge-Aware Question Answering,conclusion,Insight-tree,"in this paper, we propose a novel dynamic relevance graph network (drgn) architecture for commonsense question answering given an external source of knowledge in the form of a knowledge graph. our model learns the graph node representation while a) exploits the existing relations in kg, b) re-scales the importance of the neighbor nodes in the graph based on training a dynamic relevance matrix, c) establishes direct connections between graph nodes based on measuring the relevance scores of the nodes dynamically during training. the dynamic relevance edges help in finding the chain of reasoning when there are missing edges in the original kg. our quantitative and qualitative analysis shows that the proposed approach facilitates answering the complex questions that need multiple hops of reasoning. furthermore, since drgn uses the relevance between the question node and graph entities, it exploits the richer semantic context of the question in graph reasoning which leads to improvements in the performance on the negative questions. our proposed approach shows competitive performance on two qa benchmarks, including commonsenseqa and openbookqa.","{207853300: 'Fang et al., 2020', 174801080: 'Min et al., 2019;', 52822214: 'Yang et al., 2018', 233219869: 'Yasunaga et al., 2021', 222208994: 'Zheng and Kordjamshidi, 2020'}",https://www.aclanthology.org/2022.coling-1.116.pdf
21,216553833,Contextualized Representations Using Textual Encyclopedic Knowledge,conclusion,Insight-tree,"we presented a method to build text representations by jointly contextualizing the input with dynamically retrieved textual encyclopedic knowledge. we showed consistent improvements, in-and out-of-domain, across multiple reading comprehension benchmarks that require factual reasoning and knowledge well represented in the background collection.","{202583433: 'Das et al., 2019b', 67855846: 'Dua et al., 2019', 204823992: 'Fisch et al., 2019', 198229624: 'Joshi et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 173188058: 'Talmor and Berant, 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2004.12006v2.pdf
22,222272049,Case Study: Deontological Ethics in NLP,conclusion,Insight-tree,"two principles of deontological ethics-namely the generalization principle and respect for autonomy via informed consent-can be used to decide if an action is ethical. despite the limitations of these principles, they can provide useful insights into making nlp systems more ethical. through the four case studies discussed in this paper, we demonstrate how these principles can be used to evaluate the decisions made by nlp systems and to identify the missing aspects. for each of the case studies, we also present potential directions for nlp research to move forward and make the system more ethical.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2021.naacl-main.297.pdf
23,255546131,ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,conclusion,Insight-tree,"in this work, we propose a simple method to improve task-agnostic distillation generalization by leveraging mtl. the teacher is first augmented by mtl and then distilled. empirical results show that our method outperforms several baselines on in-domain, out-domain and low-resource tasks.",{},https://export.arxiv.org/pdf/2301.03416v1.pdf
24,255546131,ERNIE 3.0 Tiny: Frustratingly Simple Method to Improve Task-Agnostic Distillation Generalization,limitation,Insight-tree,"compared with vanilla task-agnostic distillation, mitkd has an additional multi-task finetuning stage which may require additional computation resources.    ",{},https://export.arxiv.org/pdf/2301.03416v1.pdf
25,158046817,Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs,conclusion,Insight-tree,"we propose a new gnn-based method for multihop rc across multiple documents. we introduce the hde graph, a heterogeneous graph for multiple-hop reasoning over nodes representing different granularity levels of information. we use co-attention and self-attention to encode candidates, documents, entities of mentions of candidates and query subjects into query-aware representations, which are then employed to initialize graph node representations. evaluated on wiki-hop, our end-to-end trained single neural model achieves the state-of-the-art performance on the blind test set. in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa.",{},https://arxiv.org/pdf/1905.07374v1.pdf
26,226236844,Exploring Question-Specific Rewards for Generating Deep Questions,conclusion,Insight-tree,"in this paper, we optimize three question-specific rewards via reinforcement learning on a seq2seq based question generator, aiming to improve the fluency, relevance and answerability of the generated questions. through comprehensive analytic experiments, including automatic and human evaluation, consistency validation, and meso analysis, we show that the effectiveness of a reward is poorly reflected by automatic evaluation metrics such as bleu. instead, we find a reward that correlates well with the human judgement generally has better effects on improving the question quality. in future works, we believe these observations can help to guide the design of other qg-specific rewards that target on unexplored aspects of question generation, such as the informativeness and the utility of questions.","{198229624: 'Joshi et al. 2020', 216553210: 'Pan et al. 2020', 52822214: 'Yang et al., 2018a', 202572810: 'Zhang and Bansal, 2019'}",https://www.aclweb.org/anthology/2020.coling-main.228.pdf
27,211258645,Unsupervised Question Decomposition for Question Answering,conclusion,Insight-tree,"we proposed an algorithm that decomposes questions without supervision, using 3 stages: (1) learning to decompose using pseudo-decompositions without supervision, (2) answering sub-questions with an off-the-shelf qa system, and (3) answering hard questions more accurately using sub-questions and their answers as additional input. when evaluated on hotpotqa, a standard benchmark for multihop qa, our approach significantly improved accuracy over an equivalent model that did not use decompositions. our approach relies only on the final answer as supervision but works as effectively as state-of-the-art methods that rely on strong supervision, such as supporting fact labels or example decompositions. qualitatively, we found that unsupervised decomposition resulted in fluent sub-questions whose answers often match the annotated supporting facts in hotpotqa. our unsupervised decompositions are largely extractive, which is effective for compositional, multi-hop questions but not all complex questions, showing room for future work. overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems.","{189927896: 'Jiang & Bansal 2019a', 202565945: 'Jiang & Bansal, 2019b', 174801764: 'Min et al., 2019a', 174801080: 'Min et al. 2019b', 202660724: 'Nie et al., 2019', 207870753: 'Tu et al., 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2002.09758v1.pdf
28,207756753,Adversarial NLI: A New Benchmark for Natural Language Understanding,discussion & conclusion,Insight-tree,"in this work, we used a human-and-model-in-theloop training method to collect a new benchmark for natural language understanding. the benchmark is designed to be challenging to current stateof-the-art models. annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into misclassifying, but that another person would correctly classify. we found that non-expert annotators, in this gamified setting and with appropriate incentives, are remarkably creative at finding and exploiting weaknesses. we collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult. training on this new data yielded the state of the art on existing nli benchmarks.",{},https://www.aclweb.org/anthology/2020.acl-main.441.pdf
29,221507798,KILT: a Benchmark for Knowledge Intensive Language Tasks,conclusion,Insight-tree,"we introduce kilt, a benchmark for assessing models that need to condition on specific knowledge in a defined snapshot of wikipedia to solve tasks spanning five domains. the goal is to catalyze and facilitate research towards general and explainable models equipped with task-agnostic representations of knowledge. our experiments show promising results for a general solution combining dense retrieval and seq2seq generations, although there is large room for improvements. in particular, we find that provenance of current models is generally low.",{},https://www.aclweb.org/anthology/2021.naacl-main.200.pdf
30,235313508,Question Answering Over Temporal Knowledge Graphs,conclusion,Insight-tree,"in this paper we introduce cronquestions, a new dataset for temporal knowledge graph question answering. while there exist some temporal kgqa datasets, they are all based on non-temporal kgs (e.g., freebase) and have relatively few questions. our dataset consists of both a temporal kg as well as a large set of temporal questions requiring various structures of reasoning. in order to develop such a large dataset, we used a synthetic   generation procedure, leading to a question distribution that is artificial from a semantic perspective. however, having a large dataset provides an opportunity to train models, rather than just evaluate them. we experimentally show that increasing the training dataset size steadily improves the performance of certain methods on the tkgqa task. we first apply large pre-trained lm based qa methods on our new dataset. then we inject kg embeddings, both temporal and non-temporal, into these lms and observe significant improvement in performance. we also propose a new method, cronkgqa, that is able to leverage temporal kg embeddings to perform tkgqa. in our experiments, cronkgqa outperforms all baselines. these results suggest that kg embeddings can be effectively used to perform temporal kgqa, although there remains significant scope for improvement when it comes to complex reasoning questions.",{},https://www.aclanthology.org/2021.acl-long.520.pdf
31,245634501,OpenQA: Hybrid QA System Relying on Structured Knowledge Base as well as Non-structured Data,conclusion,Insight-tree,"we propose an intelligent question-answering system based on structured and unstructured data, that is, openqa, in which users can give query questions and the model can quickly give accurate answers back to users. we integrate the kbqa structured question answering based on semantic deep learning analysis and the two-stage unstructured question answering based on machine reading comprehension into the intelligent question answering system, and return the answer with the highest probability through the answer selection module based on transformer.",{},https://arxiv.org/pdf/2112.15356v1.pdf
32,260682258,Towards General Text Embeddings with Multi-stage Contrastive Learning,conclusion,Insight-tree,"this paper presents a multi-stage contrastive learning approach to develop text embedding model that can be applied to various tasks. our model benefits from a diverse training data mixture, enabling it to achieve good generalization performance for single vector embedding. through extensive evaluation on multiple benchmarks, we demonstrate the effectiveness and versatility of our text embedding model. our future work will focus on scaling the model to support longer context, extending it to support multilingual and multi-modal applications, as well as exploring the benefits of prompts and instructions. academic paper the scientific articles usually have a higher quality due to its formal nature. for each paper, we use the title as query and its abstract as document for constructing text pairs. the articles are mined from different websites (such as arxiv, biorxiv, medrxiv, pubmed and semantic scholar) to cover a wide range of topics.","{253581733: 'Asai et al., 2023', 249097975: 'Izacard et al., 2022a', 252907685: 'Muennighoff et al., 2023', 245144556: 'Ni et al., 2022b', 254853816: 'Su et al., 2023;', 233296016: 'Thakur et al., 2021', 220302524: 'Xiong et al., 2021;', 247447562: 'Zhou et al. 2022'}",https://export.arxiv.org/pdf/2308.03281v1.pdf
33,249062555,Teaching Broad Reasoning Skills via Decomposition-Guided Contexts,conclusions,Insight-tree,"large language models demonstrate impressive reading comprehension abilities and a wide variety of reasoning skills. despite these abilities and the availability of large scale multihop qa datasets, large lm-based qa models do not reliably learn to use such reasoning skills for answering complex questions. in this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions. our transfer results on actual qa datasets also add to the line of work that shows synthetic datasets can be used to inject useful skills that transfer over to real natural language tasks. given the artifact issues in real datasets (specifically, in their contexts) and the difficulty in controlling for them via perturbations, leveraging existing multihop questions for their broad reasoning patterns but using synthetic contexts appears to be a viable alternative for carefully constructing teaching datasets, where models can learn the right way to reason. ",{},https://arxiv.org/pdf/2205.12496v1.pdf
34,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",conclusion,Insight-tree,"we introduced instructor, a single model that creates broadly-applicable text embeddings using natural language instructions. we constructed medi, a collection of diverse datasets, to finetune instructor with instructions. our extensive experiments showed that instructor achieves state-of-the-art performance on text embedding benchmarks, as well as prompt retrieval for fewshot in-context learning. we hope that researchers and practitioners will benefit from our embeddings or our datasets for tasks of their interest.",{},https://export.arxiv.org/pdf/2212.09741v3.pdf
35,254853816,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",limitations,Insight-tree,"although  table 5 in the appendix. at each step, we first randomly select a dataset and then construct a minibatch only using the examples from that dataset. in this way, we ensure that in-batch negatives are sampled from the same dataset, thereby preventing the model from using task differences to predict the negative label. we use the maximum batch size that fits the machine memory and run all our experiments on 40gb a100 gpus.training we initialize instructor with the gtr-large model (ni et al., 2021, 335m parameters) 4 and finetune it on medi using the adamw optimizer with learning rate 2 Ã 10 â5 and warmup ratio 0.1. we use a softmax temperature of 0.01 and finetune instructor for 20k steps.baselines we use the official mteb benchmark for comparisons, but here we highlight several strong baselines with the following two types. the first class of baselines is embedding models specializing in information retrieval: contriever-  , 2022). they are mainly trained on symmetric paraphrase datasets such as nli (williams et al., 2018) and the quora question pairs. 5 all of these baselines are based on pretrained language models, achieving strong performance on the mteb leaderboard. in particular, sent-t5-xxl and gtr-xxl (both with 4.8b parameters) achieve the first and second best average performances.",{},https://export.arxiv.org/pdf/2212.09741v3.pdf
36,118548248,He II Î»4686 IN Î· CARINAE: COLLAPSE OF THE WIND-WIND COLLISION REGION DURING PERIASTRON PASSAGE,conclusions,Insight-tree,"in this paper, we present data from the last 4 low excitation events in Î· car in the light of the he ii Î»4686 emission line. we summarize our results and conclusions below.",{},https://arxiv.org/pdf/1104.2276v3.pdf
37,235485084,Weakly Supervised Pre-Training for Multi-Hop Retriever,conclusion,Insight-tree,"answering complex questions includes reasoning across multiple documents. recent studies have found that reasoning requires learning sub-question detection and relevant document retrieval to predict n correct answer with supporting facts. however, building such datasets requires costly human annotation and has limited scalability. to address this issue, we proposed a weakly supervised pre-training method for multi-hop retriever, louvre. our pretraining method contains three elements: ""next document prediction"" task, ""bridge entity re-phrasing"", and a model. we demonstrated the efficacy of louvre and its robustness on few-shot settings with extensive experiments on supporting document retrieval task and end-to-end multi-hop qa task. we also showed that our method performs very well at a much lower inference cost.   calculate score of each paths/documents by jointly encode each document with the given question. as a result, reranking takes a huge portion of computation time of the end-to-end multi-hop qa pipeline. semanticretrievalmrs (nie et al., 2019) propose the document reranking model that takes output of sparse retrievers such as tf-idf. since the model outputs documents not a list of supporting documents, we use the same document rearranging method as tf-idf above. pathretriever (asai et al., 2020) and","{208267807: 'Asai et al., 2020', 153312687: 'Ding et al., 2019', 207853300: 'Fang et al., 2020;', 174801080: 'Min et al., 2019;', 202660724: 'Nie et al., 2019', 211258645: 'Perez et al. 2020', 202773198: 'Qi et al., 2019', 128345225: 'Sun et al., 2019;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2106.09983v1.pdf
38,250562913,Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation,conclusion,Insight-tree,"in this paper, an incremental learning mrc model with uncertainty-aware fixed memory and adversarial domain adaptation, ma-mrc, is proposed for continual mrc and alleviating catastrophically forgetting. inspired by the human learning process, there are two main ideas of ma-mrc: a memory that stores a small number of samples in previous seen domain data and always focuses on what the model most needs to replay; and adversarial learning the domain adaptation in a two-player game to learn better transfer representations between previous and current domain data. experimental results show that the proposed ma-mrc can achieve a good continuous learning performance without catastrophically forgetting under cda-c and cda-q settings.","{204823992: 'Fisch et al. 2019', 204800552: 'Lee et al., 2019', 226254596: 'Seonwoo et al., 2020;', 221292890: 'Su et al. 2020'}",https://export.arxiv.org/pdf/2208.05217v1.pdf
39,248426967,HYBRIDIALOGUE: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data,conclusion,Insight-tree,"in this paper, we presented a novel dataset, hy-bridialogue, for information-seeking dialogue where knowledge is grounded in both tables and text. while previous work has combined table and text modality in the question-answering space, this has not been utilized in the dialogue setting. our results in the various tasks demonstrate that there is still significant room for improvement and illustrate the need to build models that can adapt well to this hybrid format. in addition to the baseline tasks, future research can utilize hybridialogue to explore automatic multihop question decomposition.","{224803601: 'Chen et al., 2020a', 215785913: 'Chen et al., 2020b', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.findings-acl.41.pdf
40,262824840,An In-depth Survey of Large Language Model-based Artificial Intelligence Agents,conclusion,Insight-tree,"in this paper, we presented a comprehensive and systematic survey of the llm-based agents.",{},https://export.arxiv.org/pdf/2309.14365v1.pdf
41,261681729,AGent: A Novel Pipeline for Automatically Creating Unanswerable Questions,conclusion and future works,Insight-tree,"in this work, we propose agent, a novel pipeline designed to automatically generate two sets of unanswerable questions from a dataset of answerable questions. we systematically apply agent on squad and hotpotqa to generate unanswerable questions. through a two-stage process of human reviewing, we demonstrate that agent unanswerable questions exhibit a low error rate.",,https://export.arxiv.org/pdf/2309.05103v1.pdf
42,253761592,Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,conclusions,Insight-tree,"inspired by how humans accumulate skills from past experience and re-use them to solve new tasks, in this paper, we develop and conduct extensive experiments with transformer-based task-level mixture-of-expert (moe) models, in hope to provide new insights on multi-task learning and crosstask generalization in nlp. firstly, we empirically investigate importance design choices and quantify their influence on final model. secondly, in both few-shot and zero-shot settings, we demonstrate that task-level mixture-of-expert models are better at generalizing to new tasks. finally, by conducting a detailed analysis on the routing decisions, we find they have strong correlations with human-defined task characteristics, even when the decisions are learned spontaneously without no prior knowledge such as pre-computed task representations. we hope our work provide useful advice on training and interpreting multi-task models in nlp and we hope it will inspire future work in improving multitask learning and cross-task generalization in nlp.",,https://export.arxiv.org/pdf/2205.12701v2.pdf
43,8455535,Molecular Analysis of the Interaction of the Snake Venom Rhodocytin with the Platelet Receptor CLEC-2,conclusions,Insight-tree,"we have previously solved the crystal structures of both rhodocytin and clec-2, but there is no crystallographic structure of the rhodocytin-clec-2 complex. using three dimensional structures of both rhodocytin and clec-2, we have generated models of the likely modes of interaction of the venom protein and its receptor on platelets and have investigated and analysed the computational models of the interaction which we have generated [30,32,33]. using a set of analytical algorithms and approaches, we have assessed the properties of the interfacing surfaces and the contribution made to the interaction by specific intermolecular contacts, including salt bridges and hydrogen bonds. in addition to this, we have evaluated the potential flexibility of these model complexes. the model wherein two molecules of clec-2 associate with tetrameric rhodocytin provides a more plausible model in terms of the composite effects related to the number of interfacing residues, the nature of their interactions and the predicted solvation energy effects. further, it is of potential significance that the predicted dynamic motions of this complex are suggestive of a mechanism whereby this interaction might cluster the receptors on the platelet surface, which could have implications for signaling. overall, the work presented indicates that a plausible mode of binding is that of one non-disulfide linked (Î±Î²) 2 tetramer of rhodocytin with two dimers of clec-2. this analysis will be of value in the development of further studies to characterise the interaction further with a view to developing therapeutic approaches to disrupt the rhodocytin-clec-2 interaction on the platelet surface.",{},NaN
44,260440449,LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS,conclusion,Insight-tree,"we proposed long range arena (lra), a new benchmark for evaluating progress on efficient transformer research. our new benchmark is challenging and probes at model capabilities in dealing with diverse data types and structures such as text, mathematics, and visual data. our benchmark comprises of tasks ranging from 1k to 16k tokens. for the first time, we conduct an extensive side-by-side comparison of ten recently proposed efficient transformer models. the experimental results show that these tasks are very challenging even for long-range transformer models. the overall results show that there is no one-size-fits-all solution and trade-offs have to be made in terms of model quality and speed/memory. we plan to open source our code and benchmarks to facilitate future benchmarking, research and model development.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2011.04006v1.pdf
45,15781784,Access to,conclusions,Insight-tree,"we found that the deletion of one gata factor, either gata4, gata5, or gata6, can drastically alter the gene expression profiles and lineage determination of es cells induced to differentiate by retinoic acid. es cells lacking a single gata factor, either gata4, gata5, or gata6, exhibit a unique pattern of gene expression profile when differentiated. deletion of gata6 terminates the differentiation of es cells to endoderm but leads to mesoderm lineage differentiation. normally, during in vitro differentiation, the majority of es cells differentiate into primitive endoderm cells [25]. thus, the deletion of gata6 allows the selection of lineage other than yolk sac endoderm. this study demonstrates a potential approach in redirecting the lineage determination of es cells in vitro by altering the expression of gata factors.",{},NaN
46,260334056,Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering,discussion and limitations,Insight-tree,"below, we highlight several key findings of this paper and discuss some of its limitations.which evaluation metrics are best? our analysis on correctness ( Â§4) and faithfulness ( Â§5) demonstrates that widely-used metrics are not suitable for evaluating the correctness (due to errors such as elaborate answers, open-ended questions, and list of named-entities) and faithfulness (due to partially grounded responses). correlating the metrics with human judgements (table 2 and table 5) reveals that recall and gpt4-eval are the best lexical and model-based metrics for correctness and k-precision and llmcritic (gpt-4) are the best lexical and model-based metrics for faithfulness, respectively. however, these model-based metrics, especially the ones based on llms, are usually slow to run, expensive, difficult to reproduce, and may exhibit systematic biases. while we propose that recall and k-precision are the most widely-accessible and human-aligned metrics for correctness and faithfulness, respectively, we emphasize that these simple lexical-based metrics are easy to hack. one model can copy all the retrieved knowledge as the output, leading to high recall and k-precision metrics. however, such a model will be penalized heavily when evaluated for faithfulness w.r.t. irrelevant knowledge.instruction-following models according to the most human aligned and easy to use metrics (i.e., recall and k-precision), we conclude that gpt-3.5 outperforms other models on majority of the datasets in correctness w.r.t information need. however, when analyzing the faithfulness w.r.t relevant knowledge, flan-t5 is shown to be the best model in all three datasets. moreover, our further analysis on the models' faithfulness w.r.t irrelevant knowledge demonstrates that models struggle to correctly identify whether the provided knowledge is relevant or not.limitations it is worth mentioning that the experiments for evaluating the faithfulness of the models are conducted in a modified setting, where a relevant or irrelevant passage is provided in the prompt on purpose. this is different from the real-world scenario, where the retrieved passages can contain a mix of relevant and irrelevant knowledge.finally, it should also be noted that beyond qualitative investigation, we did not explore a wide range of prompts for the tasks studied in this work. recent work has shown that the performance of instruction-following models can vary greatly depending upon the provided prompt (zhao et al., 2021;liu et al., 2023b). we leave it to future works to investigate better prompts for instructionfollowing models in a retrieval-augmented setting.","{208267807: 'Asai et al. 2020;', 258615193: 'Dziri et al. 2023', 239009856: 'Paranjape et al., 2022', 240288953: 'Qi et al. 2021;', 221970302: 'Xiong et al. 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2307.16877v1.pdf
47,260334056,Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering,conclusion,Insight-tree,"we extensively study the capability of instructionfollowing models to correctly and faithfully respond to questions in three qa settings (natural, multi-hop, and conversational). first, we uncover various issues with using traditional metrics, like f1 score, to evaluate the correctness of models. through correlation with human judgement, we find that llm-based metrics (e.g. gpt-4) and token-level recall are promising metrics for evaluating the correctness w.r.t information need. moreover, our further faithfulness analysis shows that llm-based metrics like llmcritic (gpt-4) and lexical-based k-precision are more aligned with human judgements in evaluating the faithfulness of the models given the relevant knowledge.","{208267807: 'Asai et al. 2020;', 258615193: 'Dziri et al. 2023', 239009856: 'Paranjape et al., 2022', 240288953: 'Qi et al. 2021;', 221970302: 'Xiong et al. 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2307.16877v1.pdf
48,252917981,PROMPTING GPT-3 TO BE RELIABLE,conclusion,Insight-tree,"our work systematically studies the reliability of gpt-3 from four key facets: generalizability, fairness, calibration, and factuality. we develop effective prompting strategies to make gpt-3 outperform supervised models by large margins on these facets. our work reveals new insights of llms and provides practical recommendations for users of gpt-3. we hope our work can inspire more future work to: (1) examine more facets of reliability, such as avoiding harmful generations; (2) apply the prompting methods in this paper to more real-world applications, such as incorporating human feedback for collaborative multi-step planning; (3) further explore more effective prompting strategies to improve reliability, such as post-hoc calibration on language model probabilities.","{237048095: 'Chen et al., 2021b;', 204823992: 'Fisch et al., 2019', 249097975: 'Izacard et al., 2022a', 251371732: 'Izacard et al., 2022b;', 207982228: 'Longpre et al., 2019', 249062754: 'Si et al., 2022', 173188058: 'Talmor & Berant, 2019;', 247595263: 'Wang et al. 2023', 52822214: 'Yang et al., 2018', 238856959: 'Ye & Durrett, 2022'}",https://export.arxiv.org/pdf/2210.09150v2.pdf
49,254535672,From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *,conclusions,Insight-tree,"this paper presents a novel mrc-sytle pre-training model, namely pmr. pmr can fully resolve the learning objective and data format gaps that frequently appear in fine-tuning existing plms. ex-perimental results over multiple dimensions, including effectiveness in solving few-shot tasks, outof-domain generalization, demonstrate its strong benefits in bridge the pretrain-finetune gaps. pmr also shows encouraging potential in explaining the sequence classification process in an extractiondriven mrc framework.  (trischler et al., 2017), searchqa (dunn et al., 2017, hot-potqa (yang et al., 2018), bioasq (tsatsaronis et al., 2015), drop (dua et al., 2019), duorc (saha et al., 2018), race (lai et al., 2017), re-lationextraction (levy et al., 2017), textbookqa (kembhavi et al., 2017. eqa has always been treated as an mrc problem, where the question serves as the mrc query, and the passage containing the answers serves as the mrc context. for ner, we follow li et al. (2020) to formulate ner into the mrc paradigm, where the entity label together with its description serves as the mrc query, and the input text serves as the mrc context. the goal is to extract the corresponding entities as answers. we use the eq. 4 as the learning objective, where y ext i,j indicates that the input span x i:j is an answer/entity.","{231718729: 'Aghajanyan et al., 2021', 235485084: 'Seonwoo et al., 2021', 52822214: 'Yang et al., 2018', 247793456: 'Yasunaga et al. 2022'}",https://export.arxiv.org/pdf/2212.04755v1.pdf
50,264935600,CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL,conclusion,Insight-tree,"while llms incorporate vast world knowledge and corpus statistics, they may be unfamiliar with (possibly private) client db schemas, which can be very large, rendering impractical or expensive any attempt to upload the full schema in-context along with questions for text-to-sql applications.","{249017531: 'Li et al., 2023c;'}",https://export.arxiv.org/pdf/2311.01173v1.pdf
51,264828956,Poisoning Retrieval Corpora by Injecting Adversarial Passages,conclusions,Insight-tree,"we proposed a new attack for dense retrievers, in which adversarial passages are inserted into the corpus to mislead their retrieval outputs.we show that even a small number of adversarial passages can successfully attack state-of-the-art dense retrievers and generalize to queries from unseen domains.these findings have important implications for the future deployment of robust retrieval systems in real-world applications.","{249097975: 'Izacard et al., 2022', 86611921: 'Kwiatkowski et al., 2019', 233296016: 'Thakur et al., 2021', 220302524: 'Xiong et al., 2021'}",https://export.arxiv.org/pdf/2310.19156v1.pdf
52,220483148,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,conclusion,Insight-tree,"we have presented logiqa, a large-scale logical reasoning reading comprehension dataset. in addition to testing reasoning capacities of machine reading, our dataset can also serve as a benchmark for re-examining the long pursued research of logical ai in the deep learning nlp era. results show that the state-of-the-art machine readers still fall far behind human performance, making our dataset one of the most challenging test for reading comprehension.",{},https://arxiv.org/pdf/2007.08124v1.pdf
53,218516694,MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models,conclusion,Insight-tree,"in this paper, we convert eight existing qa tasks from the mrqa shared task (fisch et al., 2019b) into sentence-level retrieval tasks, by treating the sentence containing the ground-truth span as the target sentence-level answer. in additional to a new evaluation suite for sentence level retrieval, we provide strong baselines using unsupervised term-based information retrieval methods (bm25), and three neural models, off-the-self use-qa, finetuned use-qa, and bert dual encoders.","{204823992: 'Fisch et al., 2019b'}",https://www.aclweb.org/anthology/2021.adaptnlp-1.10.pdf
54,219978758,Improving QA Generalization by Concurrent Modeling of Multiple Biases,conclusion,Insight-tree,"in this paper we (1) investigate the impact of debiasing methods on qa model generalization for both single and multi-domain training scenarios, and (2) propose a new framework for improving the in-domain and out-of-domain performances by concurrent modeling of multiple biases. our framework weights each training example according to multiple biases and based on the strength of each bias in the training data. it uses the resulting bias weights in the training objective to prevent the model from mainly focusing on learning biases. we evaluate our framework using two different training objectives, i.e., multi-bias confidence regularization and multi-bias loss re-weighting, and show its effectiveness in both single and multidomain training scenarios. we further compare our framework with two state-of-the-art debiasing methods of utama et al. (2020) and mahabadi et al. (2020). we show that knowledge distillation, modeling multiple biases at once, and weighting the impact of each bias based on its strength in the training data are all important factors in improving the in-domain and out-of-domain performances. while recent literature on debiasing in nlp focuses on improving the performance on adversarial evaluation sets, this work opens new research directions on wider uses of debiasing methods. the main advantage of using our debiasing methods is that they improve the performance and generalization without requiring additional training data or larger models. future work could build upon our framework by applying it to a wide range of tasks beyond qa using task-specific bias models.","{202539031: 'Clark et al., 2019;', 86611921: 'Kwiatkowski et al., 2019', 208000835: 'Su et al. 2019', 208201969: 'Sugawara et al., 2020', 173188058: 'Talmor and Berant 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf
55,211010545,Conversations with Documents An Exploration of Document-Centered Assistance,conclusions and future work,Insight-tree,"in this paper, we explored the novel domain of document-centered digital assistance. we focused on a consumption scenario, in which individuals are a (co-)owner of a document. through a survey, we identified a set of primary capabilities people expect from a digital assistant in a document-centered scenario, as well as a large set of questions that gave us insight into the types of queries that people might pose about a document when they have an approximate or good idea what the document is about. our explorations shed light on the hierarchy of questions that might be posed, and demonstrate that the types of questions people ask in a document-centered scenario are different from the factoid questions in conventional qa datasets. we show that state-of-the-art qa models can be finetuned to perform with reasonable accuracy on the new dqa data. yet, it has proven to be an unsolved task, which makes this a fertile area for future work. this research opens a new direction for digital assistance. avenues for future work include deeper explorations of query rewriting to better tailor document-centered questions to conventional qa systems, and also exploring ways to scale up the data to a much larger and broader range of documents.  ",{},https://arxiv.org/pdf/2002.00747v1.pdf
56,250390946,Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering,conclusion,Insight-tree,"in this paper, we study cqa over structured and unstructured knowledge sources (i.e., kb and text particularly), and focus on studying the knowledge transfer between different knowledge sources. to facilitate the transfer, we first propose a unified cqa framework, simultqa to bridge kbqa and textqa systems. empirical results show that knowledge transfer enables substantial improvements on low-resource domains. more importantly, we conduct fine-grained analyses to shed more light on how knowledge is transferred to inspire future research on knowledge transfer between sources, and we conclude the paper with insights for future cqa datasets and systems.","{208267807: 'Asai et al., 2020', 211296452: 'Dhingra et al., 2020;', 67855846: 'Dua et al., 2019', 207853300: 'Fang et al., 2020', 189927857: 'Feldman and El-Yaniv, 2019;', 202660724: 'Nie et al., 2019;', 202773198: 'Qi et al., 2019', 233240823: 'Shi et al., 2021', 128345225: 'Sun et al., , 2019', 52822214: 'Yang et al., 2018', 237502990: 'Zhu et al., 2021'}",https://www.aclanthology.org/2022.suki-1.7.pdf
57,199668753,Towards Knowledge-Based Recommender Dialog System,conclusion,Insight-tree,"in this paper, we propose a novel end-to-end framework, kbrd, which bridges the gap between the recommender system and the dialog system via knowledge propagation. through a series of experiments, we show that kbrd can reach better performances in both recommendation and dialog generation in comparison with the baselines. we also discuss how the two systems benefit each other. dialog information is effective for the recommender system especially in the setting of cold start, and the introduction of knowledge can strengthen the recommendation performance significantly. information from the recommender system that contains the user preference and the relevant knowledge can enhance the consistency and diversity of the generated dialogs.","{153312687: 'Ding et al., 2019', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/D19-1189.pdf
58,263310649,Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback,conclusion,Insight-tree,"in this work, we propose a novel framework that leverages contrastive feedback to optimize large language models through reinforcement learning, namely rlcf. the capabilities of llm could be aligned with the context of information retrieval through the proposed rlcf. specifically, we first construct a group of similar documents by a dense retrieval model. subsequently, documents in the same group are fed into a llm to be optimized. the responses are obtained by the llm for these similar documents. the contrastive feedback is obtained from these responses generated by llm with respect to corresponding documents. the contrastive feedback is calculated by dense retrieval model. formally, we employ a novel reward function, batched-mrr, as the contrastive feedback, which is a variant of mrr. then, the contrastive feedback could be utilized to optimized llm through ppo algorithm, which is a widely used reinforcement learning method. we conduct experiments on two tasks of information retrieval, demonstrating the effectiveness of our proposed rlcf. the rlcf-optimized llm could generates specific queries for data augmentation, achieving promising performance on few-shot dense retrieval. besides, we introduce a brand-new setting of document summarization, which is under the context of information retrieval. to be specific, the summarizes should be specific to each document among similar documents, which is desired for users to filter out target document. to evaluate the effectiveness of summarization in the proposed setting, we introduce rouge-diff, a variant of rouge score, which is calculated in the group level. in future work, we suggest exploring more domains which could use the rlcf for optimization, such as style transfer, harmless alignment, helpfulness alignment and etc.",{86611921: '[31]'},https://export.arxiv.org/pdf/2309.17078v1.pdf
59,218595722,Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension,conclusion,Insight-tree,"in this work, we present a novel multi-grained mrc framework based on graph attention networks and bert. we model documents at different levels of granularity to learn the hierarchical nature of the document. on the natural questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers. the experiments show that our proposed methods are effective and outperform the previously existing methods by a large margin. improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals. besides, the currently existing methods actually cannot process a long document without truncating or slicing it into fragments. how to model long documents is still a problem that needs to be solved.  question: what 's the dog 's name on tom and jerry long answer: tom ( named "" jasper "" in his debut appearance ) is a grey and white domestic shorthair cat . "" tom "" is a generic name for a male cat . he is usually but not always , portrayed as living a comfortable , or even pampered life , while jerry ...","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.599.pdf
60,263787121,RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models,conclusion,Insight-tree,"this paper presents dupmae, a new retrievaloriented pre-training method where the semantic representation capacities can be jointly enhanced for all contextualized embeddings of the language models. it leverages retromae's decoding task for [cls]'s embedding and introduces a bow-based decoding task for ot embeddings. the training losses from the two tasks are added up for a unified encoder. the two types of embeddings, after dimension reduction, are aggregated for the joint semantic representation. the effectiveness of our proposed method is empirically verified, as remarkable retrieval performances are achieved on ms marco and beir throughout different situations.",{},https://export.arxiv.org/pdf/2211.08769v1.pdf
61,221978039,FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data,conclusion,Insight-tree,"forecasting is a difficult task that requires every possible advantage to do well. it would be wise to harness this pool of unstructured data for training automatic event forecasting agents. to utilize this form of data for forecasting, we proposed a question-answering task that requires forecasting skills to solve forecastqa, and provided the accompanying dataset. various baseline methods did not perform well, but this is not surprising given the inherent difficulty of forecasting. our benchmark dataset can benefit future research beyond natural language understanding and hope forecasting performance will be significantly improved.  ","{86611921: 'Kwiatkowski et al., 2019', 202773198: 'Qi et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.acl-long.357.pdf
62,202542881,Span Selection Pre-training for Question Answering,conclusion and future work,Insight-tree,"span selection pre-training is effective in improving reading comprehension across four diverse datasets, including both generated and natural questions, and with provided contexts of passages, documents and even passage sets. this style of pretraining focuses the model on finding semantic connections between two sequences, and supports a style of cloze that can train deep semantic understanding without demanding memorization of specific knowledge in the model. the span selection task is suitable for pre-training on any domain, since it makes no assumptions about document structure or availability of summary/article pairs. this allows pre-training of language understanding models in a very generalizable way.",{52822214: 'Yang et al. 2018'},https://www.aclweb.org/anthology/2020.acl-main.247.pdf
63,233296336,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,conclusion,Insight-tree,"in this paper, we propose an evaluation pipeline for pretrained models by testing their transferability without word identity information. specifically, we take an english pretrained bert off-the-shelf and fine-tune it with a scrambled english dataset. we conduct analyses across six tasks covering both classification and sequence labeling. by evaluating performance against multiple baselines, we aim to assess where bert can transfer knowledge even without word identities. we find considerable transfer for bert as compared to even powerful baselines, by only for classification tasks.","{52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2104.08410v1.pdf
64,234334398,READTWICE: Reading Very Large Documents with Memories,conclusion & future work,Insight-tree,"readtwice performs well on several qa tasks, particularly narrativeqa where long-range dependencies among entities appear to be very important. the proposed method is conceptually simple, easy to implement and is capable of reading entire books. for future work, we plan to explore new memory types, hierarchies and aggregation functions. we also aim to apply the model to other tasks, particularly long text summarization, likely to benefit from a memory-forming mechanism.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2105.04241v2.pdf
65,239016079,COVIDRead: A Large-scale Question Answering Dataset on COVID-19,conclusion and future work,Insight-tree,"in this paper, we have presented covidread, a new dataset for benchmark evaluation of methods for qa/mrc on covid-19. the dataset is created in a semi-automatic way. the dataset consists of context-question-answer triples over 100k, out of which 40k are human-annotated. researchers and editors/associate editors would be benefited out of this kind of models and the dataset. we have proposed three approaches that are based on viz. (i.) vanilla bert (ii.) bio-medical bert (i.e., biobert) and (iii.) clinical bert. our proposed models are competitive compared to the existing state-of-the-art models. our future works would include:",{221662105: '3]'},https://arxiv.org/pdf/2110.09321v1.pdf
66,252873630,Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning,conclusion,Insight-tree,"in this work, we proposed a novel counterfactual reasoning approach to reduce disconnected reasoning in multi-hop qa. we used the causal graph to explain the existing multi-hop qa approaches' behaviors, which consist of the shortcut impacts and reasoning impacts. the shortcut impacts capture the disconnected reasoning and they are formulated as the natural direct causal effects. then we constructed the counterfactual examples during the training phase to estimate the both natural direct effects of question and context on answer prediction as well as supporting facts identification. the reasoning impact represents the multi-hop reasoning and is estimated by introducing learnable parameters.","{202583433: 'Das et al., 2019', 229923812: 'Li et al., 2021', 155100120: 'Qiu et al., 2019;', 220831004: 'Zaheer et al., 2020'}",https://www.aclanthology.org/2023.acl-long.231.pdf
67,262459341,BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models,conclusion,Insight-tree,"we propose bamboo, a benchmark for comprehensively evaluating the long text modeling capabilities of llms.bamboo consists of five tasks with two length levels, enabling the evaluation of llms' main capacities across various dimensions and domains.based on the evaluation of several long context models on bamboo, we give an overall analysis of the performances of different models and tasks.we have discussed key questions of long text models, provide a qualitative analysis of long text modeling tasks, and suggest directions for improving long context modeling abilities.we believe bamboo can be employed to analyze the extensive capacities and advance the long text modeling proficiency of llms in the future.","{234093776: 'Dasigi et al., 2021', 245218982: 'Pang et al., 2022;', 258686160: 'Ratner et al., 2023', 260440449: 'Tay et al., 2021', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020'}",https://export.arxiv.org/pdf/2309.13345v1.pdf
68,207847640,Blockwise Self-Attention for Long Document Understanding,conclusion,Insight-tree,"in this work, we study the lightweight bert model with the goal of achieving both efficiency and effectiveness. we profile and analyze the memory bottlenecks of bert and focus on optimize dotproduct self-attention, which consumes quadratic memory with respect to the sequence length. to reduce both time and memory consumption, we present blockbert, which sparsifies the attention matrices to be sparse block matrices. the proposed model achieves time and memory saving without significant loss of performance.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.findings-emnlp.232.pdf
69,210859295,MANYMODALQA: Modality Disambiguation and QA over Diverse Inputs,conclusion,Insight-tree,"we presented a new challenge to the community, hoping to promote research in manymodal qa. we structured our challenge in a way that encourages research in other, more general areas, such as transfer learning and end-to-end modality disambiguation + multimodal qa. we hope that this challenge will serve as a test bed for further work and that our model will inspire directions of subsequent research. we plan to continue our work by collecting data that exhibits a stronger form of multimodality, where the question can only be answered after combining multiple modalities, and by adding new modalities, such as video and audio.",{},https://arxiv.org/pdf/2001.08034v1.pdf
70,226236740,Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps,conclusion,Insight-tree,"in this study, we presented 2wikimultihopqa-a large and high quality multi-hop dataset that provides comprehensive explanations for predictions. we utilized logical rules in the kb to create more natural questions that still require multi-hop reasoning. through experiments, we demonstrated that our dataset ensures multi-hop reasoning while being challenging for the multi-hop models. we also demonstrated that bootstrapping the multi-hop mrc dataset is beneficial by utilizing large-scale available data on wikipedia and wikidata.","{139103297: 'Chen and Durrett, 2019;', 215785913: 'Chen et al. 2020', 218486753: 'Inoue et al., 2020', 174801764: 'Min et al. 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2011.01060v2.pdf
71,221292890,Continual Domain Adaptation for Machine Reading Com-prehension,conclusion,Insight-tree,"we introduce the continual domain adaptation task for mrc. so far as we know, this is the first study on the continual learning perspective of mrc. we build two datasets cda-q and cda-c for the cda task, by re-organizing existing mrc collections into different domains with respect to the question type and passage type. we conduct preliminary experiments showing the existence of catastrophic forgetting (cf) phenomenon of existing mrc models under the cda setting. further, we propose regularization-based regbertqa and dynamic-architecture progbertqa to tackle the cda for mrc. we conduct extensive experiments to analysis the effectiveness of both methods and validate that the proposed dynamic-architecture based model achieves the best performance.","{198229624: '[17]', 86611921: '[22]'}",https://arxiv.org/pdf/2008.10874v1.pdf
72,245334850,MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding,conclusions and future work,Insight-tree,"we present a new qa task, mumuqa, along with an evaluation benchmark for multimedia news understanding. the task is challenging in the requirement of cross-media grounding over images, captions, and news body text. we demonstrate the benefit of using multimedia knowledge extraction, both for generating silver-standard training data and for a pipeline-based multimedia qa system. the multimedia baselines are still considerably behind human performance, suggesting ample room for improvement. future work will incorporate other forms of media in news, such as video and audio, to facilitate information seeking from more comprehensive data sources. another direction is to infuse the endto-end multimedia qa system with additional input from the grounding and visual attribute extraction systems.  ","{227231636: 'Chakravarti et al. 2020', 210859295: 'Hannan, Jain, and Bansal 2020', 174801080: 'Min et al. 2019', 52822214: 'Yang et al. 2018'}",https://arxiv.org/pdf/2112.10728v2.pdf
73,232307160,Self-Supervised Test-Time Learning for Reading Comprehension,conclusion,Insight-tree,"in this work, we propose test-time learning (ttl) as a new framework for unsupervised extractive question answering (eqa). we present four variants of ttl with a simple but effective context expansion method. we utilize four question-answer pair generation methods for eqa and propose using qa-srl as an additional source of qa pairs, to supplement prior methods. we show ttl enables ""understanding"" of contexts at test-time, without human-authored annotations, and significantly improves eqa, including low parameter models. we envision ttl as a framework that can direct work in reading comprehension to be viewed as a problem of ever-evolving datasets instead of a static corpus. natural language itself undergoes continuous evolution (gentner and france, 1988;traugott and dasher, 2001;hamilton et al., 2016) via changes in preference for syntactical structures; creation of new words and phrases; and changing usage frequencies and semantics for existing words. ttl can potentially be applied to such scenarios with semantic drift or domain shift. further improvements w.r.t. selection of similar contexts for k-neighbor ttl could be explored by leveraging hard sample selection, hard negative mining, bootstrapping, and contrastive learning, along with improved currculum strategies.","{219721462: 'Kamath et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020'}",https://www.aclweb.org/anthology/2021.naacl-main.95.pdf
74,232269988,Evaluating Document Coherence Modelling,conclusion,Insight-tree,"we propose the new task of detecting whether there is an intruder sentence in a document, generated by replacing an original sentence with a similar sentence from a second document. to benchmark model performance over this task, we construct a large-scale dataset consisting of documents from english wikipedia and cnn news articles. experimental results show that pretrained lms which incorporate larger document contexts in pretraining perform remarkably well in-domain, but experience a substantial drop cross-domain. in follow-up analysis based on human annotations, substantial divergences from human intuitions were observed, pointing to limitations in their ability to model document coherence. further results over a linguistic probe dataset show that pretrained models fail to identify some linguistic characteristics that affect document coherence, suggesting room to improve for them to truly capture document coherence, and motivating the construction of a dataset with intruder text at the intra-sentential level.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2103.10133v1.pdf
75,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,conclusions,Insight-tree,"we have demonstrated the utility of question decomposition as an effective means to probe pretrained multi-hop question-answering models for supporting evidence. through simulatability experiments, we show the effectiveness of this explanation form at allowing humans to predict model behavior, a sign that it helps humans to form an accurate mental model of the machine learning system (jacovi et al., 2022). this ability to predict system performance occurs at the instance level instead of a sense of trust of the overall system, which can be important if the accuracy of the system is variable based on the question.","{221448158: 'Khot et al. 2021', 174801080: 'Min et al., 2019', 211258645: 'Perez et al., 2020', 211258744: 'Tang et al. 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2204.07693v2.pdf
76,248227734,Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes,limitations,Insight-tree,"our simulability study results (section 4) are conducted on silver labels. as section 5 reveals, there is a need for higher-quality question decompositions. while we have demonstrated the potential for decomposition probes to help users build mental models of system behavior, these results are not fully realizable in real applications until decompo-sition systems improve.the probing strategy explored in this paper is particular to the qa setting and datasets that don't have predefined categories of answers. other probing strategies may exist that are not explored in this paper.it is noted that multi-hop questions do not always require multi-hop reasoning to solve. indeed we intentionally use a non-multi-hop questionanswering model to answer the original question to disadvantage the system so that explanations are required. multi-hop questions afford the use of a decompositional probing strategy. our study did not look at non-multi-hop questions, which may require other probing strategies yet to be invented.","{221448158: 'Khot et al. 2021', 174801080: 'Min et al., 2019', 211258645: 'Perez et al., 2020', 211258744: 'Tang et al. 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2204.07693v2.pdf
77,247158312,Combining Modular Skills in Multitask Learning,conclusions,Insight-tree,"in this work, we argued that a modular design is crucial to ensure that neural networks can learn from a few examples and generalise robustly across tasks by recombining autonomous facets of knowledge. to this end, we proposed a model where a subset of latent, discrete skills from a fixed inventory is allocated to each task in an end-to-end fashion. the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters. we evaluate the sample efficiency of our model on multitask instruction following through reinforcement learning and its few-shot adaptability on multitask text-to-text generation through supervised learning. in both experiments, we surpass competitive baselines where parameters are fully shared, task-specific, combined according to expert knowledge, or generated conditionally on the task. finally, we show that our model facilitates interpretability by learning an explicit hierarchy of tasks based on the skills they require. ",{233296709: 'Ye et al. 2021'},https://arxiv.org/pdf/2202.13914v2.pdf
78,221749191,Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning,conclusions,Insight-tree,"progress in multi-hop qa under the reading comprehension setting relies on understanding and quantifying the types of undesirable reasoning current models may perform. this work introduced a formalization of disconnected reasoning, a form of bad reasoning prevalent in multi-hop models. it showed that a large portion of current progress in multifact reasoning can be attributed to disconnected reasoning. using a notion of contrastive sufficiency, it showed how to automatically transform existing support-annotated multi-hop datasets to create a more difficult and less cheatable dataset that results in reduced disconnected reasoning.","{155100120: 'Xiao et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf
79,250526133,A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA,conclusion,Insight-tree,"in this paper, we propose a method that developing models that have different specialized capabilities and fuse their strengths to tackle the task objective of finqa. we first analyze the data distribution of finqa by adversarial validation, and build a new validation dataset that has a similar distribution to test set by re-splitting the training and validation set. besides, in order to reduce the influence of distribution gap, we develop and fuse several small models rather than train a large model which is easily over-fitting to training set. furthermore, we obtain a high performance by fusing the strengths of different small model. overall, we achieve the 1st place in finqa challenge with 71.93% execution accuracy and 67.03% program accuracy.",{},https://arxiv.org/pdf/2207.06490v1.pdf
80,258987734,BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language,conclusions,Insight-tree,"information retrieval in the polish language is still developing and is an intensive research area. therefore, there is a great need for resources to enable further training and more accurate evaluation of existing and new deep neural ir models. in this work, we introduced the translated beir-pl benchmark and showed the results of a broad family of ir baseline models. we would like to encourage other researchers to participate in further development of polish and multilingual ir models using our new resource.","{86611921: '[18]', 233296016: '[29]', 247411106: '[34]'}",https://export.arxiv.org/pdf/2305.19840v1.pdf
81,241583517,Team Papelo at FEVEROUS: Multi-hop Evidence Pursuit,conclusion,Insight-tree,"team papelo's system for feverous achieves .281 feverous score on the development set, with .658 label accuracy and .348 evidence recall. the largest increase in performance over the baseline comes from the label classifier, which uses a different model architecture and is trained on extracted evidence chains including irrelevant evidence. we also achieve better evidence recall through our table cell ranking module, which was trained with a multiple choice cross entropy loss similar to dpr. additional gains are achieved by our multi-hop evidence retrieval. these modules can only be effective when given good representations of the context of sentences, list items and table cells, which we have carefully constructed.","{67855846: 'Dua et al., 2019', 209202200: 'Gupta et al., 2020', 52822214: 'Yang et al. 2018'}",https://www.aclanthology.org/2021.fever-1.5.pdf
82,264038871,IntÃ©gration du raisonnement numÃ©rique dans les modÃ¨les de langue : Ãtat de l'art et direction de recherche,conclusion,Insight-tree,"cet article s'intÃ©resse Ã  l'amÃ©lioration des modÃ¨les de langue en raisonnement numÃ©rique.bien que le prÃ©-entraÃ®nement sur des donnÃ©es numÃ©riques soit maintenant une approche courante, les rÃ©sultats ne sont pas satisfaisants.notre synthÃ¨se de l'Ã©tat de l'art des approches d'intÃ©gration du raisonnement numÃ©rique dans les modÃ¨les de langue penche clairement en faveur des architectures hybrides mÃªlant gÃ©nÃ©ration de texte et calculateurs externes.nous sommes convaincus du potentiel de ces modÃ¨les, Ã  la fois en raisonnement automatique Ã  partir d'Ã©noncÃ©s textuels et dans le cadre des applications de data-to-text.nous travaillons au dÃ©veloppement d'architectures capables de traduire ces problÃ©matiques en code python, d'exÃ©cuter ces programmes puis d'incorporer les rÃ©sultats retournÃ©s dans un texte pertinent.",{},https://www.aclanthology.org/2023.jeptalnrecital-coria.11.pdf
83,232075995,Published as a conference paper at ICLR 2021 LEARNING REASONING PATHS OVER SEMANTIC GRAPHS FOR VIDEO-GROUNDED DIALOGUES,conclusion,Insight-tree,"we proposed pdc, a novel approach to learning a reasoning path over dialogue turns for videogrounded dialogues. our approach exploits the compositional semantics in each dialogue turn to construct a semantic graph, which is then used to derive an optimal path for feature propagation. our experiments demonstrate that our model can learn to retrieve paths that are most relevant to the current question. we hope our approach can motivate further study to investigate reasoning over multiple turns, especially in complex settings with interconnected dialogue flows (sun et al., 2019).","{208267807: 'Asai et al. 2020', 153312687: 'Ding et al., 2019', 52822214: 'Yang et al., 2018b'}",https://export.arxiv.org/pdf/2103.00820v2.pdf
84,202572622,PubMedQA: A Dataset for Biomedical Research Question Answering,conclusion,Insight-tree,"we present pubmedqa, a novel dataset aimed at biomedical research question answering using yes/no/maybe, where complex quantitative reasoning is required to solve the task. pubmedqa has substantial automatically collected instances as well as the largest size of expert annotated yes/no/maybe questions in biomedical domain. we provide a strong baseline using multi-phase fine-tuning of biobert with long answer as additional supervision, but it's still much worse than just single human performance. there are several interesting future directions to explore on pubmedqa, e.g.: (1) about 21% of pubmedqa contexts contain no natural language descriptions of numbers, so how to properly handle these numbers is worth studying; (2) we use binary bow statistics prediction as a simple demonstration for additional supervision of long answers. learning a harder but more informative auxiliary task of long answer generation might lead to further improvements.","{165163607: 'Clark et al., 2019'}",https://www.aclweb.org/anthology/D19-1259.pdf
85,258557836,A Frustratingly Easy Improvement for Position Embeddings via Random Padding,conclusion,Insight-tree,"in this work, we propose a simple strategy, random padding, to improve the performance of extractive qa models, especially when they are trained on short contexts but evaluated on longer contexts. our method only re-organizes the input token sequences when fine-tuning, without any modifications to the architectures of plms. experiments reveal that our simple method can effectively enhance qa models when predicting answers at the rear positions, where the position embeddings may not be sufficiently updated without random padding. we also show that our simple strategy can improve the performance of plm components in more benchmarks and tasks where accurate local context representations over longer context are necessary.","{204823992: 'Fisch et al., 2019', 198229624: 'Joshi et al., 2020', 216868500: 'Ko et al., 2020;', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.04859v1.pdf
86,263909028,FORMALLY SPECIFYING THE HIGH-LEVEL BEHAVIOR OF LLM-BASED AGENTS,conclusion,Insight-tree,"in this work, we introduced a framework for defining agents using ltl in a task-agnostic, domain general fashion.the ltl formulation of an agent can be used to construct a constrained decoder, which controls the high-level behavior of an agent at runtime.we demonstrated the utility of our framework with experiments on three datasets, where we found that the hard constraints imposed on generation by our approach can lead to an increase in agent performance.",{52822214: 'Yang et al. 2018'},https://export.arxiv.org/pdf/2310.08535v1.pdf
87,263334529,Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method,conclusion and future work,Insight-tree,"in this paper, we study different prompting methods for using llms in news claim verification.we introduce a hierarchical step-by-step (hiss) method that prompts llm to perform the verification in fine-grained steps, aiming to mitigate the omission of thoughts and fact hallucination.validated on two public datasets, hiss prompting improves the performance of llms on the task over fullysupervised sota models and its strong few-shot icl-based counterparts.hiss prompted explanations show superior explainability in their coverage readability.in the future, we will build a conversational factchecking model based on llms which can be userfriendly and incorporate human fact-checkers in the loop.","{226262339: 'Fan et al., 2020', 246652372: 'Ji et al., 2023;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.00305v1.pdf
88,263334529,Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method,limitations,Insight-tree,"despite the promising performance of llms based on few-shot icl, fact verification is a challenging research problem given the fact that performance scores are still quite low in general.there are a few limitations.firstly, in this work, we highlight that all the baselines and our proposed method solely rely on textual information.we focus on an unimodal approach utilizing language models and do not consider the potential assistance from other modalities, such as images and videos, for this task.although the exploration of multimodal approaches has gradually drawn some research attention (wang et al., 2018;silva et al., 2021;bu et al., 2023), it falls outside the scope of our current work.meanwhile, the scope of this study is limited to the verification of news claims, which represents only a subset of the broader issue of misinformation.misinformation encompasses a wide range of false or misleading information, including rumors, fake news articles, and spams (wu et al., 2019).while our focus was specifically on news claims, future research could explore the detection and mitigation of misinformation in other formats.further, our proposed prompting method heavily relies on the capabilities of backbone llms, which can come with substantial computational costs.our method leverages the advancements in multi-step reasoning exhibited by these llms, necessitating high-performance expectations.however, it is worth noting that most state-of-the-art llms are currently not open-source and only available as services.for instance, gpt-3.5 can only be accessed via api.the reliance on such llms makes deep model control infeasible, and the need for api access poses challenges in terms of cost.finally, while our approach leverages search engines to mitigate the fact hallucination issue in llms, it operates under the assumption that pertinent information is readily accessible through web search.however, not all information is indexed or available in search engines.for instance, if someone claims to have witnessed a rare meteorological phenomenon in a small town, such event might not be reported on major news websites or databases.such firsthand, non-digitized accounts might be retrieved or fact-checked.this underscores the limitation in relying solely on search engines as a primary source of external knowledge for factchecking with llms.another limitation of our method lies in the claims that are beyond established world knowledge when necessary relevant knowledge is not complete or even not available.this necessitates the model's ability to infer novel knowledge by formulating and subsequently validating appropriate hypotheses, a task that remains beyond the capabilities of existing technologies.","{226262339: 'Fan et al., 2020', 246652372: 'Ji et al., 2023;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.00305v1.pdf
89,259951081,How Is ChatGPT's Behavior Changing over Time?,conclusions and future work,Insight-tree,"our findings demonstrate that the behavior of gpt-3.5 and gpt-4 has varied significantly over a relatively short amount of time.this highlights the need to continuously evaluate and assess the behavior of llm drifts in applications, especially as it is not transparent how llms such as chatgpt are updated over time.our study also underscores the challenge of uniformly improving llms' multifaceted abilities.improving the model's performance on some tasks, for example with fine-tuning on additional data, can have unexpected side effects on its behavior in other tasks.consistent with this, both gpt-3.5 and gpt-4 got worse on some tasks but saw improvements in other dimensions.moreover, the trends for gpt-3.5 and gpt-4 are often divergent.beyond the final performances, it's interesting to observe shifts in chain-of-thought behaviors and verbosity of the models.",{},https://export.arxiv.org/pdf/2307.09009v3.pdf
90,257766470,"Natural Language Reasoning, A Survey",limitations,Insight-tree,"we introduce both limitations of the current research and intrinsic in plms. firstly, there are gaps in defeasible reasoning and reasoning path evaluation.â¢ research gap on defeasible reasoning. while defeasible reasoning is widely used in our daily life, this topic is still under-explored in nlp. [4] found that it is more challenging for chatgpt to perform abductive reasoning and inductive reasoning than deduction, among which induction is the much more difficult one. â¢ lack of effective ways to evaluate reasoning paths. it is still challenging to automatically evaluate generated reasoning paths without ground truth. evaluating reasoning paths might become increasingly important to build explainable and reliable ai systems, especially when more people contact and use chatgpt-like products nowadays.secondly, there are also limitations intrinsic to plms.â¢ soft deduction can produce invalid conclusions. transformers can only predict conclusions with probability, irrespective of whether the conclusion of deductive reasoning is necessarily true in nature, which might prevent it from precise reasoning. this characteristic can result in a sub-optimal solution to deductive problems (including arithmetic reasoning and symbolic reasoning). for example, while chatgpt is impressive on reasoning tasks, it still fails to achieve perfect performance on the simplest one-step deductive inference task [4].â¢ biases on content. plms make their prediction based on context. while llms have made huge progress in reasoning, [32] found that llms are biased by content like humans when performing deduction. for example, they perform worse in abstract or counterfactual situations than the realistic ones. such biases will hinder them from actual reasoning and lead to wrong answers, degrading downstream performance. more severely, it might cause harmful societal influences due to some social biases such as gender, which also exist in gpt4 [16].","{236459873: '1', 251135345: '[5,', 237450610: '[11]', 139103297: '[20,', 211126663: '[25]', 233297051: '[31]', 237485084: '[33,', 248780551: '[39,', 230799347: '[45,', 226236740: '[54,', 235097509: '[61,', 218486753: '[65,', 237502773: '66,', 222178328: '67]', 189927896: '68,', 204915921: '73,', 221448158: '74,', 86611921: '[78]', 235755349: '81,', 225075843: '84,', 201058633: '[86]', 174801764: '96,', 174801080: '97,', 225066758: '101,', 211258645: '104,', 240288953: '109,', 155100120: '113,', 248986946: '114]', 222141025: '[126,', 235097535: '128,', 249062828: '129]', 247594506: '130]', 219573621: '[145]', 221749191: '150]', 236771976: '151]', 247595263: '[156]', 52822214: '169]', 233219869: '170,', 220045416: '175]', 222208994: '186]'}",https://export.arxiv.org/pdf/2303.14725v2.pdf
91,261065828,Building Interpretable and Reliable Open Information Retriever for New Domains Overnight,conclusion,Insight-tree,"in this work, we propose an information retrieval (ir) pipeline built on the advances of query decomposition and event linking. specifically, we associate the input query with generated events relevant to the query through the decomposition step and then link these events to real-world facts in a knowledge base with an event-linking model. we show that with simple passage selection through bm-25 and cross-domain qa supervision, our pipeline outperforms existing state-of-the-art unsupervised or cross-domain ir models on five datasets by an average of 6%. compared with existing methods, our pipeline does not involve heavy pre-training, parameter tuning, or domain-specific supervision.","{165163607: 'Clark et al., 2019', 222125277: 'De Cao et al., 2021', 230799347: 'Geva et al., 2021', 52822214: 'Yang et al., 2018', 253237669: 'Zhou et al. 2022'}",https://export.arxiv.org/pdf/2308.04756v1.pdf
92,226955938,Unsupervised Explanation Generation for Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we aim to improve the explainability for the machine reading comprehension task, which is different from most of the previous works that were only striving for better objective evaluation scores. to achieve this goal, we propose a novel mechanism called recursive dynamic gating (rdg) to gradually refine the amount of the input information in each layer of the pre-trained language model. also, we propose an attention smoothing technique that will increase the accuracy of the rdg mechanism. experimental results on three multiple-choice machine reading comprehension datasets show that the proposed rdg mechanism could not only improve the objective evaluation scores, but also show an advantage over the traditional attention mechanism in explainability.","{209063721: 'Moon et al. 2019', 214233456: 'Sun et al., 2020', 52822214: 'Yang et al., 2018;', 211572557: 'Yang et al. 2020'}",https://arxiv.org/pdf/2011.06737v1.pdf
93,207982228,An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering,conclusion,Insight-tree,"this paper describes experiments on various competitive pre-trained models (bert, xlnet), domain sampling strategies, negative sampling, data augmentation via back-translation, and active learning. we determine which of these strategies help and hurt multi-domain generalization, finding ultimately that some of the simplest techniques offer surprising improvements. the most significant benefits came from sampling no answer segments, which proved to be particularly important for training extractive models on long sequences. in combination these findings culminated in the second ranked submission on the mrqa-19 shared task.","{67855846: 'Dua et al., 2019'}",https://www.aclweb.org/anthology/D19-5829.pdf
94,235313893,Knowing More About Questions Can Help: Improving Calibration in Question Answering,conclusion,Insight-tree,"we introduce a richer feature space for question answering calibrators with question and context embeddings and paraphrase-augmented inputs. our work suggests deciding the correctness of a qa system depends on both the semantics of the questioncontext and the confidence of the model. we thoroughly test our calibrator in domain shift, adversarial, and open domain qa settings. the experiments show noticeable gains in performance across all settings. we further demonstrate our calibrator's general applicability by using it as a reranker in extractive open domain qa. to summarize, our calibrator is simple, effective and general, with potential to be incorporated into existing models or extended for other nlp tasks.","{219721462: 'Kamath et al. 2020', 86611921: 'Kwiatkowski et al., 2019'}",https://arxiv.org/pdf/2106.01494v1.pdf
95,239616544,ITERATIVE HIERARCHICAL ATTENTION FOR ANSWER- ING COMPLEX QUESTIONS OVER LONG DOCUMENTS,conclusion,Insight-tree,"we consider on the problem of answering complex questions over long structured documents. like multi-hop open qa tasks, this problem requires not only conventional ""machine reading"" abilities, but the ability to retrieve relevant information and refine queries based on retrieved information. additionally, it requires the ability to navigate through a document, by understanding the relationship between sections of the document and parts of the question. in our framework, navigation is modeling similarly to retrieval in multi-hop models: the model attends to a document section, and uses a compact neural encoding of the section to update the query. unlike most prior multihop qa models, however, queries are updated in embedding space, rather than by appending to a discrete representation of question text. this approach is end-to-end differentiable and very fast. experiments also demonstrate that this use of hierarchical attention can significantly improve the performance on qa tasks: in fact, the dochopper model achieves the start-of-the-art results on four challenging qa datasets, outperforming the baseline models by 3-5%, while also being 3-10 times faster.","{221845203: 'Ainslie et al., 2020', 155100120: 'Zhang et al., 2019'}",https://arxiv.org/pdf/2106.00200v2.pdf
96,254926391,Multi-hop Evidence Retrieval for Cross-document Relation Extraction,conclusion,Insight-tree,"we study efficient and effective ways to extract multi-hop evidence for cross-document re and propose mr.cod. mr.cod extracts evidence paths from an open set of documents and ranks them with adapted dense retrievers as scorers. to overcome the gap between retrieval in odqa and evidence retrieval for re, we develop a contextual dpr that augments sparse queries with passage context. extensive experiments show high-quality evidence retrieved by mr.cod boosts end-to-end cross-document re performance. future works include extending mr.cod to more retrieval methods, such as generative dense retrievers.","{202583433: 'Das et al., 2019;', 189927857: 'Feldman and El-Yaniv, 2019;', 250562707: 'Feng et al., 2022', 230437663: 'Khattab et al., 2021', 221970302: 'Xiong et al., 2021a', 189898081: 'Yao et al., 2019;', 233219869: 'Yasunaga et al., 2021;'}",https://export.arxiv.org/pdf/2212.10786v2.pdf
97,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,discussion and limitations,Insight-tree,"we show that feature attributions can reveal known dataset biases and reasoning shortcuts in hotpotqa without having to perform a detailed manual analysis. this confirms the suitability of our attribution methods for at least this use case: model designers can look at them in a semi-automated way and determine how robust the model is going to be when faced with counterfactuals.our analysis also highlights the limitations of current explanation techniques. we experimented with other counterfactuals by permuting the order of the paragraphs in the context, which often gave rise to different predictions. we believe the model prediction was in these cases impacted by biases in positional embeddings (e.g., the answer tends to occur in the first retrieved paragraph), which cannot be indicated by current attribution methods. we believe this is a useful avenue for future investigation. by first thinking about what kind of counterfactuals and what kind of behaviours we want to explain, we can motivate the development of new explanation techniques to serve these needs.","{189927896: 'Jiang and Bansal, 2019', 219721462: 'Kamath et al., 2020', 174801764: 'Min et al., 2019;', 218487535: 'Gardner et al., 2020', 207870753: 'Tu et al., 2020;', 225068329: 'Wiegreffe et al., 2021;', 52822214: 'Yang et al. 2018'}",https://www.aclanthology.org/2021.emnlp-main.447.pdf
98,237498988,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,conclusion,Insight-tree,"we have presented a new methodology using explanations to understand model behavior on realistic counterfactuals. we show explanations can indeed be connected to model behavior, and therefore we can compare explanations to understand which ones truly give us actionable insights about what our models are doing.","{189927896: 'Jiang and Bansal, 2019', 219721462: 'Kamath et al., 2020', 174801764: 'Min et al., 2019;', 218487535: 'Gardner et al., 2020', 207870753: 'Tu et al., 2020;', 225068329: 'Wiegreffe et al., 2021;', 52822214: 'Yang et al. 2018'}",https://www.aclanthology.org/2021.emnlp-main.447.pdf
99,237385768,WebQA: Multihop and Multimodal QA,conclusion,Insight-tree,"webqa is a new multi-hop, multi-modal question answering challenge for our community. designed to simulate the heterogeneous information landscape one might expect during a web search, webqa covers a series of opendomain general visual queries while also forcing models to still reason about text. our task requires a system to determine relevant sources, perform aggregation and reasoning. we also propose a novel general recipe for evaluation on webqa which measures both fluency and accuracy.","{210859295: '[10]', 235097195: '[24]'}",https://arxiv.org/pdf/2109.00590v4.pdf
100,236428903,Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy,conclusion,Insight-tree,"this paper proposes a novel ""select-to-guide"" model (s2g) for multi-hop reading comprehension in more effective and convenient way. as an alternative of the existing graph modeling, the proposed graph-free s2g model consists of an evidence paragraph retrieval module which selects evidence paragraphs in a step-by-step multihop manner, and a multi-task module that simultaneously extracts evidence sentences and answer spans.","{52822214: '[6]', 189927896: '[8]', 155100120: '[9]', 207853300: '[10]', 153312687: '[11]', 158046817: '[12]', 215768725: '[16]', 207870753: '[18]', 221845203: '[27]', 174801080: '[28]', 211258645: '[29]', 160009340: '[30]'}",https://arxiv.org/pdf/2107.11823v1.pdf
101,254125751,ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,conclusion,Insight-tree,"we present a new parameter-effluent tuning method attempt, which learns to produce instance-wise prompts by interpolating multiple reusable soft prompts trained on source tasks and a new taskspecific prompt, while keeping the original lm frozen. our large-scale experiments demonstrate that attempt achieves a great trade-off between task performance and efficiency, introducing an interpretable and modular task transfer.","{231718729: 'Aghajanyan et al., 2021', 244478674: 'Gupta et al., 2022;'}",https://www.aclanthology.org/2022.emnlp-main.446.pdf
102,252815949,Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network,conclusion,Insight-tree,"to solve the problem of lacking global structure in ldqa methods, we propose compress graph selector network to capture the global structure over the long document when selecting evidence pieces. extensive experiments demonstrate the strong per-formance of the model.","{221845203: 'Ainslie et al., 2020', 234093776: 'Dasigi et al., 2021', 52822214: 'Yang et al., 2018', 218595722: 'Zheng et al., 2020a', 237502990: 'Zhu et al., 2021'}",https://www.aclanthology.org/2022.emnlp-main.336.pdf
103,237513496,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,conclusion and future work,Insight-tree,"in this paper, we have presented a novel paradigm of training a question filter to capture the semantics of a qa system's answering capability by distilling the knowledge of the answer scores from it. our experiments on three academic and one industrial qa benchmark show that the trained question models can estimate the pr/re curves of the qa system well, and can be used to effectively filter questions while only incurring a small drop in recall. an interesting future work direction is to analyze the impact/behavior of the question filters in a cross-domain setting, where the training and testing corpora are from different domains. this would allow examining the transferability of the semantics learned by the question filters. a complementary future work direction could be knowledge distillation from a sophisticated answer verification module like (rodriguez et al., 2019;kamath et al., 2020;zhang et al., 2021).","{235313893: 'Zhang et al., 2021'}",https://www.aclanthology.org/2021.emnlp-main.583.pdf
104,247593935,R E L I C : Retrieving Evidence for Literary Claims,conclusion,Insight-tree,"in this work, we introduce the task of literary evidence retrieval and an accompanying dataset, relic. we find that direct quotation of primary sources in literary analysis is most commonly used as evidence for literary claims or arguments. we train a dense retriever model for our task; while it significantly outperforms baselines, human performance indicates a large room for improvement. important future directions include (1) building better models of primary sources that integrate narrative and discourse structure into the candidate representations instead of computing them out-of-context, and (2) integrating relic models into real tools that can benefit humanities researchers.",{},https://www.aclanthology.org/2022.acl-long.517.pdf
105,252222436,Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?,conclusion and future work,Insight-tree,"in this work, we proposed a progressive pre-training strategy for adhoc retrieval, which consists of three hyperlink-based pre-training tasks, namely hyperlink prediction, symmetric hyperlink prediction, and most relevant document selection. these pre-training tasks can simulate different stages of a retrieval process, and gradually enhance the model's performance on relevance modeling. compared with existing methods, our strategy can make better use of utilize the hyperlink, which reveal the great potential of hyperlink in learning relevance modeling. the experimental results demonstrated the effectiveness of our model. in the future, we plan to integrate our method with other pre-training objectives (such as rop), which may bring further improvement.","{208267807: '1', 204823992: '[10]', 86611921: '[15]', 220302524: '[39]', 52822214: '[41]'}",https://export.arxiv.org/pdf/2209.06583v1.pdf
106,237737294,Dual-Channel Reasoning Model for Complex Question Answering,conclusion and future work,Insight-tree,"in this paper, we propose a dual-channel reasoning architecture for complex question answering. e dual-channel reasoning architecture is applied to the feature interaction framework and graph-based models to verify its general applicability. in the experiments, we show that our models   complexity significantly and consistently outperform the baseline model, especially in supporting fact prediction tasks. after more detailed experimental analysis, it is proved that the dual-channel reasoning structure has stronger step-by-step reasoning ability than the single-channel reasoning structure. in the future, we believe that the following issue will be worth studying. for the dual-channel reasoning architecture, the interaction strategy between the two channels, such as the soft parameter sharing of the homogeneous neural network components of the two channels, is worthy of further study.","{231627885: '[6]', 189927857: '7]', 52822214: '[12]', 189927896: '[13]', 207853300: '[14]', 155100120: '[15]', 160009340: '[16]', 174801080: '[25]', 202565945: '[26]', 214071925: '[27]', 153312687: '[29]'}",NaN
107,256358835,Graph Attention with Hierarchies for Multi-hop Question Answering,conclusions and future work,Insight-tree,"in this paper, we proposed two extensions to hierarchical graph network (hgn) for the multihop question answering task on hotpotqa. first, we completed the hierarchical graph structure by adding new edges between the query and context sentence nodes. second, we introduced gath as the mechanism for neural node updates, a novel extension to gat that can update node representations sequentially, based on hierarchical levels. to the best of our knowledge, this is the first time the hierarchical graph structure is directly exploited in the update mechanism for information propagation.","{207853300: 'Fang et al. 2020', 174801080: 'Min et al., 2019', 160009340: 'Nishida et al., 2019', 155100120: 'Qiu et al., 2019', 158046817: 'Tu et al., 2019'}",https://export.arxiv.org/pdf/2301.11792v1.pdf
108,5106665,A Model for Processing Temporal References in Chinese,conclusions and future work,Insight-tree,"the issues of mapping linguistic patterns to temporal relations are addressed in the paper. these mapping is preconditioned on the temporal indicators and achieved on a set of pre-de ned rules. the mapping mechanism was validated. on 7429 sentences describing temporal relevance, we achieved 92.77% accuracy in average. these relations will be useful to for information extraction, information retrieval and questionanswering application. once the corresponding frames have been instantiated and their slots lled after temporal natural language processing. the related temporal concepts will be linked together according to their chronological orders, to be applied as the knowledge to ful ll users' queries.",{},NaN
109,219792855,Pre-trained Language Models as Symbolic Reasoners over Knowledge?,conclusion,Insight-tree,this work is a first study towards understanding bert's ability to capture knowledge seen during pre-training by investigating it's reasoning and memorization capabilities. we identified factors influencing what knowledge is stored and what is forgotten and what is learnable beyond knowledge explicitly seen during training. we saw that theoretically bert is able to infer facts not explicitly seen during training via symbolic rules. future work should investigate how to enable bert during pre-training to use this capability. we see the need to incentivize plms to capture symbolic rules and factual knowledge as this could potentially improve plm's performance also on downstream tasks where reasoning capabilities or implicit knowledge leverage is needed.,"{211126663: 'Clark et al. 2020', 67855846: 'Dua et al., 2019;', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2006.10413v1.pdf
110,258866037,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,conclusion,Insight-tree,"we demonstrate the effectiveness of iter-retgen in answering questions with complex information needs.despite simple, iter-retgen outperforms retrieval-augmented methods that have a more complex workflow, which we believe could serve as a strong baseline for future research on retrieval-augmented generation.we also show that generation-augmented retrieval adaptation can further improve the performance of iter-retgen while also reducing overheads.","{196170479: 'Fan et al., 2019', 230799347: 'Geva et al., 2021', 226236740: 'Ho et al., 2020', 249097975: 'Izacard et al., 2022a', 86611921: 'Kwiatkowski et al., 2019', 236771976: 'Trivedi et al., 2022b'}",https://export.arxiv.org/pdf/2305.15294v2.pdf
111,216562224,Semantics-Aware Inferential Network for Natural Language Understanding,conclusion,Insight-tree,"this work focuses on two typical nlu tasks, machine reading comprehension and natural language inference by refining the use of semantic clues and inferential model. the proposed semantics-aware inferential network (sain) is capable of taking multiple semantic structures as input of an inferential network by closely integrating semantics and reasoning steps in a creative way. experiment results on 11 benchmarks, including 4 nli tasks and 7 mrc tasks, show that our model outperforms all previous strong baselines, which consistently indicate the general effectiveness of our model 6 .","{204823992: 'Fisch et al., 2019', 198229624: 'Joshi et al., 2019', 86611921: 'Kwiatkowski et al., 2019'}",https://arxiv.org/pdf/2004.13338v1.pdf
112,247476141,Hyperdecoders: Instance-specific decoders for multi-task NLP,conclusion,Insight-tree,"we propose a novel method for generating adapters conditioned on a model's input and show that this improves performance in multi-task settings across a variety of tasks. we explore the effectiveness of our approach for sequence classification, qa, and summarisation tasks, and find that it often outperforms strong parameter-efficient baselines. future work could examine applying our approach to other architectures (e.g. decoder-only models) or explore the tradeoffs between shared and generated parameters across different layers. an analysis of our approach suggests the primary benefits come from improved control of the encoder over the decoder, enhancing the effects of positive transfer from the shared encoder. this allows our approach to efficiently adapt a pretrained language model to multiple tasks unseen during pretraining while still benefiting strongly from positive transfer.","{231718729: 'Aghajanyan et al., 2021;', 221819379: 'Pilault et al. 2021'}",https://export.arxiv.org/pdf/2203.08304v3.pdf
113,16987041,Generating Referring Expressions that Involve Gradable Properties,conclusion,Insight-tree,"if the usefulness of nlg resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. in principle, this might be done by providing the generator with vague input-in which case no special algorithms are needed-but suitably contextualized vague input is often not available (mellish 2000). the only practical alternative is to provide the generator with ""crisp"" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. it is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to gre. far from being a peculiarity of a few adjectives, vagueness is widespread. we believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (section 9.3); r nouns that allow different degrees of strictness (section 9.5); r degrees of salience (section 9.4); and r imprecise pointing (section 9.5).",{},NaN
114,253237103,RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees,conclusion,Insight-tree,"we presented rlet, a rl-based entailment tree generation framework, which contains sentences selection and deduction generation modules and can be trained with cumulative signals across the entire reasoning tree. experiments show that rlet outperforms existing baselines on structure correctness and is applicable in practical scenarios. future directions include applying rl framework on other stepwise methods with more stable and sophisticated rl algorithms.","{222178328: 'Jhamtani and Clark, 2020;', 225075843: 'Lin et al. 2021', 222141025: 'Saha et al., 2020;', 247594506: 'Sanyal et al., 2022', 237258250: 'Wiegreffe and MarasoviÄ, 2021', 235187342: 'Xu et al., 2021;', 249062748: 'Yang et al., 2022', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.483.pdf
115,249017531,From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,conclusion,Insight-tree,"we propose fe2h, a simple yet effective framework for multi-hop qa that divides both the document selection and question answering into two stages following an easy-to-hard manner. experimental results demonstrate that since we cannot feed all of the candidate documents to the plms at a time due to the input length limitation, taking the multi-hop reasoning nature into consideration at the document selection phase significantly improves the overall performance. as for the subsequent qa phase, thanks to the great natural language understanding ability of the plms, the performance of our simple two-stage reader is better than the stateof-the-art approaches without any graph structure and explicit reasoning chains. we hope this work could facilitate more simple yet powerful multihop qa approaches with the help of the advanced plms.","{207853300: 'Fang et al., 2020', 215768725: 'Groeneveld et al., 2020', 189927896: 'Jiang and Bansal, 2019', 174801080: 'Min et al., 2019', 211258645: 'Perez et al., 2020', 158046817: 'Tu et al., 2019', 207870753: 'Tu et al., 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2205.11729v1.pdf
116,211146419,Conditional Self-Attention for Query-based Summarization,conclusion,Insight-tree,this paper introduces conditional self-attention (csa) and its variants as versatile and plugin mod-ules for conditional contextual dependency modeling. we develop an attention-only neural network built from csa and transformer for querybased summarization. it consistently outperforms vanilla transformer and other baselines for abstractive and extractive qsumm tasks on debatepedia and hotpotqa datasets.,,https://arxiv.org/pdf/2002.07338v1.pdf
117,211010520,Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension,discussion and conclusions,Insight-tree,"we have in this work investigated an rc annotation paradigm which includes a model in the loop that has to be ""beaten"" by the annotator. applying this approach with a series of progressively stronger rc models in the annotation loop, we arrived at three separate rc datasets, graduated by the difficulty of the model adversary. based on this dataset series we investigated several questions surrounding the annotation paradigm, in particular whether such datasets grow outdated as stronger models emerge, and about their generalisation to standard (non-adversarially collected) questions. we found that stronger rc models can still learn from data collected with a weak adversary in the loop, and their generalisation improves even on datasets collected with a very strong adversary. models trained on data collected with a model in the loop furthermore generalise well towards nonadversarially collected data, both on squad and on naturalquestions, yet we observe a slow deterioration with progressively stronger adversaries.",,https://arxiv.org/pdf/2002.00293v1.pdf
118,215785913,HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data,conclusion,Insight-tree,"we present hybridqa, which is collected as the first hybrid question answering dataset over both tabular and textual data. we release the data to facilitate the current research on using heterogeneous information to answer real-world questions. we design hybrider as a strong baseline and offer interesting insights about the model. we believe hybridqa is an interesting yet challenging nextproblem for the community to solve.","{139103297: 'Chen and Durrett, 2019;', 67855846: 'Dua et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 128345225: 'Sun et al., 2019'}",https://arxiv.org/pdf/2004.07347v3.pdf
119,258987552,UKP-SQuARE: An Interactive Tool for Teaching Question Answering,conclusion,Insight-tree,"in this paper, we present a novel method to teach question-answering to postgraduate nlp students following the learner-centered method of flipped classrooms. we propose to provide reading materials to the students before the class and use the ukp-square platform as a driving tool to conduct the class. this platform integrates the most popular qa pipelines and an ecosystem of tools to analyze the available models. these tools include explainability methods, behavioral tests, adversarial attacks, and graph visualizations. we provide a series of use cases for teaching based on the provided models and methods by ukp-square, showing that classes can become much more interactive by using ukp-square than in conventional lectures.","{204823992: 'Fisch et al., 2019', 198229624: 'Joshi et al., 2020', 233219869: 'Yasunaga et al., 2021'}",https://www.aclanthology.org/2023.bea-1.17.pdf
120,259089062,Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training,conclusion,Insight-tree,"in this work, we propose recontriever to further explore the potential of contrastive pre-training to reduce the demand of human-annotated data for dense retrievers. benefiting from multiple positives from the same document as well as relevance-aware contrastive loss, our model achieves remarkable performance under zero-shot cases. additional results on low data resources further verify its value under various practical scenarios.","{233296016: 'Thakur et al., 2021', 220302524: 'Xiong et al., 2021'}",https://export.arxiv.org/pdf/2306.03166v1.pdf
121,237373991,Interactive Machine Comprehension with Dynamic Knowledge Graphs,conclusion,Insight-tree,we explore to leverage graph representations in the challenging imrc tasks. we investigate different categories of graph structures that can capture text information at various levels. we describe methods that dynamically generate the graphs during information gathering. experiment results show that graph representations provide consistent improvement across settings. this evinces our hypothesis that graph representations are proper inductive biases in imrc. ,,https://www.aclanthology.org/2021.emnlp-main.540.pdf
122,235294052,On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study,conclusion,Insight-tree,"in this paper, we demonstrated that across a variety of models and datasets, training on adversarial data leads to better performance on evaluation sets created in a similar fashion, but tends to yield worse performance on out-of-domain evaluation sets not created adversarially. additionally, our results suggest that the adc process (regardless of the outcome) might matter more than successfully fooling a model. we also identify key qualitative differences between data generated via adc and sdc, particularly the kinds of questions created.",{},https://www.aclanthology.org/2021.acl-long.517.pdf
123,250048838,MVP: Multi-task Supervised Pre-training for Natural Language Generation,conclusion,Insight-tree,"in this paper, we present multi-task supervised pre-training (mvp) for natural language generation. firstly, we collect a large-scale nlg corpus, mvpcorpus, from 77 datasets over 11 diverse nlg tasks. after converting various nlg tasks into a unified text-to-text format, we propose multi-task supervised pre-training to learn an effective and general model mvp with task-specific prompts for nlg tasks. extensive experiments have demonstrated that: (1) supervised pre-training is beneficial for nlg tasks as an effective solution.",{},https://export.arxiv.org/pdf/2206.12131v3.pdf
124,222132873,"When in Doubt, Ask: Generating Answerable and Unanswerable Questions, Unsupervised",conclusions,Insight-tree,"this paper studies the impact of augmenting human-made data with synthetic data on the task of extractive question answering by using bert (devlin et al., 2018) as the language model and squad 2.0 (rajpurkar et al., 2018) as the baseline dataset. two sets of synthetic data are used for augmenting the baseline data: a set of answerable and another set of unanswerable questionsanswers. conducted experiments show that using both these synthetic datasets can tangibly improve the performance of the selected language model for eqa, while the unans data, generated by the authors, has a more pronounced impact on improving the performance. adding the unans dataset to the original data yields a gain of 5% in both f1 and em scores, whereas the ans dataset yields around a quarter of this gain. enhancing the original data with a combination of the two synthetic datasets improves the f1 score of bert on the test-set by 7% and the em score by 5% which are sizable improvements compared to the performance of the baseline models and similar efforts in the literature. the obtained results indicate the great potential of using synthetic data to complement the costly human-generated datasets: this augmentation can help provide the massive data required for training the modern language models at a very low cost.",{},https://arxiv.org/pdf/2010.01611v2.pdf
125,222132873,"When in Doubt, Ask: Generating Answerable and Unanswerable Questions, Unsupervised",limitations,Insight-tree,"the presented approach has limitations similar to (lewis et al., 2019b): although we tried to avoid using any human-labeled data for generating the synthetic question-answers, the questiongenerating models rely on manually-labeled data from ontonotes 5 (for ner system) and penn treebank (for extracting subclauses). further, the question-generation pipeline of this work uses english language-specific heuristics. hence, the applicability of this approach is limited to languages and domains that already have a certain amount of human-labeled data for question generation, and porting this model to another language would require extra preparatory efforts. an extensive amount of training examples are required to achieve tangible performance gains, and this results in substantial training times and compute costs for both generating synthetic data and training the bert model. these high training times and resource costs prevented us from performing the experiments on the full squad 2.0 dataset. nonetheless, given the homogeneity of the original dataset, we expect the synthetic training examples to bring similar performance improvements if added to the full dataset with similar proportions.",{},https://arxiv.org/pdf/2010.01611v2.pdf
126,237769419,Esta obra estÃ¡ bajo licencia internacional Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0. Revista Innova EducaciÃ³n,conclusiones,Insight-tree,"primero, se identificaron que los atributos del perfil docente establecidos por la instituciÃ³n enfocado por competencias son planificador, mediador, agente cultural y promotor los cuales se miden a travÃ©s de los atributos del desempeÃ±o docente que son mediador, planificador, evaluador, promotor y agente cultural.",{},http://www.revistainnovaeducacion.com/index.php/rie/article/download/357/256
127,252715485,Decomposed Prompting : A MODULAR APPROACH FOR SOLVING COMPLEX TASKS,conclusion,Insight-tree,"we proposed a new approach, decomposed prompting, to solve complex tasks using few-shot prompts, by decomposing them into a prompting program built out of simpler sub-tasks. drawing inspiration from software libraries, our decomposer and shared sub-tasks are designed in a modular fashion: they use their own few-shot prompts, allowing one to independently optimize each prompt, decompose a sub-task further if necessary, or even seamlessly replace it with a symbolic system. we show that decomposed prompting outperforms prior work on four different tasks and generalization settings, establishing it as an effective few-shot paradigm for solving complex tasks. we treat the number of paragraphs to retrieve (k) in nodecomp-ctxt and decomp-ctxt models as a hyperparameter. we select it based on a grid search on a set of values to maximize performance on a held out set of 100 questions for each dataset. for nodecomp-ctxt, we search k â {6, 8, 10} for gpt3 models and k â 2, 4, 6, 8 for flan-t5-* models. for decomp-ctxt, we search k â {2, 4, 6} for gpt3 and flan-t5-* models. note that the ranges are different between gpt3 and flan-t5-* as gpt3 can fit in more number of tokens. the ranges are different for nodecomp-ctxt and decomp-ctxt as k refers to number of paragraphs retrieved in each round of retrieval, and nodecomp-ctxt has only one step of retrieval whereas decomp-ctxt usually has multiple retrieval steps.",{},https://export.arxiv.org/pdf/2210.02406v2.pdf
128,233289699,What to Pre-Train on? Efficient Intermediate Task Selection,conclusion,Insight-tree,"in this work we have established that intermediate pre-training can yield gains in adapter-based setups, however, around 44% of all transfer combinations result in decreased performances. we have consolidated several existing and new methods for efficiently identifying beneficial intermediate tasks.",{},https://www.aclanthology.org/2021.emnlp-main.827.pdf
129,231718729,Muppet: Massive Multi-task Representations with Pre-Finetuning,conclusion,Insight-tree,"in this work, we propose pre-finetuning, a stage after pre-training to further refine representations before end-task finetuning. we show that we can effectively learn more robust representations through multi-task learning (mtl) at scale. our mtl models outperform their vanilla pre-trained counterparts across several tasks. our analysis shows that properly scaling mtl with heterogeneous batches and loss scaling is critical to leveraging better representations. we also show a critical point regarding the number of tasks when doing multi-task learning, where fewer tasks degrade representations compared to the pre-trained model, but more tasks than this point improve representations.",{},https://www.aclanthology.org/2021.emnlp-main.468.pdf
130,244119766,Less Is More: Domain Adaptation with Lottery Ticket for Reading Comprehension,conclusions,Insight-tree,"in this work, we propose alter, a simple and effective domain adaptation paradigm for few-shot reading comprehension. we exploit a small fraction of parameters of the over-parameterized source domain model to adapt to the target domain by first identifying and then fine-tuning the lottery subnetwork. we introduce self-attention attribution, an interpreting method for transformer, to identify better subnetworks and improve the target domain performance. further exploration on using several heuristic methods to reveal subnetwork structures find that subnetwork structures are critical to the effectiveness besides using fewer parameters. ","{198229624: 'Chen et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 221292890: 'Fan et al., 2020', 173188058: 'Talmor and Berant 2019', 52822214: 'Yang et al., 2018;'}",https://aclanthology.org/2021.findings-emnlp.95.pdf
131,237420912,FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models,conclusion,Insight-tree,"we present an effective few-shot question answering (qa) system that combines the use of pre-trained text-to-text models and a fine-tuning framework aligned with their pre-training counterpart. through experimental studies on various qa benchmarks and few-shot configurations, we show that this system can produce significant gains including in scenarios where the training data is extremely scarce (an absolute gain of 34 f1 points on average in comparison to the current standard of the fine-tuning framework). we also present extensions to multilingual and larger model settings and show that the gains translate well to these settings (eg:-up to an absolute 40 f1 point gain in comparison to xlm-roberta + a span-selection objective). through ablation studies, we study the impact of model size, fine-tuning objectives, inputoutput design and illustrate the factors leading to such strong gains. for future, as our framework doesn't explicitly enforce the answer to be a span in the input text, it'd be interesting to consider its applications to generative qa tasks.","{212657414: 'Clark et al., 2020a', 204823992: 'Fisch et al., 2019', 198229624: 'Joshi et al., 2020', 211258652: 'Puri et al. 2020', 230433978: 'Ram et al., 2021'}",https://www.aclanthology.org/2021.emnlp-main.491.pdf
132,218487313,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"we introduced a simple, unsupervised approach for evidence retrieval for question answering. our approach combines three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using glove embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, and (c) a simple stopping condition that concludes the iterative process when all terms in the given question and candidate answers are covered by the retrieved justifications. overall, despite its simplicity, unsupervised nature, and its sole reliance on glove embeddings, our approach outperforms all previous methods (including supervised ones) on the evidence selection task on two datasets: multirc and qasc. when these evidence sentences are fed into a roberta answer classification component, we achieve the best qa performance on these two datasets. further, we show that considerable improvements can be obtained by aggregating knowledge from parallel evidence chains retrieved by our method. in addition of improving qa, we hypothesize that these simple unsupervised components of air will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria.","{67855846: 'Dua et al., 2019;', 189927857: 'Feldman and El-Yaniv, 2019;', 208089867: 'Jansen and Ustalov, 2019', 202712552: 'Khot et al., 2019b', 202660724: 'Nie et al., 2019;', 202773198: 'Qi et al., 2019;', 128345225: 'Sun et al., 2019a', 202785879: 'Yadav et al., 2019b', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.414.pdf
133,258762869,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,conclusion,Insight-tree,"in this paper, we propose an effective method called berm to improve the generalization ability of dense retrieval without target domain data and additional modules. the basic idea of berm is learning the domain-invariant feature, that is, matching signal. to achieve it, we introduce a novel concept of dense retrieval to represent the matching information between two texts, the matching representation. further, we propose two requirements for matching and text representations as the constraint in the training of dense retrieval to enhance the ability to extract essential matching information from the passage according to different queries under the premise of balanced expression of the text. the two requirements unlock the ability of dense retrieval to capture matching signal without additional interaction. experimental results show that berm is a flexible method that can be combined with different dense retrieval training methods without inference overhead to improve the out-of-domain generalization ability. in domain adaptation setting, our method is also effective and performs better than baselines.","{233296016: 'Thakur et al., 2021', 238857091: 'Xin et al., 2022', 220302524: 'Xiong et al., 2021b'}",https://www.aclanthology.org/2023.acl-long.365.pdf
134,248722227,Exploring Universal Intrinsic Task Subspace via Prompt Tuning,conclusion and discussion,Insight-tree,"we study the hypothesis that plm adaptations to various tasks can be reparameterized as optimizations within a unified low-dimensional intrinsic task subspace. we develop an analysis tool ipt. it first finds a subspace by jointly decomposing the adaptive parameters of multiple tasks and then tunes parameters within the subspace for unseen data and tasks. experiments show the found subspaces contain good solutions for plm adaptations, which is strong evidence for our hypothesis.",{233296709: 'Ye et al. 2021'},https://export.arxiv.org/pdf/2110.07867v3.pdf
135,211003735,BREAK It Down: A Question Understanding Benchmark,conclusion,Insight-tree,"in this paper, we presented a formalism for question understanding. we have shown it is possible to train crowd-workers to produce such representations with high quality at scale, and created break, a benchmark for question decomposition with over 83k decompositions of questions from 10 datasets and 3 modalities (db, images, text). we presented the utility of qdmr for both open-domain question answering and semantic parsing, and constructed a qdmr parser with reasonable performance. qdmr proposes a promising direction for modeling question understanding, which we believe will be useful for multiple tasks in which reasoning is probed through questions.","{67855846: 'Dua et al., 2019;', 189927896: 'Jiang and Bansal, 2019;', 86611921: 'Kwiatkowski et al., 2019', 174801764: 'Min et al., 2019a', 174801080: 'Min et al., 2019b', 202773198: 'Qi et al. 2019'}",https://www.aclweb.org/anthology/2020.tacl-1.13.pdf
136,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,conclusion,Insight-tree,"we propose a few-shot data synthesis framework to training smaller models for efficient open domain multi-hop question answering with less than 10 human annotations. our framework consists of generation functions parameterized by llms and prompts, which requires less hand-crafted features than prior work while still achieving better performance. we show that our approach is general by extending to fact verification tasks. in experiments, we benchmark our approach on three multihop question answering and one fact verification benchmarks. the results show that our approach leads to significantly better models that rival the performance of previous methods employing models nearly three times larger in terms of parameter counts. the analysis shows the importance of the filtering steps and our approach benefits models of various sizes.",{},https://export.arxiv.org/pdf/2305.13691v1.pdf
137,258841780,Efficient Open Domain Multi-Hop Question Answering with Few-Shot Data Synthesis,limitation,Insight-tree,"we highlight three limitations on our work: (1) our approach depends on synthesizing large amounts of data, which are expensive even if we used llama 65b which are much smaller than palm 540b and gpt-3.5;(2) our approach finetunes language models and thus is not applicable to the closedsource language models, e.g., gpt-3 and palm; and (3) our approach depends on the availability of powerful llms for synthesizing finetuning data.",{},https://export.arxiv.org/pdf/2305.13691v1.pdf
138,251407400,FQuAD2.0: French Question Answering and Learning When You Don't Know,conclusion & future work,Insight-tree,"in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions.","{165163607: 'Clark et al., 2019', 211126910: ""d'Hoffschmidt et al., 2020"", 218974030: 'Rachel et al., 2020', 52822214: 'Yang et al., 2018'}",NaN
139,253581551,Data-Efficient Autoregressive Document Retrieval for Fact Verification,conclusions and future work,Insight-tree,we show that distant supervision and pre-training enables high precision autoregressive document retrieval with fewer annotated training data. while previous work has studied the utility of pre-training   (2020)).,"{251594672: 'Chen et al., 2022', 222125277: 'De Cao et al., 2020', 226278099: 'Jiang et al., 2020', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2211.09388v1.pdf
140,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,conclusion,Insight-tree,"this paper puts forward the view with empirical evidence for qa that contrary to popular belief, multi-source domain generalization (dg) is better modeled as a problem of addressing model underfitting than overfitting. our experimental results show that by simply learning the training domains well, even when the number of such domains is relatively small, strong out-of-domain generalization can be achieved without the need for cross-domain regularization. we rely on knowledge distillation in our experiments for improved source domain learning over erm. in light of these findings, we believe that focusing our efforts on adequately fitting the source domain patterns might be a more reasonable path forward for dg. that said, further research is needed on the topic before a definitive conclusion can be reached; we hope that our work will inspire future explorations of this problem.","{67855846: 'Dua et al., 2019', 204823992: 'Fisch et al., 2019'}",https://arxiv.org/pdf/2205.07257v1.pdf
141,248811545,Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering,limitations,Insight-tree,"we explore the problem of multi-source domain generalization (dg) in qa with new and existing methods. we believe that our findings will generalize to more baselines and datasets, but here we only show proof of concept for a select set of existing baselines and a single dg benchmark (which consists of multiple datasets from various domains).","{67855846: 'Dua et al., 2019', 204823992: 'Fisch et al., 2019'}",https://arxiv.org/pdf/2205.07257v1.pdf
142,258236218,Why Does ChatGPT Fall Short in Answering Questions Faithfully?,conclusion,Insight-tree,"our paper investigates the common failures of chat-gpt in complex open-domain question answering. we identify four types of errors: comprehension, factualness, specificity, and inference. we also examine the key abilities knowledge memorization, knowledge association, and knowledge reasoning, which are critical to these failures. additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on association, and the effect of decomposition on reasoning. finally, we suggest several techniques to help users more effectively use chat-gpt as a question-answering tool and enable system builders to develop better qa systems. our research contributes to the understanding of what influencing the faithfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more efficient and reliable language models.","{165163607: 'Clark et al., 2019', 252715485: 'Khot et al., 2023;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2304.10513v1.pdf
143,220045477,Benefits of Intermediate Annotations in Reading Comprehension,conclusion,Insight-tree,"we show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. however, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. we proposed a simple semi-supervision technique to expose the model to these annotations. we believe that in future they can be used more directly to yield better performance gains. we have also released these annotations for the research community at https: //github.com/ddua/intermediate_annotations. motteux was also without heirs and bequeathed sandringham, together with another norfolk estate and a property in surrey, to the third son of his close friend, emily lamb, the wife of lord palmerston. at the time of his inheritance in 1843, charles spencer cowper was a bachelor diplomat, resident in paris. on succeeding to motteux's estates, he sold the other properties and based himself at sandringham. he undertook extensions to the hall, employing samuel sanders teulon to add an elaborate porch and conservatory. cowper's style of living was extravagant he and his wife spent much of their time on the continent and within 10 years the estate was mortgaged for Â£89,000. the death of their only child, mary harriette, from cholera in 1854 led the couple to spend even more time abroad, mainly in paris, and by the early 1860s cowper was keen to sell the estate. figure 9: predicted relevant spans for question answered correctly with annotation (prediction:""charles spencer cowper"") and incorrectly without annotations (prediction:""lord palmerston"") by xlnet on quoref","{67855846: 'Dua et al., 2019', 174801764: 'Min et al., 2019;', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.497.pdf
144,263888725,"ViQuAE, a Dataset for Knowledge-based Visual Question Answering about Named Entities a Dataset for Knowledge-based Visual Question Answering about Named",conclusion.,Insight-tree,"despite the improvement brought by dpr and the multimodal fusion, there is still a lot of room for improvement, which highlights the difficulty of the task, especially for questions about non-human entities. this can be explained by the specialized image representation of arcface, whereas imagenet-resnet and clip are more general. it also highlights the need to study visual representation of non-human entities, as exemplified in section 1.","{221507798: '[35]', 222225265: '42,', 233219849: '47]', 52822214: '53]'}",https://hal-universite-paris-saclay.archives-ouvertes.fr/hal-03650618/document
145,263888725,"ViQuAE, a Dataset for Knowledge-based Visual Question Answering about Named Entities a Dataset for Knowledge-based Visual Question Answering about Named",conclusion and perspectives,Insight-tree,"we introduce a new dataset, viquae, designed as a benchmark to track the progress of kvqae systems. the dataset has been annotated with a semi-automatic pipeline that we also provide. questions in the dataset may be answered using a freely available kb of 1.5m wikipedia articles paired with images. we propose a baseline along with the benchmark that addresses kvqae as a two-stage problem: ir and rc, with both zero-and few-shot learning methods for the two stages. first, ir is carried out with well-established technologies: term-based text retrieval, cnn-based image retrieval, and face recognition, as well as recent bert-based retrieval techniques. then, rc also takes advantage of the ubiquitous bert model. while both stages could be improved, the experiments highlight the need for a better ir. indeed, our late fusion scheme neglects interaction between the modalities. future work should focus on a better multimodal representation, ideally embedding text and image in the same space, on both the query and kb sides. special attention should be paid to the representation of non-human entities. as exemplified in section 1 and demonstrated in section 7.2, humans can be clearly represented with their face, while other entities have more heterogeneous depictions. we believe that multimodal representations will also benefit the rc stage, as our experiments show that using a text-only reader is insufficient if the ir stage is noisy.","{221507798: '[35]', 222225265: '42,', 233219849: '47]', 52822214: '53]'}",https://hal-universite-paris-saclay.archives-ouvertes.fr/hal-03650618/document
146,248405719,A Thorough Examination on Zero-shot Dense Retrieval,conclusion and future work,Insight-tree,"in this paper, we thoroughly examine the zeroshot capability of dr models. we conduct empirical analysis by extensively studying the effect of various factors on the retrieval performance. in particular, we find that the factors of vocabulary overlap, query type distribution, and data scale are likely to affect the zero-shot performance of dense retriever. besides, the performance between bm25 and dr models varies significantly on different target datasets, where the dataset bias (e.g., a dataset is created based on exact match) is likely to make such comparison unfair. overall, we find that the zero-shot performance of dense retrieval models still has room to improve and deserves further study.","{86611921: 'Devlin et al., 2019;', 236477844: 'Ren et al., 2021a'}",https://export.arxiv.org/pdf/2204.12755v2.pdf
147,261951885,QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation,conclusion,Insight-tree,"in this paper, we propose qasnowball, a novel iterative bootstrapping framework that can continually generate high-quality and large-scale qa data.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2309.10326v2.pdf
148,264147039,Unsupervised Domain Adaption for Neural Information Retrieval,conclusions and future work,Insight-tree,"in this paper, we have explored the use of large language models (llms) for synthetic query generation and compared them to the rule-based independent cropping method for unsupervised domain adaptation.our results show that llm-based methods outperform independent cropping in all scenarios by a significant margin.although llms require more time, the benefits they offer in terms of performance make them a viable alternative for query generation.","{252519173: 'Dai et al., 2023', 249097975: 'Izacard et al. 2022', 233296016: 'Thakur et al., 2021', 247411106: 'Xu et al., 2022'}",https://export.arxiv.org/pdf/2310.09350v1.pdf
149,247476217,Tracing Origins: Coreference-aware Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we present intuitive methods to solve coreference-intensive machine reading comprehension tasks by following the reading process of human in which people connect the anaphoric expressions with explicit instructions. we demonstrate that all our three fine-tuning methods, including coref gnn , coref addatt and coref multiatt , are superior to the pre-trained language models that incorporate the coreference information in the pretraining stage, such as corefroberta large . as the fine-tuning methods rely on the coreference resolution models supplied by other researchers, their performance is also constrained by the accuracy of those coreference resolution models. in addition, the questions that require multistep reasoning, span multiple entities or contain multiple answer items also pose the challenges to our models. in the future, with more in-depth study on human reasoning in reading comprehension and more progress in graph neural networks, the gnnbased coreference graph can be enriched with more edge types and diverse structures to leverage more linguistic knowledge and gain better performance.","{67855846: 'Dua et al., 2019', 207853300: 'Fang et al., 2020', 198229624: 'Joshi et al., 2020;', 240288953: 'Qi et al., 2021', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2110.07961v2.pdf
150,244346065,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,conclusion,Insight-tree,"benefiting from pre-trained bert models, the field of information retrieval has seen remarkable progress in neural ir models, as exemplified by the success of vanilla bert which has become a strong, yet simple, baseline for neural ir models. to overcome the limitations of bert-based models regarding long documents, we have proposed to divide documents into blocks and to select only the most important key blocks. this is reminiscent of the way humans assess the relevance of a document for a given query: one first identifies blocks relevant to the query, blocks which are then aggregated to obtain the overall assessment of the document. in order to select blocks, we have investigated two approaches: the first one is straightforward and makes use of standard retrieval functions as tf-idf or bm25; the second one learns a single bert model used for both ranking blocks and documents. both approaches have been shown to improve over standard baselines and previous bert-based models. we have followed the same approach on another highly competitive neural ir model, namely parade, here again with improved results. all in all, selecting blocks is advantageous for the two models studied here, vanilla bert and parade. we conjecture that this selection is a way to remove passages in documents which are not relevant to the query and which are likely to bring noise when matching queries and documents.","{221845203: '[1]', 233189566: '[17]', 52822214: '[71]'}",https://export.arxiv.org/pdf/2111.09852v3.pdf
151,237048095,A Dataset for Answering Time-Sensitive Questions,conclusion,Insight-tree,"though time-sensitive facts are pervasive in our daily text corpus, there has been little prior work exploring this direction. in this paper, we build the first dataset to investigate whether existing models can understand time-sensitive facts. our experiments show that the sota models are still lagged behind humans in temporal reasoning. in order to empower the future nlp models to understand temporal information, different temporal-aware models need to be proposed. finally, this paper opens up new research directions for better modeling temporal information in text representations.","{67855846: '[Dua et al., 2019]', 86611921: '[Kwiatkowski et al., 2019]', 52822214: '[Yang et al., 2018]', 220831004: '[Zaheer et al., 2020]'}",https://arxiv.org/pdf/2108.06314v5.pdf
152,237513875,Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?,conclusion and future work,Insight-tree,"edge probing tests are the predominant method to probe for linguistic information in large language models. we use them to evaluate how the process of fine-tuning an lm for qa might change the grammatical knowledge in an encoder, and observe no significant differences between pre-trained and fine-tuned lms. more importantly, we find this phenomenon in carefully designed target tasks where the models must use the said grammatical knowledge. from similar ep test results, previous works have concluded that fine-tuning does not change the encoding of grammatical knowledge. however, our analysis provides a 'dataset bias' explanation for the consistency of the results and provides some clues as to why any representation tends to achieve very similar results for ep tests. this is different from the previous task-design criticisms of the ep tests.","{252283929: 'Choudhury et al. 2022', 202558795: 'van Aken et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.139.pdf
153,245218995,Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants,discussion and conclusion,Insight-tree,"in this work, we introduce generative annotation assistants (gaas) and investigate their potential to aid crowdworkers with creating more effective training data more efficiently. we perform a thorough analysis of how gaas can be used for improving qa dataset annotation in different settings, including different generative model training data, sampling strategies, and whether to also provide annotators with answer suggestions. we find that gaas are beneficial in both the standard and adversarial data collection settings. in the standard data collection setting, and under the assumption of no access to adversarially-collected data, gaas with prompts sampled based on likelihood provide annotation speed-ups, while prompts sampled by adversarial performance or uncertainty metrics provide benefits to both the model error rates on the collected data as well as subsequent downstream qa performance. we find that while gaas are effective for improving standard data collection, we still do not approach the performance obtained when using adversarial data collection.","{221507798: 'Petroni et al., 2021', 211258652: 'Puri et al., 2020;'}",https://www.aclanthology.org/2022.naacl-main.275.pdf
154,207756678,Do Multi-hop Readers Dream of Reasoning Chains?,conclusion,Insight-tree,"in this paper, we analyze qa models' capability in multi-hop reasoning by assessing if the reasoning chain could help existing multi-hop readers. we observed the general weakness of stat-or-the-art models in multi-hop reasoning and proposed a comatching based method to mitigate. despite the fact that co-matching is designed to encode only three input sequences to achieve limited multi-hop reasoning, we consider this as the most promising one that demonstrates the concrete reasoning capability and has the potential for real multi-hop reasoning.",{},https://www.aclweb.org/anthology/D19-5813.pdf
155,248862979,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,conclusion,Insight-tree,"in this work, we hypothesize that (i) logic ability plays a key role in a wide scope of tasks requiring general reasoning; and (ii) plms' logic ability can be further improved beyond their original linguistic ability. we correspondingly propose logigan, an unsupervised adversarial pre-training framework for logical reasoning enhancement. logigan circumvents the non-differentiable challenge of sequential gan via a novel generator-verifier scoring consensus mechanism, and enables largescale pre-training with longer target length. extensive experiments and ablation studies reveal the effectiveness and functional components of logigan, providing evidence to our major hypothesis.","{237485084: 'Deng et al., 2021;', 67855846: 'Dua et al., 2019', 52822214: 'Yang et al., 2018b'}",https://export.arxiv.org/pdf/2205.08794v2.pdf
156,236478211,GCRC: A New MRC Dataset from Gaokao Chinese for Explainable Evaluation,conclusions,Insight-tree,"in this paper, we present a new challenging machine reading comprehension dataset (gcrc), collected from gaokao chinese, consisting of 8,719 high-level comprehensive multiple-choice questions. to the best of our knowledge, this is currently the most comprehensive, challenging, and high-quality dataset in mrc domain. in addition, we spend considerable effort to label three types of information, including sentence-level sfs, ers of a distractor, and reasoning skills required for qa, aiming to comprehensively evaluate systems in an explainable way. through experiments, we observe gcrc is very challenging data set for existing models, and we hope it can inspire innovative machine learning and reasoning approach to tackle the challenging problem and make mrc as an enabling technology for many real-world applications.","{226236740: 'Ho et al., 2020', 218486753: 'Inoue et al., 2020', 214233456: 'Sun et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.findings-acl.113.pdf
157,225067629,AQUAMUSE: Automatically Generating Datasets for Query-Based Multi-Document Summarization,conclusion,Insight-tree,"we have presented aquamuse, a scalable methodology for constructing new qmds datasets, along with in-depth analyses and baseline experiments to demonstrate properties of one such dataset instance. many parts of the approach are configurable providing researchers a rich sandbox for evaluating summarization models under different task conditions. our methodology greatly reduces the cost of data collection by converting a predominantly generative human annotation task (e.g., reading documents and writing succinct summaries) to a discriminative human annotation task (e.g., deciding on sentence-document relevance). while our present work do not propose new methods for query-based summarization, we ran baseline experiments on one specific instance of the aquamuse dataset using a few popular neural approaches re-adapted with query conditioning. our experiments demonstrates that there is still much headroom for existing state-of-the-art models and we hope aquamuse will spur further advancements query focused multi-document summarization algorithms.",{},https://arxiv.org/pdf/2010.12694v1.pdf
158,253080775,WIKIWHY: ANSWERING AND EXPLAINING CAUSE-AND-EFFECT QUESTIONS,conclusion,Insight-tree,"with this paper, we release wikiwhy, a question-answering dataset enabling the analysis and improvement of llms' reasoning capability. we propose explanation between grounded cause-effect pairs to distinguish memorization of the relation from a genuine understanding of the underlying mechanics. compared to related works on explainable qa, our explanation format finds a natural middle ground that balances complexity and depth, allowing our crowdsourcing methods to produce thought-provoking examples while being highly scalable. we exploit this scalability to cover topics previously overlooked by other explanation datasets and demonstrate our proposed task to be difficult with strong baselines (our experiments feature models failing to produce satisfying explanations even under ideal conditions). finally, we motivate the development of new automatic metrics that are better able to handle the complexities of generated reasoning.","{218486753: 'Inoue et al. 2020', 222178328: 'Jhamtani & Clark, 2020', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2210.12152v2.pdf
159,233297028,Question Decomposition with Dependency Graphs,conclusion,Insight-tree,"in this work, we propose to represent qdmr structures with a dependency graph over the input tokens, and propose a graph parser and a seq2seq model that uses graph supervision as an auxiliary loss. we show that a graph parser is 16x faster than a seq2seq model, and that it exhibits better sample coplexity. moreover, using graphs as auxiliary supervision improves out-of-domain generalization and leads to better performance on questions that represent a long sequence of computational steps. last, we propose a new evaluation metric for qdmr parsing and show it better corresponds to human intuitions.","{215785913: 'Chen et al., 2020;', 67855846: 'Dua et al., 2019;', 210859295: 'Hannan et al., 2020;', 233219849: 'Talmor et al., 2021', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2104.08647v1.pdf
160,258865693,Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,conclusion,Insight-tree,"we raise concerns about the use of parallelintegrated methods to address context length restriction: (1) pcw is functionally equal with a simple weighted sum ensemble on label distribution among context windows; (2) pcw degrades the multi-step reasoning capabilities of llms in complex tasks requiring knowledge understanding. de-spite the fact that parallel-integrated methods sometimes show better classification performance when the label space is large, they merely brute-force ensemble each window's context, consequently weakening logical reasoning and knowledge comprehension.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.15262v1.pdf
161,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,conclusion,Insight-tree,"this paper presents tacr, a ","{224803601: 'Chen et al., 2020a', 225066758: 'Pan et al., 2020'}",https://export.arxiv.org/pdf/2305.14682v1.pdf
162,258865636,TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering,limitations,Insight-tree,"in this paper, we focus on the hybrid qa task, where the answers to most questions can be extracted from cell values in tables and linked passages using a reading comprehension model. although tacr performs well in cell selection, one of its limitations is that it lacks numerical reasoning ability across different cells, such as counting and comparing. to enable tacr to answer numerical questions, we will further develop its numerical reasoning capabilities in future work. another limitation of tacr is that it shows a strong ability in column selection while performing relatively worse in row selection. for future work, we plan to try to improve its row-selection accuracy. ","{224803601: 'Chen et al., 2020a', 225066758: 'Pan et al., 2020'}",https://export.arxiv.org/pdf/2305.14682v1.pdf
163,233444226,Dynabench: Rethinking Benchmarking in NLP,conclusion and outlook,Insight-tree,"we introduced dynabench, a research platform for dynamic benchmarking. dynabench opens up exciting new research directions, such as investigating the effects of ensembles in the loop, distributional shift characterisation, exploring annotator efficiency, investigating the effects of annotator expertise, and improving model robustness to targeted adversarial attacks in an interactive setting. it also facilitates further study in dynamic data collection, and more general cross-task analyses of humanand-machine interaction. the current iteration of the platform is only just the beginning of a longer journey. in the immediate future, we aim to achieve the following goals: anyone can run a task. having created a tool that allows for human-in-the-loop model evaluation and data collection, we aim to make it possible for anyone to run their own task. to get started, only three things are needed: a target model, a (set of) context(s), and a pool of annotators.","{86611921: 'Kwiatkowski et al., 2019', 208201969: 'Sugawara et al., 2020;'}",https://www.aclweb.org/anthology/2021.naacl-main.324.pdf
164,252846670,Do Question Answering Modeling Improvements Hold Across Benchmarks?,conclusion,Insight-tree,"this work studies whether qa modeling improvements hold across the diverse landscape of qa benchmarks. we develop the notion of concurrence, which quantifies the similarity between benchmarks' rankings of modeling approaches. experiments with 32 qa benchmarks and 20 diverse modeling approaches indicate that humanconstructed benchmarks largely have high concurrence amongst themselves, even when their passage and question distributions or linguistic phenomena of focus are very different. to better understand how different benchmark attributes affect concurrence, we explore downsampled benchmarks and various programmatically-generated benchmarks, the latter having high concurrence only when they target phenomena that are also useful for better performance on human-constructed benchmarks (e.g., identifying paraphrase and lexical overlap). our results indicate that the modeling improvements studied hold broadly, despite years of intense community focus on a small number of benchmarks.",{},https://www.aclanthology.org/2023.acl-long.736.pdf
165,227231411,Answer-driven Deep Question Generation based on Reinforcement Learning,conclusion and future work,Insight-tree,"deep question generation aims to generate complex questions that require reasoning over multiple pieces of information. in this paper, we propose an answer-driven end-to-end deep question generation model (addqg) based on reinforcement learning. an answer-aware initialization module with a gated connection layer and a semantic-rich fusion attention mechanism are designed to incorporate document and answer information into the generation process. reinforcement learning is further applied to integrate both syntactic and semantic metrics as the reward to enhance the training of addqg. experiments show that addqg outperforms the state-of-the-art systems on the challenging dqg dataset. ablation studies have demonstrated the effectiveness of our designs, and human evaluations show that our model can produce more coherent and answer-focused questions.","{139103297: 'Chen and Durrett, 2019;', 207870753: 'Tu et al., 2020', 52822214: 'Yang et al., 2018', 202572810: 'Zhang and Bansal, 2019'}",https://www.aclweb.org/anthology/2020.coling-main.452.pdf
166,259980634,Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,conclusions,Insight-tree,"overall, our results from the reasoning perturbation experiments suggest that question decomposition leads to more faithful model-generated reasoning. factored decomposition generates the most faithful reasoning, whereas cot decomposition generates less faithful reasoning than factored decomposition but more faithful reasoning than cot prompting. this is shown by the early answering experiments, which find comparable faithfulness between cot decomposition and cot prompting, and the adding mistakes experiments, which find cot decomposition has intermediate faithfulness.","{254408974: 'Dua et al., 2022', 225068329: 'Wiegreffe et al., 2021', 52822214: 'Yang et al., 2018', 252873674: 'Ye & Durrett, 2022;'}",https://export.arxiv.org/pdf/2307.11768v2.pdf
167,259980634,Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,conclusions,Insight-tree,"our findings studying the faithfulness of model-generated reasoning via biased contexts suggests that factored decomposition leads to more faithful reasoning than cot or cot decomposition. cot decomposition reasoning looks less faithful than cot reasoning via these metrics, but our measurements from the reasoning perturbation experiments suggest otherwise. we do not make any claims about any ordering of the methods in terms of their importance to overall faithfulness, so by simple averaging (after normalizing to a 0-1 scale), we assess cot decomposition reasoning as more faithful than cot reasoning.","{254408974: 'Dua et al., 2022', 225068329: 'Wiegreffe et al., 2021', 52822214: 'Yang et al., 2018', 252873674: 'Ye & Durrett, 2022;'}",https://export.arxiv.org/pdf/2307.11768v2.pdf
168,259980634,Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,discussion and limitations,Insight-tree,"our findings indicate that using question decomposition over cot prompting provides faithfulness gains at the cost of question-answering performance. factored decomposition generates the most faithful reasoning but leads to the worst question-answering performance. cot decomposition provides intermediately faithful reasoning and performance. we are uncertain how this observed trade-off might be affected by other improvements such as further training, especially training geared towards improving a model's ability to answer questions via decomposition. such training or other techniques may lead to pareto-dominating methods for highly faithful and performant model-generated reasoning, which we believe to be an exciting goal for future work.our work leans heavily on the methods we use to assess the faithfulness of model-generated reasoning. these methods are limited by our inability to access the ground truth for the model's reasoning. our claim that question decomposition improves reasoning faithfulness is one based on multiple, fairly independent, lines of evidence, but we are open to future tools for assessing reasoning faithfulness, perhaps those based on a mechanistic understanding of the internal computations of our models (olah, 2023), changing our conclusions. additionally, we evaluate our methods on only four question-answering tasks and on only one model (an rlhf-finetuned llm); pretrained llms may be more or less prone to generating ignored or biased reasoning, which may increase or reduce the faithfulness benefit obtained via decomposition. expanding the diversity of the tasks and models evaluated could lead to more robust conclusions about the relative performance and reasoning faithfulness of cot prompting and question decomposition approaches.","{254408974: 'Dua et al., 2022', 225068329: 'Wiegreffe et al., 2021', 52822214: 'Yang et al., 2018', 252873674: 'Ye & Durrett, 2022;'}",https://export.arxiv.org/pdf/2307.11768v2.pdf
169,259980634,Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,conclusion,Insight-tree,"we explore three prompting strategies for improving the question-answering performance while eliciting faithful reasoning from llms: chain-of-thought (cot) prompting, cot decomposition, and factored decomposition. our work shows it is possible to greatly improve the faithfulness of model-generated reasoning by prompting models to perform question decomposition while maintaining similar levels of question-answering accuracy, suggesting that there is even more headroom for progress using other techniques.","{254408974: 'Dua et al., 2022', 225068329: 'Wiegreffe et al., 2021', 52822214: 'Yang et al., 2018', 252873674: 'Ye & Durrett, 2022;'}",https://export.arxiv.org/pdf/2307.11768v2.pdf
170,214233456,Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension,conclusion,Insight-tree,"we present the first free-form multiple-choice chinese machine reading comprehension dataset (c 3 ), collected from real-world language exams, requiring linguistic, domain-specific, or general world knowledge to answer questions based on the given written or orally oriented texts. we study the prior knowledge needed in this challenging machine reading comprehension dataset and carefully investigate the impacts of distractor plausibility and data augmentation (based on similar resources for english) on the performance of state-of-the-art neural models. experimental results demonstrate the there is still a significant performance gap between the best-performing model (68.5%) and human readers (96.0%) and a need for better ways for exploiting rich resources in other languages.","{165163607: 'Clark et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.tacl-1.10.pdf
171,262217090,Automatic Answerability Evaluation for Question Generation,conclusion,Insight-tree,"this paper highlights an urgent issue in the field of question generation: the absence of an effective automatic evaluation metric to assess whether the generated questions are answerable.to address this issue, we propose a prompting-based metric on answerability (pman), leveraging chatgpt to assess answerability through chain-of-thought (cot) prompting.experiments with both manually created and model-generated samples demonstrate its reliability and strong alignment with human evaluations.applying our metric to evaluate qg models further indicates its potential to complement conventional metrics and guide future research in qg toward the generation of more answerable questions.however, pman currently fails to assess ""yes/no"" type questions with high accuracy, which remains as a direction for future research.","{248780551: 'Fei et al., 2022', 242075660: 'Ji et al., 2021'}",https://export.arxiv.org/pdf/2309.12546v1.pdf
172,254823272,FEWFEDWEIGHT: Few-shot Federated Learning Framework across Multiple NLP Tasks,conclusion,Insight-tree,"in this paper, we have presented fewfedweight, enabling few-shot learning across massive nlp tasks with federated learning. in this new framework, the global model synthesizes pseudo samples for each client model, which are weighted by an energy-based algorithm. aggregation weights of client models are estimated according to their performance during training. experiments on 118 different tasks demonstrate the effectiveness of the proposed fewfedweight.",{},https://export.arxiv.org/pdf/2212.08354v1.pdf
173,248780378,Query Generation with External Knowledge for Dense Retrieval,conclusion,Insight-tree,"we presented a novel query generation method, qgek, that generates synthetic queries in a form more similar to human labeled queries by using external knowledge. in order to use unprocessed external knowledge, we convert a query into a tripletbased template, which can include information of subjects and answers. remarkably, when dense retrieval models are trained with the queries generated from qgek, the performance has improved much compared to using the queries without external knowledge. also, we have shown that including external knowledge give rises to the distribution of the unique words similar to that of the human labeled queries. we believe that qgek can also be applied to the other generation methods by orthogonally adding some external knowledge processing modules. for future work, we plan to generate queries both close to human labeled ones and optimized for ir tasks and to allow the template to accept more general logical forms for diverse highquality queries. the code and data will be made available for public access.","{238857091: 'Xin et al., 2021'}",https://www.aclanthology.org/2022.deelio-1.3.pdf
174,258436815,"Huatuo-26M, a Large-scale Chinese Medical QA Dataset",conclusion,Insight-tree,"in this paper, we propose the largest chinese medical qa dataset to date, consisting of 26 million medical qa pairs, expanding the size of existing datasets by more than 2 orders of magnitude. at the same time, we benchmark many existing works based on the data set and found that these methods still have a lot of room for improvement in medical qa scenarios. we also demonstrate the possible uses of the dataset in practice. the experimental results show that the dataset contains rich medical knowledge that can be very helpful to existing datasets and tasks. we hope that the huatuo-26m dataset can not only help promote the research of medical qa, but also practically help doctors and patients.","{86611921: 'Kwiatkowski et al., 2019'}",https://export.arxiv.org/pdf/2305.01526v1.pdf
175,58008636,Genetic variability of myostatin and prolactin genes in popular goat breeds in Egypt,conclusion,Insight-tree,"in the end, this study is considered to be a step advancing for further studies that may add to give additional information about the genetic polymorphism of meat and growth characters of egyptian goat breeds and the improvement of these economically important traits. ",{},NaN
176,256460927,Generating Coherent Narratives with Subtopic Planning to Answer How-to Questions,conclusion and future work,Insight-tree,"we proposed a novel subtopic planning based architecture for answering how-to questions. our architecture is able to generate answers with better structure, higher diversity and more consistent quality. moreover, our subtopic selection method effectively singles out high quality subtopics with relevance and independence. both automatic and human evaluation proved the effectiveness of our methods. we consider the two directions for future research: 1) improving the answer's quality by applying end-to-end retrieval-generation models, e.g. (lewis et al., 2020b). 2) developing precise metrics to evaluate long-form and non-factoid answers. it suggests that incorporating commonsense knowledge may improve the performance of a subtopic decomposition model.",{},https://www.aclanthology.org/2022.gem-1.3.pdf
177,248496003,Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning,conclusion and future work,Insight-tree,"we propose a two-branch graph transformer network for logical reasoning of text, which is named as logiformer. firstly, we introduce two different strategies to construct the logical graph and syntax graph respectively. especially for the logical graph, we are the first to model both causal relations and negations in the logical reasoning task. secondly, we feed the extracted node sequences to the fully connected graph transformer for each graph. the topology of the graph is utilized to form the attention bias for the self-attention layers. thirdly, a dynamic gate mechanism is applied to make a fusion of the features from two branches. to improve the awareness of different question types, the question feature is updated based on the self-attention module. finally, the concatenated text sequence is passed through the feed forward layer and obtains the answer prediction. the whole reasoning process provides the interpretability, reflected by logical units with explicit relations and the visualization of the attention maps.",{232380161: '[12]'},https://arxiv.org/pdf/2205.00731v2.pdf
178,254877586,When Do Decompositions Help for Machine Reading?,conclusion,Insight-tree,"our work explored when decompositions are helpful for machine reading. we showed that decompositions are helpful when there is limited data available, or when parameters cannot be tuned. however, when enough data exists (empirically around a few hundred instances) and parameters can be fine-tuned, it is best to let the model learn the decompositions implicitly through end-to-end training. we hope that our work will help to inform readers as they create new datasets and select methods to use for complex question answering.","{67855846: 'Dua et al., 2019a;', 230799347: 'Geva et al., 2021b', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2212.10019v1.pdf
179,264426784,How Much Consistency Is Your Accuracy Worth?,conclusion,Insight-tree,"we introduce relative consistency, which complements standard contrast consistency by allowing an accuracy and consistency score pair to be examined to determine whether a higher consistency was possible with that accuracy.this facilitates the comparison of consistencies achieved by models that achieved different levels of accuracy.we show that relative consistency enriches conclusions we make about whether a model is more consistent than another, and occasionally even leads us to different takeaways.","{201058633: 'Lin et al., 2019', 247595263: 'Wang et al., 2023', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.blackboxnlp-1.19.pdf
180,264426784,How Much Consistency Is Your Accuracy Worth?,limitations,Insight-tree,"this mathematical model is based on a simplified version of contrastive datasets.contrastive datasets may have more than two edits for each original instance, which will result in a different distribution.although we provide formulas for distributions of arbitrary bundle size in appendix b, these distributions are less intuitive, more expensive to compute, and additionally have the drawback that, if a model achieves high pairwise rc on two of the elements of the bundle, it is likely to achieve high bundle rc, even if the other elements of the test set do not achieve high pairwise rc.in general, we recommend formulating questions of consistency in terms of bundles with one instance exhibiting a feature and the other instance lacking that feature.moreover, contrastive datasets may include extra data that is not contrastive; e.g., condaqa has a small number of bundles with a single instance because other instances in the bundle were filtered because they did not pass quality checks.in Â§2.3, we state the drawbacks of the distribution (5).namely, we do not consider that the distribution might be skewed due to the varying example difficulty and other inherent properties of datasets and models.","{201058633: 'Lin et al., 2019', 247595263: 'Wang et al., 2023', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.blackboxnlp-1.19.pdf
181,249926985,Questions Are All You Need to Train a Dense Passage Retriever,conclusions and future work,Insight-tree,"we introduced art, a novel approach to train a dense passage retriever using only questions. art does not require question-passage pairs or hardnegative examples for training and yet achieves state-of-the-art results. the key to making art work is to optimize the retriever to select relevant passages such that conditioning on them, the question generation likelihood computed using a large pre-trained language model iteratively improves. despite requiring much less supervision, art substantially outperforms dpr when evaluated on multiple qa datasets and also generalizes better on out-of-distribution questions.","{212657414: 'Clark et al., 2020', 249097975: 'Izacard et al., 2022', 86611921: 'Kwiatkowski et al., 2019', 248218489: 'Sachan et al., 2022', 233296016: 'Thakur et al. 2021', 220302524: 'Xiong et al., 2021'}",https://www.aclanthology.org/2023.tacl-1.35.pdf
182,244799249,ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,conclusion,Insight-tree,"we introduced colbertv2, a retriever that advances the quality and space efficiency of multivector representations. we hypothesized that cluster centroids capture context-aware semantics of the token-level representations and proposed a residual representation that leverages these patterns to dramatically reduce the footprint of multi-vector systems off-the-shelf. we then explored improved supervision for multi-vector retrieval and found that their quality improves considerably upon distillation from a cross-encoder system. the proposed colbertv2 considerably outperforms existing retrievers in within-domain and out-of-domain evaluations, which we conducted extensively across 28 datasets, establishing state-of-the-art quality while exhibiting competitive space footprint.","{239009856: 'Paranjape et al., 2022', 220302524: 'Xiong et al., 2020'}",https://www.aclanthology.org/2022.naacl-main.272.pdf
183,51940842,RETHINKING NUMERICAL REPRESENTATIONS FOR DEEP NEURAL NETWORKS,conclusion,Insight-tree,"in this work, we introduced the importance of carefully considering customized precision when realizing neural networks. we show that using the ieee 754 single precision floating point representation in hardware results in surrendering substantial performance. on the other hand, picking a configuration that has lower precision than optimal will result in severe accuracy loss. by reconsidering the representation from the ground up in designing custom precision hardware and using our search technique, we find an average speedup across deployable dnns, including googlenet and vgg, of 7.6Ã with less than 1% degradation in inference accuracy.",{},https://arxiv.org/pdf/1808.02513v1.pdf
184,256739246,Recent Advances in Long Documents Classification Using Deep-Learning,conclusion and future directions,Insight-tree,"it is beyond doubt that transformer architecture changed the way linguistic analysis is performed, and in a very short time bert has been widely accepted as the golden standard of semantic understanding. however, the greatest value of this concept may be tied to its flexibility, as it allows for extensive customization and specialization with only minimal modifications of the training procedure. while there have been numerous adaptations of successful transformer models in the past, it's highly likely that the number and quality of derivative work will increase in the near future. figuring out ways to improve an already impressive model is not easy, but growing presence of this topic in the online forums and greater availability of research papers dealing with some of the outstanding challenges could power the next wave of research in this direction. this process is already underway, and a breakthrough achieved with transformers is being actively exploited by research teams from around the world.",{},https://www.aclanthology.org/2022.icnlsp-1.12.pdf
185,189815643,Genome-Wide Analysis of the miRNA-mRNAs Network Involved in Cold Tolerance in Populus simonii Ã P. nigra,conclusions,Insight-tree,"cold tolerance mirnas and candidate target genes were identified through integrated srna and transcriptome analysis in cold treatment of populus simonii Ã p. nigra. mir genes such as mir319, mir159, mir167, mir172, mir395, mir393, mir390, and novel_63 and transcriptional factors including myb, sbp, bzip, arf, lhw, and atl showed differential expression and they might be the main contributors related to lrr receptor kinase, the arf pathway, the spl pathway, and dnaj-related photosystem ii involved in the cold tolerance of populus simonii Ã p. nigra. these results not only increase our knowledge of srnas involved in the post-transcriptional regulation of cold tolerance, but also provide candidate genes for future functional analysis of the cold tolerance-related signaling pathways in populus simonii Ã p. nigra.",{},NaN
186,235606327,On the Diversity and Limits of Human Explanations,conclusion,Insight-tree,"explanations represent a fascinating phenomenon and are actively studied in psychology, cognitive science, and other social sciences. while the growing interest in explanations from the nlp community is exciting, we encourage the community to view this as an opportunity to understand how humans approach explanations and contribute to understanding and exploring the explanation processes. this will in turn inform how to collect and use human explanations in nlp. a modest proposal is that it is useful to examine and characterize human explanations before assuming that all explanations are equal and chasing a leaderboard.",,https://www.aclanthology.org/2022.naacl-main.158.pdf
187,233168843,BOOTSTRAP INFERENCE FOR HAWKES AND GENERAL POINT PROCESSES,conclusions,Insight-tree,"in this paper we have discussed the theoretical foundations and practical implementations of bootstrap inference for self-exciting point process models. applications of the bootstrap in order to improve upon the poor quality of asymptotic approximations are scarce in the literature. classic 'recursive intensity bootstrap' (rib) schemes have been proposed in the recent literature, although without proof of their first-order validity. rib schemes can also be quite involved to implement in practice, as they generally require numerical integration for the recursive computation of the intensity for each bootstrap repetition. to improve, we have introduced a new bootstrap scheme, the 'fixed intensity bootstrap' (fib), where the conditional intensity is kept fixed across bootstrap repetitions. by doing so, conditionally on the original data the bootstrap data generating process follows a simple inhomogeneous point process with known intensity; therefore, it is very simple to implement and to use in practice. for both bootstrap schemes, we have provided a new bootstrap (asymptotic) theory, which allows to assess bootstrap validity for both bootstraps. monte carlo evidence supports the idea that the bootstrap is a valid inference method when applied to point process models.",{},https://arxiv.org/pdf/2104.03122v2.pdf
188,260460827,Neural Conversational QA: Learning to Reason v.s. Exploiting Patterns,conclusion,Insight-tree,"in this paper we show how the existing neural models exploit spurious patterns that exist in the data for the sharc task -a conversation qa that requires reasoning over rules expressed in natural language. we demonstrate how existing models can exploit spurious patterns in such conversational qa datasets and introduce an augmented version of the sharc dataset that discourages a model from exploiting such spurious clues. we also present a simple yet effective model, ur-canet, that learns embedding representation from the dialog history, dialog turns, and the history of past follow-up question and answer pairs. the network generate intermediate representations which is input to a copy decoder to generate a follow-up question. urcanet outperforms existing systems on both the original sharc corpus and the augmented sharc corpus.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/1909.03759v1.pdf
189,221738903,Asking Complex Questions with Multi-hop Answer-focused Reasoning,conclusion and future work,Insight-tree,"in this paper, we proposed a new task that asks complex questions given a collection of documents and the corresponding answer by discovering and modeling the multiple entities and their semantic relations across the documents. to solve the problem, we propose answer-focused multi-hop reasoning by leveraging different granularity levels of semantic information in the answer-centric entity graph built from natural language text. extensive experiment results demonstrate the superiority of our proposed model in terms of automatically computed metrics and human evaluation. our work provides a baseline for the new task and sheds light on future work in the multi-hop question generation scenario.",{},https://arxiv.org/pdf/2009.07402v1.pdf
190,218570919,CAiRE-COVID: A Question Answering and Multi-Document Summarization System for COVID-19 Research,conclusion,Insight-tree,"we have described our system, caire-covid, comprising of three major modules, information retrieval, question answering, and summarization, which uses the cord-19 dataset consisting of published scientific articles concerning covid-19. our system can answer user queries related to covid-19 by retrieving relevant paragraphs from articles available in the dataset, using our qa models to answer the question, and also generate two versions of a concise summary of the top paragraphs via the two summarization models. we believe that getting factual information regarding covid-19 and showing them in a comprehensible way, we can prioritise scientific facts about the virus, and help the community in the fight against the ongoing global pandemic.","{208000835: 'Su et al., 2019', 52822214: 'Yang et al., 2018b'}",https://arxiv.org/pdf/2005.03975v1.pdf
191,28464742,Constructive physics,conclusion,Insight-tree,"we familiarized ourselves with ideas joint by the common name of constructive physics. these ideas do not form the separate discipline, as the constructive mathematics is not some separate science. constructivism is the direction, which aroused in mathematics, namely, in its foundations, and in its development absorbs physics. this process is unavoidable and wholesome. i will be glad if this book helps a reader to form more definite attitude to the constructivism, even more if it excites the desire to take up the development of this direction immediately. i permit myself to enumerate some problems which solution seems to me accessible in the framework of constructivism right now.",{},https://arxiv.org/pdf/0805.2859v2.pdf
192,235097195,MIMOQA: Multimodal Input Multimodal Output Question Answering,conclusion,Insight-tree,"we presented one of the first exploration, to the best of our knowledge, of multimodal output question answering from multimodal inputs and proposed usage of publicly available textual datasets for it. we proposed strong baselines by utilizing the existing frameworks for extract textual answers and independently match them with an appropriate image. we demonstrate the value of a joint-multimodal understanding for multimodal outputs in our problem setup by developing a multimodal framework mexbert which outperformed the baselines significantly on several metrics. we also developed a proxy supervision technique in absence of labelled outputs and showed its effectiveness for improved multimodal question answering. we used some existing metrics to compare the different models and justified the usage of these metrics based on a human experiment.","{86611921: 'Kwiatkowski et al., 2019'}",https://www.aclweb.org/anthology/2021.naacl-main.418.pdf
193,257636734,Logical Reasoning over Natural Language as Knowledge Representation: A Survey,conclusion,Insight-tree,"in this paper, we propose a new concept, logical reasoning over natural language as knowledge representation (lrnl), and provide a detailed and up-to-date review of lrnl. moreover, we have introduced the philosophical foundations, advantages of lrnl, benchmarks and methods, challenges, desirable tasks & methods, and the relation of lrnl to related nlp fields ( Â§a.1).","{248986946: 'Qu et al., 2022;', 222141025: 'Saha et al., 2020', 235097535: 'Saha et al., , 2021', 249062828: 'Sanyal et al. 2022a', 247594506: 'Sanyal et al., 2022b;', 219573621: 'Talmor et al. 2020'}",https://export.arxiv.org/pdf/2303.12023v1.pdf
194,247447562,Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering,conclusion,Insight-tree,"this paper proposes hyperlink-induced pretraining (hlp), a pre-training method for openqa passage retrieval by leveraging the online textual relevance induced by hyperlink-based topology. our experiments show that hlp gains significant improvements across multiple qa datasets under different scenarios, consistently outperforming other pre-training methods. our method provides insights into openqa passage retrieval by analyzing the underlying bi-text relevance. future work involves addressing tasks like ms marco where the granularity of the information-seeking target is at the passage level. ","{86611921: 'Kwiatkowski et al., 2019'}",https://www.aclanthology.org/2022.acl-long.493.pdf
195,257813864,qaaskeR + : a novel testing method for question answering software via asking recursive questions,conclusion and future work,Insight-tree,"question answering (qa) software has been widely used in our daily life. in this paper, we propose a novel recursive metamorphic testing method qaasker + with five novel recursive metamorphic relations. qaasker + tests qa software by checking its behaviors on multiple recursively asked questions that are relevant to the same or some further enriched knowledge. it cuts off the reliance on the preannotated labels of test cases, thus enables both the flexible just-in-time test during usage and the extensible test with massive unlabeled data for qa software, which cannot be supported by the current reference-based test paradigm. we evaluate the effectiveness of qaasker + by using it to test four representative state-of-the-art qa software that covers two mainstream types of qa software, as well as a popular reallife qa application, the google search service. comprehensive results demonstrate that qaasker + can reveal quantities of valid violations that depict diverse answering issues for various kinds of mainstream qa software. besides, we also found that our recursive mrs have a better fault detection effectiveness than two representative non-recursive mrs and can even help to fix the revealed issues.","{165163607: 'Clark et al. 2019;', 202572622: 'Jin et al. 2019;', 86611921: 'Kwiatkowski et al. 2019;', 52822214: 'Yang et al. 2018'}",NaN
196,246904646,A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models,conclusions,Insight-tree,"we comprehensively survey existing works about knowledgeintensive nlp with pre-trained language models and summarize the current progress in terms of the three critical components in plmkes: knowledge sources, knowledge-intensive nlp tasks, and knowledge fusion methods. based on the discussion about the three components, we further pose several challenges that would be influential in the practical usage and propose the related future directions in response to the challenges. we hope that this paper could provide nlp practitioners with a clear picture on the topic and boost the development of the current knowledge-intensive nlp technologies.","{52822214: 'Yang et al., 2021]'}",https://arxiv.org/pdf/2202.08772v1.pdf
197,235742855,Probabilistic Graph Reasoning for Natural Proof Generation,conclusion,Insight-tree,"in this work, we propose probr, a novel probabilistic graph reasoning framework for joint question answering and proof generation. probr defines a joint distribution over all possible answers and proofs, which can directly characterize the interaction between answers and proofs. experiments prove the effectiveness of proposed probr.        1. in all ablation experiments, probr achieved the best qa performance, demonstrating that probr can capture critical information for question answering in a variety of settings. however, since some of the dataset are artificially synthesized, it is difficult to guarantee that probr will work in the real dataset as well. we leave it as future work.","{211126663: 'Clark et al. 2020', 222141025: 'Saha et al., 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2107.02418v1.pdf
198,249890380,KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP,conclusion and future work,Insight-tree,"this paper explores multi-task learning paradigms at a massive scale for data augmentation in fewshot language learning for the first time. we demonstrate that the proposed knowledge mixture training enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences). experiments verified the effectiveness of our knowda, and knowda outperforms state-of-the-art data augmentation approaches on well-established benchmarks superglue, conll'03, and wikiann in the few-shot setting. we also perform ablation studies indicating the importance of including demonstrations and the impact of different keys. moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration. [30] prashanth vijayaraghavan, ivan sysoev, soroush vosoughi, and deb roy. deepstance at semeval-2016 task 6: detecting stance in tweets using character and word-level cnns. arxiv preprint arxiv:1606.05694, 2016.",{},https://export.arxiv.org/pdf/2206.10265v1.pdf
199,238639683,Efficient Self-Supervised Metric Information Retrieval: A Bibliography Based Method Applied to COVID Literature,conclusions,Insight-tree,"we proposed a new self-supervised method to create a latent semantic space from unlabelled corpora of papers, where the spatial proximity among them represents their semantic similarity. however in unsupervised corpora of papers, such as the cord-19 that contains a large collection of the covid literature, is unknown which papers are positively and negatively related each other.",{},NaN
200,259252406,ROBUT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations,conclusion,Insight-tree,"this work proposes robut, the first benchmark for ",{},https://www.aclanthology.org/2023.acl-long.334.pdf
201,234778323,Answering Any-hop Open-domain Questions with Iterative Document Reranking,conclusion,Insight-tree,"we present a qa framework that can answer any-hop open-domain questions, which iteratively retrieves, reranks and filters documents with a graph-based reranking model, and adaptively decides how many steps of retrieval and reranking are needed for a multi-hop question. our method consistently achieves promising performance on both single-and multi-hop open-domain qa datasets.","{208267807: '[1,', 202583433: '[5]', 211296452: '[8]', 153312687: '[9]', 207853300: '[10]', 189927857: '[11]', 174801080: '[21]', 202660724: '23,', 160009340: '24]', 202773198: '[25]', 155100120: '[26]', 158046817: '[29]', 52822214: '[41]', 218595722: '[44]'}",https://export.arxiv.org/pdf/2009.07465v5.pdf
202,5742355,Back-translation for discovering distant protein homologies,conclusions,Insight-tree,"in this paper, we addressed the problem of finding distant protein homologies, in particular affected by frameshift events, from a codon evolution perspective. we search for protein common origins by implicitly aligning all their putative coding dna sequences, stored in efficient data structures called back-translation graphs. our approach relies on a dynamic programming alignment algorithm for these graphs, which involves a non-monotonic gap penalty that handles differently frameshifts and full codon indels. we designed a powerful translation-dependent scoring function for nucleotide pairs, based on codon substitution models, whose purpose is to reflect the expected dynamics of coding dna sequences.",{},https://arxiv.org/pdf/1001.4603v1.pdf
203,259316688,Analyzing Multiple-Choice Reading and Listening Comprehension Tests,conclusions,Insight-tree,"this work highlights the trade-off between contextual comprehension and world knowledge in multiple-choice reading and listening comprehension tests. we found that automated reading comprehension systems perform significantly better than random, even with limited access to the context passage. these findings provide content creators with an approach to capture the balance between comprehension and world knowledge in their questions. we further investigated to what extent a context needs to be read before the correct answer can be deduced, finding that it is possible to answer some questions across several reading/listening comprehension datasets with only access to a fraction of the context. overall, our findings guide content creators in constructing more valid and reliable assessments, ensuring accurate evaluation of language proficiency.","{253510370: '[3]', 208201969: '[5,', 220831004: '16,', 234335834: '17]', 52822214: '21]', 248779897: '[23]'}",https://export.arxiv.org/pdf/2307.01076v1.pdf
204,259316688,Analyzing Multiple-Choice Reading and Listening Comprehension Tests,limitations,Insight-tree,"a limitation for the ibm-debater dataset is that the contexts have been truncated to 512 tokens prior to any experiments despite the average length being approximately 1000 tokens to use the standard pretrained language model finetuned on race++.this research is funded by the epsrc (the engineering and physical sciences research council) doctoral training partnership (dtp) phd studentship and supported by cambridge assessment, university of cambridge and alta.","{253510370: '[3]', 208201969: '[5,', 220831004: '16,', 234335834: '17]', 52822214: '21]', 248779897: '[23]'}",https://export.arxiv.org/pdf/2307.01076v1.pdf
205,259501085,InPars Toolkit: A Unified and Reproducible Synthetic Data Generation Pipeline for Neural Information Retrieval,conclusions,Insight-tree,"we have introduced the inpars toolkit, a codebase designed to generate synthetic data using llms in a reproducible manner for neural ir tasks. the toolkit comprises an end-to-end pipeline that encompasses data generation, training, reranking, and evaluating the trained models. additionally, the codebase is integrated with two major libraries for commonly used datasets from the beir benchmark, and it supports both gpu and tpu training and inference. our goal is to make research on these methods more accessible and to pave the way for this emerging research trend in the ir community. our experiments have demonstrated that training reranker models using synthetic data and evaluating them on gpu infrastructure yielded results comparable to those obtained when training on the tpu setup. additionally, we have also made available all synthetic data generated for all beir datasets and the models finetuned on this data.",{233296016: '[16]'},https://export.arxiv.org/pdf/2307.04601v1.pdf
206,264306280,Knowledge-Augmented Language Model Verification,conclusion,Insight-tree,"in this work, we proposed knowledge-augmented language model verification (kalmv), which identifies not only the relevance of the retrieved knowledge to the input query but also the faithfulness of the reflection of knowledge in the generated answers, in order to prevent incorrect answer generations with knowledge-augmented lms.to this end, we developed a verifier that can detect errors in both the knowledge retrieval and answer generation stages by instruction-finetuning lms.further, during inference, we proposed to rectify errors by re-retrieving knowledge and re-generating answers if our kalmv detects errors, and also perform an ensemble over multiple verification outputs from different instructions, to improve the efficacy of the verifier.we validated kalmv on two question answering tasks and showed its effectiveness in significantly reducing hallucinations.we believe that kalmv will bring substantial practical impact in improving the reliability of lm-based systems, especially since it is a plug-and-play module.","{86611921: 'Kwiatkowski et al. 2019', 252693442: 'Sen et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.12836v1.pdf
207,2184647,Finding optimal solutions for vehicle routing problem with pickup and delivery services with time windows: A dynamic programming approach based on state-space-time network representations,conclusions,Insight-tree,"a new generation of transportation network companies uses mobile-phone-based platforms to seamlessly connect drivers to passengers from different origins to different destinations with specific, preferred departure or arrival times.many relevant practical aspects need to be carefully formulated for real-world planning/dispatching system deployment, such as time-dependent link travel times on large-scale regional transportation networks, and tight vehicle capacity and passenger service time window constraints.",{},https://arxiv.org/pdf/1507.02731v2.pdf
208,251224409,Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,conclusion and future work,Insight-tree,"in this paper, we present aggretriever, a singlevector dense retrieval model that exploits all contextualized token embeddings from the input to bert. we introduce a simple approach to aggregate the contextualized token embeddings into a dense vector, agg . experiments show that agg combined with the standard [cls] vector achieves better retrieval effectiveness than using the [cls] vector alone for both in-domain and zero-shot evaluations. our work demonstrates that mlm pre-trained transformers can be fine-tuned into effective dense retrievers without further pre-training or expensive fine-tuning strategies.",{233296016: 'Thakur et al. 2021'},https://www.aclanthology.org/2023.tacl-1.26.pdf
209,211258652,Training Question Answering Models From Synthetic Data,conclusion,Insight-tree,"we build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation. with our best models, we generate large question answering datasets from unlabeled wikipedia documents and finetune a 345 million parameter bert-style model achieving 88.4 em score. finetuning the resulting model on real squad1.1 data further boosts the em score to 89.4. this amounts to a 1.7 point improvement over our fully supervised baseline. finally, we generate synthetic text from a wikipedia-finetuned gpt-2 model, generate answer candidates and synthetic questions based on those answers, and then train a bert-large model to achieve similar question answering accuracy without directly using any real data at all. doing so required us to scale model size for our answer generators, question generators, and filtration models. we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.","{173188058: 'Talmor & Berant, 2019'}",https://arxiv.org/pdf/2002.09599v1.pdf
210,264463622,"Information technology capability, open technological innovation and firm growth",conclusions and discussion,Insight-tree,"conclusionsthis study contributes to the existing literature on the relationship between it capability and firm growth by examining the mediating effects of open technological innovation.our findings are consistent with previous research that suggests it capability positively influences firm growth [60,61].however, our study provides a more nuanced understanding of this relationship by showing that open technological innovation plays a partial mediating role in this relationship.specifically, our results suggest that new ventures that are able to leverage open technological innovation can better translate their it capabilities, such as it flexibility and it integrality, into firm growth.",{},NaN
211,264463622,"Information technology capability, open technological innovation and firm growth",conclusions,Insight-tree,"this study contributes to the existing literature on the relationship between it capability and firm growth by examining the mediating effects of open technological innovation.our findings are consistent with previous research that suggests it capability positively influences firm growth [60,61].however, our study provides a more nuanced understanding of this relationship by showing that open technological innovation plays a partial mediating role in this relationship.specifically, our results suggest that new ventures that are able to leverage open technological innovation can better translate their it capabilities, such as it flexibility and it integrality, into firm growth.",{},NaN
212,264463622,"Information technology capability, open technological innovation and firm growth",limitations and future research,Insight-tree,"while this study provides important contributions to the literature on the relationship between it capability, open technological innovation, and firm growth, there are several limitations that need to be addressed in future research.firstly, the cross-sectional design adopted in this study limits the ability to establish causality between the constructs over time.future research should use a longitudinal study to overcome this limitation and consolidate the results.secondly, the smaller sample size limits the generalizability of our findings.the data used in this study may not be representative of all new ventures and may be limited by the interviewees' cognition.to address this limitation, future research should focus on a specific industry in china and explore the effects of it capability on open technological innovation and firm growth under the regulatory impact of the sharing economy environment to better understand the relationship among them.additionally, there may be other variables, such as organizational learning, that mediate the relationship between it capability and firm growth.therefore, future research should investigate the role of other mediators to provide a more comprehensive understanding of the mechanisms through which it capability influences firm growth.by addressing these limitations, future research can further enhance our understanding of the complex relationship between it capability, open technological innovation, and firm growth in the context of new ventures in china.h2. it capability positively affects open technological innovation.h2a.it flexibility positively affects open technological innovation.h2b.it integration positively affects open technological innovation.",{},NaN
213,259360724,Text Alignment Is An Efficient Unified Model for Massive NLP Tasks,conclusion,Insight-tree,"we propose to unify diverse language tasks into a text pair alignment problem.this framework yields an alignment model (align) that, despite being less versatile than llms, solves a wide range of language problems efficiently with superior performance.we show that align outperforms task-specific models finetuned on several nlu tasks while having performance comparable to llms that are orders of magnitude larger.additionally, align excels in factual consistency evaluation, and can be used as an add-on to augment llms in qa tasks by identifying unanswerable questions.","{231718729: '[9]', 207756753: '39]', 165163607: '[45]', 213474484: '[46]', 86611921: '[75]', 211010520: '[79]', 67855846: '[80]', 52822214: '[81]', 201058633: '[84]', 230799347: '[86]'}",https://export.arxiv.org/pdf/2307.02729v2.pdf
214,263830734,RECOMP: IMPROVING RETRIEVAL-AUGMENTED LMS WITH COMPRESSION AND SELECTIVE AUGMENTATION,conclusion,Insight-tree,"we introduce recomp, a method which compresses retrieved documents into textual summaries before prepending them to improve in-context retrieval augmented language models.we present two compression models -an extractive compressor and an abstractive compressor.we design a training scheme which leverages end task signals from a blackbox lm to generate useful summaries and allowing the compression models to perform selective augmentation.our experiments show that our compressors can improve the efficiency of retrieval augmented lms significantly with minimal drop in performances.","{86611921: 'Kwiatkowski et al., 2019', 221507798: 'Petroni et al., 2021', 226262229: 'Xu & Lapata, 2020', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.04408v1.pdf
215,249062558,Is a Question Decomposition Unit All We Need?,conclusion,Insight-tree,the recent trend of building large lms may not be sustainable to solve evolving benchmarks. we believe that modifying data samples can significantly help the model improve performance. we study the effect of question decomposition (qd) on a diverse set of tasks. we decompose questions manually and significantly improve model performance (24% for gpt3 and 29% for roberta-squad along with a symbolic calculator). our findings indicate that human-in-the-loop question decomposition (hqd) can potentially provide an alternate path to building large lms. our approach provides a viable option to involve people in nlp research. we hope our work will encourage the community to develop human-centric solutions that actively involve humans while leveraging nlp resources.,{},https://www.aclanthology.org/2022.emnlp-main.302.pdf
216,243865275,Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation,conclusion,Insight-tree,"we present distdr, a distantly-supervised odqa system that improves over a weak retriever by iter-atively finding evidence from a corpus, and using the evidence as distant supervision for model training.without using any evidence labels, distdr matches the fully-supervised sota approaches on both multi-hop and single-hop qa benchmarks.",{},https://arxiv.org/pdf/2110.04889v1.pdf
217,246680186,Logical Reasoning for Task Oriented Dialogue Systems,conclusions,Insight-tree,"in this paper, we proposed an architecture for the integration of a reasoning model in task-oriented dialogue systems. we formulated the problem as a sequence prediction problem given a user query and context, and presented an approach for generating data and fine-tuning generative models to reason over a set of facts in the dialogue context. we demonstrated our approach for a shopping assistant and reported experimental results for different formulations of the problem. we showed that these models can learn to do logical reasoning to 1) answer questions from the dialogue context when all the information is available, 2) extract constraints when partial information is available, and 3) delegate to the dialogue policy when no reasoning is required.",{},https://www.aclanthology.org/2022.ecnlp-1.10.pdf
218,55533745,Detecting dominant changes in irregularly sampled multivariate water quality data sets,conclusions,Insight-tree,"we suggested and tested an exploratory approach for the detection of dominant changes in multivariate water quality data sets with irregular sampling in space and time.the combination of the selected methods aimed to provide a broadly applicable exploratory framework for typical existing monitoring data sets, e.g. from environmental agencies, which are often characterized by relatively low sampling frequency and irregularities of the sampling in space and/or time.in the approach, we applied a dimension-reduction method to derive multivariate water quality components and analysed their spatiotemporal features with respect to changes that concerned more than single sites, short-term fluctuations or single events.",{},NaN
219,216641884,A Survey of Document Grounded Dialogue Systems (DGDS),conclusion,Insight-tree,"the document grounded dialogue system (dgds) can mine document(s) information and discuss specific document(s) in a real human conversation. we believe that extracting unstructured document(s) information in dialogue is the future trend of the ds because a large amount of human knowledge is contained in these document(s). the research of the dgds not only possesses a broad application prospect but also facilitates the ds to better understand human knowledge and natural language. this article introduces the dgds, defines the related concepts, analyzes the current datasets and models, and provides views on future research trends in this field, hoping to be helpful for the community.","{67855846: '[21]', 158046817: '[103]', 52822214: '[125]', 52895001: '126,'}",https://arxiv.org/pdf/2004.13818v1.pdf
220,234741852,TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,conclusion,Insight-tree,"we propose a new challenging qa dataset tat-qa, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. to answer questions in tat-qa, the close relation between table and paragraphs and numerical reasoning are required. we also propose a baseline model tagop based on tat-qa, aggregating information from hybrid context and performing numerical reasoning over it with pre-defined operators to compute the final answer. experiments show tat-qa dataset is very challenging and more effort is demanded for tackling qa tasks over hybrid data. we expect our tat-qa dataset and tagop model would serve as a benchmark and baseline respectively to help build more advanced qa models, facilitating the development of qa technologies to address more complex and realistic hybrid data, especially those requiring numerical reasoning.","{224803601: 'Chen et al., , 2021', 215785913: 'Chen et al., 2020b', 67855846: 'Dua et al., 2019;', 128345225: 'Sun et al., 2019;'}",https://www.aclanthology.org/2021.acl-long.254.pdf
221,253255229,MULTI-VECTOR RETRIEVAL AS SPARSE ALIGNMENT,conclusion,Insight-tree,"in this paper, we introduce aligner, a novel sparse alignment method for multi-vector document retrieval. we first formulate different retrieval models with token-level sparse alignments and propose aligner to tackle the limitations of existing models. specifically, aligner uses pairwise alignments and unary saliences that allow us to adapt to different tasks and prune unimportant tokens, respectively. as a result, we achieve strong performance on both zero-shot and few-shot document retrieval tasks while drastically improving the run-time and storage complexity of multi-vector retrieval. with its interpretable alignments and better performance with large language models, we envision that our multi-vector retrieval model can serve as a strong standalone retriever in the future.","{249097975: 'Izacard et al., 2022;', 220302524: 'Xiong et al., 2020;'}",https://export.arxiv.org/pdf/2211.01267v1.pdf
222,235755349,Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training,conclusion,Insight-tree,"in this paper, we propose a new approach to train multi-hop qa models, not to take reasoning shortcuts of guessing right answers without sufficient evidences. we do not require annotations and generate pseudo-evidentiality instead, by regularizing qa model from being overconfident when evidences are insufficient. our experimental results show that our method outperforms baselines on hotpotqa and has the effectiveness to distinguish between evidence-positive and negative set.","{208267807: 'Asai et al., 2019', 139103297: 'Chen and Durrett, 2019;', 207853300: 'Fang et al., 2020', 215768725: 'Groeneveld et al., 2020;', 189927896: 'Jiang and Bansal, 2019', 174801764: 'Min et al., 2019', 202660724: 'Nie et al., 2019;', 155100120: 'Qiu et al., 2019;', 221749191: 'Trivedi et al. 2020', 218487313: 'Yadav et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.acl-long.476.pdf
223,232035689,Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing,conclusions,Insight-tree,"we have presented a review of existing datasets for exnlp research, highlighted discrepancies in data collection that can have downstream modeling effects, and synthesized the literature both inside and outside exnlp into a set of recommendations for future data collection.","{236459873: '[2]', 218486753: '[53]', 222178328: '[58,', 204915921: '[61]', 222310757: '[112]', 225068329: '[128]', 52822214: '[137]', 218487030: '[138]'}",https://arxiv.org/pdf/2102.12060v4.pdf
224,255186555,DEMONSTRATE-SEARCH-PREDICT: Composing retrieval and language models for knowledge-intensive NLP,conclusion,Insight-tree,"for a long time, the dominant paradigm for building models in ai has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration. however, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems.","{230799347: 'Geva et al., 2021;', 230437663: 'Khattab et al., 2021a', 239009856: 'Paranjape et al. 2022', 244799249: 'Santhanam et al., 2022b'}",https://export.arxiv.org/pdf/2212.14024v2.pdf
225,257642038,Parameter-Efficient Sparse Retrievers and Rerankers using Adapters,conclusion,Insight-tree,"retrieval models, based on plm, require finetuning millions of parameters which makes them memory inefficient and non-scalable for out-of-domain adaptation. this motivates the need for efficient methods to adapt them to information retrieval tasks. in this paper, we examine adapters for sparse retrieval models. we show that with approximately 2% of training parameters, adapters can be successfully employed for splade models with comparable or even better effectiveness on benchmark ir datasets such as ms marco and trec. we further analyze adapter layer ablation and see a further reduction in training parameters to 1.8% retains effectiveness of full finetuning. for domain adaptation, adapters are more stable and outperform finetuning, which is prone to overfitting, on tripclick dataset, adapters outperform on precision metrics torso and tail queries and performs comparably on head queries. we explore knowledge transfer between first stage rankers and rerankers as a final study. adapters underperform full finetuning when trying to reuse sparse model to rerankers. dense first stage rankers perform similarly for adapters and finetuning while sparse first stage rankers is less effective compared to finetuning. we leave this as future work. as memory-efficient adapters are effective for splade, we leave for future studying larger sparse models and their generalizability. finally, an interesting scenario could also be to tackle unsupervised domain adaptation with adapters.","{248665596: '5', 233296016: '[30]'}",https://export.arxiv.org/pdf/2303.13220v1.pdf
226,222067190,MaP: A Matrix-based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we first investigate different approaches of span extraction in mrc. to improve the current vector-based conditional approach, we propose a matrix-based conditional approach. more careful consideration of the dependencies between the start and end positions of the answer span can predict their values better. we also propose a sampling-based training strategy to address the training process of the matrix-based conditional approach. the final experimental results on a wide of datasets demonstrate the effectiveness of our approach and training strategy.",{},https://www.aclweb.org/anthology/2020.aacl-main.69.pdf
227,231740858,Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches,conclusion,Insight-tree,"although large natural datasets are crucial ingredients for training accurate, deployable nlp systems, we find that naturalness and size are not necessary qualities of benchmarks that recapitulate progress on squad. benchmarks with varying naturalness and size can offer challenges with relevance to natural language.",{},https://arxiv.org/pdf/2102.01065v1.pdf
228,252819457,Type-dependent prompt CycleQAG : Cycle consistency for Multi-hop Question Generation,conclusion,Insight-tree,"in this work, we propose type-dependent prompt cycleqag with cycle consistency. since multihop qg needs to know more diverse information because it needs to gather more scattered pieces of information for generating a question, we introduce the nce for the first time in the qg task. also, we demonstrate that the intermediate task is effective in the qg task. furthermore, we show a significant performance improvement by using prompt-style fine-tuning to make the most of the information obtained from the intermediate task. the experiments show that the proposed model outperforms in all automatic evaluations comparing with the existing text-based multi-hop model and several qg models.","{215785913: 'Chen et al., 2020', 202565869: 'Nema et al., 2019', 225066758: 'Pan et al., 2021', 216553210: 'Pan et al., 2020;', 233219849: 'Talmor et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.549.pdf
229,226226674,CHIME: Cross-passage Hierarchical Memory Network for Generative Review Question Answering,conclusions,Insight-tree,"in this paper, we have proposed chime, a cross-passage hierarchical memory network for multi-passage generative review qa. it is built on the xlnet generator  by adding a memory module consisting of a context and a answer memory which guarantees a more accurate refining process for crosspassage evidence collection and answer generation. the sequential process adopted in chime makes it possible to elaborate longer text passages and some straightforward interpretability. we have assessed experimentally a significant quality improvement using different state-of-the-art metrics to measure the lexical and semantic coherence of the generated text. we plan to further extend chime to model with multiple ground truth simultaneously and leverage the available product attributes.","{57721315: 'Nishida et al., 2019'}",https://www.aclweb.org/anthology/2020.coling-main.229.pdf
230,252692968,RECITATION-AUGMENTED LANGUAGE MODELS,conclusion & discussion,Insight-tree,"in this paper, we propose a novel recitation-augmented generation framework to improve language models' performance in the closed-book question-answering setting. we hypothesize that for knowledge-intensive nlp tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. in addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. we show promising results over three large language models and across three different closed-book qa datasets, demonstrating the effectiveness of our proposed recite-and-answer approach.","{86611921: 'Kwiatkowski et al., 2019', 209515274: 'Talmor et al., 2020', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2210.01296v2.pdf
231,261640869,Retrieving Chinese Questions and Answers Based on Deep-Learning Algorithm,conclusions and future work 6.1. conclusions,Insight-tree,"in light of the current state of open-domain reading comprehension question-answering methods, this study proposed the qarcg method based on the dual-process theory of cognitive science.the qarcg approach views open-domain question answering as a combination of retrieval and reasoning systems.system 1, responsible for retrieval, extracts triples from given supporting text and iteratively retrieves information from wikipedia, constructing a cognitive graph with reasoning paths.system 2, responsible for reasoning, learns the interaction information between paragraphs using rnn based on the built cognitive graph.it reorders and scores different reasoning paths, and predicts the answer's span based on the highest-scoring reasoning path's paragraphs.the integration of retrieval and reasoning reduces the loss in graph construction and maintains graph structure, thereby enhancing interpretability and overcoming the lack of reasoning interpretability in traditional end-to-end reading comprehension methods.additionally, it addresses the requirement for existing large-scale knowledge graphs in knowledge graph question answering.","{52822214: '5', 153312687: '[7]'}",NaN
232,234095341,VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension,conclusions,Insight-tree,"in this work we introduce and examine a powerful yet simple model for reading comprehension on long texts which we call vault, based on the hypothesis that with a large sequence length long answers can be classified effectively without computationally heavy graph-based models. we validate our approach by showing it yields f1 scores competitive with heavier methods at a fraction of the decoding cost on two very different domain benchmark datasets that require reading long texts.  compare the correct answers produced by vault with the incorrect answers produced by the ablated model from the last row of table 3 (nq) and roberta baseline from the first row of table 2 (techqa).","{218595722: 'Zheng et al., 2020'}",https://www.aclanthology.org/2021.acl-short.131.pdf
233,258841029,Improving Language Models via Plug-and-Play Retrieval Feedback,conclusion,Insight-tree,"in conclusion, this paper presents a novel pipeline, refeed, designed to improve large language models' performance in a plug-and-play framework, effectively addressing the challenges arising from knowledge-intensive tasks. by employing a retrieval method to provide automatic feedback on generated outputs and integrating this feedback to refine the outputs without the need for expensive fine-tuning, refeed offers a practical and efficient solution. we introduce two innovative modules within the refeed pipeline: diverse answer generation and an ensemble approach. these two modules further enhance refeed to produce more reliable and accurate answers by considering a wider array of retrieved documents and mitigating the risk of misleading retrieval feedback. our extensive experiments on four challenging knowledgeintensive benchmarks demonstrate the effectiveness of refeed in achieving state-of-the-art performance under the few-shot setting. we believe by continuing to refine and optimize the refeed pipeline, we can unlock its full potential and ex-pand its applicability across a diverse range of scenarios and applications.","{221507798: 'Petroni et al., 2021', 252692968: 'Sun et al., 2023', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.14002v1.pdf
234,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,conclusion,Insight-tree,"supervised dense compression algorithms have been popular and effective in-domain in recent times, however can have difficulties to generalize well to unseen domains. the algorithms are memory efficient, but lack in performance when evaluated in specialized domains which contain no training data. in order to adapt these compression algorithms under severe domain shifts, in this paper we propose a solution to jointly optimize domainadaption algorithms along with vector compression. the recent technique, gpl in combination with bpr and jpq provide a boost of 19.3 and 11.6 ndcg@10 points respectively.","{238857091: 'Xin et al. 2022', 220302524: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2205.11498v1.pdf
235,248987232,Domain Adaptation for Memory-Efficient Dense Retrieval,limitations and future work,Insight-tree,"even though we find the gpl technique to provide a boost with memory compressed models: bpr and jpq. our work has a few limitations which we briefly mention them below and for future work: different compression algorithms: in our work, we considered jpq and bpr due to its popularity and effectiveness shown in our preliminary results. in future, we can work on extending our methods to more recent memory compression algorithms such as repconc (zhan et al., 2022).better backbone models: we suspect the performances on the beir can further improved with stronger backbone models in comparison with tas-b. due to the model agnostic nature of our method, we can easily extend our work to different state-ofthe-art dense retrievers in the upcoming future.requires separate models: bpr and jpq both require training separate models for each domain or task with our technique. this can be quite cumbersome for practical use-cases involving several hundreds of domains or retrieval tasks, for which one would need to train multiple models.compute intensive: our method gpl is compute intensive: (1) for bpr+gpl, every dense retriever requires to separately compute embeddings for the whole corpus for hard-negative mining.(2) cross-encoder teacher model although very effective, slows down the training significantly as required to label during training. in future, we can explore efficient and faster teachers instead of cross-encoders for gpl such as colbert (khattab and zaharia, 2020) or tilde (zhuang and zuccon, 2021)","{238857091: 'Xin et al. 2022', 220302524: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2205.11498v1.pdf
236,252819276,Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension,conclusion,Insight-tree,"this paper presents a novel method to guide the mrc model to better perform logical reasoning tasks. we propose a holistic graph-based system to model hierarchical logical reasoning chains. to our best knowledge, we are the first to deal with context at both discourse level and phrase level as the basis for logical reasoning. to decouple the interaction between the node features and type features, we apply hierarchical interaction mechanism to yield the appropriate representation for reading comprehension. on the logical qa benchmarks (reclor, logiqa) and natural language inference benchmarks (snli and anli), our proposed model has been shown effective by significantly outperforming the strong baselines.","{153312687: 'Ding et al., 2019;', 207853300: 'Fang et al. 2020', 220483148: 'Liu et al., 2020a', 207756753: 'Nie et al., 2020', 155100120: 'Qiu et al., 2019;', 231709861: 'Sugawara et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.126.pdf
237,218486753,R 4 C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,conclusions,Insight-tree,"towards evaluating rc systems' internal reasoning, we have proposed r 4 c that requires systems not only to output answers but also to give their derivations. for scalability, we have carefully developed a crowdsourced framework for annotating existing rc datasets with derivations. our experiments have demonstrated that our framework produces high-quality derivations, and that automatic evaluation metrics using multiple reference derivations can reliably capture oracle derivations. the experiments using two simple baseline models highlight the nature of r 4 c, namely that the derivation generation task is not simply the sf detection task. we make the dataset, automatic evaluation script, and baseline systems publicly available at https://naoya-i.github.io/r4c/.","{139103297: 'Chen and Durrett, 2019;', 196170479: 'Fan et al., 2019;', 186206745: 'Jiang et al., 2019', 174801764: 'Min et al., 2019;', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.602.pdf
238,220445474,CopyBERT: A Unified Approach to Question Generation with Self-Attention,conclusion,Insight-tree,"we showed that having a unified encoder-decoder transformer model initialized with contextualized word embeddings and further extended with copy mechanism can already give state-of-the-art, without additional pre-training on generation tasks (dong et al., 2019). we also sped up the training of qg models that use bert by choosing predictions on output embeddings that are offset by one position ( Â§3.3). this work shows the significance of explicitly using self-attentions of bert like models. these models can further be used in other tasks such as abstractive summarization and machine translation to see qualitative improvements.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.nlp4convai-1.3.pdf
239,89838681,Detection of non specific toll-like receptor 3 in the marine and freshwater fishes,conclusion,Insight-tree,the results of phylogenetic analysis showed that the tlr3 gene of the catfish was closely related to i. punctatus and c. batracus as well as some freshwater fish and seawater.,{},NaN
240,222080412,KPQA: A Metric for Generative Question Answering Using Keyphrase Weights,conclusion,Insight-tree,"in this paper, we create high-quality human judgments on two genqa datasets, ms-marco and avsd, and show that previous evaluation metrics are poorly correlated with human judgments in terms of the correctness of an answer. we propose kpqa-metric, which uses the pre-trained model that can predict the importance weights of words in answers to a given question to be integrated with existing metrics. our approach has a dramatically higher correlation with human judgments than existing metrics, showing that our model-based importance weighting is critical to measure the correctness of a generated answer in genqa.","{196170479: 'Fan et al., 2019', 198229624: 'Joshi et al., 2020', 57721315: 'Nishida et al., 2019;', 52822214: 'Yang et al., 2018b'}",https://www.aclweb.org/anthology/2021.naacl-main.170.pdf
241,263671602,EVALUATING HALLUCINATIONS IN CHINESE LARGE LANGUAGE MODELS,conclusion,Insight-tree,"in this work, we create a chinese hallucination question-answering dataset named halluqa to evaluate hallucinations in chinese large language models.questions in halluqa can be used to measure imitative falsehoods and factual errors.we design a llm-based automated evaluation method and verify its effectiveness.we conduct extensive experiments on 24 large language models.all models achieve less than a 70% non-hallucination rate on halluqa, which proves the challenging nature of our dataset.according to the experimental results, we further analyze the primary hallucinations types of different models and discuss the types that different models need to prioritize and address.we hope that halluqa can help reduce hallucinations problems in chinese large language models and enhance the credibility of the models.the prompt used for evaluation is shown in figure 10.we utilize the conversational format of gpt-4.in the first turn, we include the evaluation criteria for hallucinations and evaluation guidance, and concatenate a response from the assistant indicating affirmation.in the second turn, we provide the question to be evaluated, the response, examples of correct answers, and an instruction to output ""yes"" or ""no"" as the evaluation reulst.during the evaluation, we need to replace the red placeholders with the question, examples of the correct answer, and the model's response to this question.","{52822214: 'Yang et al., 2018', 258959258: 'Yin et al. 2023'}",https://export.arxiv.org/pdf/2310.03368v4.pdf
242,264555175,Detrimental Contexts in Open-Domain Question Answering,conclusions,Insight-tree,"the reader models in retrieve-then-read pipelines are sensitive to the retrieved contexts when generating answers.damaging passages in this set can lead to incorrect responses.filtering damaging passages results in increases in em scores without the need for architectural modifications.despite  shortcomings in evaluating qa with exact match, we demonstrate that by filtering passages, models can achieve 10% higher em scores using subsets of context that are 20x times smaller.","{226278099: 'Jiang et al., 2020', 258615193: 'Kamalloo et al. 2023', 86611921: 'Kwiatkowski et al., 2019;', 221507798: 'Petroni et al., 2021;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.18077v1.pdf
243,264555175,Detrimental Contexts in Open-Domain Question Answering,limitations,Insight-tree,"identifying the behavior of black-box models is challenging.while we identify different subsets of evidence that the model reacts well to when generating a correct answer, there is no guarantee that these subsets of information correspond to what humans users would consider useful.furthermore, some of the reasons the model changed its prediction, such as generating a more specific answer, would be correct if multiple references were available for evaluating the models.however, these alternative answers are not available in the datasets, which means we are optimizing the models for a limited subset of truly valid answers.lastly, our approach may not be practical for decoder-only llms where the order of context/answer choices varies the outcome.to assess the answerability of the given n candidate contexts, o(n!) inferences are required for llms, while fid only needs one inference due to its order-invariance property.we report a limitation, evaluation and position of established modelling techniques that can help guide the community for future research.if models can effectively leverage external information, they should be capable of using text as an interpretable source of information rather than relying solely on knowledge that is stored within inaccessible model parameters.this approach may contribute to a future with nlp models that are more interpretable and controllable.","{226278099: 'Jiang et al., 2020', 258615193: 'Kamalloo et al. 2023', 86611921: 'Kwiatkowski et al., 2019;', 221507798: 'Petroni et al., 2021;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.18077v1.pdf
244,258865410,Decomposing Complex Queries for Tip-of-the-tongue Retrieval,conclusion,Insight-tree,"we study a real-world information-seeking settingtip of the tongue retrieval-in which users issue long, complex queries for re-finding items despite being unable to articulate identifying details about those items. we introduce a simple but effective approach to handling these complex queries that decomposes them into sub-queries or clues that are routed to expert retrievers for specialized scoring. our simple framework allows for modular composition of different retrievers and leveraging of pretrained models for specific modalities such as clip for document images. we observe improvements of up to 7% relative gain for recall@5 when incorporating query decomposition into existing retrievers on our newly-introduced whatsthatbook, a large challenging dataset of real-world, tip-of-thetongue queries for books.","{249097975: 'Izacard et al., 2022', 86611921: 'Kwiatkowski et al., 2019;', 52822214: 'Yang et al., 2018b;', 254044526: 'Zhao et al. 2022'}",https://export.arxiv.org/pdf/2305.15053v1.pdf
245,204800552,Domain-agnostic Question-Answering with Adversarial Training,conclusion,Insight-tree,"we leverage adversarial learning to learn domaininvariant features. in our experiments, the proposed method consistently improves the performance of baseline and it is applicable to any qa model. in future work, we will try adversarial learning for pre-training model with diverse set of domains.","{86611921: 'Kwiatkowski et al., 2019'}",https://www.aclweb.org/anthology/D19-5826.pdf
246,235097509,Breadth First Reasoning Graph for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, we proposed a novel gnn model of bfr-graph. specifically, the reasoning message starts from the question node and passes to the next sentences node hop by hop until all the edges have been passed. we also construct the reasoning graph as a weighted graph and present a more interpretable way to aggregate scores of different levels from gnn. on hotpotqa leaderboard, bfr-graph achieved state-of-the-art on answer span prediction.","{208267807: 'Asai et al. 2020', 153312687: 'Ding et al., 2019;', 207853300: 'Fang et al. 2020', 215768725: 'Groeneveld et al. 2020', 174801080: 'Min et al. 2019', 160009340: 'Nishida et al. 2019', 155100120: 'Qiu et al. 2019', 207870753: 'Tu et al., 2020', 158046817: 'Tu et al., 2019', 52822214: 'Yang et al. 2018'}",https://www.aclweb.org/anthology/2021.naacl-main.464.pdf
247,228375214,Multilingual Transfer Learning for QA Using Translation as Data Augmentation,conclusion,Insight-tree,"in this work, we highlight open challenges in the existing multilingual approach by (lewis et al. 2020) and (clark et al. 2020). specifically, we show that large pretrained multi-lingual lms are not enough for this task. we produce several novel strategies for multilingual qa that go beyond zero-shot training and outshine the previous baseline built on top of mbert. we present a translation model that has 14 times more training data. further, our at and laf strategies utilize translation as data augmentation to bring the language-specific embeddings of the lm closer to each other. these approaches help us significantly improve the cross-lingual transfer. empirically, our models demonstrate strong results and all approaches improve over the previous zs strategy. we hope these techniques spur further research in the field such as exploring other multilingual lms and invoking additional networks on top of large lms for multilingual nlp.",{212657414: 'Clark et al. 2020'},https://arxiv.org/pdf/2012.05958v1.pdf
248,261823125,CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration,conclusion,Insight-tree,"in our paper, we present a novel approach for automatic data augmentation by llm generated counterfactual instances diverse in surface form and semantic content. our results show that augmenting training data of smaller models with llm generated cfs consistently improves generalization capabilities of the underlying models across six ood extractive qa datasets. we further show that models trained on cf augmented data are easier to calibrate, both when considering the standard confidence-based setup as well as the explanationaugmented calibration setup. finally, show that rationale-augmented calibrator models prefer concise explanations, rather than comprehensive ones. by highlighting the fact that more diverse cf instances improve the quality of the models' internal representations by covering a broader part of the input space we pave the way for future works exploring the relation between surface form and semantic diversity of data used for augmentation and the models' generalization performance.","{238856938: 'Paranjape et al. 2022', 253098276: 'Si et al., 2022', 52822214: 'Yang et al., 2018', 238856959: 'Ye and Durrett 2022', 237498988: 'Ye et al. 2021', 235313893: 'Zhang et al., 2021'}",https://export.arxiv.org/pdf/2309.07822v2.pdf
249,252383151,Activity report analysis with automatic single or multi span answer extraction,conclusion,Insight-tree,"we present a simple multispan architecture, mseqa, for multi type question answering by classifying each sentence as a probable answer candidate. we show that when a combination of single/multi-span classifier with multispan tagging is used, the model provides robust answers for multi-span tasks without degrading its performance on single-span questions. as future work, we would like to further process selected sentences from the multi-span tagger and consolidate them into one fluent answer. we also plan to explore ways to put this question answering capability onto edge devices for various applications.  ","{67855846: '7]', 52822214: '[9]'}",https://export.arxiv.org/pdf/2209.09316v1.pdf
250,233240821,Are Multilingual BERT models robust? A Case Study on Adversarial Attacks for Multilingual Question Answering,conclusion,Insight-tree,"we have shown several novel adversaries that successfully attack mbert for mlqa. specifically, we show that the language of the adversarial statement impacts the attack with priority given to english and the language of the question regardless of the other languages in the qa pair. we also show that including such attack strategies while training our defense brings back performance without the need for complex neural network engineering. not only do the strategies improve results for their corresponding attack, they help for all our attacks indicating model robustness. in the future, we plan to expose vulnerabilities on other multilingual lms and datasets and explore more sophisticated defense strategies.","{52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2104.07646v1.pdf
251,252873216,Benchmarking Long-tail Generalization with Likelihood Splits,conclusion,Insight-tree,"with the saturation of static, single-metric leaderboards, there is growing consensus for the development of holistic evaluation benchmarks. this includes evaluation of systems on aspects of performance beyond just single error rate on indistribution data; aspects such as performance on out-of-distribution data (linzen, 2020), and evaluating generalizability, robustness and fairness (ethayarajh and jurafsky, 2020). in this work, we describe an approach to benchmark long-tail generalization, a necessary skill for nlp systems that truly understand language. we demonstrate the challenge posed by our splits to state-of-the-art models on several tasks; standard evaluation overestimates model performance on long-tail utterances. instead of releasing a random split as the only metric on official benchmarks, our simple method can be used, for a wide range of tasks, to expose additional challenges in the collected data at no annotation cost. benchmarking long-tail generalization, in this manner, can test model behavior on a broad set of generalization challenges, which may be missed by evaluations that test specific skills in isolation.",{},https://www.aclanthology.org/2023.findings-eacl.71.pdf
252,234334015,ExpMRC: explainability evaluation for machine reading comprehension,conclusion,Insight-tree,"in this paper, we propose a comprehensive benchmark for evaluating the explainability of mrc systems. the proposed expmrc benchmark contains four datasets, including squad, cmrc 2018, race + , c 3 , covering span-extraction mrc and multiple-choice mrc in both en- glish and chinese. expmrc aims to evaluate the mrc system to give not only correct predictions on the final answer but also extract correct evidence for the answer. we set up several baseline systems to thoroughly evaluate the difficulties of expmrc. the experimental results show that both traditional and state-of-the-art pre-trained language models still underperform human performance by a large margin on most of the subsets, indicating that more efforts should be made on designing an effective approach for evidence extraction. we hope the release of the dataset will further accelerate the research on the explainability and interpretability of mrc systems, especially for the unsupervised approaches.","{214233456: '[6]', 52822214: '[23]', 155100120: '[24,', 247763195: '[27]'}",https://arxiv.org/pdf/2105.04126v1.pdf
253,258865346,On Degrees of Freedom in Defining and Testing Natural Language Understanding,conclusions,Insight-tree,"the prevalence of exaggerated claims about the achievements of foundation models motivates us to reconsider how we define and evaluate nlu. our formulation of nlu using the response-dependent interpretation mitigates the issues of the turing and octopus tests; it stipulates that observers and target conditions, including tasks and abilities, must be specified. however, current practices for creating nlu datasets are yet to be aligned, which provides researchers with the freedom to choose convenient strategies. to organize essential practices using a standard guideline, we introduce the validity ar-gument, which guides stakeholders to collect and interpret evidence for validating that the test subject executes its intended behavior. our proposed checklist helps researchers find relevant practices for benchmarking nlu, but we continually revise it by investigating potential refutation to promote more credible nlu studies.","{211003735: 'Wolfson et al., 2020'}",https://export.arxiv.org/pdf/2305.15130v1.pdf
254,240288835,MetaICL: Learning to Learn In Context,conclusion,Insight-tree,"in this paper, we introduced metaicl, a new fewshot learning method where an lm is meta-trained to learn to in-context learn, i.e. condition on training examples to recover the task and make predictions. we experiment with a large, diverse collection of tasks, consisting of 142 unique tasks in total and 52 unique target tasks, using seven different settings. metaicl outperforms a range of strong baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer, and outperforms or matches 8x bigger models. we identify ingredients for success of metaicl such as the number and diversity of meta-training tasks. we also demonstrate that, while metaicl is better than recent work using natural instructions, they are complementary and the best performance is achieved by integrating metaicl with instructions.","{233296709: 'Ye et al., 2021'}",https://www.aclanthology.org/2022.naacl-main.201.pdf
255,218521479,VisBERT: Hidden-State Visualizations for Transformers,conclusion,Insight-tree,"visbert establishes a novel method to analyze the behavior of bert models, in particular regarding the question answering task. our method allows a fine-grained analysis of each of the bert layers and depicts how each input token changes in each step. additionally, visbert reveals four phases in bert's transformations that are common to all of the datasets we examined and that mirror the traditional nlp pipeline, cf. [10]. we establish this behaviour on three diverse question answering datasets and make all three models available for users to make their own analyses on their own data, as well as the code to reproduce this visualization.","{202558795: '[11]', 52822214: '[16]'}",https://arxiv.org/pdf/2011.04507v1.pdf
256,254097925,Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering,conclusion,Insight-tree,"open-domain and multi-hop qa is an important problem for both humans and computers. to compare how humans and computers search and answer complex questions, our interface collects human question answering data as agents search with traditional and neural search engines alongside question answering models that suggest queries and answers. humans often use shorter queries, apply dynamic search chains, and use world knowledge. future qa models should have the ability to generate novel queries, ""discard"" irrelevant results, and explicitly check answers. moreover, computer agents for qa should also be able to use diverse retrievers to find evidence to answer questions, learning from the insights found in human data. with an agent trained on our data, we could have the ""best of both worlds"" to combine the ingenuity and tacit knowledge of humans with an indefatigable agent with access to all the world's information.","{208267807: 'Asai et al., 2020;', 240288953: 'Qi et al., 2021', 202773198: 'Qi et al., 2019', 52822214: 'Yang et al., 2018', 237490850: 'Zhang et al., 2021'}",https://export.arxiv.org/pdf/2212.03296v1.pdf
257,250562707,Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge,conclusion,Insight-tree,"we have proposed a new approach to multi-hop question answering over tabular and textual data. the approach, referred to as dehg, takes question answering as a problem of reasoning answers on the basis of a heterogeneous information graph. dehg employs bert in encoding of questions and passages respectively and generates pointers in decoding of answer generation. experimental results show that dehg significantly outperforms the state-of-the-art methods.","{215785913: 'Chen et al., 2020b;', 86611921: 'Kwiatkowski et al., 2019;', 225066758: 'Pan et al., 2021;', 128345225: 'Sun et al., 2019;', 52822214: 'Yang et al., 2018;'}",https://aclanthology.org/2022.findings-naacl.12.pdf
258,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,conclusion,Insight-tree,"we introduce answer-based claim decomposition, which aims to decompose a question into a series of true/false claims. through experiments on three datasets with gpt-3.5, including a new challenge dataset obscureqa, we show how our technique can be used to perform fine-grained self-evaluation. we find that there is a significant difference in the proportion of claims satisfied for incorrect and correct responses, but there is no indication that gpt-3.5 believes that the gold answer satisfies more abcd claims than its incorrect answers. finally, to investigate the reliability of our approach, we conduct an error analysis and based on our findings, suggest remedies to overcome these errors.","{252917981: 'Cheng et al., 2023', 165163607: 'Clark et al., 2019', 211258645: 'Perez et al., 2020;', 237491981: 'Si et al., 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.14750v1.pdf
259,258865288,Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation,limitations,Insight-tree,"in our preliminary experiments, we apply answerbased claim decomposition to factual trivia questions where answers are entities spanning a few words. however, we did not examine if our technique would be effective on other types of qa datasets, such as truthfulqa (lin et al., 2022b), which exploits imitative falsehoods and contains longer desired responses, or boolqa (clark et al., 2019), which has ""yes"" or ""no"" as the only possible answers.further, due to financial constrains, we test abcd and fine-grained self-evaluation through preliminary experiments on a subset of data from our three datasets. however, given that our results were statistically significant ( Â§5.1), we believe that the number of questions selected were sufficient for our study. in addition, since we only examined a subset of questions from our newly-collected dataset obscureqa, this opens up future research directions leveraging our dataset. we believe that obscureqa could be used to evaluate llms on a variety of facets, including benchmarking the academic knowledge of state-of-the-art llms, and given that this dataset frequently elicits untruthful responses, studying confidence and uncertainty calibration techniques.","{252917981: 'Cheng et al., 2023', 165163607: 'Clark et al., 2019', 211258645: 'Perez et al., 2020;', 237491981: 'Si et al., 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.14750v1.pdf
260,258558037,Empowering Language Model with Guided Knowledge Fusion for Biomedical Document Re-ranking,conclusion,Insight-tree,"in this work, we proposed an effective approach to re-rank the documents by utilizing the knowledge graph and integrating the external knowledge into the plms. to effectively fuse the language and graph information in the knowledge-enriched framework, we introduced a mutual informationbased objective function, which ensures the fused representations are non-redundant and informative in nature. extensive experiments on biomedical and open-domain datasets show the effectiveness of the proposed approach. ","{233296016: 'Thakur et al. 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.04344v1.pdf
261,240288953,Answering Open-Domain Questions of Varying Reasoning Steps from Text,conclusion,Insight-tree,"in this paper, we presented iterative retriever, reader, and reranker (irrr), a system that uses a single model to perform subtasks to answer open-domain questions of arbitrary reasoning steps. irrr achieves competitive results on standard opendomain qa benchmarks, and establishes a strong baseline on b qa, the new unified benchmark we present, which features questions with mixed levels of complexity. ","{221970302: 'Xiong et al., 2021', 52822214: 'Yang et al. 2018', 211562797: 'Zhao et al., 2020a'}",https://www.aclanthology.org/2021.emnlp-main.292.pdf
262,247447492,What Makes Reading Comprehension Questions Difficult?,conclusion,Insight-tree,"to make an nlu benchmark useful, it has to consist of examples that are linguistically diverse and difficult enough to discriminate among state-ofthe-art models. we crowdsource multiple-choice reading comprehension questions for passages extracted from seven different sources and analyze the effects of passage source on question difficulty and diversity. although we expect that the difficulty of a passage affects the difficulty of questions about that passage, the collected questions do not show any strong correlation between the human-machine performance gap and passage source, length, or readability measures. our manual annotation of comprehension types reveals that questions requiring numerical or logical reasoning are relatively difficult. we also find several trends between passage sources and comprehension types.",{},https://www.aclanthology.org/2022.acl-long.479.pdf
263,248427125,Generative Retrieval for Long Sequences,conclusion,Insight-tree,"we show that generative retrieval, which has been originally proposed for retrieving short sequences such as entities, can also be considered for retrieving longer sequences. we particularly find that generative retrieval can have an advantage over bi-encoder in certain situations, such as retrieving structured information (e.g., reasoning chains or graphs) and retrieving an arbitrary number of items. given that generative retrieval inherently has gpu memory and speed benefits, it can be a practical alternative for general retrieval tasks in the future. ",,https://arxiv.org/pdf/2204.13596v1.pdf
264,201058633,Reasoning Over Paragraph Effects in Situations,conclusion,Insight-tree,"we present ropes, a new reading comprehension benchmark containing 14,322 questions, which aims to test the ability of systems to apply knowledge from reading text in a new setting. we hope that ropes will aide efforts in tying language and reasoning together for more comprehensive understanding of text.","{67855846: 'Dua et al., 2019', 86611921: 'Kwiatkowski et al., 2019;', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/D19-5808.pdf
265,252624423,eRock at Qur'an QA 2022: Contemporary Deep Neural Networks for Qur'an based Reading Comprehension Question Answers,conclusion and future work,Insight-tree,"we attempted to solve quranqa shared task using bert (devlin et al., 2018) from scratch as well as fine-tuned over two different pre-trained variants. moreover we opted for data augmentation and weight-decay regularization techniques to improve performance over the task.","{212657414: 'Clark et al., 2020'}",https://www.aclanthology.org/2022.osact-1.11.pdf
266,253398396,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,conclusion,Insight-tree,"in this paper, we proposed robustlr, a diagnostic benchmark to test the logical robustness of deductive reasoning models. in robustlr, we propose two evaluation sets, logical contrast and logical equivalence, each probing different logical reasoning abilities. overall, we find that fine-tuning lms such as roberta and t5 on deductive reasoning datasets is not sufficient to learn the semantics of the logical operators conjunction, disjunction, and negation. although well-aligned training dataset improves model performance, the models still find it challenging to understand negations, both in logical contrast and logical equivalence sets. we demonstrate some interesting shortcoming of lms designed for logical reasoning, that can eventually enable building better reasoning models.","{139103297: 'Chen and Durrett, 2019;', 211126663: 'Clark et al., 2020', 220483148: 'Liu et al., 2021', 52822214: 'Yang et al., 2018b'}",https://www.aclanthology.org/2022.emnlp-main.653.pdf
267,253398396,ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners,limitation,Insight-tree,"a key limitation of the work is the synthetic nature of the dataset. while it is ideal to explore more natural theories, it makes the systematic logical perturbation process very challenging. thus, in this work, we resort to using synthetic datasets, but aim to bridge this gap in future works. another limitation is the complexity of the datasets we explore. we use fairly simple logical rules and constructs for robustlr. some more complex forms of logical reasoning-based theories can potentially reveal even more limitations of deductive reasoning models. another interesting aspect we do not explore in this scope is potential techniques to improve these models on deductive reasoning tasks. this might involve trying different inductive biases in the form of architectural designs, more specialized datasets, etc.","{139103297: 'Chen and Durrett, 2019;', 211126663: 'Clark et al., 2020', 220483148: 'Liu et al., 2021', 52822214: 'Yang et al., 2018b'}",https://www.aclanthology.org/2022.emnlp-main.653.pdf
268,253098851,Iteratively Prompt Pre-trained Language Models for Chain of Thought,conclusion & future work,Insight-tree,"we explore an iterative prompting framework towards driving a ""chain of thought"" from plms for multi-step reasoning tasks. we show the superiority of this iterative scheme, and also the effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme. in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors. in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities. the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely, a human can track along the plm's chain of thought and make edits and corrections at different steps, similarly as in (mo et al., 2022a), which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process. we leave these investigations as future work.","{230799347: 'Geva et al., 2021', 226236740: 'Ho et al., 2020', 218486753: 'Inoue et al., 2020', 230437663: 'Khattab et al., 2021;', 250390946: 'Mo et al., 2022b', 202773198: 'Qi et al., 2019;', 219573621: 'Talmor et al., 2020b', 221970302: 'Xiong et al., 2021;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.174.pdf
269,256630981,LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models,conclusions,Insight-tree,"in this paper, we construct a synthetic nonmonotonic reasoning benchmark, logicnmr, with explicit facts and rules, to capture the iterative update on the knowledge base.we probe whether the pre-trained language models have truly mastered the non-monotonic reasoning ability.the experimental results show that even though the fine-tuned language models all achieve a high accuracy, they perform worse on generalization, proof-based traceability and robustness to irrelevant information.consequently, we cannot give a positive answer to the research problem whether the language models master the non-monotonic reasoning ability.it suggests us to explore a better approach to take advantage of the language models to conduct non-monotonic reasoning tasks.","{211126663: 'Clark et al. 2020', 233297051: 'Dalvi et al. 2021', 189927896: 'Jiang and Bansal, 2019', 247518855: 'Li et al. 2022', 220483148: 'Liu et al., 2020', 243865235: 'Tian et al., 2021', 248496003: 'Xu et al. 2022', 52822214: 'Yang et al., 2018;'}",https://aclanthology.org/2022.findings-emnlp.265.pdf
270,256630981,LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models,limitations,Insight-tree,"although we construct a dataset to probe the non-monotonic reasoning ability of language models and conduct some experiments, we have to admit that there are still some limitations.first, only three language models are used in this paper.more language models with different architectures should be evaluated.second, the synthetic rules of logicnmr are too strong.we will relax some restrictions of generating rules, such as query extraction way.third, we limit the default theory to only one extension to reduce reasoning complexity, resulting in simpler non-monotonic inference patterns.a future work is to probe non-monotonic reasoning ability in a more general and systematic way, such as by allowing plural extensions.","{211126663: 'Clark et al. 2020', 233297051: 'Dalvi et al. 2021', 189927896: 'Jiang and Bansal, 2019', 247518855: 'Li et al. 2022', 220483148: 'Liu et al., 2020', 243865235: 'Tian et al., 2021', 248496003: 'Xu et al. 2022', 52822214: 'Yang et al., 2018;'}",https://aclanthology.org/2022.findings-emnlp.265.pdf
271,174799117,Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution,conclusion,Insight-tree,"we presented a neural approach for resolving cross-document event and entity coreference. we represent a mention using its text, context, andinspired by the joint model of lee et al. (2012)we make an event mention representation aware of coreference clusters of entity mentions to which it is related via predicate-argument structures, and vice versa. our model achieves state-of-the-art results, outperforming previous models by 10.5 conll f 1 points on events, and providing the first cross-document entity coreference results on ecb+. future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions.","{52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/P19-1409.pdf
272,249049410,Generative Multi-hop Retrieval,conclusion,Insight-tree,"in this paper, we show that the bi-encoder approach has limitations in multi-hop retrieval; the bottleneck problem becomes a more severe problem as the number of hops increases, and is more susceptible to error propagation. we present generative multi-hop retrieval (gmr), an encoder-decoder model that performs retrieval by generating the entire target sequences with the aid of constrained decoding. we show that gmr is more robust on multi-hop retrieval tasks where it achieves higher or comparable performance in five datasets. we also introduce two corpus memorization methods, lm memorization and multi-hop memorization, to further improve gmr's performance. our experimental results demonstrate that in multi-hop retrieval, a generative approach is highly competitive with bi-encoder methods and deserves further explorations in the community.",{},https://www.aclanthology.org/2022.emnlp-main.92.pdf
273,254563995,Transcoding unicode characters with AVX-512 instructions,conclusion,Insight-tree,"it is not a priori obvious that character transcoding is amenable to simd processing.earlier work achieved high speeds but it required kilobytes of lookup tables. 2 our work indicates that the avx-512 instruction-set extensions enables high speed for tasks such as character transcoding-without lookup tables and using few instructions.it suggests that some features of the avx-512 instruction-set extensions might serve as a reference for future instruction-set extensions.in particular, we find masked simd instructions (move, load, store, compress) with byte-level granularity useful.both intel and amd support avx-512 instructions.they also both offer specialized compilers, tuned for their processors.future work could compare the performance of our routines on more varied intel and amd processors (e.g., intel rocket lake and sapphire rapids, amd zen 4), using specialized compilers (e.g., from intel and amd) and hand-tuned assembly.we could extend our benchmarks to cover a wider range of string.",{},https://export.arxiv.org/pdf/2212.05098v3.pdf
274,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,limitations of realfp.,Insight-tree,"our realfp dataset includes only one explanation program to a given fp whereas in practice, there can be multiple creative decompositions that lead to the correct answer. to encourage models that are capable of capturing this diversity in the output space, it would be interesting to (a) collect alternative solutions similar to say, image captioning datasets where it is the norm to train and evaluate against multiple ground truth candidates and (b) increasing the number of templates in the synthfp dataset, thereby biasing the model towards exploring multiple solutions by pre-training on a richer synthetic dataset. further, the work doesn't include other variants of fps -e.g. binary yes/no questions, comparisons, or fps involving probability and risk quantification. finally, note that our real-world dataset, by virtue of how it is collected, has a high us-centric bias, both in terms of cultural context and vocabulary.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.582.pdf
275,239998631,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,conclusion,Insight-tree,"in this work, we propose fermi problems (fps) as a reasoning challenge for ai systems. apart from introducing abstraction as a crucial reasoning skill, our work requires the combined application of various reasoning skills including creative decomposition of problems, commonsense reasoning, mathematical reasoning, etc. we collect two datasets -realfp with â¼1k real-world questions and syn-thfp with 10k templated questions. based on these datasets, we propose three concrete tasks of increasing difficulty that encompass the fp challenge. the baseline models we provide, despite being based on state-of-the-art language models and even with substantial fine-tuning, struggle on our challenge tasks. they are, on average, off by two orders of magnitude from the correct estimate and perform only slightly better than predicting a constant number. we thus hope to establish fermi problems as a hard reasoning challenge that motivates further advances in ai reasoning systems.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.582.pdf
276,201646361,Movie Plot Analysis via Turning Point Identification,conclusions,Insight-tree,"we proposed the task of turning point identification in screenplays as a means of analyzing their narrative structure. we demonstrated that automatically identifying a sequence of key events and segmenting the screenplay into thematic units is feasible via an end-to-end neural network model. in future work, we will investigate the usefulness of tps for summarization and question answering. we will also scale the tripod dataset and move to a multi-modal setting where tps are identified directly in video data.",{},https://www.aclweb.org/anthology/D19-1180.pdf
277,260681466,TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage,conclusion,Insight-tree,"in this paper, we have introduced a structured framework specially designed for llm-based ai agents, with an emphasis on their abilities in task planning and tool usage.this framework, coupled with our design of two distinct types of agents assigned for the inference process, allows for a comprehensive evaluation of the capabilities of current open-source llms, thereby yielding critical insights into their effectiveness.furthermore, our research highlights the significant potential of llms in managing complex tasks, revealing the exciting prospects they hold for future research and development.as we continue to explore and improve upon these models, we move closer to unlocking their full potential in a wide range of real-world applications.",{},https://export.arxiv.org/pdf/2308.03427v2.pdf
278,257557511,Secret-Keeping in Question Answering,conclusion and future work,Insight-tree,"we have introduced the task of secret-keeping as an important, and under-explored problem in question answering. we identify a lack of suitable secretkeeping metrics and define secrecy, paranoia and information leakage to address the gap. we design and implement a secret-keeping approach that is model-agnostic, only requiring access to predefined secrets, and the output of a qa system to, detect the disclosure of secrets. we have identified a rich field for future work in secret-keeping including: â¢ reducing paranoia and information leakage.","{86611921: 'Kwiatkowski et al., 2019', 236447339: 'Rogers et al., 2023'}",https://export.arxiv.org/pdf/2303.09067v1.pdf
279,230799347,Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies,conclusion,Insight-tree,"we present strategyqa, the first dataset of implicit multi-step questions requiring a wide-range of reasoning skills. to build strategyqa, we introduced a novel annotation pipeline for eliciting creative questions that use simple language, but cover a challenging range of diverse strategies. questions in strategyqa are annotated with decomposition into reasoning steps and evidence paragraphs, to guide the ongoing research towards addressing implicit multi-hop reasoning.","{211010520: 'Bartolo et al., 2020', 165163607: 'Clark et al., 2019', 67855846: 'Dua et al., 2019;', 189927896: 'Jiang and Bansal, 2019', 204915921: 'Khot et al., 2020a;', 174801080: 'Min et al., 2019', 211258645: 'Perez et al., 2020;', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2101.02235v1.pdf
280,235421967,A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering,conclusion,Insight-tree,"to alleviate the spurious solution problem in weakly supervised qa, we propose to explicitly exploit the semantic correlations between a question and its solution via mutual information maximization. during training, we pair a task-specific model with a question reconstructor which guides the task-specific model to predict solutions that are consistent with the questions. experiments on four qa datasets demonstrate the effectiveness of our learning method. as shown by automatic and manual analyses, models trained with our method are more resistant to spurious solutions during training, and are more precise in locating information that is relevant to the questions during inference, leading to higher accuracy of both answers and solutions.","{67855846: 'Dua et al., 2019', 202558815: 'Min et al., 2019', 173188058: 'Talmor and Berant, 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.acl-long.318.pdf
281,248986946,Interpretable Proof Generation via Iterative Backward Reasoning,conclusion,Insight-tree,"this paper presents ibr, a proof generation model via iterative backward reasoning for rule-based qa tasks. we equip the reasoning procedure with detailed hidden state tracking by predicting nodes and edges in the proof path iteratively backward from the question, and allow the model to reason on the elaborate representations of nodes and his- tory paths. our model is more interpretable than previous at-once models, and is also more effective and efficient than former iterative models. experiments also demonstrate the superiority of ibr to various baselines on proof generation under various settings.  we implement our model based on pytorch along with huggingface-transformers toolkit 6 . we use roberta large model 7 as our backbone encoder to generate token-level representations. table 10 shows the implementation details of ibr, including learning rates for different modules. all linear layers used in our model have one layer. the model trained after 8 epochs will be used in the evaluation. we remove functional words without lexical meaning like ""a"" and ""the"" from facts, rules, and questions to shorten the input length, so each training epoch takes about 2 hours. we select these hyper-parameters according to tuning them empirically based on the performance. all experiments are run on nvidia tesla-v100 gpus. the main experiment performance of ibr fluctuates by one point.","{198229624: 'Joshi et al., 2020;', 222141025: 'Saha et al., 2020', 235742855: 'Sun et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.naacl-main.216.pdf
282,211258744,Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?,conclusion,Insight-tree,"we propose a new way to evaluate whether multi-hop qa systems have learned the ability to perform reasoning over multiple documents by asking sub-questions. an automatic approach is designed to generate sub-questions for a multihop question. on a human-verified test set, we show that all three existing top models give worse performance on the subquestions compared to our proposed model with an explicit question type classification component and a single-hop qa component. as an initial step towards a more explainable qa system, we hope our work could motivate the construction of multi-hop qa datasets with explicit reasoning paths annotated and the development of better multi-hop qa models.","{153312687: '[Ding et al., 2019]', 189927857: 'Feldman and El-Yaniv, 2019;', 189927896: 'Jiang and Bansal [2019]', 174801080: '[Min et al., 2019]', 160009340: 'Nishida et al., 2019;', 155100120: '[Qiu et al., 2019;', 52822214: '[Yang et al., 2018;'}",https://arxiv.org/pdf/2002.09919v1.pdf
283,256358906,KN O WDA: ALL-IN-ONE KNOWLEDGE MIXTURE MODEL FOR DATA AUGMENTATION IN LOW- RESOURCE NLP TASKS,conclusion and future work,Insight-tree,"this paper explores multi-task learning paradigms at a massive scale for data augmentation in low-resource nlp tasks for the first time. we demonstrate that the proposed knowledge mixture training enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences). experiments verified the effectiveness of our knowda, and knowda outperforms state-of-the-art data augmentation approaches on the popular benchmarks fewglue, conll'03, and wikiann. we also perform ablation studies indicating the importance of including demonstrations and the impact of different keys. moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration.","{231718729: 'Aghajanyan et al., 2021', 244478674: 'Aribandi et al. 2022', 211010520: 'Bartolo et al., 2020a', 233296709: 'Ye et al. 2021'}",https://export.arxiv.org/pdf/2206.10265v2.pdf
284,248266450,A Survey on Multi-hop Question Answering and Generation,conclusions and future work,Insight-tree,"multi-hop qa has been researched quite frequently in the recent years with multiple diverse models proposed that aim to model the multi-step retrieval-reasoning process and achieve promising improvements on existing datasets and benchmarks. such systems capable of performing multistep reasoning have a variety of applications ranging from chat-bot assistants that are capable of interactive conversations, to search engines that are capable to retrieve results that may be relevant but not reachable directly from the query text. at the same time the task of mhqa is significantly more challenging than its single hop counterpart. since paragraphs multiple hops away from the question could share few common words and little semantic relation with the question [34], the task to retrieve such contexts is challenging and suffers from semantic drift. the ability of current models to combine multiple contexts for reasoning is also limited. further challenges for solving mhqa is the difficult process of creating datasets that require the models to perform multi-hop reasoning, as well as the task of evaluating the models' abilities to do so without any hacks. some challenging benchmarks and evaluation methods have been recently proposed that bring out some surprising and interesting observations. these results point out to several limitations of existing systems and call for further research.","{221845203: '[1]', 231627885: '[14,', 139103297: '[17]', 215785913: '[21]', 202583433: '[28]', 234093776: '[29]', 153312687: '34,', 233296201: '[36]', 220045477: '[37,', 67855846: '[38]', 207853300: '[40]', 189927857: '41,', 214802355: '[54]', 235097509: '[61]', 218486753: '[62]', 222178328: '[66]', 237450545: '[68,', 204915921: '[72]', 202712552: '[73]', 174801764: '[100]', 174801080: '102,', 225066758: '[107]', 202773198: '[112]', 155100120: '[113]', 237592852: '[131]', 224705407: '[137]', 211258744: '[141]', 203610361: '[142]', 221749191: '[144]', 128344862: '[145]', 207870753: '[146]', 207756678: '[151]', 202583429: '[167]', 202785879: '[169]', 218487313: '[170]', 234776197: '171]', 52822214: '[173]', 220045416: '[176]', 234778323: '[181]'}",https://arxiv.org/pdf/2204.09140v1.pdf
285,230433978,Few-Shot Question Answering by Pretraining Span Selection,conclusion,Insight-tree,"we explore the few-shot setting of extractive question answering, and demonstrate that existing methods, based on fine-tuning large pretrained language models, fail in this setup. we propose a new pretraining scheme and architecture for span selection that lead to dramatic improvements, reaching surprisingly good results even when only an order of a hundred examples are available. our work shows that choices that are often deemed unimportant when enough data is available, again become crucial in the few-shot setting, opening the door to new methods that take advantage of prior knowledge on the downstream task during model development. ","{86611921: 'Kwiatkowski et al., 2019'}",https://www.aclanthology.org/2021.acl-long.239.pdf
286,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,conclusion,Insight-tree,"we presented oreolm, a novel model that incorporates symbolic kg reasoning with existing lms. we showed that oreolm can bring significant performance gain to open-domain qa benchmarks, both for closed-book and open-book settings, as well as encoder-only and encoder-decoder models. additionally, oreolm produces reasoning paths that helps interpret the model prediction. in future, we'd like to improve oreolm by training to conduct more reasoning steps, supporting locial reasoning, and apply oreolm to a broader range of knowledge-intensive nlp tasks.","{153312687: 'Ding et al. 2019', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018', 233219869: 'Yasunaga et al., 2021;'}",https://export.arxiv.org/pdf/2211.08380v1.pdf
287,253522964,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,limitations,Insight-tree,"limited reasoning steps in our experiments, we show that using reasoning step t = 2 has better performance to t = 1 on one-hop and multi-hop (mostly two) qa datasets. thus, it's a natural question about whether we could extending reasoning steps more? as previous kg reasoning mostly could support very long path (with lstm design)though we didn't spend much time exploring before the paper submission, we indeed try using t = 3, but currently it didn't get better results. we hypothesize the following reasons: 1) a large portion of our current model's improvement relies on the weakly supervised relation pre-training. to do it, we construct a k-hop (k=2 now) subgraph, and sample dependency graph based on it. the larger k we choose, the more noise is included into the generated relation label, in an exponential increasing speed. thus, it's harder to get accurate reasoning path ground-truth for high-order t . another potential reason is that within transformer model, the representation space in lower and upper layer might be very different, say, encode more syntax and surface knowledge at lower layers, while more semantic knowledge at upper layers. currently we adopt a mlp projection head, wishing to map integrated knowledge into the same space, but it might have many flaws and need further improvement. table requires pre-training and gpu resources our current design has a huge entity embedding table, which should be learned through additional supervision and could not directly fine-tune to downstream tasks. this is restricts our approach's usage.","{153312687: 'Ding et al. 2019', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018', 233219869: 'Yasunaga et al., 2021;'}",https://export.arxiv.org/pdf/2211.08380v1.pdf
288,229923145,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,conclusion,Insight-tree,"in this paper, we develop general methods to improve the performance of large pre-trained seq2seq models for jointly producing nl rationales and answer predictions. specifically, we introduce sentence markers into seq2seq models to tackle explanation fabrication, we enable larger input passage sizes using the fusion-in-decoder architecture, and we infuse knowledge by fine-tuning on restructured qa datasets. we show that a universal model can perform favourably compared to the best task-specific fine-tuned models. our methods improve the state of the art on rationale extraction metrics and task accuracy on multiple eraser benchmarks while reducing the extent to which seq2seq models fabricate explanations to justify incorrect predictions, thereby improving the reliability and verifiability of the generated rationales.","{173188058: 'Talmor and Berant 2019', 218487733: 'Vu et al. 2020', 52822214: 'Wiegreffe et al. 2020'}",https://www.aclanthology.org/2021.emnlp-main.301.pdf
289,225040262,Stronger Transformers for Neural Multi-Hop Question Generation,conclusion,Insight-tree,"in this work, we propose a series of strong transformer models for multi-hop qg. to effectively encode the context documents and the answer, we introduce answer type embeddings and a new sublayer to incorporate the extracted entity-centric graph. we also propose an auxiliary contrastive objective to identify the supporting facts and a data filtering approach to balance the training-test distribution mismatch. experiments on the hotpotqa dataset show that our models outperform the current best approaches by a substantial margin of 5 bleu points. our analysis further reveals that graph-based components may not be the most critical in improving the performance, but can render complementary strengths to the transformer.","{211296452: 'Dhingra et al., 2020', 216553210: 'Pan et al., 2020;'}",https://arxiv.org/pdf/2010.11374v1.pdf
290,238857091,Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations,conclusion and future work,Insight-tree,"in this paper, we present modir, a new representation learning method that improves the zero-shot generalization ability of dense retrieval models. we first show that dense retrieval models differ from classification models in that they emphasize locality properties in the representation space. then we present a momentum-based adversarial training method that robustly pushes text encoders to provide a more domain invariant representation space for dense retrieval. our experiments demonstrate that, compared with ance, a recent sota dr model, modir's improvements are robust overall and significant on datasets where zerodr's evaluation is more accurate.","{220302524: 'Xiong et al., 2021'}",https://www.aclanthology.org/2022.findings-acl.316.pdf
291,247292369,Feeding What You Need by Understanding What You Learned,conclusion,Insight-tree,"we design a competency assessment framework for mrc capabilities, which describes model skills in an explainable and multi-dimensional manner. by leveraging the framework, we further uncover and disentangle the connections between various data properties and model performance on a specific task, as well as propose a capability boundary breakthrough curriculum (cbbc) strategy to maxi-mize the data value and improve training efficiency. the experiments performed on four benchmark datasets verified that our approach can significantly improve the performance of existing mrc models. our work shows a deep understanding of model capabilities and data properties helps monitor the model skills during training and improves learning efficiency. our framework and learning strategy are also generally applicable to other nlp tasks. ",{},https://www.aclanthology.org/2022.acl-long.403.pdf
292,253157979,TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack,conclusion,Insight-tree,"we present tasa, an automatic adversarial attack method for qa models. it generates twin answer sentences, perturbed answer sentence (pas), and distracting answer sentence (das), to construct a new adversarial context in a qa sample. it can deceive models and misguide them to an incorrect answer based on their pitfalls that overly rely on matching sensitive keywords during predicting answers. in experiments, tasa achieves remarkable attack performance on five datasets and three victim models with satisfactory sample quality. our additional analysis also proves that it is possible to get more robust qa models via tasa in the future. during fine-tuning bert on different qa datasets, we set the maximum input sequence length as 384, using an adam optimizer whose initial learning rate is 6.25eâ5 with the batch size 32. the epoch number is 3 and the final model after all epochs will be saved as the victim model. spanbert we also use the huggingfancetransformers to implement the model, along with spanbert-large-cased version 5 to initialize the weights. it contains 24 layers with a hidden size of 1024. a linear layer is added to predict the start and end positions of the answer span.","{52822214: 'Yang et al., 2018, and'}",https://www.aclanthology.org/2022.emnlp-main.821.pdf
293,261556862,Cognitive Architectures for Language Agents,conclusion,Insight-tree,"we proposed cognitive architectures for language agents (coala), a conceptual framework to systematically understand and build language agents.our framework draws inspiration from the rich history of symbolic artificial intelligence and cognitive science, connecting decades-old insights to frontier research on large language models.we believe this approach provides a path towards developing more general and more human-like artificial intelligence.",{},https://export.arxiv.org/pdf/2309.02427v2.pdf
294,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,conclusion,Insight-tree,"we propose a novel extension of soft prompt tuning (pt) -vector quantized input-contextualized prompt tuning (vip), designed to have two desirable characteristics -(i) contextualizing the soft prompt tokens w.r.t input text using a learnable sentence encoder (ii) discretizing the contextual prompts using a vector quantization network. on an extensive set of language understanding tasks -superglue, qa, nli, ner, and relation classification, vip outperforms pt baseline. further, our generalization studies on out-of-domain evaluations of qa and nli and multi-task settings over 4 tasks also show that vip is able to learn richer and more robust prompt representations than pt.",{},https://www.aclanthology.org/2022.emnlp-main.455.pdf
295,248986718,Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding,limitations,Insight-tree,"in this section, we point out the limitations of vip and its potential future directions.â¢ pretraining prompt contextualizer. the sentence encoder in vip is trained from scratch for each downstream task. however, following the prompt pre-training proposed in gu et al. (2021), a possible future work is to pretrain the prompt contextualizer in a task-agnostic way.â¢ larger parameter size. vip framework demands a larger parameter size than the baseline soft prompt tuning, owing mainly to the codebook. in appendix a.2 we show that by reducing the number of vip-prompt tokens and codebooksize, we can reduce the parameter size to onethird while compromising performance slightly on superglue. more extensive experimental analysis and better techniques for compressing the codebook, we leave as future work.â¢ more hyperparameters. other than the standard hyperparameters of the sentence encoder, the quantizer introduces new hyperparameters -codebook-size, multinomial sample size, and the temperature constant Ï to scale logits. while vip needs additional hyperparameters, in all our experiments across 20 training datasets from 5 tasks, we fix all hyperparameters related to codebook and sentence-encoder. this shows that our model is indeed not sensitive to the hyperparameters and does not need very specific tuning for each task/setting.â¢ training challenges. learning the codebook requires an ema style updating scheme instead of the standard gradient update. with the plm being frozen, this needs more careful handlingfor e.g. a critical hyperparameter is the value of the temperature constant Ï . a very high value can lead to representation collapse of the codebook while very low values can lead to sparse codebook usage. however, as discussed above, Ï is independent of the task and depends on the initial norm of codebook vectors.â¢ impact on small-scale datasets. we posit that due to the larger parameter size of vip, it performs worse than pt in tasks with lesser training data, e.g. scores on the cb dataset in table 1. this is due to the larger parameter size of vip. indeed, by reducing the parameter size of vip (in appendix table 8), we achieve much better performance on cb.â¢ t5-base as backbone plm. due to resource limitations, in all our experiments we use t5base as the backbone. following lester et al. (2021) where larger plms are shown to improve prompt-tuning performance, we speculate vip to showcase a similar effect. also, though we use t5 as plm in this work, our vip architecture can be used in bert or gpt style prediction or generation as well. however, a formal analysis of this is left as future work.â¢ data and model bias. the language understanding tasks and datasets were predominantly in the english language, and thus limit our claims to the english language. gender, age, race, and other socioeconomic biases may exist in these datasets, and models trained on these datasets may propagate these biases. it is likely that additional biases are also embedded within the t5-base plm that was used as the backbone of vip.",{},https://www.aclanthology.org/2022.emnlp-main.455.pdf
296,258461315,"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",conclusion,Insight-tree,"in this paper, we point out the challenges that should be considered in introducing ir into llm from perspectives of reasoning and knowledge.then, we propose a novel framework called searchain for enabling ir and llm to interact with each other effectively.searchain not only stimulates the knowledge-reasoning ability of llm but also uses ir to provide the knowledge that llm really needs based on the external knowledge base, which improves accuracy and credibility.besides, searchain can mark references to supporting documents for the knowledge involved in the generated contents, which improves the traceability of the contents.in addition, the interaction between ir and llm in searchain transforms the topology of reasoning from chain to tree, which enables llm to dynamically modify reasoning direction.experimental results on complex knowledge-intensive tasks show searchain performs better than all baselines.in future work, we will consider how to improve the efficiency of the framework and how to introduce more tools to interact with llm for more tasks.","{221507798: '[3]', 247595263: '[18]', 196170479: '32', 52822214: '[33]', 236771976: '[34]', 226236740: '[35]', 252692968: '[42]', 244799249: '[43]'}",https://export.arxiv.org/pdf/2304.14732v6.pdf
297,211296452,DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE,conclusion,Insight-tree,"we present drkit, a differentiable module that is capable of answering multi-hop questions directly using a large entity-linked text corpus. drkit is designed to imitate traversal in kb over the text corpus, providing ability to follow relations in the ""virtual"" kb over text. we achieve state-of-the-art results on the metaqa dataset for answering natural language questions, with a 9 point increase in the 3-hop case. we also developed an efficient implementation using sparse operations and inner product search, which led to a 10-100x increase in queries/sec over baseline approaches. a metaqa: implementation details we use p = 400 dimensional embeddings for the mentions and queries, and 200-dimensional embeddings each for the start and end positions. this results in an index of size 750mb. when computing a eâm , the entity to mention co-occurrence matrix, we only retain mentions in the top 50 paragraphs matched with an entity, to ensure sparsity. further we initialize the first 4 layers of the question encoder with the transformer network from pre-training. for the first hop, we assign z 0 as a 1-hot vector for the least frequent entity detected in the question using an exact match. the number of nearest neighbors k and the softmax temperature Î» were tuned on the dev set of each task, and we found k = 10000 and Î» = 4 to work best. we pretrain the index on a combination of the metaqa corpus, using the kb provided with metaqa for distance data, and the wikidata corpus.   indexing only entity-mentions in single-hop questions over all spans. note that drkit-entities has a high hits@1 performance on the rare relations subset, showing that there is generalization to less frequent data due to the natural language representations of entities and relations.","{202583433: 'Das et al. 2019b', 153312687: 'Ding et al. 2019', 189927857: 'Feldman & El-Yaniv, 2019;', 186206745: 'Jiang et al., 2019;', 202558815: 'Min et al. 2019', 202773198: 'Qi et al., 2019', 155100120: 'Xiao et al., 2019'}",https://arxiv.org/pdf/2002.10640v1.pdf
298,257833654,Quantifying the Academic Quality of Children's Videos using Machine Comprehension,conclusion and future work,Insight-tree,"in this research, we demonstrate the ability of an rc model to assess academic quality by introducing a new dataset consisting of questions and answers from children's videos. we then determine the academic value of the top channels by measuring the number of textbook questions answered correctly by the model. our analysis of over 80,000 videos posted on the top 100 channels provides a comprehensive evaluation of the academic quality of content on ytk and utilizes a large dataset of middle school textbook questions on various topics. our findings reveal the academic topics covered in these children's videos, and we compare the quality of the channels.",{86611921: '[20]'},https://export.arxiv.org/pdf/2303.17201v1.pdf
299,235435668,Question Answering Infused Pre-training of General-Purpose Contextualized Representations,conclusion,Insight-tree,"in this work, we pre-trained token-level contextual representations that are useful for downstream fewshot learning. our key idea was to use questionanswer pairs to define what information should be encoded in passage representations. we showed that these representations are useful for a variety of standard nlp tasks in zero-and few-shot settings, including paraphrase detection, named entity recognition, and sentiment analysis, across nine total datasets. looking forward, we hope to see more work on designing pre-training objectives that align with downstream needs for few-shot learning. ",{},https://www.aclanthology.org/2022.findings-acl.59.pdf
300,244119706,Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval,conclusion,Insight-tree,"in this work, we propose a framework that enhances dual-encoders with cross-embeddings for answer retrieval. a novel geometry alignment mechanism is introduced to align the geometry of dual-encoders with cross-embeddings. extensive experimental results show that our method significantly improves dual-encoders model and outperforms the state-of-the-art method on multiple answer retrieval datasets.",{},https://arxiv.org/pdf/2206.02978v1.pdf
301,254877499,Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,conclusions,Insight-tree,"chain-of-thought prompting has significantly improved llms' ability to perform multi-step reason-  [1984][1985][1986][1987][1988][1989] was performed by jane siberry. jane siberry was born in toronto. the castle in toronto is the casa loma. so the answer is: casa loma. table 2: example cots generated by gpt3 with different methods. since nor relies on parametric knowledge, it often makes a factual error in the first sentence derailing the full cot. oner can retrieve relevant information closest to the question and is less likely to make such errors early on, but it still makes errors later in the cot. as ircot performs retrieval after each step, it is often able to prevent such errors in each step. more examples are in app. d.","{221970302: 'Xiong et al. 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.acl-long.557.pdf
302,123758373,Understanding Dataset Design Choices for Multi-hop Reasoning,discussion and conclusion,Insight-tree,"there exist several other multi-hop reasoning datasets including worldtree , openbookqa (mihaylov et al., 2018), and mul-tirc (khashabi et al., 2018). these datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content. we leave a detailed investigation of these for future work.",{},https://arxiv.org/pdf/1904.12106v1.pdf
303,259165244,KoLA: Carefully Benchmarking World Knowledge of Large Language Models,conclusion and future work,Insight-tree,"this paper presents kola, a carefully designed knowledge-oriented llm assessment benchmark. we design a cognitive ability taxonomy for more helpful diagnostic results, adopt both known and evolving data sources for better fairness, and employ contrastive metrics for high applicability. in the  figure (b) shows the scatter plots of model performance on evolving tasks and its non-evolving counterparts (e.g., 3-5 v.s. [3][4][5][6]. the x-axis and y-axis of each subplot represent the standard scores (z value) correspondingly.",{},https://export.arxiv.org/pdf/2306.09296v2.pdf
304,258676557,Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning,conclusion,Insight-tree,"in this paper, we propose three solution program centric auxiliary pretraining tasks at both the whole program level and sub-program level. at the wholeprogram level, we propose the variable integrity ranking pretraining task, which guides the model to distinguish required and irrelevant variables in the noisy input. to further enhance the model's ability to learn the underlying reasoning process, we propose two additional pretraining tasks: variable operator prediction and variable keyphrase masking. these tasks help the model perform accurate sub-program construction. experimental results demonstrate the effectiveness of our method. variable integrity ranking achieves the most improvement on both the retriever and program solver. the sub-program level tasks substantially improve results on plms of different scales. our approach achieves 3.56% execution accuracy and 3.74% program accuracy improvement on the competitive roberta-large baseline.","{237485084: 'Deng et al., 2021;', 67855846: 'Dua et al., 2019', 230433978: 'Ram et al., 2021;', 52822214: 'Yang et al., 2018', 248780469: 'Zhao et al., 2022', 234741852: 'Feng et al. 2021'}",https://export.arxiv.org/pdf/2305.07475v1.pdf
305,257365721,GlobalNER: Incorporating Non-local Information into Named Entity Recognition,conclusion,Insight-tree,"to improve ner with non-local information from the internet, we propose a transformer-based query generation method and a mention-aware re-ranker, mentionscore. these can favor the recall of the retrieved results, select the non-local sentences specifically related to each mention in the local sentence and lead to a state-of-the-art performance of 61.56 micro-f1 score on wnut17 dataset is achieved.  table 1. the experimental results of adopting different query strategies and reranking settings on wnut17 dataset with reference sentences retrieved by google search retrieval. all our experiments report the average performance of 10 models trained with different seeds. the superscript "" * "" denotes the experiments reported in clner paper. clner w/ cl is the best setting of wang et al. (2020), which utilized retrieved sentences and was trained with the technique cooperative learning, while clner w/o cl only used the retrieved sentences. baseline#2 implemented in our system is the counterpart to clner w/o cl. m&s+ms is the setting of using mention as query and mentionscore as re-ranker to form the external context and evaluate with the model baseline#1. m_g&s+ms uses gold mention as query. compared to baseline#2, paired single-tail ttest for p-value < 0.05 is superscripted with â  and p-value < 0.01 with â¡ to denote the t-test is passed.",{},https://export.arxiv.org/pdf/2303.02915v1.pdf
306,257767698,Augmenting ebooks with Recommended Questions using Contrastive fine-tuned T5,conclusion and future scope,Insight-tree,"the use of ai is constantly evolving in diverse applications. this study investigates the potential advantages of a natural language processing approach for education. this research presented an education question generating (eqg) approach that augments the ebook content with generated edu-questions to provide students with an effective learning platform. through experiments, we assessed the model's performance on a question generation task both before and after contrastive training. we discovered that a contrastive trained model can produce more pertinent questions on the input text and can comprehend key concepts more effectively. experiments on qa dataset, prml (bishop, 2006) and ncert 2 ebook shows that our model succeeds to produces complex questions at scale.",{},https://www.aclanthology.org/2022.icon-main.15.pdf
307,260682695,Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies,conclusion,Insight-tree,"in this paper, we present a comprehensive survey of self-correcting large language models with automated feedback. we broadly categorize and analyze various self-correction strategies, including training-time, generation-time, and post-hoc corrections. we also discuss the major application areas of self-correction, including correcting factual errors, enhancing reasoning abilities, and improving code generation, among others. finally, we outline a number of potential future directions and associated challenges in this field. our goal with this paper is to provide a com-prehensive and useful resource for readers interested in the development of this rapidly evolving domain. to aid in this effort, we create a continually-updated reading list in a github repository: https://github.com/teacherpeterpan/ self-correction-llm-papers. ","{253080775: 'Ho et al., 2023', 256846551: 'Ribeiro et al., 2023;', 247595263: 'Wang et al., 2023c', 249062748: 'Yang et al., 2022a;', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2308.03188v2.pdf
308,249062996,LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA,conclusion,Insight-tree,"this work introduces lepus, a method to perform unsupervised re-ranking of multi-document paths for question answering based on large language models. given a question, the document path is encoded into a prompt and the document path is scored as the probability of generating the question given the prompt. experiments on a standard multihop qa benchmark show the strong performance of lepus in the zero-shot setting, displaying comparable performance to fully-supervised retrievers. we also analyze our approach showing the utility of using multi-hop prompts as opposed to singlehop ones. lastly, our work shows that language models can indeed function as strong unsupervised re-rankers for multi-hop question answering.","{208267807: 'Asai et al. 2020', 211296452: 'Dhingra et al., 2020', 202660724: 'Nie et al., 2019', 211258645: 'Perez et al., 2020', 240288953: 'Qi et al. 2021', 235485084: 'Seonwoo et al. 2021', 221970302: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2205.12650v1.pdf
309,5669835,"Automatic extraction of dierences between spoken and written languages, and automatic translation from the written to the spoken language",conclusion,Insight-tree,"in this study, w e extracted dierences between spoken and written languages and examined the extracted dierences by using spoken and written data constructed by the communications research laboratory and the national institute for japanese language. we also tried transforming written language into spoken language by using extracted dierences as the transformation rules.",{},http://www.lrec-conf.org/proceedings/lrec2002/pdf/27.pdf
310,17412630,sRNA Antitoxins: More than One Way to Repress a Toxin,conclusions,Insight-tree,"there are numerous toxin-antitoxin loci found within bacterial chromosomes; the type i and type iii pairs represent only a fraction of what has been described to date [1]. the type i and type iii loci are unique in that they utilize rna as antitoxins, yet they use their rna antitoxins very differently, illustrating the immense versatility of rna as a regulatory molecule. it is important to note that to date these antitoxins have been shown to act either through interaction with toxin mrna or protein.",{1376816: '[43]'},NaN
311,189927896,"Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA",conclusion,Insight-tree,"in this work, we identified reasoning shortcuts in the hotpotqa dataset where the model can locate the answer without multi-hop reasoning. we constructed adversarial documents that can fool the models exploiting the shortcut, and found that the performance of a state-of-the-art model dropped significantly under our adversarial examples. we showed that this baseline can improve on the adversarial evaluation after being trained on the adversarial data. we next proposed to use a control unit that dynamically attends to the question to guide the bi-attention in multi-hop reasoning. trained on the regular data, this 2-hop model is more robust against the adversary than the baseline; and after being trained with adversarial data, this model achieved further improvements on the adversarial evaluation and also outperforms the baseline. overall, we hope that these insights and initial improvements will motivate the development of new models that combine explicit compositional reasoning with adversarial training.",{52822214: 'Yang et al. 2018'},https://arxiv.org/pdf/1906.07132v1.pdf
312,225039884,On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries,conclusion,Insight-tree,"we introduce squall, the first large-scale semantic parsing dataset with both hand-produced target logical forms and manually-derived lexical alignments between questions and sql queries. our dataset enables finer-grained supervision than existing datasets have previously supported. we incorporate the alignments into encoder-decoder-based neural models through supervised attention and an auxiliary task of column prediction. experiments confirm our intuition that finer-grained supervision is helpful to model training. our oracle studies also show that there is large unrealized further potential for our annotations. thus, it remains an exciting challenge for future research to use our lexical alignment annotations more effectively. our annotation cost analysis shows that collecting additional lexical alignments is more costeffective for improving model accuracy than having only logical forms. we hope that our findings will help future dataset design decisions and extensions of other existing datasets. one potential future direction is to further investigate the utility of lexical alignments in a cross-dataset/domain evaluation setting  ",{},https://www.aclweb.org/anthology/2020.findings-emnlp.167.pdf
313,259949923,SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval Lexical Inverted Index CPU for Interpret- System Matching for Inference Inference ability,conclusion,Insight-tree,"in this work, we presented sprint, a unified python toolkit focused on sparse neural retrieval. the toolkit extends the evaluation of several neural sparse retrievers on a common interface and easily allows practitioners to search their custom datasets using sparse retrieval. evaluation of a custom dataset using our toolkit is straightforward, as we effectively use an inference pipeline to unify evaluation across all different sparse retrievers.","{86611921: '[17]', 233296016: '[40]', 220302524: '44]', 221971009: '[49]'}",https://export.arxiv.org/pdf/2307.10488v1.pdf
314,258078778,Datamator: An Intelligent Authoring Tool for Creating Datamations via Data Query Decomposition,"conclusion, limitations, and future work",Insight-tree,"in this paper, we presented the authoring tool datamator, developed for creating datamations. to the best of our knowledge, it is the first tool that supports datamation design and generation. given a dataset and a question, datamator can automatically decompose the question into a sequence of data analysis operators and generate a datamation based on unit visualization. datamator also allows the user to modify and edit the generated results. our user studies showed that datamator is highly rated for generating datamations to explain data analysis processes. its editing function also showed to be effective in correcting the automatically generated results.","{236493173: '[14]', 211003735: '[45]', 52822214: '[47]'}",https://export.arxiv.org/pdf/2304.03126v3.pdf
315,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",limitations,Insight-tree,"for object detection, while unified-io generally produces accurate outputs (see appendix a.4), we find the recall is often poor in cluttered images. prior work (chen et al., 2022b) has shown this can be overcome with extensive data augmentation techniques, but these methods are not currently integrated into unified-io. our use of a pre-trained vq-gan greatly simplifies our training and is surprisingly effective for dense prediction tasks. however, it does mean unified-io has limited image generation capabilities (recent works (yu et al., 2022b) have shown this method can be greatly improved but was not available at the time of development). we also found in a small-scale study that our model does not always understand prompts not in the training data (see appendix 4.5).","{165163607: 'Clark et al., 2019;'}",https://export.arxiv.org/pdf/2206.08916v2.pdf
316,249848272,"UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS",conclusion,Insight-tree,"we have presented unified-io, a unified architecture that supports a large variety of computer vision and nlp tasks with diverse inputs and outputs, including images, continuous maps, binary masks, segmentation masks, text, bounding boxes, and keypoints. this unification is made possible by homogenizing each of these modalities into a sequence of discrete tokens. the 2.9b parameter unified-io xl model is jointly trained on 90+ datasets, is the first model to perform all 7 tasks on the grit benchmark and obtains impressive results across 16 other vision and nlp benchmarks, with no benchmark fine-tuning or task-specific modifications.","{165163607: 'Clark et al., 2019;'}",https://export.arxiv.org/pdf/2206.08916v2.pdf
317,214802013,Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,conclusions,Insight-tree,in this paper we propose the problem of recovering reasoning chains in multi-hop qa from weak supervision signals. our model adopts an cooperative game approach where a ranker and a reasoner cooperate to select the most confident chains. experiments on the hotpotqa and medhop benchmarks show the effectiveness of the proposed approach. ,"{153312687: 'Ding et al., 2019;', 202660724: 'Nie et al., 2019;', 155100120: 'Xiao et al., 2019;', 52822214: 'Yang et al., 2018', 189898081: 'Yao et al., 2019'}",https://arxiv.org/pdf/2004.02393v1.pdf
318,263829433,DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries,conclusion and future work,Insight-tree,"this paper introduces a novel task, scientific document retrieval using multi-level aspect-based queries (doris-mae), aimed at modeling the process of information retrieval in the context of scientific research.we also present a dataset for doris-mae generated using the anno-gpt framework.","{237450545: '33,', 221448158: '[36,', 86611921: '[40]', 244799249: '[66]', 233296016: '69]', 52822214: '87]'}",https://export.arxiv.org/pdf/2310.04678v3.pdf
319,261049680,Large Language Models as Zero-Shot Conversational Recommenders ACM Reference Format,limitations of llms as zero-shot crs,Insight-tree,"finding 10 -llm recommendations suffer from popularity bias in crs. popularity bias refers to a phenomenon that popular items are recommended even more frequently than their popularity would warrant [8]. figure 8 shows the popularity bias in llm recommendations, though it may not be biased to the popular items in the target datasets. on redial, the most popular movies such as avengers: infinity war appear around 2% of the time over all ground-truth items; on reddit, the most popular movies such as everything everywhere all at once appears less than 0.3% of the time over ground-truth items. but for the generated recommendations from gpt-4 (other llms share a similar trend), the most popular items such as the shawshank redemption appear around 5% times on redial and around 1.5% times on reddit. compared to the target datasets, llms recommendations are more concentrated on popular items, which may cause further issues like the bias amplification loop [8]. moreover, the recommended popular items are similar across different datasets, which may reflect the item popularity in the pre-training corpus of llms.finding 11 -recommendation performance of llms is sensitive to geographical regions. despite the effectiveness in general, it is unclear whether llms can be good recommenders across various cultures and regions. specifically, pre-trained language models' strong open-domain ability can be attributed to pre-training from massive data [5]. but it also leads to llms' sensitivity to data distribution. to investigate llms recommendation abilities for various regions, we take test instances from the reddit dataset and obtain the production region of 7,476 movies from a publicly available movie dataset 13 by exact title matching, then report the recall@1 for the linked movies grouped by region. we only report regions with more than 300 data points available to ensure enough data to support the result. as shown in figure 9 the current best model, gpt-4's performance on recommendation is higher for movies produced in english-speaking regions. this could be due to bias in the training data -the left of figure 9 show item on reddit forums are dominated by movies from english-speaking regions. such a result highlights large language model's recommendation performance varies by region and culture and demonstrates the importance of cross-regional analysis and evaluation for language model-based conversational recommendation models.","{222125277: '[7,', 199668753: '[10,'}",https://export.arxiv.org/pdf/2308.10053v1.pdf
320,261049680,Large Language Models as Zero-Shot Conversational Recommenders ACM Reference Format,conclusion and discussion,Insight-tree,"we investigate large language models (llms) as zero-shot conversational recommendation systems (crs). through our empirical investigation, we initially address a repetition shortcut in previous standard crs evaluations, which can potentially lead to unreliable conclusions regarding model design. subsequently, we demonstrate that llms as zero-shot crs surpass all fine-tuned existing crs models in our experiments. inspired by their effectiveness, we conduct a comprehensive analysis from both the model and data perspectives to gain insights into the working mechanisms of llms, the characteristics of typical crs tasks, and the limitations of using llms as crs directly. our experimental evaluations encompass two publicly available datasets, supplemented by our newly-created dataset on movie recommendations collected by scraping a popular discussion website. this dataset is the largest public crs dataset and ensures more diverse and realistic conversations for crs research. we also discuss the future directions based on our findings in this section.","{222125277: '[7,', 199668753: '[10,'}",https://export.arxiv.org/pdf/2308.10053v1.pdf
321,263605809,"MERGE, THEN COMPRESS: DEMYSTIFY EFFICIENT SMOE WITH HINTS FROM ITS ROUTING POLICY",conclusions,Insight-tree,"sparse mixture-of-experts (smoe) is a promising framework to scale up the model capacity, which enjoys roughly unchanged training and inference flops at the cost of significantly increased memory overheads.the memory requirements and expert redundancy highly limit its practical usage.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.01334v1.pdf
322,226281918,Synonym Knowledge Enhanced Reader for Chinese Idiom Reading Comprehension,conclusion,Insight-tree,"in this paper, we give a quantitative analysis to prove that the literal meanings of many chinese idioms are far from their semantics, and also verify that the synonymic relationship can mitigate this inconsistency, which is beneficial for chinese idiom reading comprehension. we propose the synonym knowledge enhanced reader to fully utilize the relationship. experimental results show that our model achieves state-of-the-art performance among different settings of chid, a large-scale chinese idiom reading comprehension dataset. similar to chinese idioms, the inconsistency exists in a number of other language elements, e.g., slangs in english, where the potential use of the synonymic relationship among them requires a further exploration.","{67855846: 'Dua et al., 2019', 214233456: 'Sun et al., 2020', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/2020.coling-main.329.pdf
323,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",conclusions,Insight-tree,"we created a novel dataset socratiq to support research on automatic socratic question generation. we applied latest research in prompt-based conditional text generation to fine-tune existing large language models from gpt, t5, and prophetnet to learn soqg. through our study and the release of this novel dataset, we take a first step towards enabling future research on models for soqg as well as impactful applications in areas such as counseling and education (inkster et al., 2018;fitzpatrick et al., 2017).","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.eacl-main.12.pdf
324,258378353,"Socratic Question Generation: A Novel Dataset, Models, and Evaluation",limitations,Insight-tree,"we note the following limitations in our work that also comprise our future research directions. first, while a human socratic method practitioner will know what type of socratic question to ask based only on context, our prompt-based models assume the availability of question-type for generating a type-sensitive question. in fact, when only contexts were used for qg (gpt, t5, prophetnet baselines in section 3), the generated questions matched the desired question-type (those of the available reference questions) in only 37-40% of the cases. furthermore, the question-type identification of automated methods using context alone was very poor with overall accuracy comparable to that of random assignments (section 4.1).secondly, though we showcased the potential use of soqg in designing chatbots and dialog systems for applications such as counseling, we note that the current evaluation has only been at the singleturn level. we hope to extend socratiq to capture back and forth discussions on cmv to provide multiturn data and also deduce via forum votes and other indicators if the discussion indeed resulted in changed minds and enabled alternate perspectives. furthermore, considering the special purpose of socratic questions in shaping perspectives and enabling introspection and reflection, a comprehensive evaluation would require measuring these aspects over the multi-turn sessions.finally, our dataset was created by re-purposing the cmv subreddit data available in english, a high-resource language for which large-scale pretrained language models (plms) are readily available. obtaining high classification and generation performances via fine-tuning of plms will be a challenge that needs addressing in low-resource languages.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.eacl-main.12.pdf
325,222140958,Context Modeling with Evidence Filter for Multiple Choice Question Answering,conclusions,Insight-tree,we propose evidence filter to alleviate the effect of unrelated sentences and enhance the saliency of evidences potentially without human efforts. results on openbookqa indicate the effectiveness of our method. our future work is to enhance the evidence filter by more complex components.,"{139103297: 'Chen and Durrett, 2019', 153312687: 'Ding et al., 2019', 155100120: 'Qiu et al., 2019;', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2010.02649v1.pdf
326,238744204,Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?,conclusion,Insight-tree,"in this paper, we propose spar, a salient-phrase aware dense retriever, which can augment any dense retriever with the lexical matching capacity and out-of-domain generalization from a sparse retriever. this is achieved by training a dense lexical model Î» to imitate the behavior of the teacher sparse retriever, the feasibility of which remained unknown until this work. we show that spar outperforms previous state-of-the-art dense and sparse retrievers, matching or even exceeding more complex hybrid systems, on various in-domain and outof-domain evaluation datasets. for future work we plan to explore if a dense retriever can be trained to learn lexical matching directly without relying on a teacher model. this way, we can avoid imitating the errors of the sparse retriever, and devise new ways of training dense retrievers that can potentially surpass hybrid models. moreover, there are several intriguing findings in this work that may warrant further study, such as why spar's acc@k improves relatively to the hybrid model as k increases, and why joint training is less effective than post-hoc vector concatenation.","{249097975: 'Izacard et al., 2022', 86611921: 'Kwiatkowski et al., 2019', 233296016: 'Thakur et al., 2021', 220302524: 'Xiong et al., 2021'}",https://export.arxiv.org/pdf/2110.06918v3.pdf
327,263671701,DSPY: COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES,conclusion,Insight-tree,"this paper introduced dspy, a new programming model for designing ai systems using pipelines of pretrained lms and other tools.we presented three new concepts introduced in this abstraction (dspy signatures, modules, and teleprompters), and showed in two very different case studies that it supports rapid development of highly effective systems that use relatively small lms.we have maintained open-source versions of this framework for close to a year.in this period, we have seen and created a large number of programs that were compiled to high-quality systems by dspy, spanning tasks from information extraction to low-resource synthetic data generation.in the interest of space and to maintain reasonable scope in this paper, we leave reporting on such tasks under controlled experimental conditions to future work.while in-context learning has proved transformative over the past 2-3 years of lm research, we argue that the true expressive power in this emerging paradigm is in building sophisticated text transformation graphs in which composable modules and optimizers (teleprompters) come together to leverage lms in more systematic and reliable ways.","{202773198: 'Qi et al., 2019;', 247595263: 'Wang et al. 2022b', 258309779: 'Yao et al. 2022'}",https://export.arxiv.org/pdf/2310.03714v1.pdf
328,237485084,ReasonBERT: Pre-trained to Reason with Distant Supervision,conclusion and future work,Insight-tree,"we propose reasonbert, a novel pre-training method to enhance the reasoning ability of language models. the resulting model obtains substantial improvements on multi-hop and hybrid qa tasks that require complex reasoning, and demonstrates superior few-shot performance. in the future, we plan to use our query-evidence pairs collected by distant supervision to improve the retrieval performance for open-domain qa, as well as empower reasonbert to handle more types of reasoning, like comparison and numeric reasoning, in natural language understanding. ","{226278099: 'Jiang et al., 2020', 198229624: 'Joshi et al., 2020', 86611921: 'Devlin et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.494.pdf
329,202541222,Large Scale Question Answering using Tourism Data,conclusion,Insight-tree,"in the spirit of defining a question answering challenge that is closer to a real-world qa setting, we introduce the novel task of identifying the correct entity answer to a given user question based on a collection of unstructured reviews describing entities. we harvest a dataset of over 48,000 qa pairs, which enables end to end training of models.","{196170479: 'Fan et al., 2019'}",https://arxiv.org/pdf/1909.03527v2.pdf
330,219559168,Knowledge-Aided Open-Domain Question Answering,conclusion,Insight-tree,"this paper investigates how the performance of open-domain question answering can be improved through enhancing document retrieval and answer reranking. the central idea is to consider both question-document and document-document relationships in the document retriever and the answer reranker. more specifically, with the aid of external knowledge resources, we first construct question-document graphs and document-document graphs using knowledge triples, and then encode such relational knowledge in the document retrieval and answer ranking components. we evaluated our model on several open-domain question answering datasets including squad-open, quasar-t and triviaqaunfiltered. we observed that our method can boost the overall performance of open-domain question answering consistently on these datasets. extensive experiments show that modeling the questiondocument and document-document relationships can contribute to the improvement consistently.","{153312687: '[10]', 202773198: '[26]'}",https://arxiv.org/pdf/2006.05244v1.pdf
331,211832082,"A Survey of Antimicrobial Resistance Determinants in Category A Select Agents, Exempt Strains, and Near-Neighbor Species",conclusions,Insight-tree,"both pcr and microarrays are valuable tools for the tracking the genetic underpinnings of amr resistance. here, we used two complementary technologies-microarray analysis and hrma-to survey 127 select agents, exempt strains, and near-neighbor species for a broad variety of resistance mechanisms acquired through both horizontal transfer and gene mutations. to our knowledge, this is the largest survey of category a agents, exempt strains, and near-neighbor species for genes covering multiple mechanisms of amr.",{},NaN
332,248571884,Better Retrieval May Not Lead to Better Question Answering,conclusion,Insight-tree,"in this work we investigate the impact of evidence quality on question answering on two open-domain qa datasets, strategyqa and hotpotqa. results show that (1) strategyqa is less sensitive to the evidence quality than hotpotqa and (2) boolean questions are less sensitive to the evidence quality than extractive questions. further study shows this is mainly because the model does not yield a sufficiently high score on those examples with perfect evidence retrieval. finally, we recommend that for the implicit decomposition open-domain question answering problems with boolean questions such as strategyqa, researchers start by improving qa performance given the gold paragraphs rather than improving evidence quality.","{230799347: 'Geva et al. 2021', 215768725: 'Groeneveld et al., 2020', 202565945: 'Jiang and Bansal, 2019;', 86611921: 'Kwiatkowski et al., 2019', 174801764: 'Min et al., 2019', 202660724: 'Nie et al., 2019', 240288953: 'Qi et al., 2021', 155100120: 'Qiu et al., 2019;', 221970302: 'Xiong et al., 2020;', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020;', 233219392: 'Zhao et al., 2021'}",https://arxiv.org/pdf/2205.03685v1.pdf
333,212747830,Invited Review . Pre-trained Models for Natural Language Processing: A Survey,conclusion,Insight-tree,"in this survey, we conduct a comprehensive overview of ptms for nlp, including background knowledge, model architecture, pre-training tasks, various extensions, adaption approaches, related resources, and applications. based on current ptms, we propose a new taxonomy of ptms from four different perspectives. we also suggest several possible future research directions for ptms.","{198229624: '[47]', 52822214: '[217]', 207870753: '[220]'}",https://arxiv.org/pdf/2003.08271v4.pdf
334,259164782,"When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants",conclusion,Insight-tree,"we present an empirical efficiency analysis of vanilla transformers and their self-attention-based efficient variants across modalities, metrics and input context sizes. we find substantial differences across modalities and metrics when analyzing the tipping point for efficient variants. finally, the layerwise analysis finds that self-attention is not the only bottleneck. we recommend that all efficient model papers should report such cross-modal, layerwise profiling results on multiple efficiency metrics covering a variety of use-cases to provide a full picture of the benefits of the model.","{226281978: 'Tay et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.acl-short.141.pdf
335,102353837,BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis,conclusions,Insight-tree,we proposed a new task called review reading comprehension (rrc) and investigated the possibility of turning reviews as a valuable resource for answering user questions. we adopted bert as our base model and proposed a joint post-training approach to enhancing both the domain and task knowledge. we further explored the use of this approach in two other review-based tasks: aspect extraction and aspect sentiment classification. experimental results show that the post-training approach before fine-tuning is effective.,{},https://www.aclweb.org/anthology/N19-1242.pdf
336,244909409,Hybrid Autoregressive Inference for Scalable Multi-hop Explanation Regeneration,conclusion,Insight-tree,"this work presented scar, a hybrid autoregressive architecture for scalable explanation regeneration. an extensive evaluation demonstrated that scar achieves performance comparable with that of state-of-the-art crossencoders while being â 50 times faster and intrinsically scalable, and confirmed the impact of the hybridisation on semantic drift and question answering. this work demonstrated the effectiveness of hybrid architectures for explainable inference at scale, opening the way for future research at the intersection of latent and explicit models. as a future work, we plan to investigate the integration of relevance and explanatory power in an end-to-end differentiable architecture, and explore the applicability of the hybrid framework on additional natural language and scientific reasoning tasks, with a focus on real-world scientific inference problems.","{211296452: 'Dhingra et al. 2019', 208089867: 'Jansen and Ustalov 2019;', 222178328: 'Jhamtani and Clark 2020;', 204915921: 'Khot et al. 2020;', 225075843: 'Lin et al. 2021', 231883811: 'Valentino, Thayaparan, and Freitas 2021', 221970302: 'Xiong et al. 2021', 52822214: 'Yang et al. 2018;', 233219392: 'Zhao et al. 2021;'}",https://arxiv.org/pdf/2107.11879v2.pdf
337,214802166,Prerequisites for Explainable Machine Reading Comprehension: A Position Paper,conclusion,Insight-tree,"in this position paper, we overviewed issues and future directions of mrc. we focused specifically on the situation model in psychology for what we should ask of reading comprehension and the substantive validity in psychometrics for how we should correctly evaluate it. we conclude that future datasets should (i) evaluate the capability of the situation model for understanding contextdependent situations and for grounding to nontextual information and (ii) ensure the substantive validity by improving the question quality and designing a white-box task formulation. ","{86611921: 'Kwiatkowski et al., 2019', 174801764: 'Min et al. 2019', 208201969: 'Sugawara et al. 2020'}",https://arxiv.org/pdf/2004.01912v1.pdf
338,212644640,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,conclusion,Insight-tree,"in this paper, we introduce a novel framework to characterise machine reading comprehension gold standards. this framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach. furthermore we applied the framework to analyse popular state-of-the-art gold standards for machine reading comprehension: we reveal issues with their factual correctness, show the presence of lexical cues and we observe that semantics-altering grammatical modifiers are missing in all of the investigated gold standards. studying how to introduce those modifiers into gold standards and observing whether state-of-the-art mrc models are capable of performing reading comprehension on text containing them, is a future research goal. a future line of research is to extend the framework to be able to identify the different types of exploitable cues such as question or entity typing and concrete overlap patterns. this will allow the framework to serve as an interpretable estimate of reading comprehension complexity of gold standards. finally, investigating gold standards under this framework where mrc models outperform the human baseline (e.g. squad) will contribute to a deeper understanding of the seemingly superb performance of deep learning approaches on them.","{139103297: 'Chen and Durrett, 2019b', 67855846: 'Dua et al., 2019', 189927896: 'Jiang and Bansal, 2019', 202785879: 'Yadav et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.lrec-1.660.pdf
339,246016282,Reasoning over Hybrid Chain for Table-and-Text Open Domain QA,conclusion,Insight-tree,"in this paper, we present a chain-centric reasoning and pre-training (carp) framework for table-andtext question answering. when answering the questions given retrieved table and passages, carp first extracts explicit hybrid chain to reveal the intermediate reasoning process leading to the answer across table and text. the hybrid chain provides a guidance for qa, and explanation of the intermediate reasoning process. to enhance the extraction model with better reasoning ability and alleviate data sparsity problem, we design a novel chaincentric pre-training method. this method synthesizes the reasoning corpus in a larger scale and of higher reasoning complexity, which is achieved by automatically synthesizing heterogeneous reasoning paths from tables and passages in wikipedia and reversely generating multi-hop questions. we find that the pre-training task boosts performance on the hybrid chain extraction model, especially for questions requiring more complex reasoning, which leads to significant improvement on the performance of the qa model. the hybrid chain also provides better interpretability of the reasoning process. our system achieves the state-of-the-art result on a ","{214802013: 'Feng et al., 2020'}",https://arxiv.org/pdf/2201.05880v1.pdf
340,218613640,Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond,conclusion,Insight-tree,"this work comprehensively reviews the studies of mrc in the scopes of background, definition, development, influence, datasets, technical and benchmark highlights, trends, and opportunities. we first briefly introduced the history of mrc and the background of contextualized language models. then, we discussed the role of contextualized language models and the influence of mrc to the nlp community. the previous technical advances were summarized in the framework of encoder to decoder. after going through the mechanisms of mrc systems, we showed the highlights in different stages of mrc studies. finally, we summarized the trends and opportunities. the basic views we have arrived at are that 1) mrc boosts the progress from language processing to understanding; 2) the rapid improvement of mrc systems greatly benefits from the progress of clms; 3) the theme of mrc is gradually moving from shallow text matching to cognitive reasoning.","{153312687: 'Ding et al. 2019', 67855846: 'Dua et al. 2019;', 204823992: 'Fisch et al. 2019', 186206745: 'Jiang et al. 2019;', 198229624: 'Joshi et al. 2020', 57721315: 'Nishida et al. 2019', 155100120: 'Qiu et al. 2019b;', 52822214: 'Yang et al. 2018'}",https://arxiv.org/pdf/2005.06249v1.pdf
341,243832767,Dataset of Fake News Detection and Fact Verification: A Survey,conclusion,Insight-tree,"our survey provides extensive reviews of fake news datasets by: (1) summarizing the definition of fake news, relevant concepts related to fake news, and the areas covered by existing survey papers on fake news research as a basis for discussion;",{},https://arxiv.org/pdf/2111.03299v1.pdf
342,219573577,Multi-hop Reading Comprehension across Documents with Path-based Graph Convolutional Network,conclusion,Insight-tree,"in this paper, we propose a novel approach for multi-hop reading comprehension across documents. our approach extends the entity graph by introducing reasoning entities, which can form the reasoning path from question to candidates. in addition, our approach incorporates the question in the multi-hop reasoning through a new gate mechanism to regulate how much useful information propagating from neighbors to the node. experiments show that our approach achieves state-of-the-art accuracy both for single and ensemble models.","{201698166: '[Pennington et al., 2014]'}",https://arxiv.org/pdf/2006.06478v2.pdf
343,252780709,Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering,conclusion,Insight-tree,"in this paper, we examined the multi-hop reasoning capabilities of generative qa models, finding that overall models take shortcuts when answering multi-hop questions, not demonstrating convincing multi-hop reasoning capability. when trained only on single-hop questions, models generalize poorly to multi-hop questions, while approximation using the concatenation of single-hop questions and sparql queries improves the multi-hop performance significantly. further directions include better approximations of multi-hop questions and advanced modeling techniques that encourage compositional ability.","{208267807: 'Asai et al., 2020', 139103297: 'Chen and Durrett, 2019;', 153312687: 'Ding et al., 2019;', 67855846: 'Dua et al., 2019', 189927896: 'Jiang and Bansal, 2019;', 86611921: 'Kwiatkowski et al., 2019', 235755349: 'Lee et al., 2021', 174801764: 'Min et al., 2019a;', 174801080: 'Min et al., 2019b;', 202773198: 'Qi et al., 2019;', 155100120: 'Qiu et al., 2019;', 211258744: 'Tang et al. 2021', 221749191: 'Trivedi et al., 2020;', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.152.pdf
344,237552879,Evaluation Paradigms in Question Answering,conclusion,Insight-tree,"we identify two core motivations for qa research over the past twenty years. we link one to the usercentered goals of the cranfield paradigm and propose the manchester paradigm to describe research working towards building human-like, intelligent qa systems. in at least the short-term, this distinction is important as it illuminates the goals of industry and academic stakeholders; ultimately, this makes it easier to ensure that both research agendas are valued. in the long term, we suspect that the best qa agents will benefit from the insights of user-oriented tasks and the longer-range efforts towards natural language understanding (bender and koller, 2020; linzen, 2020).","{211258645: 'Perez et al., 2020', 237491981: 'Si et al., 2021', 219965751: 'Zeng et al., 2020;'}",https://www.aclanthology.org/2021.emnlp-main.758.pdf
345,233296924,Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation,discussion and conclusion,Insight-tree,"in this work, we develop a synthetic adversarial data generation pipeline for qa, identify the best components, and evaluate on a variety of robustness measures. we propose novel approaches for answer candidate selection, adversarial question generation, and synthetic example filtering and relabelling, demonstrating improvements over existing methods. furthermore, we evaluate the final models on three existing robustness measures and achieve state-of-the-art results on adversarialqa, improved learnability of various comprehension skills for checklist, and improved domain generalisation for the suite of mrqa tasks.",{},https://www.aclanthology.org/2021.emnlp-main.696.pdf
346,252819460,CausalQA: A Benchmark for Causal Question Answering,conclusion,Insight-tree,"we constructed webis-causalqa-22, the first large benchmark dataset of 1.1 million causal questionanswer pairs, which serves to advance research in causal question answering. to ensure diversity of questions, we extracted them using seven hand-crafted high-precision lexical rules to capture as many subtypes of causal questions as possible. these rules were derived from a new typology of causal questions, which in turn is based on relevant related work on question typologies. a manual analysis of a sample of questions was used to characterize causal questions in terms of two dimensions: (1) their semantic properties, i.e., according to which element of the causal structure the question is asked (antecedent, consequent, or the causal chain) and (2) their pragmatic interpretation, i.e., the underlying intention or assumed information need of the questioner (e.g., prevention of medical problems). furthermore, a subsequent analysis of the causal questions contained in a search engine log showed that a significant proportion of 5% of question queries are causal. finally, we evaluated the state-of-the-art model unifiedqa on our corpus as an initial baseline for causal question answering.","{208267807: 'Asai et al. 2020', 196170479: 'Fan et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 237263476: 'Talmor et al., 2021', 52822214: 'Yang et al., 2018'}",NaN
347,218610721,Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction,conclusion,Insight-tree,"we proposed the olp task and a method to create an olp benchmark. we created the large olp benchmark olpbench, which will be made publicly available 4 . we investigated the effect of leakage of evaluation facts, non-relational information, and entity-knowledge during model selection using a prototypical open link prediction model. our results indicate that most predicted true facts are genuinely new. jointly in the context of lp.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.209.pdf
348,254877343,HYRR: Hybrid Infused Reranking for Passage Retrieval,conclusion,Insight-tree,"we proposed a generic training framework for rerankers based on a hybrid retriever. while the hybrid retriever is composed of term-based and neural models, the reranker is a neural cross-attention model which learns from negatives examples generated by the hybrid retriever. the proposed approach is robust and outperforms several strong baselines on ms marco passage ranking task and beir benchmark dataset, which demonstrates that it is practical and generalized. we observe that a model trained with robust training instances (in this case, from the hybrid retriever) produces a reranker that outperforms matched-training rerankers for termbased or neural retrievers.","{86611921: 'Kwiatkowski et al., 2019', 245144556: 'Dai et al., 2022', 233296016: 'Thakur et al., 2021'}",https://export.arxiv.org/pdf/2212.10528v1.pdf
349,247450729,Efficient Long Sequence Encoding via Synchronization,conclusion,Insight-tree,"in this work, we propose transync framework with flexible synchronization mechanisms for encoding long sequences. we demonstrate the feasibility of our method in reasoning tasks with long context, and also show its high adaptability to different scenarios. we consider our work to be valuable as an easy solution to address the long context issue in qa, and to be potentially applicable to other long sequence modeling tasks.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2203.07644v1.pdf
350,252365096,VIMQA: A Vietnamese Dataset for Advanced Reasoning and Explainable Multi-hop Question Answering,conclusion,Insight-tree,"in this work, we propose vimqa, a multi-hop vietnamese qa dataset. it is highly necessary and important to facilitate the development of vietnamese qa models that can perform advanced reasoning and provide explainable answers with supporting facts. then, we also propose a pipeline for collecting multi-hop qa examples that can be generalized for all languages. we also prove the efficiency of our pipeline via the detailed analysis in our vimqa dataset. the experimental results indicate that vimqa is challenging for competitive approaches in both single and multiple hop qa. it reveals that our vimqa dataset is a good resource for vietnamese and cross-lingual qa models, especially in vietnamese multi-hop qa tasks for reasoning and explaining the comprehension and coherence of text understanding.",{},NaN
351,262013357,Investigating Answerability of LLMs for Long-Form Question Answering,conclusion,Insight-tree,"with the emergence of llms like chatgpt and open-source successful llms, it is extremely important to understand the capabilities and limitations of different llms.in order to test deeper reasoning abilities of llms by referring to longer contexts, we evaluate answers generated by llms on questions generated by chatgpt on summaries of long documents.results show that our proposed method of question generation poses a challenging setup for llms and shed light on performance gaps between massive llms and open-source llms.we hope our analysis motivates future research directions such as leveraging longer contexts in a constrained sequence length setting and developing better long-form text generation for smaller llms.","{196170479: 'Fan et al., 2019', 245218982: 'Pang et al., 2022'}",https://export.arxiv.org/pdf/2309.08210v1.pdf
352,258418300,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,conclusion,Insight-tree,"we have proposed ugr, a novel unified generative retriever, which can robustly serve different retrieval tasks for knowledge-intensive language tasks. to unify retrieval tasks, we formulated the retrieval problem as a conditional generation problem and introduced an n-gram-based identifier for relevant contexts at different levels of granularity. to learn different retrieval tasks with a single model, we mapped the descriptions of tasks to a few prompt tokens for keeping task specifications. empirical results on the kilt benchmark demonstrated the superiority of the proposed method. efficiently integrating knowledge from different retrieval tasks in ugr has the potential to save significant time and computational resources in both academic and industrial environments. however, ugr needs a complex scoring function to solve the identifier repetition problem; we encourage future work that explores other effective and efficient semantic identifiers for generative retrieval. beyond kilt, training a more general unified generative retrieval model to serve different retrieval applications under multiple corpora and modalities seems a promising future direction.","{251594672: '6,', 222125277: '8,', 196170479: '[11,', 221507798: '[34]', 52822214: '46]'}",https://export.arxiv.org/pdf/2304.14856v1.pdf
353,207999127,Relation Module for Non-answerable Prediction on Reading Comprehension,conclusion,Insight-tree,"in this work we propose a new relation module that can be applied on any mrc reader and help increase the prediction accuracy on non-answerable questions. we extract high level semantics from multi-head self-attentive pooling. the semantic object pairs are fed into the relation network which makes a guided decision as to whether a question is answerable. in addition we augment the context vector with plausible answers, allowing us to extract objects focused on the proposed answer span, and differentiate from other objects that are not as relevant in the context. our results on the squad 2.0 dataset using the relation module on both bidaf and bert models show improvements from the relation module. these results prove the effectiveness of our relation module.",{},https://www.aclweb.org/anthology/K19-1070.pdf
354,258588167,Unsupervised Dense Re-trieval Training with Web Anchors,conclusion,Insight-tree,"we train an unsupervised dense retrieval model, anchor-dr, leveraging the rich web anchors. in particular, we design a contrastive learning task: anchor-document prediction to continuously pretrain anchor-dr. additionally, we apply predefined rules and train a query classifier to filter out uninformative anchors. experiments on two public datasets: msmarco and beir show that anchor-dr significantly outperforms the state-of-the-art dense retrievers on unsupervised retrieval. our analyses provide a further comparison of the patterns of information contained in our contrastive learning pairs and query-document pairs in test datasets.","{238744204: '[3]', 249097975: '[14]', 233296016: '[24]', 220302524: '[26]', 247793456: '[27]'}",https://export.arxiv.org/pdf/2305.05834v1.pdf
355,264487241,Length is a Curse and a Blessing for Document-level Semantics,conclusion,Insight-tree,"in this work, we questioned the length generalizability of contrastive learning-based text encoders.we observed that, despite their seemingly strong representational power, this ability is strongly vulnerable to length-induced semantic shifts.we formalized length attack, demystified it, and defended against it with la(ser) 3 .we found that, teaching the models ""my longer-self = myself"" provides a standalone semantic signal for more robust and powerful unsupervised representation learning.","{233296016: 'Thakur et al., 2021'}",https://export.arxiv.org/pdf/2310.16193v1.pdf
356,202767252,BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels,conclusion and future work,Insight-tree,"in this paper, we have presented the bipar, a bilingual parallel machine reading comprehension dataset on novels. from bilingual parallel passages of chinese and english novels, we manually created diversified parallel questions and answers of different types via crowdsourced workers with a multi-layer quality control system. although bipar is an extractive mrc dataset, in-  table 6: fine-grained results in terms of different answer types and question categories on the monolingual task. the left side of the slash is the f1 score on the english data, while the right is on chinese. all f1 scores are calculated on the 100 questions as described in section 4.3.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/D19-1249.pdf
357,226262276,Scene Restoring for Narrative Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we focus on narrative machine reading comprehension. inspired by human behaviors, we propose a novel method to restore the scene for the narrative passage. specifically, we introduce the event knowledge from atomic and build a three-dimensional graph to describe the scene. to encode the scene graph, we propose graph dimensional-iteration network (gdin). we conduct experiments on two relevant datasets, rocstories and cosmosqa. the result shows our method achieves state-of-the-art. further experimental investigation shows that (1) compared with concept knowledge, the event knowledge we choose is more suitable for narrative mrc; (2) our proposed graph models the scene more effectively than the unstructured text and the unified plane graph do; (3) our proposed gdin encodes the scene graph efficiently by iterating multiple steps.","{174801080: 'Min et al., 2019'}",https://www.aclweb.org/anthology/2020.emnlp-main.247.pdf
358,256846917,"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",conclusion,Insight-tree,"this work extensively covers research efforts on backdoor learning for nlp. to this end, we systematically and comprehensively survey state-of-the-art research studies on backdoor attacks and defenses. additionally, we thoroughly review and analyze various aspects of backdoor learning, including techniques, model architectures, evaluation metrics, and benchmark datasets. we argue that for backdoor learning to contribute to actual robustness, research studies should take into account an expansive view and strive to answer questions related to why such attacks and defenses are successful. it is crucial to determine whether any given technique is booming due to limitations and weaknesses associated with the target model (inherent incapability arising from intrinsic properties of a target model) or whether it is due to weaknesses or limitations in the dataset itself. finally, we offer insights into open challenges and future research directions worth pursuing.  [98] 2022 gradient-based adversarial attacks amazon, trojanai sentiment analysis generic bert, gpt, lstm, gru azizi et al. [6] 2021 sequence-to-sequence (seq-2-seq) generative model mr, yelp, ag news, hs sentiment analysis, topic classification black-box bert, cnn, lstm chan et al. [14] 2020 conditional adversarially regularized autoencoder snli, yelp, mnli sentiment analysis, nli generic bert, roberta, xlnet chan et al. [16] 2021 backdoor sentence insertion imdb, dbpedia sa, sc, pr grey-box lstm chan et al. [21] 2021 trigger construction imdb, sst-5 sa white-box bert,lstm eger et al. [35] 2019 visual text perturbations imdb, mr sa black-box bert,lstm nguyen et al. [119] 2020 input-aware trigger generator via diversity loss mnist, object recognition sa, sc resnet,lstm acc, asr qi et al. [131] 2021 syntactic trigger-based attack sst-2, ag news, olid sa, sc white-box bert,lstm qi et al. [132] 2021 invisible triggers via learnable combination of word substitution sst-2, ag news, olid sa, sc white-box bert,lstm wallace et al. [159] 2019 gradient guided search over token snli, squad, olid nli black-box bert, gpt-2 yang et al. [180] 2021 poisoned word embeddings sst-2, imdb, snli sa, sc, nli black-box bert yang et al. [182] 2021 negative data augmentation and modifying word embeddings yelp, imdb, twitter sa, toxic detection black-box bert zhang et al. [192] 2021 re-weighted training of language models webtext, twitter toxic detection, qa white-box bert, gpt-2, xlnet zhang et al. [195] 2021 neuron-level backdoor attack olid, gtsrb, sst-2, enron toxic and spam detection, sa black-box bert, roberta, vggnet li et al. [83] 2021 knowledge distillation gtsrb, cfair-10 image recognition black-box resnet, vggnet garg et al. [44] 2020 backdoor injection by adversarial weight perturbation mr, cfair-10 generic black-box wordcnn, lstm chen et al. [19] 2021 task-agnostics label replacement foundation model sst-2, qnli, rte generic white-box bert, gpt-2 gan et al. [39] 2021 triggerless genetic clean-labels sentence generation sst-2, ag news, olid sc, sa black-box bert","{216035859: '[5,'}",https://export.arxiv.org/pdf/2302.06801v1.pdf
359,219171199,A Cognitive Method for Automatically Retrieving Complex Information on a Large Scale,conclusions,Insight-tree,"we present a new framework 2scr-ir sensor to tackle multi-hop retrieval problems on a large scale, which retrieves reasoning paths over the cognitive graph to provide users with useful explicit evidence chains. our retriever model learns to sequentially retrieve evidence paragraphs to construct reasoning paths, which is subsequently re-ranked by the sensor that determines the final information presented as the one extracted from the best reasoning path. our retriever obtains state-of-the-art results using the hotpotqa dataset, which shows the efficiency of our framework. the state-of-theart performance on squad is achieved, demonstrating the robustness of our method. besides, our analysis shows that 2scr-ir can produce reliable and explainable reasoning chains. in the future, we may incorporate new advances in building cognitive graphs from the web context to solve more difficult reasoning problems.","{153312687: '[6]', 189927857: '[7]', 202773198: '[8]', 208267807: '[15]', 155100120: '[16]'}",NaN
360,252212320,SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval,conclusion,Insight-tree,"in this paper, we proposed a novel uni-encoder model, sparse retriever using a dual document encoder (spade), to alleviate the trade-off between effectiveness and efficiency of the ir system.we adopted a dual document encoder for lexical and semantic matching and developed a co-training strategy to mitigate the training intervention between encoders.we also utilized document-and corpus-level pruning during model training, enabling efficient retrieval using the inverted index.experimental results showed that spade achieves state-of-the-art performance among uni-encoder models with acceptable query latency, notably preferable for commercial ir systems.",{220302524: '[67]'},https://export.arxiv.org/pdf/2209.05917v3.pdf
361,233219505,Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews,conclusion,Insight-tree,"we conducted an extensive exploratory study comparing app issue classification tools and pre-trained transformer-based models in various settings. we conducted the experiments on six available datasets and a highly imbalanced dataset, which is a combination of the six datasets. domain-specific ptms were trained using different sizes of app review data we collected from google play and these customized ptms were also studied here. our results confirm that ptms are achieving higher scores in binary and multi-class classification compared to prior approaches, but the over-the-shelf ptms are not always the best models to be used in all scenarios. instead, cptms have the highest scores and are able to perform better than other models in all settings. moreover, incorporating app specific data in the pre-training of ptms reduces the prediction time. one of the future directions of this research is assessing domain-specific ptms in other areas of app reviews and exploring ways to increase performance in zero-shot setting.            ","{212747830: '[86]', 52822214: '[124]', 207870753: '[127]', 102353837: '[130]'}",https://arxiv.org/pdf/2104.05861v3.pdf
362,248218748,Towards Fine-grained Causal Reasoning and QA,conclusion,Insight-tree,"we explored the efficacy of current state-of-the-art methods for causal reasoning tasks by considering a novel fine-grained reasoning setting and developing a dataset with rich human labels. experimental results using the state-of-the-art pre-trained language models provide the evidence that there is much room for improvement on causal reasoning tasks, and a need for designing better solutions to correlation discovery related to event causality analysis and why/what-if qa tasks.","{208201969: 'Sugawara et al., , 2020', 220831004: 'Zaheer et al., 2020'}",https://arxiv.org/pdf/2204.07408v1.pdf
363,215238353,Is Graph Structure Necessary for Multi-hop Reasoning?,conclusions,Insight-tree,"this study set out to investigate whether graph structure is necessary for multi-hop reasoning tasks and what role it plays. we established that with the proper use of pre-trained models, graph structure may not be necessary for multi-hop reasoning. in addition, we point out that the adjacency matrix and graph structure can be regarded as some kind of task-related prior knowledge. we find both graph-attention and graph structure can be replaced by self-attention or transformer.","{160009340: 'Nishida et al., 2019', 155100120: 'Qiu et al. 2019', 158046817: 'Tu et al. 2019b'}",https://arxiv.org/pdf/2004.03096v1.pdf
364,218502712,Query Reformulation using Query History for Passage Retrieval in Conversational Search,conclusion,Insight-tree,"we present hqe and ntr, both conversational query reformulation methods stacked on a successful multi-stage ir pipeline. the effectiveness of our methods are attested by experiments on the cast benchmark dataset, the results of which suggest that the two methods have different advantages in fusing context information into conversational user utterances for downstream ir models. finally, this work elevates the state of the art in cast benchmarks and provides simple but effectives baselines for future research.",{52822214: '[58]'},https://arxiv.org/pdf/2005.02230v1.pdf
365,233033850,Discrete Reasoning Templates for Natural Language Understanding,conclusion,Insight-tree,"we propose using reasoning templates for tackling reading comprehension tasks that involve reasoning over multiple paragraphs. we show that this approach is competitive with state of the art models on a subset of drop's subtraction questions, while requiring much less training data and providing better visibility of the model's decision making. in future work, we plan on extending to further templates and investigate how to learn templates instead of working from a predefined set.","{67855846: 'Dua et al., 2019', 174801080: 'Min et al., 2019', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/2021.eacl-srw.12.pdf
366,202773198,Answering Complex Open-domain Questions Through Iterative Query Generation,conclusion,Insight-tree,"in this paper, we presented golden (gold entity) retriever, an open-domain multi-hop question answering system for scalable multi-hop reasoning. through iterative reasoning and retrieval, golden retriever greatly improves the recall of gold supporting facts, thus providing the question answering model a much better set of context documents to produce an answer from, and demonstrates competitive performance to the state of the art. generating natural languages queries for each step of reasoning, golden retriever is also more interpretable to humans compared to previous neural retrieval approaches and affords better understanding and verification of model behavior. we start from the wikipedia dump file containing the introductory paragraphs used in hotpotqa that yang et al. (2018) provide, 10 and add the fields corresponding to wikipedia page titles and the introductory paragraphs (text) into the index.",{52822214: 'Yang et al. 2018'},https://www.aclweb.org/anthology/D19-1261.pdf
367,159248505,Evidence Implementation Plan,conclusion,Insight-tree,the research aims to reach a point where a benefit for change is actually happening at the end of the research's period.this could be measured through the tools that are mentioned above.thus if (pm-em) was negative and a shift in the training procedures and the management's policies had an impact on service n. saadeh,{},NaN
368,248798649,TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,conclusion & future work,Insight-tree,"in this paper, we proposed a tag-level qa model called tie to better understand the topological information contained in the structured web pages. our model explicitly captures two of the most informative topological structures of the web pages, logical and spatial structures, by dom trees and npr graphs, respectively. with the proposed twostage pipeline, we conduct extensive experiments on the websrc dataset. our tie successfully achieves sota performances and the contributions of its key components are validated.","{224803601: 'Chen et al., , 2021b', 215785913: 'Chen et al., 2020c', 231698419: 'Chen et al. 2021c', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.naacl-main.132.pdf
369,234679223,QAConv: Question Answering on Informative Conversations,conclusion,Insight-tree,"qaconv is a new dataset that conducts qa on informative conversations such as emails, panels, and channels. we show the unique challenges of our tasks in both chunk mode with oracle partial conversations and full mode with a retrieval stage. we find that state-of-the-art qa models have limited dialogue understanding and tend to predict our answerable qa pairs as unanswerable. we provide a new testbed for qa on conversation tasks to facilitate future research.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2105.06912v2.pdf
370,258762903,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader *,conclusions,Insight-tree,"this work presents a novel mrc-style pre-training model called pmr.pmr can fully resolve the learning objective and model architecture gaps that frequently appear in fine-tuning existing mlms.experimental results from multiple dimensions, including effectiveness in solving few-shot tasks and ood generalization, show the benefits of bridging the gap between pre-training and fine-tuning for span extraction tasks.pmr also shows promising potential in explaining the sequence classification process and unifying nlu tasks.","{208267807: '[2,', 237420912: '[6,', 67855846: '[13]', 204823992: '[15]', 202542881: '18,', 198229624: '[19]', 230433978: '45]', 235485084: '50]', 52822214: '[66]', 247793456: '[67]', 248572452: '68]'}",https://export.arxiv.org/pdf/2212.04755v3.pdf
371,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,nodes via discourse unit delimitation,Insight-tree,"it is studied that clause-like text spans delimited by discourse relations can be discourse units that reveal the rhetorical structure of texts [34], [41]. we further observe that such discourse units are essential logical propositions in logical reasoning, such as premise or conclusion. as the example shown in figure 3, the ""while"" in the passage indicates a comparison between the attributes of the fig. 2. the discourse-aware graph networks (dagns) pipeline mainly consists of (1) logic graph construction (2) logic representation learning. the logic graph construction module takes a logical qa data point as input and constructs logic graphs. the logic representation learning module then performs graph reasoning upon the constructed logic graphs. besides, the encoder provides fundamental embeddings for the pipeline.""analog system"" and that of the ""digital system"". the ""because"" in the option uncovers that ""error cannot occur in the emission of digital signals"" as a premise to the conclusion ""digital systems are the best information systems"".this observation is agreed with informal logic theories [35], [36], which study uncovering logical structure from the texts and have conventional in-line logical indicators. for example, acknowledged premise indicators include ""since"", ""because"", ""given that"". conclusion indicators include ""therefore"", ""so"", ""consequently"", and so forth. most of these indicators are discourse connectives. some discourse parsers [42], [43] perform discourse unit segmentation. however, discourse parsing is still challenging, and the parsers are not general to new data, such as logical reasoning questions. for example, segbot [43] is good on the rst-dt dataset but does not work well on the standardized exam texts as in the reclor dataset. thus, we customize discourse unit delimitation strategy for logical texts.we use the penn discourse treebank (pdtb 2.0) [34] to help draw discourse connectives. pdtb 2.0 contains discourse relations that are manually annotated on the 1 million wall street journal (wsj) corpus and are broadly characterized into ""explicit"" and ""implicit"" connectives. the former ones are explicitly present in sentences such as discourse adverbial ""instead"" or subordinating conjunction ""because"", whereas the latter ones are inferred by pdtb annotators between successive pairs of text spans split by punctuation marks such as ""."" or "";"". we take all the ""explicit"" connectives as well as common punctuation marks to form our discourse-aware delimiter library, presented in table 1. each logical text is split into elementary discourse units (edus) by all the delimiters in the library. the edus are taken as graph nodes v.nodes with topic-related terms. the desired key terms are those real nouns or phrases that repeatedly appear in the text. such nouns or phrases are instantiations of logical variables in propositions. as a result, replacing such terms with abstract variables or terms in other topics does not change the process of reasoning. for example, in figure 3, the first two sentences indicate a comparison of ""signal"" between ""analog system(s)"" and ""digital system(s)"". performing abstraction by replacing ""signal"" with variable Î³, ""analog system(s)"" with variable Î±, and ""digital system(s)"" with variable Î², the propositions are free from the topic of electronics, but the comparison relation is retained.we use a sliding window to collect the recurring phrases. given the input logical text, stemming is first applied to handle morphological diversity. then, the sliding window loops over ngrams and records the reoccurrence. next, all the stop words and overlapped substrings are filtered. the resulting topic-related terms are attached to the nodes according to which text segment they belong.binary node types. the text of logical reasoning qa consists of two possible structures: (passage, question, options) or (dialogue context, candidate responses). we regard passage or dialogue context as context texts that carry the main logical reasoning structure, whereas regard (question, options) or candidate responses as candidate texts that are added to the context texts and should remain their logical consistency.according to the discourse unit delimitation, the graph nodes are naturally from the context texts or the candidate texts. therefore, we define two disjoint and independent node sets: context node set v u and candidate node setv v . v u âªv v = v and v u â©v v = â.the interplay between the two node sets formulates logical consistency between the context and the candidate texts.","{232380161: '[1]', 220483148: '[3]', 155100120: '11', 207853300: '[12]', 222208994: '[13]', 52822214: '[15]', 215768766: '[32]', 67855846: '[65]'}",https://export.arxiv.org/pdf/2207.01450v2.pdf
372,250264874,Discourse-Aware Graph Networks for Textual Logical Reasoning,conclusion,Insight-tree,this paper explores a structure-based solution to textual logical reasoning that explicitly models the logical reasoning process. the challenges include: (1) uncovering the inference structure from plain texts for effective structural constraints. (2) learning the inference processes rather than the knowledge for effective logical reasoning.,"{232380161: '[1]', 220483148: '[3]', 155100120: '11', 207853300: '[12]', 222208994: '[13]', 52822214: '[15]', 215768766: '[32]', 67855846: '[65]'}",https://export.arxiv.org/pdf/2207.01450v2.pdf
373,221761662,On the Transferability of Minimal Prediction Preserving Inputs in Question Answering,conclusion,Insight-tree,"we empirically verify the surprising invariance of mppis to random seed, model architecture, and pretraining, as well as their wide transferability across domains. these results suggest that mppis may not be best explained by poorly calibrated neural estimates of confidence or dataset-specific bias. examining their relationship to generalization and adversarial robustness, we highlight the ability to maintain in-domain performance but significantly alter out-domain performance and robustness. we hope our results encourage a more systematic analysis of hypotheses regarding model behavior outside the human interpretable distribution of examples.  (2017), we borrowed the implementation and hyper-parameters from hitvoice (https://github.com/hitvoice/drqa) and train on 1 nvidia tesla v100 gpu. 8",,https://www.aclweb.org/anthology/2021.naacl-main.101.pdf
374,248299683,STANDING ON THE SHOULDERS OF GIANT FROZEN LANGUAGE MODELS,conclusions,Insight-tree,"while fine-tuning huge lms can often yield excellent performance, this approach is expensive at training time, requires serving a plethora of models at runtime, and provides poor adaptability in the face of variations in the targeted task. this paper has shown that a better alternative exists: freezing a single, huge pretrained lm and learning much smaller neural modules that specialize the lm to different tasks. while prompt tuning, prefix tuning, and other existing frozen model methods cited above can be seen as a simple instantiations of this idea, this paper shows that much more complex architectures can achieve much stronger performance.","{86611921: 'Kwiatkowski et al., 2019'}",https://arxiv.org/pdf/2204.10019v1.pdf
375,237485098,Zero-Shot Dialogue State Tracking via Cross-Task Transfer,conclusion,Insight-tree,"in this paper, we present transferqa, a unified generative model that performs dst without using any dst training data. transferqa uses the textto-text transfer learning framework that seamlessly combines extractive qa and multi-choice qa for tracking both categorical slots and non-categorical slots. to enable our model to zero-shot ""none"" value slots, we introduce two effective ways to construct unanswerable questions, i.e., negative question sampling and context truncation. the experimental results on the multiwoz and sgd datasets demonstrate the effectiveness of our approach in both zero-shot and few-shot settings. we also show that improving the ""none"" value slot accuracy has","{204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2109.04655v1.pdf
376,259361037,MultiVENT: Multilingual Videos of Events with Aligned Natural Text,conclusion,Insight-tree,"we introduce multivent, a multimodal, multilingual dataset grounded in natural language documents for event-centric video retrieval and information acquisition. this dataset consists of 2,396 videos covering 260 current events reported in five target languages (arabic, chinese, english, korean, and russian) paired with multilingual natural language video descriptions and long-form event-centric text documents. we use this dataset to characterize online news coverage and how models can use this online content for information acquisition. we propose a multilingual video retrieval benchmark using multivent and present multiclip, multilingual video retrieval model to serve as a baseline for the task. we evaluate this model and related retrieval approaches on msr-vtt and multivent to illustrate the importance of pretraining on multilingual data for evaluation on multivent. in future work, we aim to explore the effect that joint vision-ocr embeddings can have on video retrieval in text-heavy contexts. also in future work, a repaq-adjacent system [25] for automatically extracting question-answer pairs from video content and video-document pairs could be developed and applied to multivent. through this, a framework for teaching models to perform open-domain question-answering tasks with multimodal background corpora could be established, expanding the domain of questions a model can answer.","{86611921: '22,', 235364000: '29,'}",https://export.arxiv.org/pdf/2307.03153v1.pdf
377,257985191,CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval,conclusion,Insight-tree,"this paper proposes a multi-view contextual masked auto-encoding pre-training architecture for better passage retrieval. experiment results show that multi-view representation and multi-view decoding paradigms significantly contribute to effec-tive retrieval performance. our method also shows good robustness and stability. in the future, we will further explore incorporating new pre-training paradigms to get more effective and robust retrievers.","{232147859: 'Guo et al., 2022;'}",https://export.arxiv.org/pdf/2304.03158v1.pdf
378,250719623,Evidence Prediction Method Based on Sentence Selection for Legal Documents,conclusion,Insight-tree,"for legal documents with clear structure and rigorous expression, it is helpful to improve human work efficiency to let machines understand and read legal documents. e purpose of reading comprehension in the legal field is to train the machine model through legal documents so that it can answer various questions according to the given case description. an excellent reading and understanding system in the legal field can assist judges, lawyers, and other professionals in their work and also make it easy for people to understand the basic situation of each case. it has a wide range of application prospects, such as crime prediction, evidence prediction, legal provisions recommendation, and intelligent court trial. is paper mainly studies the evidence prediction in the legal field. taking the prediction of reading and understanding evidence in the legal field as the research task, this paper puts forward a prediction method of evidence based on sentence selection for legal documents. a sentence selection module is designed to remove irrelevant sentences, and questions and answers are used to infer evidence, which has achieved good results. rough experiments, it is found that the score of joint f1 proposed in this paper is 70.07%, which is more accurate than the mainstream model.",{},https://downloads.hindawi.com/journals/am/2022/1926347.pdf
379,248227284,TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval,conclusion,Insight-tree,"we introduce a method to train bi-encoders on unstructured text and knowledge graph types through a type-enforced contrastive loss. our loss can improve retrieval of rare entities for ambiguous mentions, while maintaining strong overall performance on open-domain nlp tasks. we hope our work inspires future work on integrating structured data into pretrained models. s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon. any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of nih, onr, or the u.s. government.",,https://www.aclanthology.org/2022.findings-acl.169.pdf
380,254069686,Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering,conclusion,Insight-tree,we first showed that entropy maximization often fails to transfer to unseen perturbation types. maximizing the entropy terms for various types of perturbations is effective in mitigating this problem. the failure of entropy maximization to improve out-of-distribution generalization may be caused by the unnaturalness of the perturbed inputs. modifying the perturbation functions to effectively improve outof-distribution generalization is future work.,"{204823992: 'Fisch et al. 2019', 208201969: 'Sugawara et al. , 2020', 173188058: 'Szegedy et al. 2013;'}",https://export.arxiv.org/pdf/2211.16093v1.pdf
381,252819220,KHANQ: A Dataset for Generating Deep Questions in Education,conclusions and future works,Insight-tree,"in this paper, we propose khanq, a dataset for generating in-depth educational questions. each sample in khanq is carefully annotated as context, prompt, and question to form a clean dataset. we evaluate the performance of state-of-the-art question generation models on khanq. we find that although it is feasible for the model to generate fluent and complex questions, the ability to understand and reason over the context and the prompt is still far from reaching the human level.","{235678938: 'Cao and Wang, 2021', 226236844: 'Xie et al., 2020;'}",https://www.aclanthology.org/2022.coling-1.518.pdf
382,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,"this paper presented a unified web-augmented framework for a wide range of knowledge-intensive tasks, called uniweb. we convert 16 tasks into a text-to-text generation task for training. we propose a search engine assisted learning method to selectively retrieve documents from the web through google search. furthermore, to reduce the discrepancy between the encoded and retrieved knowledge, we design a pre-training task, i.e., continual knowledge learning, to integrate the retrieved knowledge into llms. experiments on 16 tasks show the effectiveness of our web-augmented model compared to previous retrieval-augmented models. in future work, we will investigate the effect of web content in detail and consider applying our model to more types of downstream tasks.","{244478674: 'Aribandi et al., 2022'}",https://export.arxiv.org/pdf/2305.10998v2.pdf
383,258762384,Beijing Key Laboratory of Big Data Management and Analysis Methods,limitations,Insight-tree,"for web-augmented models including our work, the deterioration of search results from search engine highlights the importance of deriving an effective method to interact with the huge web.search engines are often perceived as black-box and non-transparent for end users. therefore, many works proposed ""leaning to search"" to decompose complex questions into simpler queries, which may improve the performance of web-based models (nakano et al., 2021;komeili et al., 2021). in our model, we used a commercial search engine as the retriever to work with the whole web as a knowledge source. since the web is not curated and well-structured like wikipedia, we may encounter unexpected safety issues, including misinformation and harmful contents. while we have relied on the security control of the search engine, more attention should be paid to better understand the risks and provide effective ways to mitigate them. we hope our simple approach and strong results could encourage more future work by the community to tackle these questions. to encourage the community to investigate the question and ensure reproducibility, after the reviewing process, we will release the search urls used in our experiments.as for the potential concern, since we use the search engine to access real-time information, we do not have a tight control over retrieved results as traditional end-to-end retrieval (guu et al., 2020;lewis et al., 2020b). not only the changes of search engine logic, but also the newly published information, might create discrepancies over the course of time. this is also an issue we have to tackle to build a stable web-based solution for llms. â¢ commonsense reasoning is intended to utilize commonsense knowledge to reason about certain aspects of the given text . therefore, we consider the given text as input and the prediction as output.â¢ natural language inference is the task of determining whether the given ""hypothesis"" logically follows from the ""premise"" (storks et al., 2019).it acquires deep knowledge about the relationship between hypothesis and premise. we consider the premise as input and the hypothesis as output.for each category, we choose several representative tasks to construct our pretraining corpus. the detailed information of these included tasks is listed in table 6. to mitigate the huge disparity between dataset sizes, we follow  to use the temperature-scaled mixing strategy with a rate of t = 2 for setting the proportion of data coming from each task. during pretraining, for each task example, we use bm25 to retrieve top-10 passages from ccnet as our external knowledge. the input texts are concatenated with the retrieved passages using manually-written prompts. the final input is constructed in the following format: the ""option"" string is applied only when the input text is provided with several candidate answers. the blanks ""[passage n ]"" and ""[option n ]"" is filled with the retrieved passages and candidate answers. the blank ""[task instruction]"" aims to indicate the task for our model, which is task-specific and detailed in table 7.","{244478674: 'Aribandi et al., 2022'}",https://export.arxiv.org/pdf/2305.10998v2.pdf
384,256231186,An Experimental Study on Pretraining Transformers from Scratch for IR,conclusion,Insight-tree,"foundation models come with the promise to be highly general and modular. it is believed that they contain a wide ""knowledge"" due to their pretraining on a large collection, which is then believed to be the source of their improved performance. we have examined how this pretraining collection influence the performance of ir models. our research question was to assess how much of this implicit knowledge, beneficial to the final performance, comes from pretraining on a large external collection. this is why we have experimented on a variety of collections, domains and languages to study how pretraining from scratch actually performed compared to their de facto approach of simple finetuning. while we were expecting the standard pretrained models to work better, we surprisingly revealed that pretraining from scratch works better for first-stage retrieval on msmarco, tripclick and several non-english languages on the mr.  tydi benchmark. in particular, the flops regularization played a critical role in those results, suggesting that regularization or better pretraining techniques could further improve the results. furthermore, pretrained models from scratch also behave well in the zero shot scenario for sparse models such as splade. nevertheless, pretraining from a large collection has a slight advantage when training rerankers. overall, these results, specific to ir, challenge the foundation model hypothesis for small models, ie that a more general model encapsulating the world knowledge would be better than a smaller one in a specific domain application. furthermore, our study makes a contribution to the debate between general purpose and specific purpose models. in a way, our experiments showed that less is more. in addition, pretrained language models come also with many challenges such as the societal bias in the data they have been trained on. we hope that our study could convince practitioners, both from industry and academia, to reconsider specific purpose models by pretraining from scratch. last but not least, doing so enable to better control efficiency, data bias and replicability, which are key research questions for the ir community.","{248665596: '[13,', 220302524: '[53,'}",https://export.arxiv.org/pdf/2301.10444v1.pdf
385,252199900,Domain Adaptation for Question Answering via Question Classification,conclusion,Insight-tree,"in this paper, we propose a novel framework for qa domain adaptation. the proposed qc4qa combines question classification with self-supervised adaptation techniques. qc4qa leverages question classes to reduce domain discrepancies and resemble target data distribution in training. different from existing works, qc4qa achieves superior performance by introducing a simple question classifier and incorporating the question class information in the training objective. we demonstrate the efficiency and effectiveness of qc4qa compared to state-of-the-art approaches by achieving a substantially better performance on multiple datasets.",{},https://www.aclanthology.org/2022.coling-1.153.pdf
386,86611921,Natural Questions: A Benchmark for Question Answering Research,conclusion,Insight-tree,"we argue that progress on qa has been hindered by a lack of appropriate training and test data. to address this, we present the natural questions corpus. this is the first large publicly available data set to pair real user queries with high-quality annotations of answers in documents. we also present metrics to be used with nq, for the purposes of evaluating the performance of question answering systems. we demonstrate a high upper bound on these metrics and show that existing methods do not approach this upper bound. we argue that for them to do so will require significant advances in nlu. figure 5 shows example questions from the data set. figure 6 shows example question/answer pairs from the data set, together with expert judgments and statistics from the 25-way annotations.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/Q19-1026.pdf
387,231709861,Benchmarking Machine Reading Comprehension: A Psychological Perspective,conclusion,Insight-tree,"in this paper, we outlined current issues and future directions for benchmarking machine reading comprehension. we visited the psychology study to analyze what we should ask of reading comprehension and the construct validity in psychometrics to analyze how we should correctly evaluate it. we deduced that future datasets should evaluate the capability of the situation model for understanding context-dependent situations and for grounding to non-textual information and ensure the substantive validity by creating shortcut-proof questions and designing an explanatory task formulation. ","{213474484: 'Rogers et al. 2020', 212644640: 'Schlegel et al. 2020', 208201969: 'Sugawara et al. 2020', 52822214: 'Yang et al. 2018'}",https://www.aclweb.org/anthology/2021.eacl-main.137.pdf
388,231632613,Understanding in Artificial Intelligence,conclusion,Insight-tree,"the paper considers the components an artificial intelligence system that understands should have. that is, a system that not only learns statistical relationships within the data, but is capable of forming a human-like understanding â â â â table 1: cap1: hierarchical and compositional knowledge representation, cap2: multimodal structure-to-structure mapping, cap3: integrates symbolic and non-symbolic knowledge, cap4: supports symbolic reasoning with uncertainties of the input data. this is most certainly a truly difficult problem to solve and the purpose of the paper is not to claim a general solution for solving it, but to look at several research streams and some of their latest developments. furthermore, several benchmarks are described, which have been used to study certain characteristics of an ai that understands. the work also contributes to a growing interest in artificial intelligence systems that are interpretable and transparent; properties that are crucial in domains, such as medical diagnosis.",{},https://arxiv.org/pdf/2101.06573v1.pdf
389,257913786,QUADRo: Dataset and Models for QUestion-Answer Database Retrieval,conclusion,Insight-tree,"in this paper, we have described our study to scale qa-based on q/a db to open domain applications. this required to build a large db, which we built only using publicly available q/a pairs, reaching a significant size of â 6.3m items. to enable retrieval from these large dbs, inspired by the latest neural ir technology, we modeled neural retrieval for q/a pairs. we proposed two different methods based on only questions, and on q/a pairs, where questions and answers can be seen as context. we analyzed the significant impact of using architectures with separate encoder versus dual encoders, in accuracy and efficiency.","{86611921: 'Kwiatkowski et al., 2019'}",https://export.arxiv.org/pdf/2304.01003v1.pdf
390,252734741,Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt,conclusion,Insight-tree,"in this paper, we introduce rospr, a method that efficiently enhances zero-shot generalization capabilities of a meta-trained lm by retrieving promptspecific source prompt embeddings (soft prompts) for a given target task.we accomplish this by first training the soft prompts for hard prompt of the source tasks.after training source prompt embeddings, we construct the source prompt library by storing the mean representation of training instances as keys and the corresponding prompt embeddings as values.at inference, we search for training instances stored in the library similar to sample instances from the target task, retrieve the corresponding prompt embedding, select the most frequently retrieved embedding, and append it to each of the target task instances for prediction.our results show that rospr efficiently enhances the zero-shot performance of the backbone model while introducing minimal additional parameters during inference.we additionally provide analysis of which factors attribute to the performance of rospr and find that heuristic cues such as the answer choice format are critical for generalization performance, implying that it may play a role similar to demonstrations in in-context learning.","{211010520: 'Bartolo et al., 2020', 201058633: 'Lin et al., 2019', 207756753: 'Nie et al., 2020', 221507798: 'Petroni et al., 2021', 213474484: 'Rogers et al., 2020', 239009558: 'Vu et al. 2022'}",https://export.arxiv.org/pdf/2210.03029v4.pdf
391,252734741,Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt,limitations,Insight-tree,"although we show the effectiveness of rospr by applying it on t0-3b (sanh et al., 2021), we did not evaluate our method on different model scales such as the t0-11b variant and other lm architectures such as decoder-only lms due to limited computational resources.this leaves future works on applying rospr to even larger lms and diverse lm architectures (wang et al., 2022a).moreover, it is hard to apply var to target tasks without answer choices such as free-form generation because variance among options cannot be obtained.however, rospr and rospr+inter can still be utilized and we leave applying rospr on zero-shot task location of free-form generation as future work (scialom et al., 2022).","{211010520: 'Bartolo et al., 2020', 201058633: 'Lin et al., 2019', 207756753: 'Nie et al., 2020', 221507798: 'Petroni et al., 2021', 213474484: 'Rogers et al., 2020', 239009558: 'Vu et al. 2022'}",https://export.arxiv.org/pdf/2210.03029v4.pdf
392,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,limitations of retrieval,Insight-tree,"our focus in this paper has been on recognizing valid chains of reasoning, assuming a retrieval step that retrieves a reasonable pool of candidates to start with (section 3.2). however, the retrieval step itself is not perfect: for qasc, designed so that at least one valid chain always exists, the retrieved pool of 10 contains no valid chains for 24% of the questions (upper bound in table 2), capping the overall system's performance. to gauge the performance of our model when coupled with an improved retrieval system, we ran an experiment where, at test time, we explicitly add the gold chain to the candidate pool if it does not get retrieved (and even if there is some other valid chain already in the pool). we find the p@1 score rises from 0.54 (table 2) to 0.82 (upper bound is now 1.0). this indicates the model scoring algorithm is performing well, and that improving the retrieval system, e.g., by considering may more chains per question or modifying the search algorithm itself, is likely to have the biggest impact on improving the overall system. note also that the corpus itself is an important component: finding valid chains requires the corpus to contain a broad diversity of general facts to build chains from, hence expanding/filtering the corpus itself is another avenue for improvement.","{204915921: 'Khot et al., 2020', 218487030: 'Ye et al., 2020;'}",https://arxiv.org/pdf/2010.03274v1.pdf
393,222178328,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,summary and conclusion,Insight-tree,"explaining answers to multihop questions is important for understanding why an answer may be correct, but there is currently a dearth of suitable, annotated data. to address this, and promote progress in explanation, we contribute three new explanation datasets, including one with over 98k annotated reasoning chains -by far the largest repository of annotated, corpus-derived explanations to date. we also have shown this data can significantly improve explanation quality on both in-domain (qasc) and out-of-domain (obqa) tasks. finally, we have proposed and explored using a lightweight method to achieve a delexicalized representation of reasoning chains. while preserving explanation quality (despite removing details), this representation appears to be more robust to certain perturbations.","{204915921: 'Khot et al., 2020', 218487030: 'Ye et al., 2020;'}",https://arxiv.org/pdf/2010.03274v1.pdf
394,4697510,An Interoperable System toward Cardiac Risk Stratification from ECG Monitoring,conclusions,Insight-tree,"close cooperation with cardiologists was assessed to define the chosen cardiac risk subdomains. in the long term, the system will allow one to not only include other indices but also work on a large scale of a number of patients. achieving multicentric connectivity in different hospitals will represent an increase in the scale of collected data. this approach paves the way toward intelligent systems that will take advantage of the current state of knowledge and advances on machine learning models and big data systems so that the crs in large patient databases can be analyzed.",{},NaN
395,258841283,Query Rewriting for Retrieval-Augmented Large Language Models,conclusion,Insight-tree,"this paper introduces the rewrite-retrieve-read pipeline, where a query rewriting step is added for the retrieval-augmented llm.this approach is applicable for adopting a frozen large language model as the reader and a real-time web search engine as the retriever.further, we propose to apply a tuneable small language model the rewriter, which can be trained to cater to the frozen retriever and reader.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.14283v3.pdf
396,258865893,Getting MoRE out of Mixture of Language Model Reasoning Experts,conclusion,Insight-tree,"we proposed the more framework where we construct a pool of specialized qa models that excel at different reasoning types, and then train an answer selector to select the best answer among them.experiments on 12 datasets covering four reasoning types demonstrate that more achieve better generalizability than all baselines.more importantly, the inter-expert agreement features in more offer useful signals for training effective calibrators that improve selective qa and also improve human verification of the system's final predictions.","{204823992: 'Fisch et al., 2019', 238198206: 'Friedman et al. 2021', 237513496: 'Garg and Moschitti 2021', 219721462: 'Kamath et al. 2020', 204915921: 'Khot et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 240288835: 'Min et al., 2022', 244896105: 'Puerto et al. 2023', 240288953: 'Qi et al., 2020', 236447339: 'Rogers et al., 2021', 252917981: 'Si et al. 2023a', 249062754: 'Chen et al., 2022', 237263476: 'Talmor et al., 2021', 236771976: 'Trivedi et al., 2021', 247595263: 'Wang et al. 2023', 248227734: 'Xie et al. 2022', 52822214: 'Yang et al., 2018', 238856959: 'Ye and Durrett, 2021;', 235313893: 'Zhang et al., 2021'}",https://export.arxiv.org/pdf/2305.14628v2.pdf
397,220831004,Big Bird: Transformers for Longer Sequences,limitations,Insight-tree,"we demonstrate a natural task which can be solved by the full attention mechanism in o(1)layers. however, under standard complexity theoretic assumptions, this problem requires Ï(n)-layers for any sparse attention layers withÃµ(n) edges (not just bigbird). (hereÃµ hides poly-logarthmic factors.)consider the simple problem of finding the corresponding furthest vector for each vector in the given sequence of length n. formally, task 1. given n unit vectors {u 1 , . . . , u n }, find f (u 1 , . . . , u n ) â (u 1 * , . . . , u n * ) where for a fixed j â [n], we define j * = arg max k u k â u j 2 2 . finding vectors that are furthest apart boils down to minimize inner product search in case of unit vectors. for a full-attention mechanism with appropriate query and keys, this task is very easy as we can evaluate all pair-wise inner products.the impossibility for sparse-attention follows from hardness results stemming from orthogonal vector conjecture(ovc) [1,2,7,97]. the ovc is a widely used assumption in fine-grained complexity. informally, it states that one cannot determine if the minimum inner product among n boolean vectors is 0 in subquadratic time. in app. c, we show a reduction using ovc to show that if a transformer g â t h=1,m=2d,q=0 d for any sparse directed graph d can evaluate the task 1, it can solve the orthogonal vector problem. proposition 1. there exists a single layer full self-attention g â t h=1,m=2d,q=0 that can evaluate task 1, i.e. g(u 1 , ..., u n ) = [u 1 * , . . . , u n * ], but for any sparse-attention graph d with o(n) edges (i.e. inner product evaluations), would requireÏ(n 1âo(1) ) layers.we give a formal proof of this fact in app. c.","{198229624: '[43]', 86611921: '[53]'}",https://arxiv.org/pdf/2007.14062v1.pdf
398,220831004,Big Bird: Transformers for Longer Sequences,conclusion,Insight-tree,we propose bigbird: a sparse attention mechanism that is linear in the number of tokens.,"{198229624: '[43]', 86611921: '[53]'}",https://arxiv.org/pdf/2007.14062v1.pdf
399,260164780,WE BAR E N A : A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS,conclusion,Insight-tree,"we present webarena, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents.webarena includes fully functional web applications and genuine data from four major categories, providing a realistic platform for agent interaction.it further supports a wide range of tools and external knowledge bases, fostering a focus on human-like problem-solving.additionally, we curate a comprehensive benchmark consisting of 812 examples that focus on translating high-level natural language intents into specific web interactions.we also offer metrics to programmatically ascertain whether tasks have been completed according to the desired objectives.our experiments show that even gpt-4 only achieves a limited end-to-end task success rate of 14.41%, significantly lagging behind the human performance of 78.24%.these findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within webarena environment.","{86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2307.13854v3.pdf
400,215828216,ETC: Encoding Long and Structured Data in Transformers,conclusions,Insight-tree,"this paper introduced the extended transformer construction, or etc, a novel extension of the original transformer model designed specifically to (1) scale up the input length to sequences longer than 512 tokens (scaling linearly in the size of the input), and (2) allow ingesting structured inputs. etc also allows lifting weights from existing bert models, saving significant computational resources while training. the key ideas that enable etc to achieve these are a new global-local attention mechanism, coupled with relative position encodings.","{86611921: 'Kwiatkowski et al., 2019'}",https://arxiv.org/pdf/2004.08483v2.pdf
401,196170479,ELI5: Long Form Question Answering,conclusion,Insight-tree,"we introduce the first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers. we show that abstractive models generate coherent answers and are competitive with extractive models in human evaluation. proposed models are far from human performance, in part due to the inability to exploit the long full web text. we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers.",{},https://arxiv.org/pdf/1907.09190v1.pdf
402,209202200,Published as a conference paper at ICLR 2020 NEURAL MODULE NETWORKS FOR REASONING OVER TEXT,conclusion,Insight-tree,"we show how to use neural module networks to answer compositional questions requiring symbolic reasoning against natural language text. we define probabilistic modules that propagate uncertainty about symbolic reasoning operations in a way that is end-to-end differentiable. additionally, we show that injecting inductive bias using unsupervised auxiliary losses significantly helps learning.","{67855846: 'Dua et al., 2019', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/1912.04971v2.pdf
403,254854129,Visconde: Multi-document QA with GPT-3 and Neural Reranking,conclusion,Insight-tree,"this paper describes a system for multi-document question answering that uses a passage reranker to retrieve documents and large language models to reason over them and compose an answer. our system rivals state-of-the-art supervised models in three datasets: iirc, qasper, and strategyqa. our results suggest that using gpt-3 as a reader is close to human-level performance as long as relevant passages are provided, while current retrievers are the main bottleneck. we also show that inducing the model to give explanations before answering a question improves effectiveness.","{189927857: '6', 226262208: '[7]', 230799347: '[9]', 211258645: '23,', 52822214: '[36,'}",https://export.arxiv.org/pdf/2212.09656v1.pdf
404,209063721,Memory Graph Networks for Explainable Memory-grounded Question Answering,conclusions,Insight-tree,"we introduce episodic memory qa, the task of answering personal user questions grounded on memory graph (mg). the dataset is generated with synthetic memory graphs with simulated attributes, and accompanied with 100k qa pairs composed via bootstrapped scripts and manual annotations. several novel model components are proposed for unique challenges for the episodic memory qa: 1) memory graph networks (mgn) extends the conventional memory networks by enabling dynamic expansion of memory slots through graph traversals, which also naturally allows for explainable predictions. 2) several neural module networks are proposed for the proposed task, each of which takes queries and memory graphs as input to infer answers. 3) the main episodic memory qa net aggregates answer prediction from each neural module to generate final answer candidates. the empirical results demonstrate the efficacy of the proposed model in the memory qa reasoning.",{},https://www.aclweb.org/anthology/K19-1068.pdf
405,252531182,Evaluation of Question Answering Systems: Complexity of judging a natural language,conclusion,Insight-tree,"the question-answering task is among the oldest challenges in artificial intelligence and it is still one of the most important tasks in natural language processing to this day as it enables humans to interact with a machine in a natural way. for dealing with domain-specific information and different formats of data which could be either structured or unstructured data there are three main paradigms for constructing qa systems: (1) information retrieval-based question answering, (2) knowledge base question answering, and (3) generative question answering.","{86611921: '[111]', 212657414: '[112]'}",https://export.arxiv.org/pdf/2209.12617v1.pdf
406,202785879,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,conclusion,Insight-tree,"we introduced rocc, a simple unsupervised approach for selecting justification sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer. we coupled this method with a state-of-the-art bert-based supervised question answering system, and achieved a new state-ofthe-art on the multirc and arc datasets among approaches that do not use external resources during training. we showed that rocc-based qa approaches are more robust across domains, and generalize better to other related tasks like entailment. in the future, we envision that rocc scores can be used as distant supervision signal to train supervised justification selection methods.",{},https://www.aclweb.org/anthology/D19-1260.pdf
407,221640701,Accelerating Real-Time Question Answering via Question Generation,conclusion,Insight-tree,"in this paper, we present ocean-q, a novel method that uses qg to reduce the computational cost of rtqa, and address the bottleneck of encoding coming question which cannot be cached offline. experiments on squad-open demonstrate that ocean-q is able to accelerate the fastest rtqa system by 4 times, while only losing the accuracy by 3+%. to further improve qg quality, we introduce a new data augmentation method and leverage multi-task learning and diverse beam search to boost rtqa performance. currently, there still exists a gap between ocean-q and the state-of-the-art rtqa models. future work that includes designing a better qg model may help closing the gap.  data augmentation, s min , s max and n are set to 0.5, 1.0 and 2 respectively. for multi-task learning, we use squad, hotpotqa and newsqa dataset, with mixture ratio 0.4 by searching from 0.1 to 0.9. for diverse beam search, the diverse strength rate Î³ is set to 4.0. for rtqa, we generate 20 different questions with each answer.","{52822214: 'Yang et al., 2018', 202572810: 'Zhang and Bansal, 2019'}",https://arxiv.org/pdf/2009.05167v2.pdf
408,248218489,Improving Passage Retrieval with Zero-Shot Question Generation,conclusions and future work,Insight-tree,"in this work, we propose upr, an approach to perform unsupervised passage re-ranking for opendomain retrieval. to re-rank, upr computes a relevance score for question generation conditioned on each retrieved passage using pre-trained language models. extensive experiments across a wide range of qa datasets show that an unsupervised pipeline consisting of retriever and upr greatly outperforms strong supervised retriever models. in addition, upr further improves the performance of supervised retrievers. on the open-domain qa task, by just performing inference using re-ranked passages and a pre-trained reader model, we achieve new state-of-the-art results.",,https://www.aclanthology.org/2022.emnlp-main.249.pdf
409,245353743,AN INFERENCE APPROACH TO QUESTION ANSWERING OVER KNOWLEDGE GRAPHS A PREPRINT,conclusion & future work,Insight-tree,"we presented a simple approach for converting question answering over knowledge graphs into an inference problem. leveraging existing models of natural language inference, as well as proposing a new model, we have shown state of the art results on metaqa dataset. our model is simple and amenable for domain adaptation, to solve the problem of qa over kgs from newer domains with lesser training data. to the best of our knowledge, this is the first attempt in treating qa over kg as an inferencing problem and the results are exciting. the work is preliminary and provides a good starting point for discussion and further research.",{},https://arxiv.org/pdf/2112.11070v1.pdf
410,248780279,Modeling Multi-hop Question Answering as Single Sequence Prediction,conclusion,Insight-tree,"in this work, we propose a generative question answering (qa) approach that models multi-hop qa as a single sequence prediction task. it learns to generate an answer along with a reasoning path to improve its capability of multi-hop reasoning. our experiments on prominent multi-hop qa benchmarks, hotpotqa and iirc, validate the promise and effectiveness of our proposed method path-fid and its extension pathfid+. future work will explore (1) our pathfid approach more closely with text retrieval models in open-domain qa scenarios and (2) more explicit grounding on the input information to make our approach even more interpretable and controllable.   in figure 4 and 5, we visualize the correlation between supporting evidence and answer prediction performances for comparison and bridge question types, respectively. to obtain these plots, we first split the examples into 10 buckets where n-th bucket contains the examples with support-f1 score in (10 * (n â 1), 10 * n] percentile for n = {1, 2, . . . , 10}. then, we take the average answer prediction accuracy (both em and f1) over these examples for each bucket, and report this number on the y-axis of the plot at the corresponding support-f1 bucket on the x-axis, while dropping the empty buckets. note that x = 0 corresponds to examples with support-f1 score of 0. also note that the size of a data point on the figure reflects the number of examples in the corresponding bucket as also indicated by the legend. from figures 4 and 5, we can observe that the accuracy of the generated answers is significantly lower, 30% for bridge and 10% for comparison, for the first bucket with zero support-f1 compared to buckets with positive support-f1 score. this suggests that the model has a difficult time figuring out the an-swer when the supporting evidence prediction is poor. another observation that holds for both categories is the general trend of increased answer quality as the supporting fact prediction improves. combining these two points provide additional evidence (in addition to table 2 in the main paper) implicitly supporting the answer generation process of pathfid being grounded on the generated supporting facts, which is generated as the prefix of the answer segment in the full decoded reasoning path sequence during inference.","{202558815: 'Chen et al., 2019;', 202660724: 'Nie et al., 2019;', 160009340: 'Nishida et al., 2019;', 202583429: 'Xiong et al., 2019;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.acl-long.69.pdf
411,247188085,Read before Generate! Faithful Long Form Question Answering with Machine Reading,conclusion,Insight-tree,"we propose a new end-to-end framework rbg that jointly models answer generation and machine reading to tackle the faithfulness issue in lfqa. experiments on two lfqa datasets, eli5 and ms marco, demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics. the detailed analysis further proves the competency of our method in generating fluent, relevant, and more faithful answers. we also propose to evaluate the factual correctness of lfqa model by answering questions of extractive qa tasks (e.g., natural questions), which may be helpful to evaluate the faithfulness of lfqa model efficiently.","{221507798: 'Petroni et al., 2021', 224705407: 'Su et al., 2020a', 221662105: 'Su et al. 2020b', 235212203: 'Su et al. 2021'}",https://www.aclanthology.org/2022.findings-acl.61.pdf
412,260155126,ARB: Advanced Reasoning Benchmark for Large Language Models,limitations and conclusion,Insight-tree,"in this paper, we presented arb, a novel benchmark for evaluating advanced reasoning capabilities in large language models. our dataset is composed of various problems from the sciences and law, sourced from graduate-level exams and professional resources. despite advancements in current llms, their performance remains very low on the quantitative subjects, in arb's tasks. we also proposed a rubric-based self-evaluation method, enabling llms to grade their own reasoning. this method is not yet reliable enough to replace human grading. we hope this method can be extended to more reliable and cheap testing of complex model outputs.",{},https://export.arxiv.org/pdf/2307.13692v2.pdf
413,118587388,Searching for an Attractive Force in Holographic Nuclear Physics,limitations of the n c â â limit and holography,Insight-tree,"the large n c limit is inherent in all holographic qcd methods, and this poses a problem for the aspects of nuclear physics that are different between the small n c and the large n c regimes. in particular, the bulk nuclear matter at zero temperature and pressure (but finite density) forms a quantum liquid for small n c -such as real-life n c = 3 -but becomes a crystalline solid for large n c .to see how this works, consider a condensed matter analogy -some atoms which attract to each other at long or medium distances but have repulsive hard cores. semi-classically, at zero temperature and pressure such atoms always form some kind of a crystal; it takes strong quantum effects to put the atoms into some other phase such as liquid or super-solid.of particular importance is the kinetic energy of the zero-point quantum motion of atoms confined to narrow potential wells,k â¼ Ï 2 2 2m atom (well diameter) 2 ,(2.1)or rather its ratio k/u to the potential binding energy u per atom. according to newton bernardes [18], this ratio is related to the de bour parameter Î» b of the inter-atomic potential ask u â 11Î» 2 b , Î» b = r c â 2mÇ« , (2.2)where r c is the radius of the atomic hard core and Ç« is the maximal depth of the potential. for small de bour parameters, the quantum corrections to the semi-classical approximation are weak and the crystal remains stable at zero pressure. for larger Î» b , the quantum corrections due to kinetic energy become important, and when Î» b exceeds a critical value somewhere between 0.2 and 0.3 [19], the crystal melts into a quantum liquid. 4 for example, helium atoms have Î» b = 0.306 and hence k/u â 1 while neon atoms have Î» b = 0.063 and hence k/u â 0.05; consequently, at zero temperature and zero pressure helium is a quantum liquid while neon is a crystalline solid.to see how the k/u ratio of the nuclear matter depends on the number of colors, we note that in the large n c limit, the leading nuclear forces are proportional to n c . specifically, according to kaplan and manohar [20],v ( r, i 1 , i 2 , j 2 , j 2 ; n c ) = n c Ã a c (r) + n c Ã a s (r)(i 1 i 2 )(j 1 j 2 ) + n c Ã a t (r)(i 1 i 2 ) 3(nj 1 )(nj 2 ) â (j 1 j 2 ) (2.3) + o(1/n c ).for the same n c -independent radial profiles a c , a s , a t of the central, spin-spin, and tensor potentials. classically, such potentials would like to arrange a many-nucleon system in some kind of a crystal with n c -independent nearest-neighbor distance â¼ 1 fm, while the binding energy of a nucleon in such a crystal would be proportional to the n c . indeed, all models of nuclear matter based on semi-classical models of nucleons form such crystals, for example skyrmion crystals of ref. [6]. in the quantum theory, nucleons in such a lattice have zeropoint kinetic energies (2.1) where the well diameter is independent on n c while the nucleon's mass m â n c , hence k â 1/n c andk u â n â1 c n +1 c = 1 n 2 c . (2.4)we may estimate the coefficient of this proportionality using the de bour parameter Î» b . the maximal depth of the central potential between two nucleons is about 100 mev for n c = 3, so we take it to be Ç« â¼ n c Ã 30 mev for large n c . likewise, we take the nucleon mass to be m n â¼ n c Ã 300 mev and hard-core radius r c â¼ 0.7 fm regardless of n c . consequently,Î» b = r c â 2mÇ« â¼ 2 n c =â k u â¼ 45 n 2 c (2.5)and hence liquid nuclear matter for n c 8 and solid nuclear matter for n c 8.the numerical coefficient in eq. (2.5) and hence our estimate n crit c â¼ 8 for the dividing line between liquid and solid bulk nuclear matter (at low pressures and temperatures) should be taken with a large grain of salt. also, the transition between liquid nuclear matter for n c = 3 and crystalline nuclear matter for large n c may go through some exotic phases at intermediate values of n c , perhaps something like a quantum supersolid, perhaps something more exotic without known condensed-matter analogues. but regardless of the details of this transition, in the large n c limit the potential energy of interacting near-static nucleons becomes much larger than the nucleons' kinetic energies, and the bulk nuclear matter at t = 0, p = 0 conditions becomes a conventional semi-classical crystal. the structure of such crystals can be modeled holographically -and indeed there is active research in this direction (for instance [7]) -but we have no experimental data to compare to the models because real-life nuclei with n c = 3 are liquid rather than solid.meanwhile, instead of trying do build holographic models of complete nuclei we focus on holographic models of the nuclear forces. but even at the level of the two-body forces, the large n c limit maybe different from the real-life case of just 3 colors. of particular concern is the isoscalar attractive force due to exchanges of the Ï(600) scalar mesons between the nucleons. in real life, this is a major component of the net attractive force -especially at the medium-long distances between the nucleons -but in the large n c limit this component may weaken or disappear because the Ï(600) meson itself may become heavier or even disappear from the scalar meson spectrum.the Ï(600) (also known as f 0 (600)) is the lightest isoscalar true-scalar meson. in real life, it appears as a very broad resonance of two pions -so broad that its central mass is somewhat controversial and different experimentalists locate it anywhere between 400 mev and 700 mev, and sometimes even higher, cf. references in the particle data group's listing [21]. but the real controversy about the Ï(600) resonance is its physical origin. unlike the heavier i g = 0 + , j cp = 0 ++ mesons f 0 (980), f 0 (1370), etc., the Ï(600) meson does not exist in the non-relativistic quark model 5 so for many years r. l. jaffe and others [22,23,24,25] were claiming that the Ï(600) is not a true qq meson but a qqqq tetraquark. specifically, it's a molecule-like bound state of two pions which exists because the Ï-meson exchanges in the t-channel induce an attractive s-channel force between the pions. if this claim is true, then the Ï resonance goes away in the large n c limit because the forces between pions become weak as 1/n c . but many other authors (see [26] for a sample) identify the Ï(600) with the Ï field of the linear sigma model of the chiral symmetry breaking. or rather, the massive Ï(x) field parametrizing fluctuations of magnitude of the symmetry-breaking vev ÏÏ gives rise to primordial sigma quanta, while the real sigma mesons Ï(600) are quantum mixtures of those primordial quanta with the |ÏÏ states (and to lesser extent with the other i g = 0 + , j p c = 0 ++ mesons). from this point of view, the non-relativistic quark model is irrelevant because the quarks do not become non-relativistic until after the chiral symmetry has already been broken. indeed, the nrqm does not see that the pions are (pseudo) goldstone bosons, so the fact that it does not see the sigma meson at all is simply another limitation of the nrqm as far as the chiral symmetry breaking is concerned. if this point of view is right, then the sigma meson exits for all n c . for large n c limit, this meson is mostly a quantum of the Ï(x) field -its mixing with |ÏÏ and other states becomes weak -and it's a narrow resonance rather than a broad hump we have for n c = 3, but it remains a dominant resonance in the i g = 0 + , j p c = 0 ++ ÏÏ channel, and its mass should not be too different from the real-life 600 mev.the other mesons -scalar or vector, isoscalar or isovector -are unlikely to be disturbed by the large n c limit, so their contributions to the nuclear forces would be similar to real-life qcd. if the Ï(600) meson remains in the spectrum in the large n c limit and if its mass remains similar to the real-life 600 mev, then the entire nuclear potential (2.3) for n c â â would be similar to what it is in real life, except for the overall factor n c . in particular, the net central potential v c (r) would be repulsive at short distances (the hard core) but 5 in the non-relativistic quark model, all 0 ++ mesons have s = 1 and l = 1. consequently, the lightest 0 ++ meson should be heavier than the lightest 1 ââ mesons Ï(770) or Ï(787) that have s = 1 but l = 0. depending on the assumptions one makes about the forces between the quark and the antiquark, this argument identifies the lightest true qq meson with i g = 0 + and j p c = 0 ++ as either f 0 (980) or f 0 (1370). in any case, the Ï(600) resonance is way too light to be a p-wave qq state, so it has to be something else.attractive at medium and long distances:r v c (r) n c â â limit with a light Ï meson (2.6)on the other hand, if the Ï(600) meson disappears from the spectrum for large n c , or if it becomes heavier than the lightest vector meson, then the dominant attractive force would become shorter-ranged than the repulsive force, and the net force at medium and long distances would be repulsive rather than attractive:r v c (r) n c â â limit without the Ï meson (2.7)in this scenario, at large n c the nuclear force is repulsive at all distances, and there are no bound nuclei at all, liquid or crystalline.so what really happens to the sigma-meson and to the nuclear forces at large n c ? the best way to settle this controversy would be to find the Ï resonance and its mass in a lattice qcd calculation for several values of n c . such a calculation would require a realistic pion mass (unlike most present-day lattice calculations extrapolating from m Ï â¥ 350 mev) andrather large lattices to distinguish the sigma resonance from the two-pion continuum, so it may be too hard for the present-day computers. but thanks to the moore's law, finding the Ï resonance on a lattice should become possible in a not-too-distant future.alternatively, we may try to resolve the issue using holography. although a holographic model of real qcd -or rather, of qcd with large n c -is yet to be constructed, several known models seem to be qualitatively similar, so we can compare their predictions for the meson spectra in general, and for the lightest true scalar meson in particular. however, the models that seem qualitatively similar to qcd may not be similar enough, and their predictions could be widely off target. indeed, the predictions of different models have turned out to be quite different from each other. for example, in the sakai-sugimoto model which we use in this article, the lightest true scalar meson is more than twice as heavy as the lightest vector meson. consequently -as we shall see in painful detail in section 5 -the net nuclear force is everywhere repulsive and looks like (2.7) rather than like (2.6). on the other hand, in the highly-non-antipodal version of the dymarsky-kuperstein-sonnenschein model [27], the lightest j cp = 0 ++ meson is much lighter than any other mesons (except the pions) [29]. however, this lightest scalar is a pseudo-goldstone boson of the approximate conformal symmetry of the flavor sector, so it is not clear how much attractive force it can mediate. as of this writing, it is not clear if the net nuclear potential in this model looks like the real-life potential (2.6) or like the everywhere-repulsive potential (2.7) we calculate in this paper for the sakai-sugimoto model. but suppose tomorrow somebody discovers a holographic model of the real qcd andmiracle of miracles -it has a realistic spectrum of mesons, including the Ï(600) resonance, and even the realistic yukawa couplings of those mesons to the baryons. even for such a model, the two-body nuclear forces would not be quite as in the real world because the semi-classical holography limits n c â â, Î» â â suppress the multiple meson exchanges between baryons. although in this case, the culprit is not the large number of colors but the large 't hooft coupling Î» = n c g 2 ym . indeed, from the hadronic point of view, nuclear forces arise from the mucleons exchanging one, two, or more mesons, and in real life the double-meson exchanges are just as important as the single-meson exchanges. in particular, since the lightest mesonic state with i g = 0 + , j cp = 0 ++ quantum numbers is a pair of un-bound pions, the longest-range isoscalar attractive force between nucleons comes from exchanges of two pions rather than of any single mesons. in holography, the single-meson exchanges happen at the tree level of the string theory while the multiple meson exchanges involve string loops (k â 1 loops for k mesons), and the loop amplitudes are suppressed by the powers of 1/Î» relative to the tree amplitudes.naively, one would expect the loop amplitudes to carry additional factors of 1/n c (which is dual to the string coupling) rather than 1/Î», or maybe both 1/Î» and 1/n c factors, but the naive power-of-n c counting does not work for loop amplitudes involving baryons made of n c quarks. 6 indeed, in honest qcd with a large number of colors, the multi-meson-exchange contributions to the non-relativistic effective potential for the baryons are not suppressed by powers of 1/n c [27]. however, the extra powers of n c due to n c quarks in a baryon are not accompanied by the extra powers of Î», so in holography, the contributions of the multiple meson exchanges are suppressed, albeit by powers of 1/Î» rather than 1/n c .to see how this works in a general holographic model of qcd with n c â« n f , note that such a model starts with a string-theoretic construction where the colors and the flavors live on separate branes. for large n c and large Î», the color branes become black branes producing curvature and fluxes through the bulk, which provide a non-trivial background for degrees of freedom living in the bulk itself as well as on the flavor branes. the bulk degrees of freedom are dual to the pure-color sector of qcd (glueballs, etc.), while the vector and scalar fields living on the flavor branes are dual to the qq mesons. the flavor fields have rather weak couplings to each other: in 5d terms,g 5d,flavor â¼ â r kk â Î»n c (2.8)so the 4d mesons -which are modes of the 5d vector and scalar fields with wave functionsÏ â¼ r â1/2kk -have couplings to each other of the orderg m m m â¼ 1 â Î»n c . (2.9)a holographic baryon is made from some brane spanning only the compact dimensions that is connected to the flavor branes by n c strings, although this construction is often equivalent to an instanton of the 5d flavor gauge fields. consequently, the baryon-meson coupling is enhanced by an extra factor of n c ,g m bb â¼ n c Ã 1 â Î»n c = â n c â Î» . (2.10)at the tree level of the baryon-meson theory, scattering of two baryons proceeds through a single-meson exchange, which produces a o(n c /Î») amplitude,a tree â¼ g 2 m bb â¼ n c Î» . (2.11)at the one-loop level, there are two types of diagrams, the triangle diagrams such asa â â¼ g 3 m bb Ã g m m m â¼ n c Î» 2 (2.12)and the box and crossed-box diagrams (2.13) with amplitudesa â¼ g 4 m bb â¼ n 2 c Î» 2 . (2.14)that carry an extra power of n c . however, banerjee et al showed [27] that for non-relativistic baryons, the box and the crossed-box diagrams almost cancel each other from the effective potential between the baryons, with the un-canceled part having a lower power of the n c . banerjee et al did not pay any attention to the powers of Î», but clearly the un-canceled sub-leading terms in the box and crossed-box diagrams cannot carry higher powers of the 't hooft coupling than the leading terms (2.14), thusa uncanceled â¼ n c Î» 2 â¼ a â â¼ 1 Î» a tree . (2.15)in other words, the contribution of the double-meson exchange and other one-loop processes to the 2-body nuclear potential carries the same power of n c but is suppressed by a factor 1/Î» compared to the tree-level singe-meson exchange.to be precise, the large Î» limit suppresses exchanges of the un-bound meson pairs but not of the meson-meson resonances -which become narrow (because of weak g m m m ) and act as single mesons exchanged between the two baryons. in particular, in the isoscalar 0 ++ channel that gives rise to the dominant attractive force between nucleons, the Î» â â limit suppresses the contribution of the unbound two-pion continuum, but it replaces it with a discrete set of f 0 resonances. in a good holographic model of qcd (which alas has not been found yet), the overall strength of the 0 ++ channel should be similar to the real qcd, so it would produce a similar isoscalar attractive force at short distances. however, the range of this attractive force would be significantly shorter: instead of decaying with distance like exp(â2m Ï r) as in real life, the holographic attractive force decays as exp(âm 0 r) where m 0 is the mass of the lightest isoscalar 0 ++ meson, presumably Ï(600 mev).in principle, the isoscalar 1 ââ channel that gives rise to the dominant repulsive force suffers from similar corrections in the Î» â â limit. but in practice, the strongest and the longest-range contribution to this channel comes from exchanges of a single Ï(787) meson, so suppressing the multi-meson exchanges in this channel would not make a qualitative difference. thus altogether, the net effect of large 't hooft coupling on the central nuclear potential -besides the overall 1/Î» factor -is the shortening of the attractive tail at long distances:r Î» Ã v c (r)blue: real qcd, Î» â¼ 1 red: best possibility for holographic qcd, Î» â« 1 (2.16) however, this optimistic picture presumes a holographic model of qcd that correctly reproduces (a) the overall strength of the isoscalar 0 ++ and 1 ââ channels, and (b) the mass spectra of vector and scalar mesons, especially the masses of the lightest 0 ++ and 1 ââ mesons Ï(600) and Ï(787). but thus far, no known model satisfies these requirements, not even approximately, so the nuclear forces they produce could be much more different from the real life than (2.16). in particular, the nuclear force we calculate in this paper for the sakai-sugimoto model turns out to be everywhere repulsive:r v c (r)/n c blue: real qcd red: sakai-sugimoto model (2.17)now let's go back to the large n c limit -in holography or in honest qcd -and consider yet another general problem with baryons made from many quarks: how to separate the nucleons with i = j = 1 2 from the other kinds of baryons such as â with i = j = 3 2 ? in real life, there is a large mass gap between the nucleons and the â baryons -almost 300 mev -but for large n c this gap shrinks as 1/n c . at the same time, the two-baryon potential grows like n c , so for large n c it becomes stronger than the gap. consequently, two interacting nucleons may ""forget"" their individual spins and isospins and mix up with other baryonic species such as â. in fact, for large n c there is a whole lot of baryonic species with i = j ranging from 1 2 (for odd n c ) or 0 (for even n c ) all the way up to n c /2, and a strongly-interacting nucleon might mix up with all of them. while such mixing would not affect the isoscalar spin-blind central force between two baryons, it might significantly enhance the isovector spin-spin and tensor forces.therefore, comparing the two-baryon forces in the large n c limit to the real-life twonucleon forces is rather tricky. one has to carefully keep track of the spin and isospin degrees of freedom of the two baryons, expand the interaction hamiltonian into central, spinspin, and tensor forces as in eq. (2.3), and then compare the radial profiles a c (r), a s (r), and a t (r). moreover, the spin and isospin degrees of freedom require quantum mechanical treatment because semi-classically, we do not get definite spins or isospins even for standalone single baryons. instead, we get skyrmions, or instantons, or some other kind of solitons with a definite orientation of the su(2) isospin relative to the su(2) spin ; in quantum terms, they become superpositions of baryons with all possible i = j = 1 2 , 3 2 , . . . , â. consequently, a force between two such semiclassical baryons is not a force between two nucleons but rather a superposition of forces between different baryonic species.this problem affected the first holographic calculation of the nuclear forces by k. y. kim and i. zahed [30]. their baryons were semiclassical instantons in the sakai-sugimoto model, so instead of definite |i, i z , j, j z they had a definite direction n in s 3 = su(2) isospin Ã su(2) spin /su(2) common . consequently, kim and zahed [30] found that the force between two baryons depends on the angle between n 1 and n 2 -it was attractive for some angles and repulsive for other -but they could not interpret this angular dependence in terms of the isovector spin-spin and tensor forces. by comparison, hashimoto, sakai, and sugimoto [16] made a similar calculation using properly quantized collective coordinates for each instanton.consequently, they obtained the force between two nucleons rather than some mixed-up baryons, and they could see how this force depends on each nucleon's i z and j z . in particular, they saw that at medium-short distances, the net force between two nucleons is always repulsive. evidently, the attraction kim and zahed saw for some relative orientations of semiclassical baryons happens only for high spins and isospins, but not for nucleons withi = j = 1 2 .on the other hand, the analysis of hashimoto et al was limited to the first-order perturbation theory for nucleons that are far enough from each other to avoid the strong mixing of spins and isospins. this approach will not work for the hard-core region at very short distances where the interactions are much stronger than the gaps between states of the individual baryons. in the hard core, the semiclassical analysis of kim and zahed might work better than the perturbative expansion of hashimoto et al, although comparing the semi-classical large-n c results to the real-life nuclear forces might be problematic.to summarize, the large n c limit of nuclear physics suffers from three major problems. the third problem of baryon mixing is only technical, and it can be solved -at least for the medium and long distances between the nucleons -by following hashimoto et al rather than kim and zahed. but there are no ways around the first problem of different phase structures of nuclear matter with n c = 3 and with n c â â. even at high pressures and densities, there is a difference: for n c = 3, squeezing nucleons together makes them merge into a quark liquid, while for n c â â the nucleons always retain their individual identities and a would-be quark liquid suffers from the ""chiral density wave"" instability [4]. it is possible that at some intermediate pressures and densities the n c = 3 nucleons form a crystal -just like helium solidifies at high pressures -before merging into a quark liquid. if this intermediate-pressure phase of real nuclear matter is ever observed in a lab, or can be reliably shown to exist in some exotic but observable places like inferiors of neutron stars, it would be very interesting to compare its properties to the holographic models. until then, we can only speculate.finally, the second problem -concerning the fate of the Ï(600) resonance in the large n c limit and its effect on the attractive nuclear force -is solvable in principle, but it has not been solved yet. in holography, this problem is aggravated by using qcd-like models in lieu of the presently unknown holographic dual of the real qcd. the meson spectra of such models are not quite realistic; for example, in the sakai-sugimoto model (both antipodal and non-antipodal versions) there is no Ï resonance and the lightest scalar meson has more than twice the mass of the lightest vector meson. consequently, we shall see in section 5 that in this model, the attractive force is both weaker and shorter-ranged than the repulsive force, so the net nuclear force is always repulsive. this could be a peculiar failing of the sakai-sugimoto model, or it could be the general problem of holography or even of the large n c limit. hopefully, future research will resolve this issue.",{},https://arxiv.org/pdf/1003.2621v2.pdf
414,52822214,"HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",conclusions,Insight-tree,"we present hotpotqa, a large-scale question answering dataset aimed at facilitating the development of qa systems capable of performing explainable, multi-hop reasoning over diverse natural language. we also offer a new type of factoid comparison questions to test systems' ability to extract and compare various entity properties in text.",{},https://www.aclweb.org/anthology/D18-1259.pdf
415,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,discussion and conclusion,Insight-tree,"while wic and english wsd rely exclusively on dictionaries (pilehvar and camacho-collados, 2019) to obtain word senses and example sentences, our data collection depends on wikipedia, wic, & nlp models and our annotation depends on experts.",{},https://www.aclanthology.org/2023.eacl-main.1.pdf
416,250462558,PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,limitations,Insight-tree,"our dataset is currently limited to multi-word, english noun-phrases. furthermore, it is expected to contain around a 5% error on pr-pass (i.e. the best human performance is 95% em). on pr-page, there may be more than one correct target phrase; however, we only label one phrase as the correct answer per document. we use only phrases that contain at least one wic word. hyperparameters we train each bert-based classifier for a maximum of 100 epochs with early stopping monitored on validation accuracy (patience of 10 epochs). we use a batch size of 200 and adam optimizer with learning rate Î± = 0.0001, Î² 1 = 0.9, Î² 2 = 0.999, and Ïµ = 10 â8 .",{},https://www.aclanthology.org/2023.eacl-main.1.pdf
417,246485449,JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we proposed the japanese question answering dataset, jaquad. we collected the contexts from japanese wikipedia articles and 39k+ questions were manually annotated by fluent japanese speakers. jaquad has the same format as squad, and the characteristics of the data are generally similar to korquad 1.0. in the experiments, we fine-tuned a japanese pre-trained language model with jaquad as a baseline and achieved 78.92% for f1 score and 63.38% for em on test set. the baseline reaches promising results, but there is plenty of room for improvement. extension of the dataset, such as covering longer answers, is left for future work. the dataset and our experiments are available at https://github.com/skelterlabsinc/jaquad.","{211126910: '[6]', 86611921: '[10]', 52822214: '[24]'}",https://arxiv.org/pdf/2202.01764v1.pdf
418,233219869,QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering,conclusion,Insight-tree,"we presented qa-gnn, an end-to-end question answering model that leverages lms and kgs.",{},https://www.aclweb.org/anthology/2021.naacl-main.45.pdf
419,207853300,Hierarchical Graph Network for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, we propose a new approach, hierarchical graph network (hgn), for multi-hop question answering. to capture clues from different granularity levels, our hgn model weaves heterogeneous nodes into a single unified graph. experiments with detailed analysis demonstrate the effectiveness of our proposed model, which achieves state-of-the-art performance on hotpotqa benchmark. currently, in the fullwiki setting, an off-theshelf paragraph retriever is adopted for selecting relevant context from large corpus of text. future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.","{153312687: 'Ding et al., 2019', 189927896: 'Jiang and Bansal, 2019a', 202565945: 'Jiang and Bansal 2019b', 174801764: 'Min et al., 2019a', 174801080: 'Min et al., 2019b', 160009340: 'Nishida et al., 2019', 202773198: 'Qi et al., 2019', 158046817: 'Tu et al., 2019', 155100120: 'Xiao et al., 2019'}",https://arxiv.org/pdf/1911.03631v1.pdf
420,255570137,Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding,conclusion,Insight-tree,"in this paper, we propose cross-model comparative loss, a simple task-agnostic loss function, to improve the utility of neurons in nlu models. comparative loss is essentially a ranking loss based on the comparison principle between the full model and its ablated models, with the expectation that the less ablation there is, the smaller the task-specific loss. to ensure comparability among multiple ablated models, we progressively ablate the models and provide two controlled ablation methods based on dropout and context cropping, applicable to a wide range of tasks and models.","{233296201: '[23,', 237552879: '54]', 207870753: '56]', 52822214: '[67]'}",https://export.arxiv.org/pdf/2301.03765v1.pdf
421,248780114,When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning,conclusion,Insight-tree,"we examined the three main strategies for transfer learning in natural language processing: training on an intermediate supporting task to aid the target task (stilts), training on the target and supporting task simultaneously (mtl), or training on multiple supporting tasks alongside the target task (mtl all ). we provide the first comprehensive comparison between these three methods using the glue dataset suite and show that there is a simple rule for when to use one of these techniques over the other. this simple heuristic, which holds true in more than 92% of applicable cases, states that multi-task learning is better than intermediate fine tuning when the target task is smaller than the supporting task and vice versa. additionally, we showed that these pairwise transfer learning techniques outperform the mtl all approach in almost every case.  pairwise oracle uses the best supplementary task for the given target task using the best pairwise method (stilts or mtl). all scores are the average of 5 random seeds. note that mtl all was run with three different sampling methods (top half). we find that on almost every task, pairwise approaches are better than mtl all . bold scores indicate the best score in the column for the given section.",,https://www.aclanthology.org/2022.acl-short.30.pdf
422,221819379,Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA,conclusion,Insight-tree,"we believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. in a large-scale 24-task nlp experiment, ca-mtl outperforms fully tuned single task models by 2.3% for bert large and by 1.2% for roberta large using 1.12 times the number of parameters, while single task fine-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. when a bert vanilla mtl model sees its performance drop as the number of tasks increases, ca-mtl scores continue to climb. performance gains are not driven by a single task as it is often the case in mtl. each ca-mtl module that adapts a transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. this evidence shows that ca-mtl is able to mitigate task interference and promote more efficient parameter sharing. we showed that mt-uncertainty is able to avoid degrading performances of low resource tasks. tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. overall, ca-mtl offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. extending such ideas will be an objective for future work.",{},https://arxiv.org/pdf/2009.09139v3.pdf
423,237593105,NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset,conclusion,Insight-tree,"in this work, we present a new qa datasets with complex numerical questions and interpretable reasoning graph. we also introduce an automatic evaluation metric for the generated reasoning process. we finally present an initial model producing the reasoning process while answering questions. the experiments show that noahqa is challenging and will become an interesting direction in both numerical qa and explainable qa.","{86611921: 'Kwiatkowski et al., 2019', 222141025: 'Saha et al., 2020;', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2109.10604v2.pdf
424,234335834,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,conclusion and future work,Insight-tree,"in this paper, we focus on the task of logical reasoning of text. following a three-step logical reasoning paradigm, we first propose a neuro-symbolic logicdriven context extension framework. it identifies logical expressions as elementary units of logical inference and symbolically deduces the implicitly mentioned expressions, and verbalizes them as an extended context into a pre-trained model to match the answer. we also introduce a logic-driven data augmentation algorithm, which augments literally similar but logically different instances and employs contrastive learning to help our model better capture logical information. experimental results confirm the general effectiveness of our lreasoner, and it even surpasses human performance on the reclor dataset. in the future, we will explore to model different logical reasoning types and directly incorporate symbolic logic into the model structure.","{220483148: 'Liu et al., 2020'}",https://www.aclanthology.org/2022.findings-acl.127.pdf
425,257834209,SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES,conclusion,Insight-tree,"in this paper, we introduce a domain-agnostic general-purpose architecture, the semi-parametric inducing point network (spin) and use it as the basis for induced point neural process (ipnps). unlike previous semi-parametric approaches whose computational cost grows quadratically with the size of the dataset, our approach scales linearly in the size and dimensionality of the data by leveraging a cross attention mechanism between datapoints and induced latents. this allows our method to scale to large datasets and enables meta learning with large contexts. we present empirical results on 10 uci datasets, a gaussian process meta learning task, and a real-world important task in genomics, genotype imputation, and show that our method can achieve competitive, if not better, performance relative to state-of-the-art methods at a fraction of the computational cost.","{52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020'}",https://export.arxiv.org/pdf/2205.11718v2.pdf
426,257532206,Semantic matching based legal information retrieval system for COVID-19 pandemic,conclusion,Insight-tree,"we propose a semantic matching network for pairwise relation learning. moreover, we introduce auxiliary contrastive learning to help network better distinguish the sentences. our experiments show that our method leads to performance improvements under a variety of encoder designs. based on the network we proposed, we design and implement a legal ir system for the covid-19 pandemic. the system can identify: (1) the crime cases entered by the user and find the most similar cases to be pushed to the user as answers, and (2) the crime cases documented by the user and give the reference legal gist applicable to the case. meanwhile, the study could benefit developing a more comprehensive legal ir system and similar systems. in the future, it is worth doing more experiments on more data set to analyse the effect of various neural models for such tasks and accordingly improving the system.","{233206583: 'Esteva et al. 2021', 208310122: 'Zhong et al. 2020b'}",NaN
427,254853987,Source-Free Domain Adaptation for Question Answering with Masked Self-training,conclusion,Insight-tree,"in this paper, we explore the possibility of transferring knowledge for unsupervised domain adaptation on question answering, without access to initial domain data. we proposed a novel self-trainingbased approach, mdaqa. we specially design an attention mask module to automatically keep key knowledge from the source domain and learn to mitigate domain shift between source and target domains. the module can be easily integrated into existing language models. our comprehensive experiments on well-known benchmark datasets demonstrate that mdaqa outperforms previous methods by a clear margin. it can also achieve decent performance even when the available target domain data is highly limited. this makes mdaqa have a very wide range of application scenarios.","{204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 250562913: 'Wu et al., 2022;', 52822214: 'Yang et al., 2018', 247518803: 'Yue et al., , 2022', 237364113: 'Yue et al., 2021'}",https://export.arxiv.org/pdf/2212.09563v1.pdf
428,254853987,Source-Free Domain Adaptation for Question Answering with Masked Self-training,limitation,Insight-tree,"since the use of source-free uda is mostly discussed in the medical field at the moment, ideally, more experiments on medical datasets would be more convincing. however, since available medical qa datasets are extremely limited and hard to access, currently we still conduct experiments on commonly used general-purpose qa datasets.","{204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 250562913: 'Wu et al., 2022;', 52822214: 'Yang et al., 2018', 247518803: 'Yue et al., , 2022', 237364113: 'Yue et al., 2021'}",https://export.arxiv.org/pdf/2212.09563v1.pdf
429,259376626,A Question Answering Benchmark Database for Hungarian,conclusions,Insight-tree,"we presented a new qa benchmark database in hungarian, that in several aspects, goes beyond squad-type datasets: it is not limited to single contiguous short extractive answer spans, contains yes/no questions, non-contiguous multispan short answers, long answers, questions requiring arith-model short answers long answers with multispan no multispan with multispan no multispan  table 5: performance of extractive reader models on short and long answer spans with and without multispan answers. metic reasoning, and other questions where the answer cannot be simply copied from the text. the annotation was created using a customized label-studio-based annotation platform. the annotators were encouraged to get actively involved in selecting the texts to be annotated and to abandon annotation of uninteresting or low quality texts in order to make the annotation task less boring and demotivating. we also trained and evaluated baseline models for document retrieval and reader models for answer span extraction. cross-lingual knowledge transfer naturally facilitated by multilingual transformer models was found to be beneficial for the quality of the trained models.","{165163607: 'Clark et al., 2019', 212657414: 'Clark et al. 2020', 67855846: 'Dua et al., 2019', 202572622: 'Jin et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.law-1.19.pdf
430,258833449,Fact-Checking Complex Claims with Program-Guided Reasoning,conclusion and future work,Insight-tree,"we proposed programfc, a few-shot neurosymbolic model for fact-checking that learns to map input claims to a reasoning program consisting of a sequence of sub-task function calls for answering a question, for fact-checking a simple claim, and for computing a logical expression. then factchecking is performed by executing that program. programfc combines the advantages of symbolic programs, such as explainability, with the flexibility of end-to-end neural models. using codex as the program generator, programfc demonstrates promising performance on hover and feverous with only a small number of incontext demonstrations and no additional training. we also investigated the impact of model size and the benefits of programs for retrieval, and we analyzed the errors. the results indicated that pro-gramfc effectively balances model capability, learning efficiency, and interpretability.",{52822214: 'Yang et al. 2018'},https://www.aclanthology.org/2023.acl-long.386.pdf
431,202583433,Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering,conclusion,Insight-tree,we introduce an entity-centric approach to ir that finds relevant evidence required to answer multihop questions from a corpus containing millions of paragraphs leading to significant improvement to an existing qa system.,{52822214: 'Yang et al. 2018'},https://www.aclweb.org/anthology/D19-5816.pdf
432,264426555,Implications of Annotation Artifacts in Edge Probing Test Datasets,conclusion,Insight-tree,"ep tests are classification tasks to measure an llm's ability to encode syntactic and semantic knowledge.however, in many ep datasets, there is not a significant difference between the random vs pre-trained encoders, which raises questions about the validity of the tests (the ""classifier knowledge"" problem).we analyze 17 datasets across 10 datasets to find various biases and show that the ep classifiers are more prone to use heuristic mechanisms when random encoders are used instead of the pre-trained ones.when the dataset biases are removed, the pre-trained encoders do show a significant difference from the random ones as expected.information-theoretic probes have been proposed before to solve the ""classifier knowledge"" problem, we show why they might not be necessary.future work would extend the findings of this study to fine-tuned models.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.conll-1.39.pdf
433,238583118,A Few More Examples May Be Worth Billions of Parameters,conclusions,Insight-tree,"in this work, we present an empirical investigation on the relationships between (1) a task's format, (2) the number of labeled examples available for said task, and (3) the number of parameters the model tackling the task has. through our extensive experiments, we determine that task format greatly affects the relative performance improvement that can be expected from increased training set size and parameter count. for tasks that do not require the recollection of specific external information -i.e. classification, multiple choice, and extractive qa -we find that more labeled data and larger models both reliably improve performance. in fact, for some of these tasks, adding a few hundred labeled examples is more beneficial than scaling up the model size by billions of parameters. it seems then, from a practitioner's perspective, that for many tasks where data is very sparse, the tried-and-true strategy of simply collecting more training data will often be a more effective strategy than attempting to scale to larger, more computationally-demanding models. however, the picture is very different for open qa tasks; for such tasks, we find that increasing the size of the training data barely improves performance, leaving parameter inflation as the only reliable approach to improve accuracy. finally, we provide a hypothesis to explain these results and conclude with a practical corollary -when possible, changing the format from open qa into a more ""self-contained"" one will allow labeled data to bridge performance gaps between moderately-sized models and much larger ones.  table 2 provides the results from our main experiment (section 3, figure 2) in tabular form.","{86611921: 'Kwiatkowski et al., 2019', 230433978: 'Ram et al., 2021;', 208201969: 'Sugawara et al., 2020', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2110.04374v1.pdf
434,215768725,A Simple Yet Strong Pipeline for HotpotQA,conclusion,Insight-tree,"our work shows that on the hotpotqa tasks, a simple pipeline model can do as well as or better than more complex solutions, such as graph networks, cross-document attention, or ner. powerful pre-trained models allow us to score sentences one at a time, without looking at other paragraphs. by operating jointly over these sentences chosen from multiple paragraphs, we arrive at answers and supporting sentences on par with state-of-theart approaches. this result shows that supporting sentence identification in hotpotqa is itself not a multi-hop problem, and suggests focusing on other multi-hop datasets to demonstrate the value of more complex retrieval techniques.","{208267807: 'Asai et al., 2020', 204915921: 'Khot et al., 2020', 174801764: 'Min et al., 2019a', 174801080: 'Min et al., 2019b', 202660724: 'Nie et al., 2019', 160009340: 'Nishida et al. 2019', 155100120: 'Xiao et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf
435,263608648,ADAPTING LLM AGENTS THROUGH COMMUNICATION,conclusion,Insight-tree,"we introduced learning-through-communication (ltc), a paradigm that adapts llm agents to new tasks and environments via communication-based iterative learning.within this ltc framework, we have designed three communication modes for common tasks including decision-making, knowledgeintensive reasoning, and numeric reasoning.these communication modes facilitate interactions between llm agents and their environments, as well as other agents such as gpt-4 and humans.the history of these interactions can be autonomously organized into training data for ppo training so that the agent can adapt to the new task.our approach represents a closed loop where the agent self-interacts with the environment or other agents, and learning to improve itself with minimal human intervention.empirically, we have demonstrated that ltc performs strongly in success rate and efficiency across three different tasks: alfworld, hotpotqa, and gsm8k.it consistently outperforms existing llm agent and instruction tuning baselines, showing the promise of the ltc paradigm in adapting llm agents to new tasks and environments with minimal human effort.as for future work, we plan to explore more diverse communication patterns for different tasks, and involve the communication with human during the iterative learning process.we will open source our code to facilitate further research in this line.",{52822214: '[64]'},https://export.arxiv.org/pdf/2310.01444v2.pdf
436,222177127,"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment",conclusion and future work,Insight-tree,"we propose a novel ea model (attrgnn) and contribute a hard experimental setting for practical evaluation. attrgnn can integrate both attribute and relation triples with varying importance for better performance. experimental results under the regular and hard settings present significant improvements of our proposed model, and the severe dataset bias can be effectively alleviated in our proposed hard setting.","{52822214: 'Yang et al., , 2018'}",https://arxiv.org/pdf/2010.03249v1.pdf
437,164284,A Rewriting Logic Approach for Automatic Composition of Web Services,conclusions,Insight-tree,"in this work, we showed how the formalism of graphs can be used to improve the composition of web services and make it automatic. more precisely, we have proposed rewriting logic and its maude language as a support for a graph-based approach for automatic composition of web services. the proposed model has made possible the exploration of different composition schemas as well as the formal analysis of service compositions. our contribution has broadly followed two main steps:",{},https://arxiv.org/pdf/1411.5153v1.pdf
438,222133899,Towards Interpretable Reasoning over Paragraph Effects in Situation,conclusion and future work,Insight-tree,"in this paper, we aim to answer ropes questions in an interpretable way by leveraging five neural network modules. these modules are trained in an end-to-end manner and each module provides transparent intermediate outputs. experimental results demonstrate the effectiveness of each module, and analysis on intermediate outputs presents good interpretability for the inference process in contrasted with ""black box"" models. moreover, we find that with explicitly designed compositional modeling of inference process, our approach with a few training examples achieves similar accuracy to strong baselines with full-size training data which indicates a better generalization capability. meanwhile, extending these models to a larger scope of question types or more complex scenarios is still a challenge, and we will further investigate the trade-off between explainability and scalability.   table 8 shows one labelled example, and the process of adding auxiliary supervision label contain the following steps: table 7: detailed parameters used in answer prediction, we provide search bounds for each hyperparameter and list out the hyperparameters combination for out best model and baseline model. other unmentioned parameters keep same as the one used in bert.",{},https://arxiv.org/pdf/2010.01272v1.pdf
439,248157463,ASQA: Factoid Questions Meet Long-Form Answers,conclusion,Insight-tree,"in contrast to existing datasets for long-form qa, asqa admits a clear notion of correctness that we use to define an overall metric of performance (dr). our empirical evaluations demonstrate that dr correlates well with the human judgment; and there is a large gap between human performance and the strong baselines. thus, we believe that asqa is an appealing task for the qa community. our analysis suggests that strong performance on asqa is contingent upon both high-quality retrieval and summarization. these aspects constitute important directions for future work on asqa.","{196170479: 'Fan et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.566.pdf
440,248157463,ASQA: Factoid Questions Meet Long-Form Answers,limitations,Insight-tree,"we now make two remarks that we urge the reader to consider when interpreting the results of this work.inter-annotator agreement in section 3.3, we observed that inter-annotator agreement in asqa is higher than in eli5. we note, however, that the high inter-annotator agreement in asqa is contingent upon the high inter-annotator agreement in the ambigqa dataset. indeed, ambigqa disambiguations serve as a shared source of information between the two asqa annotators working on the same instance, potentially inflating the level of agreement.that said, min et al. (2020) observe that human annotators have a decent level of agreement in constructing the disambiguations in ambigqa, thereby supporting the observation that asqa is more objective than eli5.evaluation metrics second, we caveat that our accuracy metrics (str-em and disambig-f1) only measure the recall of the required information in the long answers. in cases where the long answer hallucinates incorrect disambiguations or facts, the accuracy metrics may still be high as long as the correct disambiguations are included. we note, however, that this unnecessary extra information may still be penalized by the rouge-l metric. moreover, in the presence of distractors, we also expect the accuracy of the roberta model used for reading comprehension to degrade, thereby effectively penalizing a low precision.on a separate note, the disambig-f1 metric requires a high-accuracy qa system. hence, for domains that are significantly different from wikipedia, fine-tuning the roberta squadv2 model on the task might be important to ensure the effectiveness of the disambig-f1 metric. and receive an answer within a day. to support this mechanism, we allowed annotators to ""park"" an annotation task they were unsure about and return to it after they have their concerns resolved.annotators' well-being for this study, we recruited annotators who were fully dedicated to our task (8 hours a day for 5 days a week). to reduce the pressure on annotators and allow them to work at a comfortable pace, we gave annotators one hour to answer each question and recommended answering ten or more questions per day. on average, it took annotators 15 minutes to answer each question with the time consumption slightly decreasing as annotators get familiar with the task. the compensation rate for the task was set to be $17.8/hour which is higher than the minimum hourly wage in the us.","{196170479: 'Fan et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.566.pdf
441,218487272,What-if I ask you to explain: Explaining the effects of perturbations in procedural text,conclusion,Insight-tree,"explaining the effects of a perturbation is critical, and we have presented the first system that can do this reliably. quartet not only predicts meaningful explanations, but also achieves a new state-of-the-art on the end-task itself, leading to an interesting finding that models can make better predictions when forced to explain. our work opens up new directions for future research: 1) can additional background context from the web improve explainable reasoning? 2) can such structured explanations be applied to other nlp tasks? we look forward to future progress in this area.",{216035859: 'Asai and Hajishirzi 2020'},https://www.aclweb.org/anthology/2020.findings-emnlp.300.pdf
442,214795146,R 3 : A Reading Comprehension Benchmark Requiring Reasoning Processes*,conclusion,Insight-tree,"in this work, we present r 3 , a large-scale reading comprehension dataset in which a qa system is required to give answers to questions over diverse natural language, but also needed to present the reasoning processes. we hope this dataset can facilitating the development of explainable qa systems.","{67855846: 'Dua et al., 2019', 174801764: 'Min et al. 2019a', 174801080: 'Min et al., 2019b;', 52822214: 'Yang et al., 2018'}",NaN
443,234776197,If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"we introduced a simple unsupervised approach for retrieving candidate evidence chains that after reranking achieves state-of-the-art evidence retrieval performance on two multi-hop qa datasets: qasc and multirc. we highlight the importance of generating and feeding candidate evidence chains by showing several benefits over the widely followed approach that retrieves evidence sentences individually. further, we introduced few attention and embedding analyses demonstrating that jointly retrieving and reranking chains assist in learning compositional information, which is also beneficial to the downstream qa task. overall, our work highlights the strengths and potential of joint retrieval+reranking approaches for future works.","{139103297: 'Chen and Durrett, 2019', 189927857: 'Feldman and El-Yaniv, 2019;', 202712552: 'Khot et al., 2019b', 202660724: 'Nie et al., 2019;', 202773198: 'Qi et al., 2019', 202785879: 'Yadav et al., 2019b;', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/2021.naacl-main.363.pdf
444,248512731,Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI,conclusion,Insight-tree,"in order to provide an epistemologically grounded characterisation of natural language explanations, this paper attempted to bridge the gap in the notion of scientific explanation (salmon, 2006;salmon, 1984), studying it as both a formal object and as a linguistic expression. the combination of a systematic survey with a corpus analysis on natural language explanations (jansen et al., 2014;jansen et al., 2018),","{233297051: 'Dalvi et al., 2021', 222178328: 'Jhamtani and Clark, 2020;', 231883811: 'Valentino et al., 2021c', 237258250: 'Wiegreffe and Marasovic, 2021;', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2205.01809v2.pdf
445,255372929,Inflected Forms Are Redundant in Question Generation Models,conclusion,Insight-tree,"in this paper, we discover two major issues in the existing neural qg models. to tackle the two issues, we propose this enhancing approach for qg and apply the approach to two typical sequenceto-sequence models, i.e., the pointer generator network and unilm. we further conduct extensive experiments using squad and marco datasets. the experimental results show that improved versions of models can significantly enhance the quality of qg and speed up the decoding.",{},https://export.arxiv.org/pdf/2301.00397v1.pdf
446,252283929,"Machine Reading, Fast and Slow: When Do Models ""Understand"" Language?",conclusion,Insight-tree,"making progress towards trustworthy nlp models requires specific definitions for the behavior expected of these models in different situations. we propose a framework for rc model analysis that involves: (a) the definition of the expected 'reasoning' steps; (b) analysis of model behavior. we contribute such definitions for two linguistic 'skills' (comparison and coreference resolution), and use parallel explainability techniques to investigate whether rc models based on bert family encoders answer such questions correctly for the right reasons. we find that to be the case for comparison, but not for coreference. moreover, we find that, even for comparison, the models 'break' when encountering out-of-distribution counterfactual perturbations, suggesting that they memorize specific lexical patterns rather than learn more general reasoning 'skills'. as such, more research is needed on developing definitions and tests for specific 'skills' expected of nlu models, as well as on more faithful interpretability techniques.","{218487111: 'Dunietz et al. 2020', 226236740: 'Ho et al., 2020', 216868500: 'Ko et al. 2020', 86611921: 'Kwiatkowski et al., 2019', 174801764: 'Min et al., 2019;', 237513875: 'Choudhury et al. 2022', 213474484: 'Rogers et al., 2020;', 212644640: 'Schlegel et al., 2020;', 229923926: 'Wu et al., 2021', 52822214: 'Yang et al., 2018', 237498988: 'Ye et al. 2021'}",https://www.aclanthology.org/2022.coling-1.8.pdf
447,234341179,Improving Cross-Lingual Reading Comprehension with Self-Training,conclusion,Insight-tree,this paper presents the first self-training approach to improve cross-lingual machine reading comprehension. the experiments were conducted on largescale datasets in four different languages. the results showed that our approach improved the perfor-mance significantly compared to the baseline with 1 -16 em and 1 -8 f1 scores. we also analyzed how self-training improves cross-lingual reading comprehension in several aspects and found that improvements are correlated to zero-shot performance but not the number of pseudo-labels.,"{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2105.03627v1.pdf
448,219747814,Viticulture in the Laetanian Region (Spain) during the Roman Period: Predictive Modelling and Geomatic Analysis,conclusions,Insight-tree,"the general objective of the present paper was to calculate the suitabilities for archeological sites in the laetanian region related to intensive and specialized roman viticulture in order to determine the underlying factors of their distribution. therefore, the archaeological dataset of 82 documented wine-pressing facilities was used as response variable in a predictive modelling approach. furthermore, 15 topographical and 6 socio-economic location characteristics were taken into account, which may have had, as stated by other authors, an influence on the distribution of the sites. in a first step, two models were developed where the variable selection was based on expert knowledge from previous studies on roman agriculture and viticulture. however, this work additionally aimed at semi-automating the process of variable selection. hence, the variables were selected using statistical distribution metrics. the resulting models with automated variable selection showed good performance. they serve well as a first approximation of suitability modelling when no a priori knowledge on operating processes and interactions influencing the spatial distribution of sites is available. consequently, this modeling approach can be used for theory building. however, it is of special interest that the best prediction performance was obtained by an expert knowledge model utilizing a combination of predictor variables that is based on the specific recommendations on viticulture by lucius junius moderatus columella, the prominent ancient roman agronomist. the model was used to make the first assumptions and theories about the underlying factors that had an impact on the development of viticulture in the laetanian region. the results indicate that the accessibility of a location and its connectivity to the local and regional distribution centres and trade routes, determined by terrain steepness, was decisive for the settlement of winemaking facilities. moreover, on the basis of the predictive model and findings on experimental archaeology, the maximum number of winegrowing facilities that could have existed in the laetanian region in roman times was extrapolated to 360. as the applied modeling approach focusses on the average 50% and 75% of wine pressing facilities, some site locations could not be explained by the predictive model. thus, future research should particularly focus on those locations to find reliable hypotheses for settlement under these ""marginal"" and ""less suitable"" conditions. this is expected to provide more detailed insights into the complexity of ancient rural settlement and viticulture in the laetanian region.",{},https://web.archive.org/web/20200602163431/https:/res.mdpi.com/d_attachment/geosciences/geosciences-10-00206/article_deploy/geosciences-10-00206.pdf
449,237194607,MeDiaQA: A Question Answering Dataset on Medical Dialogues,conclusion,Insight-tree,"in this paper, we construct mediaqa, a novel dataset for qa on medical dialogues and propose a method media-bert based on the state-of-the-art pretrained language model. the proposed dataset shows the distinctiveness of medical dialogues compared with other normal domains in the context of qa. due to the unique characteristics, it is challenging for existing qa models compared with human performance. we hope our dataset will lead a deeper research on machine reading comprehension in medical domain.",{},https://arxiv.org/pdf/2108.08074v1.pdf
450,236429000,Thought Flow Nets: From Single Predictions to Trains of Model Thought,conclusion,Insight-tree,"in this paper, we introduced a task-agnostic self-correction formalism that turns a model's single output prediction into an evolving sequence of predictions-the thought flow. we take inspiration from hegel's dialectics and propose a correction module along with a gradient-based update rule that sequentially updates a model's output distributions in the direction of an increasing self-estimate of correctness. we apply our method to question answering models and conduct extensive experiments including human evaluation. we find that thought flows (i) can increase f 1 -scores up to 9.3%, (ii) exhibit complex self-correction patterns and (iii) provide significant improvements in human interaction and system perception including task performance and perceived system correctness and naturalness. a potential next step to further improve performance is learning to stop.",{},https://export.arxiv.org/pdf/2107.12220v2.pdf
451,258236093,CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval,conclusions,Insight-tree,this paper proposes to pre-train with the mixtureof-textual-experts to counter the imbalanced discrimination issue in existing dual encoders. textual-specific experts are introduced for individual modeling of the distinct traits of queries and passages. results on large-scale web bench-marks show steady improvement in retrieval performances. quantitive analysis shows a more balanced distribution of query-passage embeddings for dual-encoders.,{},https://export.arxiv.org/pdf/2304.10195v1.pdf
452,248780575,Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking,conclusion,Insight-tree,"we introduce an effective dicos-dst that dynamically selects the relevant dialogue contents corresponding to each slot from a combination of three perspectives. the dialogue collaborative selector module performs a comprehensive selection for each turn dialogue based on its relation to the slot name, its connection to the current turn dialogue, and the implicit mention oriented reasoning. then only the selected dialogue contents are fed into state generator, which explicitly minimizes the distracting information passed to the downstream state prediction. our dicos-dst model achieves new state-of-the-art performance on the multiwoz benchmark, and achieves competitive performance on most other dst benchmark datasets. the potential relationship among the above perspectives is a promising research direction, and we will explore it for more than dialogue selection in the future. ","{215745470: 'Gao et al., 2020', 215768725: 'Groeneveld et al., 2020;', 155100120: 'Qiu et al., 2019;', 207870753: 'Tu et al., 2020;', 158046817: 'Tu et al., 2019'}",https://www.aclanthology.org/2022.acl-long.165.pdf
453,256597851,LIQUID: A Framework for List Question Answering Dataset Generation,conclusion,Insight-tree,"herein, we introduced liquid, a framework that automatically generates list qa datasets from unlabeled corpora to alleviate the data scarcity problem in this field. our synthetic data significantly improved the performance of the current supervised models on five benchmark datasets. we thoroughly analyzed the effect of each component in liq-uid and generated data quantitatively and qualitatively.   table 10: distribution of answer types for the synthetic and bioasq 9b data. table 9 presents the distribution of the number of answers. similar to the results in the general domain (table 5), the synthetic data were more skewed toward smaller numbers of answers than the labeled data, but some answers (14.2%) had four or more spans.","{250390687: 'Li et al. 2022', 211258652: 'Puri et al. 2020;', 52822214: 'Yang et al. 2018;', 215768766: 'Ye et al. 2020'}",https://export.arxiv.org/pdf/2302.01691v2.pdf
454,258959550,Nonparametric Decoding for Generative Retrieval,limitations,Insight-tree,"np decoding uses k-means clustering to reduce the number of contextualized embeddings, the performance varies by how the contextualized embeddings are clustered. as the process is relatively inconsistent, reducing the number with other methods would make the model performance more consistent. also, as it is not trivial to add new contextualized token embeddings on top of preconstructed ce due to the clustering step, we did not perform on dynamic corpus setup where new items are added or updated. np decoding is applicable to all generative retrieval models including gmr or seal which needs all token embeddings, however, we focused on generative retrieval models with representative output as the retrieval target in this work. also, while it is a general approach applicable to all encoder-decoder models, we focused on applying the method to t5.",{},https://export.arxiv.org/pdf/2210.02068v3.pdf
455,258823123,CRITIC: LARGE LANGUAGE MODELS CAN SELF- CORRECT WITH TOOL-INTERACTIVE CRITIQUING,conclusion,Insight-tree,"we propose critic, a novel plug-and-play framework that empowers frozen llms to self-verify and self-correct by interacting with the external environment. leveraging the intuition of critical thinking with external feedback, critic enables llms to validate their knowledge and improve their answers through introspection without requiring further training. experiments on diverse tasks and datasets have consistently shown the effectiveness, generality, and interoperability of critic. moreover, we shed light on the unreliability of llms in self-verification, highlighting the potential of external tool interaction to solve this problem. we hope our findings will inspire further exploration into the truthfulness of language models, ultimately leading to more trustworthy ai systems. ","{246652372: 'Ji et al., 2023'}",https://export.arxiv.org/pdf/2305.11738v2.pdf
456,221970302,ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS WITH MULTI-HOP DENSE RETRIEVAL,conclusion,Insight-tree,"in this work, we generalized the recently proposed successful dense retrieval methods by extending them to the multi-hop setting. this allowed us to handle complex multi-hop queries with much better accuracy and efficiency than the previous best methods. we demonstrated the versatility of our approach by applying it to two different tasks, using a variety of downstream modules. in addition, the simplicity of the framework and the fact that it does not depend on a corpus-dependent graph structure opens the possibility of applying such multi-hop retrieval methods more easily and broadly cross different domains and settings.","{208267807: 'Asai et al., 2020', 211296452: 'Dhingra et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 174801080: 'Min et al., 2019;', 202660724: 'Nie et al., 2019', 211258645: 'Perez et al., 2020', 202773198: 'Qi et al., 2019', 211003735: 'Wolfson et al. 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2009.12756v1.pdf
457,128345225,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,conclusions,Insight-tree,"pullnet is a novel integrated qa framework for (1) learning what to retrieve from a kb and/or corpus and (2) reasoning with this heterogeneous data to find the best answer. unlike prior work, pullnet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. in each iteration, a graph cnn is used to identify subgraph nodes that should be expanded using ""pull"" operations on the corpus and/or kb. this iterative process makes it possible to retrieve a small graph that contains just the information relevant to a multi-hop question.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/D19-1242.pdf
458,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,number of premises needed for conclusions,Insight-tree,"we show the accuracy of the examples with different numbers of premises needed to reach the conclusions in figure 3. under the few-shot prompting setting, gpt-3 and","{236459873: 'Aggarwal et al., 2021;', 237450610: 'Bostrom et al., 2021', 215785913: 'Chen et al., 2020b;', 211126663: 'Clark et al., 2021', 67855846: 'Dua et al., 2019;', 232478685: 'Nan et al., 2022;', 219573621: 'Talmor et al., 2020;', 243865235: 'Chen et al., 2021', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2209.00840v1.pdf
459,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,performance on conclusions with different labels,Insight-tree,confusion matrices in figure 4 for the fine-tuning and 8-shot nl prompt results both show that lms are significantly better at making the correct predictions for conclusions with labels of true than the conclusions with labels of false or unknown. the accuracy on examples with false or unknown conclusions is 54.41% with fine-tuning and 36.91% with few-shot prompting. they also tend to make more predictions of true than the other labels.,"{236459873: 'Aggarwal et al., 2021;', 237450610: 'Bostrom et al., 2021', 215785913: 'Chen et al., 2020b;', 211126663: 'Clark et al., 2021', 67855846: 'Dua et al., 2019;', 232478685: 'Nan et al., 2022;', 219573621: 'Talmor et al., 2020;', 243865235: 'Chen et al., 2021', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2209.00840v1.pdf
460,252070866,FOLIO: Natural Language Reasoning with First-Order Logic,conclusion,Insight-tree,"we introduced folio, an expert-written dataset for first-order logic (fol) reasoning equipped with parallel fol formulas. the examples in folio are created based on real-world knowledge with natural language. it exhibits a large number of distinct logic patterns and a large vocabulary. experiments show that the performance of one of the most capable llms publicly available is only slightly better than chance with few-shot prompting on hyblogic, a subset of folio, and llms are especially bad at predicting the correct truth values for false and unknown conclusions.","{236459873: 'Aggarwal et al., 2021;', 237450610: 'Bostrom et al., 2021', 215785913: 'Chen et al., 2020b;', 211126663: 'Clark et al., 2021', 67855846: 'Dua et al., 2019;', 232478685: 'Nan et al., 2022;', 219573621: 'Talmor et al., 2020;', 243865235: 'Chen et al., 2021', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2209.00840v1.pdf
461,235364000,Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study,conclusion,Insight-tree,"we conduct a comprehensive analysis on the book qa task, taking the representative narrativeqa dataset as an example. firstly, we design the book qa techniques by borrowing the wisdom from the cutting-edge open-domain qa research and demonstrate through extensive experiments that (1) evidence retrieval in book qa is difficult even with the state-of-the-art pre-trained lms, due to the factors of rich writing style, recurrent book plots and characters, and the requirement of high-level story understanding; (2) our proposed approaches that adapt pre-trained lms to books, especially the prereading technique for the reader training, are consistently helpful.","{202558815: 'Min et al., 2019', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2106.03826v1.pdf
462,253018998,Inferring Implicit Relations in Complex Questions with Language Models,conclusion,Insight-tree,"we propose the task of implicit relation inference, which decouples inference of reasoning steps from their execution. we introduce implicitrela-tions, a benchmark that includes more than 2,000 annotated implicit relations. we show large lms can infer implicit relations across multiple types of questions and reasoning skills, but this success does not translate to an improvement in answering implicit reasoning questions. our work sheds light on capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for improving the ability of models to infer implicit relations.","{230799347: 'Geva et al., 2021', 239998631: 'Kalyan et al., 2021', 225075843: 'Lin et al., 2021;', 174801080: 'Min et al., 2019;', 211258645: 'Perez et al., 2020;', 219573621: 'Talmor et al., 2020;', 237263476: 'Talmor et al., 2021', 211003735: 'Wolfson et al., 2020;', 52822214: 'Yang et al., 2018;', 233219869: 'Yasunaga et al., 2021;'}",https://export.arxiv.org/pdf/2204.13778v2.pdf
463,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,conclusion,Insight-tree,"this work proposes a novel method that extracts distant and incidental signals from parallel news to facilitate general question representation. such parallel news signals intuitively bridge the reasoning gap in pre-trained language models due to reporting biases. to support this intuition, we train a model named decompt5 on such distant supervision and show that it improves 20%-30% on two semantic parsing benchmarks, namely overnight and torque, that directly evaluate query understanding. with decompt5 as the basis, we design a well-motivated question-answering pipeline decompentail that follows a decomposition, correction, and entailment scheme. we show that decompentail improves on strategyqa and hotpotqa by 3.7% and 8%, respectively.","{165163607: 'Clark et al., 2019', 230799347: 'Geva et al. 2021', 248666080: 'Khot et al., 2022', 174801080: 'Min et al. 2019', 211258645: 'Perez et al. 2020', 52822214: 'Yang et al., 2018b'}",https://www.aclanthology.org/2022.emnlp-main.142.pdf
464,253237669,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,limitations,Insight-tree,"in this section, we discuss some of the limitations of our work, and motivate future works. limited question formats. our proposed qa pipeline operates on binary yes/no questions. while binary questions are very general, as most other questions can be re-written into similar forms, such transformations have not been designed or evaluated, which motivates future works. limited factual correction coverage. we use gpt-3 as the backbone for our factual correction step. although it is shown to be effective, it is not as deterministic as wikipedia-based ir approaches, and we cannot easily interpret why it makes mistakes and understand how to improve.","{165163607: 'Clark et al., 2019', 230799347: 'Geva et al. 2021', 248666080: 'Khot et al., 2022', 174801080: 'Min et al. 2019', 211258645: 'Perez et al. 2020', 52822214: 'Yang et al., 2018b'}",https://www.aclanthology.org/2022.emnlp-main.142.pdf
465,208193847,D-NET: A Simple Framework for Improving the Generalization of Machine Reading Comprehension,conclusions,Insight-tree,"in this paper, we describe a simple baseline system that baidu submitted for the mrqa 2019 shared task. our system is built on a framework of pre-training and fine-tuning, namely d-net. d-net employs the techniques of pre-trained lan-guage models and multi-task learning to improve the generalization of mrc models and we conduct the experiments to examine the effectiveness of these strategies.",{},https://www.aclweb.org/anthology/D19-5828.pdf
466,235097557,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,conclusion and future work,Insight-tree,"in this paper, we propose a novel method for accelerating bert inference, called tr-bert, which prunes bert at token-level granularity. specifically, tr-bert utilizes reinforcement learning to learn a token selection policy, which is able to select general meaningful tokens in the bottom layers and select task-relevant tokens in the top layers. experiments on eleven nlp tasks demonstrate the effectiveness of tr-bert as it accelerates bert inference by 2-5 times for various performance demand. besides, tr-bert achieves a better quality and speed trade-off on long-text tasks, which shows its potential to process large amounts of information in the real-world applications.","{202660724: 'Nie et al., 2019', 173188058: 'Talmor and Berant, 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2021.naacl-main.463.pdf
467,248377036,Evaluating Extrapolation Performance of Dense Retrieval,conclusions,Insight-tree,"in this paper, we propose a simple yet effective method that evaluates the extrapolation performance of dr models, i.e., how dr models perform on queries that are distinct from the training queries. with the proposed evaluation method, we first revisit how existing dr models perform in the extrapolation regimes. results lead to several non-trivial findings that have been concealed by the existing evaluation protocol. concretely, dr is substantially more vulnerable to extrapolation than the interaction-based deep neural ranking models, and pretraining is a more effective method to improve the extrapolation ability of dr than finetuning techniques. then we further interpret our extrapolation performance by investigating its relationship with the domain transfer ability. results suggest that the extrapolation performance is a potential indicator of the domain transfer ability, further highlighting the feasibility of our methods to evaluate the generalization ability of dr models. although this paper focuses on evaluating how dr models extrapolate, the methodologies can also be used for other models in the future.","{86611921: '[27]', 233296016: '[39,', 220302524: '[47]'}",https://arxiv.org/pdf/2204.11447v1.pdf
468,250390687,MultiSpanQA: A Dataset for Multi-Span Question Answering,conclusion,Insight-tree,"we present multispanqa, a reading comprehension dataset where answers consist of multiple discrete spans. as part of this, we proposed a method for classifying the semantic structure of answers, based on the semantic relation between answer spans. we also provide an expanded version of the dataset which includes unanswerable questions and single-answer questions, to make it both more challenging and more realistic. we additionally presented a number of models for multi-span qa extraction, and found that the best-performing model was sequence tagging-based, augmented by a span number prediction module and span adjustment module.","{67855846: 'Dua et al., 2019', 196170479: 'Fan et al., 2019', 86611921: 'Kwiatkowski et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.naacl-main.90.pdf
469,235212203,Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance,conclusions,Insight-tree,"in this work, we propose qfs-bart, an abstractive summarization model for query focused summarization. we use a generalizing qa model to make explicit answer relevance scores for all words in the document and combine them to the encoder-decoder attention. we also leverage pretrained model (e.g. bart) and two-stage finetuning method which further improve the summarization performance significantly. experimental results show the proposed model achieves state-ofthe-art performance on debatepedia dataset and outperforms several comparable baselines on duc 2006-7 datasets. hal daumÃ© iii and daniel marcu. 2006  in this paper, we introduce a two-step architecture: 1) retrieve answer-related sentences given the query, rank them by the confidence score (generated from equation 4) and concatenate them. 2) use our qfs-bart to produce an abstractive summary.","{208000835: 'Su et al. 2019', 226262229: 'Xu and Lapata, 2020b'}",https://arxiv.org/pdf/2105.12969v2.pdf
470,245437737,Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics,"summary, conclusion, and future work",Insight-tree,"textual novelty detection has an array of use-cases starting from search and retrieval on the web, nlp tasks like plagiarism detection, paraphrase detection, summarization, modeling interestingness, fake news detection, and so forth. however, less attention is paid to the document-level variant of the problem in comparison to sentence-level novelty detection. in this work, we present a comprehensive account of our experiments so far on document-level novelty detection. we study existing literature on textual novelty detection as well as our earlier explorations on the topic. here we assert that we would need to perform information assimilation from multiple premises to identify the novelty of a given text. our current approach performs better than our earlier approaches. also, we show that our method could be suitably applied to allied tasks like plagiarism detection and paraphrase detection. we point out some limitations of our approach, which we aim to explore next.",{128344862: 'Trivedi et al. 2019'},NaN
471,262044969,Selecting which Dense Retriever to use for Zero-Shot Search,conclusion,Insight-tree,"this paper proposes a novel research direction for zero-shot dense retrieval.while traditional information retrieval research in this area concentrates on developing universal domain-agnostic dr models, our work shifts the focus towards developing a method to rank and select pre-trained state-of-the-art dr models that are best suited for a specific target domain corpus.we acknowledge that the proposed direction does not contradict traditional research on training zero-shot dr models, but rather complements it.as newly developed dr models are likely to have varying effects on different domains, selecting the best model is still beneficial.to explore this research direction, we adapt various methods from computer vision and machine learning, along with some approaches designed for ir.we outline our reasoning and challenges with the investigated approaches and present empirical results on a popular zeroshot benchmark dataset.our findings shed light on future research avenues within this research direction.we believe that an effective method for selecting a good dr model can provide a principled way for search engine developers to identify the most suitable model for their application, ultimately enhancing user experience.","{249097975: '[24]', 233296016: '[41]', 245131402: '44]', 220302524: '[46]'}",https://export.arxiv.org/pdf/2309.09403v1.pdf
472,221655732,Multi-Hop Fact Checking of Political Claims,conclusions,Insight-tree,"in this paper, we studied the novel task of multi-hop reasoning for fact checking of real-world political claims, which encompasses both evidence retrieval and claim veracity prediction. we presented politihop, the first political fact checking dataset with annotated evidence sentences. we compared several models on politihop and found that the multi-hop architecture transformer-xh slightly outperforms bert in most of the settings, especially in terms of evidence retrieval, where bert is easily fooled by named entity overlaps between the claim and evidence sentences. the performance of transformer-xh is further improved when retrieving more than two evidence sentences and the number of hops larger than one, which corroborates the assumption of the multi-hop nature of the task. in the first setting, the models are trained for 4 epochs on liar-plus. in the second setting, the models are trained for 8 epochs on politihop. in the third setting, models are trained for 4 epochs on liar-plus, followed by 4 epochs on politihop. in every setting, models are evaluated on the dev set and the model with the best label prediction macro-f1 score is saved, which enables early stopping. for the fourth setting, we pre-train the model for 2 epochs on the fever dataset, followed by 4 epochs on liar-plus, the fine-tune on politihop for 4 epochs.",{},https://arxiv.org/pdf/2009.06401v3.pdf
473,202712552,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,conclusion,Insight-tree,"we focus on the task of question answering under partial knowledge: a novel task that lies inbetween open-domain qa and reading comprehension. we identify classes of knowledge gaps when reasoning under partial knowledge and collect a dataset targeting one common class of knowledge gaps. we demonstrate that identifying the knowledge gap first and then reasoning by filling this gap outperforms previous approaches on the openbookqa task, with and even without additional missing fact annotation. this work opens up the possibility of focusing on other kinds of knowledge gaps and extending this approach to other datasets and tasks (e.g., span prediction).  boiling point means temperature above which a liquid boils figure 10: visualization of the models behavior with the predicted span, top predicted relation, and the top fact used by model. the heat map shows the confidence of the model for all the relations for each input sentence (first five) and conceptnet sentencized tuple (last but one) and the back-off tuple (last one) to capture the knowledge in the embeddings.",{},https://www.aclweb.org/anthology/D19-1281.pdf
474,257279774,UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers,conclusion,Insight-tree,"we present udapdr, a novel strategy for adapting retrieval models to new domains.udapdr uses synthetic queries created using generative models, such as gpt-3 and flan-t5 xxl, to train multiple passage rerankers on queries for target domain passages.these passage rerankers are then distilled into colbertv2 to boost retrieval accuracy while keeping query latency competitive as compared to other retrieval systems.we validate our approach across the lotte, beir, nq, and squad datasets.additionally, we explore various model configurations that alter the generative models, prompting strategies, retriever, and passage rerankers used in our approach.we find that udapdr can boost zero-shot retrieval accuracy on new domains without the use of labeled training examples.we also discuss several directions for future work.","{230437663: 'Khattab et al., 2021;', 255186555: 'Khattab et al., 2022', 86611921: 'Kwiatkowski et al., 2019', 221507798: 'Petroni et al., 2021', 244799249: 'Santhanam et al., 2022b', 233296016: 'Thakur et al., 2021', 245131402: 'Wang et al., 2022'}",https://export.arxiv.org/pdf/2303.00807v3.pdf
475,202583429,Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering,conclusion,Insight-tree,"this paper introduces an important sub-problem of bridge reasoning for the task of multi-hop qa in the open-domain setting. we propose a bridge reasoner that utilizes multiple types of evidence to derive the passages that cover the answers. the reasoner significantly improves the coverage of answer passages than ir methods. with the predicted passages, we show that a standard reading comprehension model is able to achieve similar performance as the state-of-the-art method that requires bert in multiple modules.",{52822214: 'Yang et al. 2018'},https://www.aclweb.org/anthology/D19-5806.pdf
476,2787275,A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains,conclusions,Insight-tree,"we propose to approximate a pomdp by using a region observable pomdp. the region observable pomdp has more informative observations and hence is easier to solve. a method for determining approximation quality is described, which allows one to make the tradeo between approximation quality and computational time by starting with a coarse approximation and re ning it gradually. simulation experiments have shown that when there is not much uncertainty in the e ects of actions and observations are informative, a pomdp can be accurately approximated by a region observable pomdp that can be solved exactly. however, this becomes infeasible as the degree of uncertainty increases. other approximate methods need to be incorporated in order to solve region observable pomdps whose radiuses are not small. proof of proposition 3: because of proposition 1 and lemma 4, it su ces to show that 3. if x , return nil, else return b.",{},https://arxiv.org/pdf/cs/9711103v1.pdf
477,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,limitations and difficulties of present work,Insight-tree,the test results have indicated that the system developed needs further enhancement such as optimization of the system performance and efficiency. some of the suggestions are: fig. (19). blast alignment results.,{},NaN
478,12625226,Development of an Integrated Suite of Software in Analysing of Large DNA Databases,conclusion,Insight-tree,"the results obtained from the tests have shown that the integrated software tool performs as expected. the work suggested that by integrating blast and fasta (two widely used and freely available algorithms), plus an additional implementation of psa and tr analysis tools, with the rest of the supporting tools (database management) developed, it is entirely possible to have an initial working version of the software tool for criminal dna analysis and de-tection work. the system has great potential and that the results obtained during the tests were satisfactory. the following observations can be made: â¢ psa algorithm is suitable for smaller sequence alignments as compared to blast and fasta, which are designed for large sequence database searches.",{},NaN
479,235790764,Summary-Oriented Question Generation for Informational Queries,conclusion,Insight-tree,"we tackle the problem of question generation targeted for human information seeking using automatic question answering technology. we focus on generating questions for news articles that can be answered by longer passages rather than short text spans as suggested questions. we build a bert-based pointer-generator network as the qg model, trained with the natural questions dataset. our method shows state-of-the-art performance in terms of bleu, meteor, and rouge l scores on our nq question generation dataset. we then apply our model to the out-of-domain news articles without further training. we use a qa system to evaluate our qg models as there are no gold questions for comparison. we also conduct a human evaluation to confirm the qa evaluation results.",{},https://www.aclanthology.org/2021.dialdoc-1.11.pdf
480,215745470,From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap,conclusion,Insight-tree,"task-oriented dialogue systems aim to help users to achieve a variety of tasks. it is not unusual to have hundreds of different domains in modern taskoriented virtual assistants. how can we ensure the dialogue system is robust enough to scale to different tasks given limited amount of data? some approaches focus on domain expansion by training on several source domains and then adapting to the target domain. while such methods can be successful in certain cases, it is hard for them to generalize to other completely different out-of-domain tasks.",{},https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf
481,251223486,Few-shot Adaptation Works with UnpredicTable Data,limitations & future work,Insight-tree,"the unpredictable dataset may contain inaccuracies, biases, and inappropriate content. we do not recommend using this dataset to train models for deployment, but release this primarily as a research resource. we do not introduce any new model capabilities that lead to different risks than the usual risks associated with model usage. our work highlights the unpredictability of model behavior given various training datasets which calls for heightened vigilance for behavior changes after finetuning. our design choices in using table data for fsl training led to a dataset that is quite different than typical nlp datasets, so specific results from training on our dataset may not fully generalize to other kinds of datasets. further work may consider other methods for converting tables to tasks, other sources of tables besides wtc, or other structured datasets besides tables. our experiments focused on modestly-sized models (gpt-2 large, 750m parameters) so our conclusions may not hold for larger models. our evaluations are limited to multiple-choice tasks. future work may extend our analyses with larger models and other tasks including freeform generation.",{},https://www.aclanthology.org/2023.acl-long.102.pdf
482,251223486,Few-shot Adaptation Works with UnpredicTable Data,conclusion,Insight-tree,"we produced unpredictable, a dataset of 413,299 diverse few-shot learning tasks from internet tables. finetuning on unpredictable improves the fsl ability of lms. however, the size of our dataset is not the key factor in its success. we find that certain narrow datasets (even ones made of trivia) are even more helpful than diverse, curated nlp datasets. finetuning on these narrow datasets leads to strong improvements on the same test tasks as finetuning on diverse, curated nlp datasets. this suggests that finetuning on these datasets cause domain-agnostic fsl gains, though we were unable to find clear patterns to explain why this happens for some data and not others. our results question common wisdom that task diversity is necessary for adapting lms to fsl. we hope our work spurs investigation on what data causes few-shot learning to emerge, both to develop better datasets and to better understand how training data leads to unexpected behaviors or failures.",{},https://www.aclanthology.org/2023.acl-long.102.pdf
483,67855846,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,conclusion,Insight-tree,"we have presented drop, a dataset of complex reading comprehension questions that require discrete reasoning over paragraphs. this dataset is substantially more challenging than existing datasets, with the best baseline achieving only 32.7% f1, while humans achieve 96%. we hope this dataset will spur research into more compre-hensive analysis of paragraphs, and into methods that combine distributed representations with symbolic reasoning. we have additionally presented initial work in this direction, with a model that augments qanet with limited numerical reasoning capability, achieving 47% f1 on drop.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/1903.00161v1.pdf
484,257405222,AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation,discussion and conclusion,Insight-tree,"in this study, a series of scalable augmentation methods are proposed to produce surrogate queries for training dense retrievers without using any annotated query-document pairs. we achieve state-ofthe-art performance on two collections of widely used benchmarks (beir and six odqa datasets), demonstrating that the efficacy of synthetic querydocument pairs for training dense retrievers, greatly bridging the gap between unsupervised dense models and bm25 and inspiring us to rethink the necessity of using real queries.",{},https://export.arxiv.org/pdf/2212.08841v2.pdf
485,237940507,More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering,conclusion,Insight-tree,"in this survey, we reviewed 47 textual qa benchmark datasets and their corresponding evaluation metrics. a novel taxonomy of textual qa tasks is provided from an application scenario point of view. a detailed description is provided for each dataset, which covers the task definition, contraction method, statistics, and evaluation measures. key features and detailed statistics among the benchmark datasets are summarized and compared in the form of tables. detailed description and distribution analysis of evaluation metrics are provided. finally, we summarized the trends of the recent textual qa benchmark contraction methods and give our opinions on the future directions of the textual qa benchmark research. we hope this work serves as a good introduction to textual qa tasks.","{231709697: '[9]', 52822214: '[13]', 226236740: '[14]', 215785913: '[15]', 196170479: '[16]', 233189637: '[18]', 221507798: '[25]', 53116244: '[30]', 230433817: '[33]', 86611921: '[48]'}",https://arxiv.org/pdf/2109.12264v2.pdf
486,251224044,Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation,conclusion,Insight-tree,"in this paper, a unified framework is presented for the pre-training of sentence representation. on top of the consecutive mae style pre-training on generic and domain-specific data, and with proper collaboration with contrastive learning, the generated model may support a wide variety of sentence representation tasks, including zero-shot retrieval, in-domain retrieval, and sentence embeddings. the experiment studies demonstrate that the proposed framework helps to achieve strong performances on benchmarks, like beir, ms marco, natural questions, sts and transfer tasks in senteval. for future works, we'll make extensions by scaling up the size of the encoding network and having it pre-trained on more unsupervised data, which will push to the empirical limit of the current method. we'll also explore its effectiveness in other languages other than english, and its impact to more applications beyond dense retrieval and nli.","{86611921: 'Kwiatkowski et al., 2019'}",https://export.arxiv.org/pdf/2208.00231v1.pdf
487,252367252,ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots,conclusion,Insight-tree,"in this work, we proposed the screenqa task. we annotated a large-scale screenqa dataset, which contains more than 80,000 question-answer pairs. compared to other vision-language multimodal problems, such as document image understanding and visual question answering, screenqa poses its unique challenges: rich in text, diverse in apps, and blended with icons and symbols. we hope to use the screenqa task and the dataset to encourage the community to look into this screen content understanding problem, as it enables new technologies and new user experiences. figure 4: data annotation interfaces for question and answer collection. a) question annotation was performed in a sequential manner, the later and non-overlapping annotators can see all previous questions to diversify question framing and avoid duplication. we also used the sequential process to provide more feedback and training to the annotators for quality improvement. b) the answer annotators were tasked to determine if the question is valid and if the question is answerable from the screen context. if both are positive, the annotators need to answer the questions by 1) selecting or drawing the bounding boxes of ui elements, 2) fill the text for each selected/drawn bounding box on right right, and 3) ranking them appropriately. the annotators were also tasked to review and make necessary corrections if the question has grammatical errors or typos.  a) the two question annotation passes were capped at five and three questions, respectively, resulting in the maximum eight questions in total. b) the cases when a single bounding box forms a sufficient answer amount to 92% of the questions, hence removed from the chart for the clarity of the long tail. anything beyond 11 bounding boxes is less than 0.05%, accumulatively less than 0.1%.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2209.08199v1.pdf
488,257365136,MULTITASK PROMPT TUNING ENABLES PARAMETER-EFFICIENT TRANSFER LEARNING,conclusion,Insight-tree,"we introduced and studied multitask prompt tuning (mpt), which learns a single transferable prompt by decomposing and distilling knowledge from multiple source tasks and their task-specific source prompts. mpt decomposes the task prompt as the hadamard product of a shared prompt matrix and a rank-one task-specific matrix. the shared component is then transferred and adapted to target tasks for further tuning. empirically we found this approach enables parameter-efficient transfer learning to target downstream tasks across diverse nlp benchmarks, even outperforming the full finetuning baseline in some cases, despite tuning much fewer task-specific parameters.","{231718729: 'Aghajanyan et al., 2021a;', 204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 207756753: 'Nie et al., 2020', 218487733: 'Vu et al., 2020;', 239009558: 'Vu et al. 2022'}",https://export.arxiv.org/pdf/2303.02861v1.pdf
489,226278099,HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification,conclusion,Insight-tree,"we present hover, a fact extraction and verification dataset requiring evidence retrieval from as many as four wikipedia articles that form reasoning graphs of diverse shapes. we show that the performance of existing state-of-the-art models degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of robust many-hop reasoning in achieving strong results. we hope that hover will encourage the development of models capable of performing complex many-hop reasoning in the tasks of information retrieval and verification.   document retrieval, sentence selection, and claim verification. the fine-tuning is done with a batch size of 16 and the default learning rate of 5e-5 without warmup. we set k r = 20, k p = 5, Îº p = 0.5, and Îº s = 0.3 based on the memory limit and the dev set performance. we select our system with the best dev-set verification accuracy and report its scores on the hidden test set. the entire pipeline is visualized in fig. 2. for document retrieval and sentence selection tasks, we fine-tune the bert on 4 nvidia v100 gpus for 3 epochs. the training of both tasks takes around 1 hour. for claim verification task, we fine-tune the bert on a single nvidia v100 for 3 epochs. the training finishes in 30 minutes.","{208267807: 'Asai et al., 2020', 139103297: 'Chen and Durrett 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf
490,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,conclusion,Insight-tree,"in this paper, we propose mr. right, a multimodal retrieval dataset for information retrieval. mr. right covers three types of text-based search queries with different modality information, including text-related, image-related, and mixed, to simulate real-world search situations. further, our dataset provides documents with texts and images to develop multimodal representation. we build our end-to-end multimodal retrieval model for mr. right to unify features across modalities. compared to the previous text and image retrieval frameworks, multimodal retrieval shows improvements on different queries and points out the balance between modalities. however, current multimodal models still have a significant gap to human performance, showing the potential of mr. right as a challenge in multimodal retrieval. we believe mr. right can breathe new insights into information retrieval for more robust retrieval systems.",{},https://export.arxiv.org/pdf/2209.13764v1.pdf
491,252567884,Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text,limitations and future work,Insight-tree,"in mr. right, we only consider text-based queries, which may limit the search modalities from users. we can expand our dataset with additional domain queries and documents such as images, audio, and video. further, mr. right focuses on the materials in wikipedia. we can explore other sources such as news, blogs, or commercial websites. mr. right is a preliminary attempt to explore multimodal retrieval, and there are still challenges we need to analyze and study in future work. ",{},https://export.arxiv.org/pdf/2209.13764v1.pdf
492,246016165,Double Retrieval and Ranking for Accurate Question Answering,conclusion,Insight-tree,"in this paper, we propose, dar, a transformer architecture based on two reranking heads: (i) the answer reranker (as2 model) and the answer support q: how many viewers did ""family guy"" premier to? c1: family guy officially premiered after fox's broadcast of super bowl xxxiii on january 31, 1999, with ""death has a shadow. c2: the show debuted to 22 million viewers, and immediately generated controversy regarding its adult content. c3: at the end of its first season, the show was #33 in the nielsen ratings, with 12.8 million households tuning i.","{202773198: 'Qi et al., 2019', 221970302: 'Xiong et al., 2020;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.findings-eacl.130.pdf
493,207853131,Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,conclusion,Insight-tree,"we use neural representations of large-scale commonsense knowledge graphs (comet) to generate contextualized knowledge graphs on demand for zero-shot question answering. our approach dynamically constructs a knowledge graph of commonsense inferences related to a presented context and conditions on it to evaluate answer options for a posed question. we use probabilistic inference to reason over the constructed graph to select the most likely answer to a question. our approach exceeds the performance of large-scale pretrained language models at the zero-shot setting by 8.5% on the socialiqa dataset. furthermore, on both the socialiqa and storycommonsense datasets, dynamically generating a contextualized commonsense knowledge graph performs better than using comet to directly answer questions.",{},https://arxiv.org/pdf/1911.03876v1.pdf
494,254246737,What is Not in the Context? Evaluation of Few-shot Learners with Informative Demonstrations,conclusion,Insight-tree,"this work introduces a task of conceptual few-shot learning, that reflects on language models' ability to learn in-context and apply a specific, possibly novel reasoning concept. we find that current incontext few-shot learners are largely insensitive to the concepts presented in demonstrations, suggesting that their ability to learn in context might be conditioned by other factors, such as their memo-rization capacity, rather than their level of task comprehension. conceptual few-shot learning poses a challenge of more controllable and scalable fewshot learning to future few-shot learners.","{233296924: 'Bartolo et al., 2021', 218486753: 'Inoue et al., 2020', 240288835: 'Min et al. 2022a', 232035689: 'Wiegreffe and MarasoviÄ, 2021'}",https://export.arxiv.org/pdf/2212.01692v1.pdf
495,221971009,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,conclusion,Insight-tree,"in short, we propose sparta, a novel ranking method, that learns sparse representation for better open-domain qa. experiments show that the proposed framework achieves the state-of-the-art performance for 4 different open-domain qa tasks in 2 languages and 11 retrieval qa tasks. this confirm our hypothesis that token-level interaction is superior to sequence-level interaction for better evidence ranking. analyses also show the advantages of sparse representation, including interpretability, generalization and efficiency. our findings also suggest promising future research directions. the proposed method does not support multi-hop reasoning, an important attribute that enables qa systems to answer more complex questions that require collecting multiple evidence passages. also, current method only uses a bag-ofword features for the query. we expect further gain by incorporating word-order information.","{198229624: 'Joshi et al., 2020'}",https://www.aclweb.org/anthology/2021.naacl-main.47.pdf
496,259341944,Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment,conclusion,Insight-tree,"we proposed a novel unified pseudo-labeling framework (upl-ea) that addresses the problem of confirmation bias for pseudo-labeling-based entity alignment across kgs. upl-ea employs an entity alignment model based on the global-local aggregation architecture to generate informative entity embeddings. in addition, upl-ea includes two modules to combat confirmation bias: within-iteration optimal transport (ot)-based pseudo-labeling and cross-iteration pseudo-label calibration. the ot-based pseudo-labeling module utilizes ot modeling to eliminate conflicted misalignments (type i pseudo-labeling errors) within each iteration. the pseudo-label calibration module employs pseudo-labels from multiple consecutive iterations to reduce pseudo-label selection variability, thus preventing the accumulation and propagation of inevitable one-to-one misalignments (type ii pseudo-labeling errors) across iterations. our extensive experiments on benchmark datasets show that upl-ea outperforms state-of-the-art baselines with limited amounts of prior alignment seeds. the competitive performance of upl-ea demonstrates its superiority in addressing confirmation bias and its potential for pseudo-labeling-based entity alignment across kgs.",{},https://export.arxiv.org/pdf/2307.02075v1.pdf
497,257019916,"Complex QA & language models hybrid architectures, Survey",conclusion,Insight-tree,"in this paper, we present a comprehensive survey of language model hybrid architectures for answering complex questions. we review the various skills required and typical approach, datasets and metrics that are used, the current limits of large language models for complex qa, the potential of hybrid architectures, better training and prompting strategies for this goal. we also identify the main challenges and research avenues for solving more complex questions including knowledge capitalization. we identify the need to address multi-sensitivity data in language models architectures and potential approaches. finally, we outline research topics and highlight the potential of exploration in this field. this paper aims to provide a comprehensive and useful resource for readers interested in the development of complex non-factoid question answering.","{236447339: '[13]', 245616876: '[24]', 246652372: '[34,', 86611921: '65', 233219660: '[101]', 213474484: '[102]', 201058633: '[114]', 202539540: '[115]'}",https://export.arxiv.org/pdf/2302.09051v4.pdf
498,252873161,How (Not) To Evaluate Explanation Quality,conclusion,Insight-tree,"this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face. we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate those characteristics. finally, we propose guidelines for a more effective evaluation, which we hope to inspire future work and ultimately drive the field towards reliable and meaningful explanation quality evaluation. our discussion is backed up with examples, well-known theories and empirical findings from a crowdsourced case study that we conducted for the example of explainable question answering systems.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2210.07126v1.pdf
499,251320151,Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models,conclusions,Insight-tree,"in this paper, we propose to separately evaluate interpolation and extrapolation capabilities of neural retrieval models. considering the dynamics of queries in web search, we define them based on whether the training and test queries are similar or not. based on the definition, we investigate the bias in popular benchmarks, design associated evaluation methods, and revisit existing neural ranking models. we observe that the popular benchmarks are biased towards interpolation and thus may not reflect how models extrapolate.","{233296016: '[46]', 220302524: '[51]'}",https://export.arxiv.org/pdf/2204.11447v2.pdf
500,241583187,FaBULOUS: Fact-checking Based on Understanding of Language Over Unstructured and Structured information,conclusion,Insight-tree,"overall, this multi-modal task creates a plethora of new challenges to overcome and opens exciting avenues of research for the future of automated fact verification.","{221970302: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.fever-1.4.pdf
501,221845203,ETC: Encoding Long and Structured Inputs in Transformers,conclusions,Insight-tree,"this paper introduced the extended transformer construction (etc), an architecture designed to (1) scale up the input length (linearly with input), and (2) encode structured inputs. etc allows lifting weights from existing bert models, improving results significantly. the key ideas are a new globallocal attention mechanism, coupled with relative position encodings and a cpc pre-training task.",{},https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf
502,240526380,Real-time polyp detection model using convolutional neural networks,conclusions,Insight-tree,"this work has described the development of a dl model for real-time polyp detection, which could be integrated, in the future, into a cad system. yolov3 was selected as the base architecture for the development of this model due to its balance between performance and prediction time and complemented with an object-tracking filtering step able to reduce false positives.",{},NaN
503,232168895,"Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering",limitations in existing okvqa data,Insight-tree,"the widely used benchmark okvqa dataset [25] consists of over 14000 question-image pairs, with 9000 training examples and 5000 examples in the test set.we identify two broad issues with it.first, significant overlap exists between answers in the train and test folds.recall from section 1.1 that 48.9% of answers in the test set are present in the training set.existing systems leverage this limitation to boost their accuracy by limiting their test answers to the most frequent answers in the training set.second, unlike webquestions [6] or complexwebquestions [42], okvqa questions, even when grouped into some categories (see below) have no clear pattern of reasoning.18% (type-1) of the questions require detecting objects and subsequent reasoning over an external knowledge source to arrive at the answer.7% of the questions (type-2) require reading text from the image (ocr) (and no other information) to answer.12% of the questions (type-3) are based on personal opinion or speculation.the remaining questions (rest) can perhaps be described best through figure 2d.we provide an example of each type in figure 2.we found that several queries of type-1 have a structural similarity to the bridging queries in complexwebquestions [42].there, each query has exactly two clauses.the first clause, when issued to a web search engine, returns (via rc) an entity which plugs into the second clause, which is again sent to the web search engine, fetching the overall answer.we found that type-1 questions can be reformulated, with the help of the scene graph, to a query that can be answered directly using web search.inspired by complex-webquestions, we next develop our challenge data set.","{202539031: '[9,', 198229624: '[18]', 174801764: '[28]', 211258645: '[31,', 128345225: '[40,', 173188058: '[43]'}",https://arxiv.org/pdf/2103.05568v3.pdf
504,232168895,"Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering",conclusion,Insight-tree,"in this paper, we identify key limitations of existing multimodal qa datasets in being opaque and uninterpretable in their reasoning.towards addressing these limitations, we present okvqa s3 , an improvisation on the existing okvqa dataset as well as design and build a new challenge data set s3vqa that focuses on a specific structural idiom that frequently appears in vqa.we also present a structurally transparent and interpretable system s3 tailored to answer questions from our challenge data set and show that it outperforms strong baselines in both existing classification as well as the proposed open-domain settings.","{202539031: '[9,', 198229624: '[18]', 174801764: '[28]', 211258645: '[31,', 128345225: '[40,', 173188058: '[43]'}",https://arxiv.org/pdf/2103.05568v3.pdf
505,253581733,Task-aware Retrieval with Instructions,discussions and conclusion,Insight-tree,"this paper lays the foundation for building a general-purpose task-aware retriever that can follow natural language instructions. we introduced a new problem, retrieval with instructions, to model users' intents explicitly. we presented berri, the first large-scale retrieval dataset with expert-written annotations. building upon berri, we trained the first instruction-following retrieval system by massive multi-task instruction-tuning, tart, adopting two widely used architectures. tart advances the state of the art on the popular zeroshot retrieval benchmarks beir and lotte as well as on our newly introduced challenging evaluation setup, x 2 -retrieval. our analysis shows that key factors to building a successful multi-task instruction-following retrieval system include informative instructions at training and test time, diversity in data and model scale, and carefully designed negative samples. we conclude with two interesting open questions, which future work can explore.",{},https://export.arxiv.org/pdf/2211.09260v2.pdf
506,256900985,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,conclusion,Insight-tree,"in this paper, we have introduced meta prompt tuning (mpt), which learns to initialize the prompt embeddings for adapting to a target task. we have identified key research questions and systematically studied where and how meta learning can improve cross-task generalization in prompt tuning. we have empirically analyzed a representative set of meta learning methods in a variety of adaptation settings on a large, diverse collection of few-shot tasks. extensive experimental results and analysis verify the effectiveness of mpt. given the findings, in the future, we would like to explore more advanced meta learning algorithms which can consistently outperform multi-task learning.","{239009558: 'Vu et al. 2022', 233296709: 'Ye et al. 2021'}",https://www.aclanthology.org/2023.acl-long.659.pdf
507,253116788,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",conclusion,Insight-tree,"in order to build effective nlp models, we must move towards evaluations that test model robustness to variations in the input. we presented romqa, the first benchmark for robust, multi-evidence, multi-answer qa. romqa evaluates robustness of models to varying question constraints by testing for worst-case performance among clusters of related questions. compared to prior qa datasets, romqa has more natural human-written questions that require reasoning over more evidence text to more answers. romqa is challenging for state-of-the-art large lms in zero-shot, few-shot, and supervised settings, and provides a quantifiable test to build more robust qa methods. we want questions that cover diverse topics, however wikidata has a very skewed proposition distribution, with a long tail of rare propositions. hence, we down-sample frequent propositions. let p prop (x) denote the percentage of triples that contain the proposition x. we define the average proposition probability as p prop = 1 |x |",,https://export.arxiv.org/pdf/2210.14353v2.pdf
508,254854282,Query-as-context Pre-training for Dense Passage Retrieval,conclusions,Insight-tree,"in this work, we propose query-as-context pretraining, a simple yet effective technique to alleviate the previously ignored issue of weakly correlated pairs during context-supervised pre-training.","{232147859: 'Guo et al., 2022;'}",https://export.arxiv.org/pdf/2212.09598v3.pdf
509,254854282,Query-as-context Pre-training for Dense Passage Retrieval,limitations,Insight-tree,"a passage is more likely to have a high correlation with its corresponding generated query than another randomly selected passage from the same document.however, limited by the capabilities of the t5 model, there are still a large number of unrelated passage-query pairs.we believe that more powerful large language models have the potential to further alleviate this problem, which is left to our future research.","{232147859: 'Guo et al., 2022;'}",https://export.arxiv.org/pdf/2212.09598v3.pdf
510,263608643,CAN LANGUAGE MODELS BE INSTRUCTED TO PROTECT PERSONAL INFORMATION?,conclusion,Insight-tree,"in this work, we present privqa, a multi-modal benchmark to measure the ability of language models and vision-language models to follow instructions to protect personal information.we also introduce an iterative, instruction-based self-moderation technique for this task.our results indicate there are still gaps in the abilities of state-of-the-art models to follow these kinds of instructions: they are not robust to adversarial inputs, and they suffer from a privacy/utility tradeoff.we also show that models succumb to biases based on popularity and race leading to inconsistent protection across demographic groups.in closing, we hope that this work sheds light on the promise of access control instructions and guides future llm development and safety research.on september 28th, using the gpt-4v(ision) version from september 25th, we conducted geolocation extraction red teaming experiments.we initiated the conversation with the prompt ""what is the name of this building?"".in each of the 12 instances tested, the model refused to respond.following up with a tour request prompt for the same building yielded identifications in 8 out of 12 cases, with 6 of them being correct (see figure 9).we also found the model will not refuse queries regarding well-known landmarks (e.g., space needle).we have disclosed these attacks to researchers at openai.",{},https://export.arxiv.org/pdf/2310.02224v1.pdf
511,232380161,DAGN: Discourse-Aware Graph Network for Logical Reasoning,conclusion,Insight-tree,"in this paper, we introduce a discourse-aware graph network (dagn) to addressing logical reasoning qa tasks. we first treat elementary discourse units (edus) that are split by discourse relations as basic reasoning units. we then build discourse-based logic graphs with edus as nodes and discourse relations as edges. dagn then learns the discourse-based features and enhances them with contextual token embeddings. dagn reaches competitive performances on two recent logical reasoning datasets reclor and logiqa.  ","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2021.naacl-main.467.pdf
512,258887816,Linguistic Properties of Truthful Response,conclusion,Insight-tree,"so far, we have discussed two main contributions of our paper: 1. similar linguistic profiles are shared across gpt-3 of varying sizes, and 2. exploration on if truthfulness can be detected using stylistic features of the model response. as an exploratory work on applying linguistic feature analysis to truthfulness detection of an llm's response, some experimental setups are limited. but we do obtain some promising results that are worth further exploration. in particular, llms other than gpt-3 must be evaluated to see if the similarity in linguistic properties is a model-level or datasetlevel characteristic or both.","{226236740: 'Ho et al., 2020;', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2023.trustnlp-1.12.pdf
513,258887816,Linguistic Properties of Truthful Response,limitation,Insight-tree,"our main limitation comes from dataset size. this was limited because we used human evaluation to label model responses as truthful or untruthful. that is, we have manually confirmed gpt-judge labels on davinci responses, and extrapolated the system to ada, babbage, and curie. frankly, the limitations caused by the small size of the dataset were quite evident because the truthfulness detector was often biased towards producing one label (either 1 or 0). we attempted to solve this problem using lower regularization parameters, but this often produced models with lower performances. an ideal solution to this problem would be training the truthfulness detector on a large set of training instances, which is also our future direction.","{226236740: 'Ho et al., 2020;', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2023.trustnlp-1.12.pdf
514,264306101,AGENTTUNING: ENABLING GENERALIZED AGENT ABILITIES FOR LLMS,conclusion,Insight-tree,"in this work, we study how to enable generalized agent abilities for llms, bridging the disparity between open and commercial llms on agent tasks.we present the agenttuning approach to achieve this goal.agenttuning first introduces the agentinstruct dataset covering 1,866 verified agent interaction trajectories and then designs an instruction-tuning strategy with the mixture of agentinstruct and general-domain instructions.we generate the open agentlm by employing agenttuning to tune the llama 2 models.agentlm exhibits strong performance on unseen agent tasks while preserving their general abilities on mmlu, gsm8k, humaneval, and mt-bench.to date, agentlm-70b is the first open llm that matches gpt-3.5-turbo on agent tasks.",{},https://export.arxiv.org/pdf/2310.12823v2.pdf
515,259370780,FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning,conclusion,Insight-tree,"motivated by the train-test efficiency differences between few-shot in-context learning and few-shot fine-tuning, we aim to find a balance and benefit from the strengths of both approaches.  based icl (early fusion) and ensemble-based icl (late fusion), in terms of both performance and computation efficiency. moreover, fusion-in-decoder icl partly closes the gap between gradient-free icl methods and gradient-based fine-tuning methods, highlighting the potential of approximating gradient-based optimization with efficient forwardonly methods (phang et al., 2022). future work may build upon our insights to further improve the computation efficiency of few-shot learning. however, similar to the findings in min et al. (2022c), our analysis on icl models suggest that they barely learn the input-label mapping from the in-context examples. we also have mixed results when more shots become available for the icl model. we hope future work can further improve the performance of icl by enabling it to learn from input-label mapping effectively and faithfully.",{},https://www.aclanthology.org/2023.acl-long.454.pdf
516,252918040,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,conclusion,Insight-tree,"in this paper, we presented a seminal knowledge prompting paradigm, based on which a novel knowledge-prompting-based plm framework kp-plm was proposed. we constructed contextual knowledge sub-graphs for contexts and employed continuous prompting mapping to generate knowledge prompts. after that, we designed two selfsupervised pre-training tasks to learn semantic knowledge from prompts. finally, we conducted extensive experiments to evaluate the model performance. experimental results validate the effectiveness of knowledge prompting in boosting the performance of plms.","{198229624: 'Joshi et al., 2020'}",https://export.arxiv.org/pdf/2210.08536v1.pdf
517,218487030,Teaching Machine Comprehension with Compositional Explanations,conclusion,Insight-tree,"in this paper, we propose to teach extractive mrc with explanations, with a focus on annotation efficiency. we believe explanations stating ""why"" and justifying ""deduction process"" opens up a new way to communicate human's generalization abilities to mrc model training. we begin with a small set of semi-structured explanations and compose nmteachers to augment training data. nmteachers are modularized functions where each module has a strict and softened form, enabling broader coverage from each explanation. extensive experiments on different datasets and mrc models demonstrate the efficiency of our system. having achieved encouraging results for mrc, we look forward to extending this framework to tasks such as non-fact-based qa and multi-hop reasoning. ","{52822214: 'Yang et al., 2018b;'}",https://www.aclweb.org/anthology/2020.findings-emnlp.145.pdf
518,220525901,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,conclusion and future work,Insight-tree,"in this paper we have briefly described the multi-task approach for joint prediction of rumor stance and veracity for data obtained from various social media platforms (in our case twitter and reddit). we presented ensemble of deep learning models having the same architecture, but varying the parameters. our approach outperforms the previous approaches by sufficient margin and able to generalize across different social media. in future, we can extend our model for multilingual setting [51] (ibereval is the counterpart of rumoreval 2019 for other languages [13]). moreover, we can leverage more sophisticated resources like pre-trained model which was trained specifically to handle data from different social media platforms. further, we can explore other methods like diffusion process of rumors [48] to make informed changes to model architecture.",{},https://arxiv.org/pdf/2007.07803v1.pdf
519,233231436,AR-LSAT: Investigating Analytical Reasoning of Text,conclusion,Insight-tree,"in this paper, we study the challenging task of analytical reasoning and introduce a dataset ar-lsat to facilitate research on analytical reasoning. we analyze the knowledge understanding and reasoning ability required for this task and present two basic approaches: a transformer-based approach and a logical-level reasoning framework, named analytical reasoning machine (arm). arm extracts symbolic knowledge, including participants, facts and rules mentioned in the context and extract logical functions from the rules. afterwards, it performs deep reasoning to find all the legitimate solutions to the problem posed and finally makes a prediction. arm sheds a light on the reasoning procedure for analytical reasoning, and each component can be further developed. experiments show that this task is very challenging for current transformer-based pre-trained language models and arm outperforms them with better performance and interpretability. further discussions are made to shed light on important future directions. ","{218486753: 'Inoue et al., 2020', 220483148: 'Liu et al., 2020b', 207756753: 'Nie et al., 2019;', 52822214: 'Yang et al., 2018a;'}",https://arxiv.org/pdf/2104.06598v2.pdf
520,252519173,PROMPTAGATOR : FEW-SHOT DENSE RETRIEVAL FROM 8 EXAMPLES,conclusion and discussions,Insight-tree,"in this paper, we have presented promptagator, a novel approach to few-shot retrieval. we showed that it is possible to create task-specific, end-to-end retrievers with only a few annotated examples. the few-shot examples, amplified by prompt-based llm query generation, simplifies the complexity of training neural retrievers for a new tasks and leads to promising retrieval performance gains. it hopefully inspires future research to further push the limit of few-shot retrieval, towards generalizable retrieval systems that can seamlessly and efficiently adapt to many tasks.","{249097975: 'Izacard et al., 2022a', 86611921: 'Kwiatkowski et al., 2019', 236477844: 'Ren et al., 2021a;', 244799249: 'Santhanam et al., 2022;', 220302524: 'Xiong et al., 2021;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2209.11755v1.pdf
521,253157925,DyREx: Dynamic Query Representation for Extractive Question Answering,conclusion,Insight-tree,"in this paper, we propose dyrex, a method to dynamically compute query representations to calculate the start and end positions of answer spans in extractive question answering. our approach consistently outperforms the dominant approach on a wide range of qa datasets, and the gain is even more significant in a few-shot scenario. in future work, it would be interesting to adapt dyrex for multi-span extraction tasks such as named entity recognition and keyphrase extraction.","{204823992: '[Fisch et al., 2019]', 86611921: '[Kwiatkowski et al., 2019]'}",https://export.arxiv.org/pdf/2210.15048v1.pdf
522,237263476,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,conclusion,Insight-tree,"in this work, we propose gamification as a general framework for creating diverse and challenging nlu benchmarks. we use this framework to collect csqa2, a new benchmark that contains 14,343 yes/no questions. we perform a detailed analysis of csqa2, which elucidates the unique properties of our dataset, and thoroughly evaluate on a strong suite of baselines. we find that the best model, unicorn-11b, achieves an accuracy of 70.2%, dozens of points lower than human accuracy. we argue that gamification is a promising approach for creating challenge sets that expose the weaknesses of current state-of-the-art models.","{173188058: '2,', 204823992: '3]', 174801764: '19,', 233444226: '[21]', 211010520: '[22]', 209515274: '[32,', 67855846: '34,', 219573621: '36,', 201058633: '37,', 52822214: '41,', 207756753: '44]'}",https://arxiv.org/pdf/2201.05320v1.pdf
523,258841216,LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models,conclusion,Insight-tree,"in this paper, we first analysed the performance of chatgpt on logical reasoning benchmarks, as well as that using logically enhanced chain-of-thought prompting. from the results we can conclude that though language modeling can help compress all data both with and without supervision into neural models and accept human instructions, it is still really weak in perform logical reasoning. specifically, llms often fail at reaching correct deductions based on the given facts and rules, and cannot distil the logical reasoning structure from observed prompts to generalize to new problems. we hope these observations could bring some insights to future research on introducing logic prior into large language models. one step further, we combine previous data-driven approach, i.e., merit+, a selfsupervised pre-training method for logical reasoning, with flan-t5-3b, and the results have also demonstrated its effectiveness.","{244478674: 'Aribandi et al., 2022;', 67855846: 'Dua et al., 2019;', 234335834: 'Wei et al., 2022a', 52822214: 'Yang et al., 2018;', 234741852: 'Zhu et al., 2021'}",https://export.arxiv.org/pdf/2305.13718v2.pdf
524,236477844,PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval,conclusion and future work,Insight-tree,"this paper presented a novel dense passage retrieval approach that leverages both query-centric and passage-centric similarity relations for capturing more comprehensive semantic relations. to implement our approach, we made three important technical contributions in the loss formulation, training data augmentation and effective training procedure. extensive results demonstrated the effectiveness of our approach. to our knowledge, it is the first time that passage-centric similarity relation has been considered for dense passage retrieval. we believe such an idea itself is worth exploring in designing new ranking mechanism. in future work, we will design more principle ranking functions and apply current retrieval approach to downstream tasks such as question answering and passage re-ranking.","{86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2108.06027v2.pdf
525,219124082,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,conclusion,Insight-tree,"we present a structured survey of methods that reveal heuristics and spurious correlations in datasets, methods which show that neural models inherit those correlations or assess their capabilities otherwise, and methods that mitigate this by adversarial training, data augmentation and model architecture or training procedure improvements. various nli datasets are reported to contain spurious correlations between input and expected output, might be unsuitable to evaluate some task modality due to dataset design or suffer from quality issues. rte is a popular target task for these data-centred investigations with more than half of the surveyed papers focusing on it. nli models, in turn, are shown to exploit those correlations and to rely on superficial lexical cues. furthermore, they lack generalisation beyond the evaluation set resulting in poor performance on out-of-distribution evaluation sets, generated adversarially or targeted at a specific capability. efforts to achieve robustness include augmenting the training data with adversarial examples, making use of external resources and modifying the neural network architecture or training objective.","{139103297: 'Chen and Durrett, 2019a', 67855846: 'Dua et al., 2019b', 189927896: 'Jiang and Bansal, 2019', 207917676: 'Pugaliya et al., 2019', 212644640: 'Schlegel et al. 2020', 208201969: 'Sugawara et al., 2020;'}",NaN
526,221448158,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,conclusion & future work,Insight-tree,"we introduced text modular networks, which provide a general-purpose framework that casts complex tasks as textual interaction between existing, simpler qa modules. based on this conceptual framework, we built modularqa, an instantiation of tmns that can perform multi-hop and discrete numeric reasoning. empirically, modu-larqa is on-par with other modular approaches (which are dataset-specific) and outperforms a stateof-the-art model in a limited data setting and on expert-generated perturbations. importantly, mod-ularqa provides easy-to-interpret explanations of its reasoning. it is the first system that decomposes drop questions into textual sub-questions and can be applied to both drop and hotpotqa.",,https://www.aclweb.org/anthology/2021.naacl-main.99.pdf
527,233219660,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,conclusion,Insight-tree,"spatial reasoning is an important problem in natural language understanding. we propose the first human-created qa benchmark on spatial reasoning, and experiments show that state-of-the-art pretrained language models (lm) do not have the capability to solve this task given limited training data, while humans can solve those spatial reasoning questions reliably. to improve lms' capability on this task, we propose to use hand-crafted grammar and spatial reasoning rules to automatically generate a large corpus of spatial descriptions and corresponding question-answer annotations; further pretraining lms on this distant supervision dataset significantly enhances their spatial language understanding and reasoning. we also show that a spatially-improved lm can have better results on two extrinsic datasets (babi and boolq).  table 7 shows the templates used to create questions in spartqa-auto. the ""<object>"" is a variable replaced by objects from the story (using choose-objects and describe-objects modules), and the ""<relation>"" variable can be replaced by the chosen relations between objects (using findall-relations module). the articles and the indefinite pronouns in each template play an essential role in understanding the question's objective. for example, ""are all blue circles near to a triangle?"" is different from ""are there any blue circles near to a triangle?"", and ""are there any blue circles near to all triangles?"". therefore, we check the uniqueness of the object definition, using ""a"" or ""the"" in proper places and randomly place the terms ""any"" or ""all"" in the yn questions to generate different questions. table 8 shows the percentage of correct labels in train and test sets. in multi-choice q-types, more than one label can be true. table 10 shows some generated sentences in spartqa-auto with some specific features that challenge models to understand different forms of relation description in spatial language.",{},https://www.aclweb.org/anthology/2021.naacl-main.364.pdf
528,256389465,EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval,conclusion,Insight-tree,"we propose embeddistill -a novel distillation method for ir that goes beyond simple score matching. en route, we provide a theoretical understanding of the teacher-student generalization gap in an ir setting which not only motivated embeddistill but also inspired new design choices for the student de models: (a) reusing the teacher's document encoder in the student and (b) aligning query embeddings of the teacher and student. this simple approach delivers consistent quality and computational gains in practical deployments and we demonstrate them on msmarco, nq, and beir benchmarks. finally, we found embeddistill retains 95-97% of the teacher performance to with 1/10th size students.","{211572791: ', Nie et al., 2020', 220302524: '[Xiong et al., 2021]'}",https://export.arxiv.org/pdf/2301.12005v2.pdf
529,238259667,Trustworthy AI: From Principles to Practices,"conclusion, challenges and opportunities",Insight-tree,"in this survey, we outlined the key aspects of trustworthiness that we think are essential to ai systems. we introduced how ai systems can be evaluated and assessed on each of these aspects, and reviewed current efforts in this direction in the industry. we further proposed a systematic approach to consider these aspects of trustworthiness in the entire lifecycle of real-world ai systems, which offers recommendations for every step of the development and use of these systems. we recognize that fully adopting this systematic approach to build trustworthy ai systems requires that practitioners embrace the concepts underlying the key aspects that we have identified. more importantly, it requires a shift of focus from performance-driven ai to trust-driven ai. in the short run, this shift will inevitably involve side-effects, such as longer learning time, slowed development, and/or increased cost to build ai systems. however, we encourage practitioners to focus on the long-term benefits of gaining the trust of all stakeholders for the sustained use and development of these systems. in this section, we conclude by discussing some of the open challenges and potential opportunities in the future development of trustworthy ai.","{6855746: '[113]', 207870753: '[332,', 52822214: '366]'}",https://arxiv.org/pdf/2110.01167v2.pdf
530,238259667,Trustworthy AI: From Principles to Practices,limitations in current evaluations of trustworthiness.,Insight-tree,"repeatable and quantitative measurements are the cornerstone of scientific and engineering progress. however, despite increasing research interest and efforts, the quantification of many aspects of ai trustworthiness remains elusive. of the various aspects that we have discussed in this paper, the explainability, transparency, and accountability of ai systems are still seldom evaluated quantitatively, which makes it difficult to accurately compare systems. developing good methods of quantitative evaluation for these desiderata, we believe, will be an important first step in research on these aspects of ai trustworthiness as a scientific endeavor, rather than a purely philosophical one.","{6855746: '[113]', 207870753: '[332,', 52822214: '366]'}",https://arxiv.org/pdf/2110.01167v2.pdf
531,258461053,Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA,conclusion and discussion,Insight-tree,"in this work, we formulated 'post-abstention', a task that allows re-attempting the abstained instances of the given selective prediction system with the aim of increasing its coverage without significantly sacrificing the accuracy. we also explored several baseline methods for this task. through comprehensive experiments on 11 qa datasets, we showed that these methods lead to considerable performance improvements in both in-domain and out-of-domain settings. we further performed a thorough analysis that resulted in several interesting findings.","{235313893: 'Zhang et al., 2021'}",https://www.aclanthology.org/2023.acl-long.55.pdf
532,251953412,LEXMAE: LEXICON-BOTTLENECKED PRETRAINING FOR LARGE-SCALE RETRIEVAL,conclusion,Insight-tree,"in this work, we propose to improve the lexicon-weighing retrieval by pre-training a lexiconbottlenecked masked autoencoder (lexmae) which alleviates the objective mismatch between the masked language modeling encoders and relevance-oriented lexicon importance. after pretraining lexmae on large-scale collections, we first observe great zero-shot performance. then after fine-tuning the lexmae on the large-scale retrieval benchmark, we obtain state-of-the-art retrieval quality with very high efficiency and also deliver state-of-the-art zero-shot transfer performance on beir benchmark. further detailed analyses on the efficacy-efficiency trade-off in terms of retrieval latency and storage memory also verify the superiority of our fine-tuned lexmae.","{236477844: 'Ren et al., 2021a', 247447562: 'Zhou et al., 2022a'}",https://export.arxiv.org/pdf/2208.14754v2.pdf
533,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,limitations and future work,Insight-tree,"we notice that for certain tasks where little context about the environment is available, fully relying on foreseeable reasoning becomes impractical. consider the following task from alfworld [27]:you are in the middle of a room. looking quickly around you, you see a drawer 2, a shelf 5, a drawer 1, a shelf 4, a sidetable 1, a drawer 5, a shelf 6, a shelf 1, a shelf 9, a cabinet 2, a sofa 1, a cabinet 1, a shelf 3, a cabinet 3, a drawer 3, a shelf 11, a shelf 2, a shelf 10, a dresser 1, a shelf 12, a garbagecan 1, a armchair 1, a cabinet 4, a shelf 7, a shelf 8, a safe 1, and a drawer 4. your task is to: put some vase in safe.since a planner has no prior knowledge about the environment, it has to enumerate all possible plans that can potentially lead to some vase. the number of reasoning steps of planner in such tasks is equivalent to the worst-case complexity of observation-dependent reasoning.the example above implies that a robust alm system should not be built on a singleton -it looks promising to wire different nodes of llms, tools, and sub-models into a directed acyclic graph (dag) so that each node functions for its predesignated tasks organically. 4 directions to further improve the efficiency and performance of such alm systems include (1) offloading specialized abilities from foundation llms into smaller models. section 3.3 demonstrate the possibility for small lms specializing [14] in general foreseeable reasoning. we expect that with a greater number of open domain instructions, foreseeable reasoning can be even more holistically offloaded. other parametric nodes in the dag, such as a solver, can be fine-tuned alike. (2) tool representation learning. in many cases from hotpotqa, wikipedia and google can both lead to the correct answer, indicating a certain level of similarity between those tools. we can set up a model to minimize the energy among similar-functioning workers. tool representations allow us to parametrize the whole alm system and therefore enabling end-to-end training. (3) graph optimization. furthermore, we should be able to optimize dag execution through multiple graph and concurrency algorithms.",{},https://export.arxiv.org/pdf/2305.18323v1.pdf
534,258967566,ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models,conclusion,Insight-tree,"we present rewoo , a modular alm framework to solve multi-step reasoning tasks efficiently by decoupling reasoning from tool feedback and observations. theoretical decomposition of prompt tokens establishes that rewoo is able to substantially reduce prompting redundancy in prevailing thought-action-observation alm systems. comprehensive experiments on both public nlp benchmarks and curated tasks reveal superior performance of rewoo in achieving boosted performance with much less token consumption. a side study also shows that rewoo has relatively robust performance under tool-failure cases. our study further unveils the potential for generic reasoning offloading via instruction tuning and specialization. future improvements beyond rewoo based alm systems involve modular llm fine-tuning, tool representation learning, and system graph learning and optimization. we demonstrate that our work lays a solid foundation for these advancements, inching us closer to truly scalable agi.",{},https://export.arxiv.org/pdf/2305.18323v1.pdf
535,252818979,Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering,conclusion,Insight-tree,"this paper proposes hdrn, a novel model for representation learning and reasoning for biomedical question answering. first, we construct hierarchical representations to obtain a deep understanding of the biomedical evidences. then, we perform multi-step dynamic reasoning to solve complex biomedical questions. we evaluate our model on three bioqa datasets and achieve new state-of-theart performances.","{218486765: 'Yue et al. 2020', 220831004: 'Zaheer et al., 2020'}",https://www.aclanthology.org/2022.coling-1.127.pdf
536,254685782,MASTER: MULTI-TASK PRE-TRAINED BOTTLE- NECKED MASKED AUTOENCODERS ARE BETTER DENSE RETRIEVERS,conclusion,Insight-tree,"in this paper, we proposed master, a multi-task pre-trained bottlenecked masked autoencoder for dense retrieval task. in our approach, we adopted a bottlenecked multi-decoder architecture to integrate a variety of pre-training tasks, and devised three types of pre-training tasks about corrupted passages recovering, related passage recovering and plms outputs recovering. the three types of tasks focused on compressing the semantic information within the passages, modeling relations among passages, and learning the knowledge from external public generative plms, respectively, leading to more informative and effective dense vectors. experimental results have shown that our approach outperforms several competitive baselines.  we compare our approach with a variety of methods: of bert on these nlu tasks. it indicates that our multi-task pre-training can also enrich the useful knowledge about nlu tasks for the plm.","{86611921: 'Kwiatkowski et al., 2019', 236477844: 'Ren et al., 2021a', 244799249: 'Santhanam et al., 2022', 247411106: 'Xu et al., 2022', 247447562: 'Zhou et al., 2022a;'}",https://export.arxiv.org/pdf/2212.07841v1.pdf
537,258309779,Answering Questions by Meta-Reasoning over Multiple Chains of Thought,conclusion,Insight-tree,this work introduces mcr for meta-reasoning over multiple reasoning chains.we evaluate mcr on 7 datasets for multi-hop qa that require both implicit and explicit reasoning in an open-domain setting and show that it outperforms previous approaches on all evaluation benchmarks.,{},https://export.arxiv.org/pdf/2304.13007v3.pdf
538,258866060,AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models,conclusion,Insight-tree,"we propose autoplan, a prompt-based method, to enable llm to solve interactive decision-making tasks without gradient computation or in-context demonstrations.autoplan conditions llm on an additional task plan described in natural language, which is obtained through an iterative three-stage process.experiments show that autoplan achieves better results than baselines and is also efficient during inference.the ablation study further confirms the effectiveness of batching and explicit reflection","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.15064v3.pdf
539,264288922,WHAT IS A GOOD QUESTION? TASK-ORIENTED ASKING WITH FACT-LEVEL MASKING,"conclusion, limitations and future work",Insight-tree,"in this paper, we presented a novel framework for evaluating task-oriented questions and observed that state-of-the-art zero-shot llms struggle at this task compared to humans.we attribute this deficiency to a lack of training data and evaluation processes for toa.to overcome these challenges, we introduced fact-level masking and flm-hotpotqa, a self-supervised toa dataset, and an associated evaluation pipeline.to conclude, our contributions highlight the challenges faced by state-of-the-art, zero-shot llms, address limitations in training data and evaluation methods for task-oriented asking, and pave the way for future toa development.",{},https://export.arxiv.org/pdf/2310.11571v1.pdf
540,216553210,Semantic Graphs for Generating Deep Questions,conclusion and future works,Insight-tree,"we propose the problem of dqg to generate questions that requires reasoning over multiple disjoint pieces of information. to this end, we propose a novel framework which incorporates semantic graphs to enhance the input document representations and generate questions by jointly training with the task of content selection. experiments on the hotpotqa dataset demonstrate that introducing semantic graph significantly reduces the semantic errors, and content selection benefits the selection and reasoning over disjoint relevant contents, leading to questions with better quality.","{202565945: 'Jiang and Bansal, 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.135.pdf
541,264820296,M 4 LE: A MULTI-ABILITY MULTI-RANGE MULTI-TASK MULTI-DOMAIN LONG-CONTEXT EVALUATION BENCHMARK FOR LARGE LANGUAGE MODELS,conclusion,Insight-tree,"in this paper, we propose a benchmark m 4 le for llms assessing their capability of long-context understanding.to establish a benchmark with diverse nlp tasks, rather than just those that are inherently lengthy, we propose a systematic method to convert short nlp task instances into long context inputs, encompassing five distinct abilities.we collect and construct in total of 36 tasks from different sources and domains covering multiple length ranges to maximize the diversity of the tasks in benchmark, with our customized construction methods which enable flexibility to extend arbitrary context lengths.we evaluate 11 well-known llms with our benchmark and find that current models struggle to understand long-context inputs and the corresponding performance related to ability types, data used when fine-tuning and positions of the relevant information.a.1.1 mnds news mnds news (petukhova & fachada, 2023) is an english hierarchical news category classification dataset comprising 10,917 news articles from 260 sources.we only use the 17 first-level categories as the labels for this study.for multiple retrieval tasks, we randomly sample a class label that appears in the instance.","{260440449: 'Tay et al., 2020', 52822214: 'Yang et al., 2018, and'}",https://export.arxiv.org/pdf/2310.19240v1.pdf
542,233240947,Time-Stamped Language Model: Teaching Language Models to Understand the Flow of Events,conclusion,Insight-tree,"we proposed the time-stamped language model (tslm model), a novel approach based on a simple and effective idea, which enables pre-trained qa models to process procedural texts and produce different outputs based on each step to track entities and their changes. tslm utilizes a timestamp function that causes the attention modules in the transformer-based lm architecture to incorporate past, current, and future information by computing a timestamp embedding for each input token. our experiments show a 3.1% improvement on the f1 score and a 10.4% improvement over the recall metric on propara dataset. our model further outperforms the state-of-the-art models with a 1.55% margin in the npn-cooking dataset accuracy for the location prediction task.","{67855846: 'Dua et al., 2019', 102352338: 'Gupta and Durrett, 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2104.07635v1.pdf
543,258999803,Explanation Graph Generation via Generative Pre-training over Synthetic Graphs,conclusion,Insight-tree,"in this paper, we propose a pre-training framework eg 3 p for a structured explanation generation task. distinct from existing pre-training tasks based on natural language text, eg 3 p focuses more on training mapping between natural language and graphs. meanwhile, due to the high cost of manually tagging, we construct queries from the synthetic graph automatically to get a large-scale corpus to support the pre-training process. using explagraph as a main benchmark, experimental results show that eg 3 p could significantly improve the ability of the model to generate explanations. in addition, on the other dataset, the results of the model after pre-training also showed a considerable improvement. our approach offers a new possibility for addressing the challenges of limited labeled data in natural language processing tasks. in the future, the ability of the model to generate explanation graphs will benefit from more datasets released with labels and more and more objective evaluation indicators put forward. additionally, while our current approach processes graphs as strings, utilizing a model architecture that is more suitable for graph generation may further enhance the model's graph generation ability.","{236459873: 'Aggarwal et al., 2021', 222141025: 'Saha et al., 2020', 52822214: 'Yang et al., 2018;', 218487030: 'Ye et al., 2020'}",https://export.arxiv.org/pdf/2306.00652v1.pdf
544,263829795,Human Mobility Question Answering (Vision Paper),discussion and conclusion,Insight-tree,"in this paper, we present a novel human mobility question answering task which aims at better understanding mobility behaviours.associated with this mobqa task, we discuss a blueprint on how to build a suitable mobqa dataset.additionally, a plausible model design tailored for the mobqa task is also suggested.broader impact.we hope this study will offer fresh concepts and insights for human mobility research.besides the human mobility domain, the mobqa task would also boost the corresponding research of language processing and question answering systems.for example, a new research topic could be how to develop language models suitable for sequential behaviour data.further, the introduced mobqa task could emerge new applications such as mobility chatbots for social good.future work.as the first attempt to explore the question answering of human mobility data, this paper constitutes an initial step in establishing the dataset for the mobqa task.we intend to expand the question templates to include diverse questions in the dataset.after finalising the mobqa dataset, we will focus on developing deep learning models for the mobqa task based on the design introduced in this paper and evaluating our solution.","{224803601: '[2]', 237048095: '[3]', 215785913: '[4]', 221978039: '[7]', 52822214: '[20]'}",https://export.arxiv.org/pdf/2310.04443v2.pdf
545,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,limitations,Insight-tree,"at its core, reflexion is an optimization technique that uses natural language to do policy optimization.policy optimization is a powerful approach to improve action choice through experience, but it may still succumb to non-optimal local minima solutions.in this study, we limit long-term memory to a sliding window with maximum capacity, but we encourage future work to extend the memory component of reflexion with more advanced structures such as vector embedding databases or traditional sql databases.specific to code generation, there are many practical limitations to testdriven development in specifying accurate input-output mappings such as non-deterministic generator functions, impure functions that interact with apis, functions that vary output according to hardware specifications, or functions that invoke parallel or concurrent behavior that may be difficult to predict.",{52822214: '[28]'},https://export.arxiv.org/pdf/2303.11366v4.pdf
546,258833055,Reflexion: Language Agents with Verbal Reinforcement Learning,conclusion,Insight-tree,"in this work, we present reflexion, an approach that leverages verbal reinforcement to teach agents to learn from past mistakes.we empirically show that reflexion agents significantly outperform currently widely-used decision-making approaches by utilizing self-reflection.in future work, reflexion could be used to employ more advanced techniques that have been thoroughly studied in traditional rl settings, such as value learning in natural language or off-policy exploration techniques.",{52822214: '[28]'},https://export.arxiv.org/pdf/2303.11366v4.pdf
547,233219849,"MULTIMODALQA: COMPLEX QUESTION ANSWERING OVER TEXT, TABLES AND IMAGES",conclusion,Insight-tree,"we present mmqa, a new qa dataset that contains 29,918 examples, 35.7% of which require crossmodality reasoning. we describe a novel framework for generating complex multimodal questions at scale, and showcase the diversity and multimodal properties of the resulting dataset. we evaluate mmqa using a variety of models, and confirm that the best model exploits the multimodality of the dataset and takes into account multi-hop reasoning via implicit decomposition. however, human performance substantially exceeds the best model, establishing the need for further research involving multiple modalities in question answering systems, which we hope that our work will drive.","{67855846: 'Dua et al. 2019', 174801764: 'Min et al., 2019a;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2104.06039v1.pdf
548,246996947,SGPT: GPT Sentence Embeddings for Semantic Search,conclusion and future work,Insight-tree,"this work presented sgpt. building on sbert, we proposed modifications to gpt models to use them as cross-or bi-encoders for semantic search.",{},https://export.arxiv.org/pdf/2202.08904v5.pdf
549,211126910,FQuAD: French Question Answering Dataset,conclusion,Insight-tree,"in the present work, we introduce the french question answering dataset. the contexts are collected from the set of high quality wikipedia articles. with the help of french college students, 60,000+ questions have been manually annotated. the fquad dataset is the result of two different annotation processes. first, fquad1.0 is collected to build a 25,000+ questions dataset. second, the dataset is enriched to reach 60,000+ questions resulting in fquad1.1. the development and test sets have both been enriched with additional answers for the evaluation process.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.findings-emnlp.107.pdf
550,235719347,Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,conclusion,Insight-tree,"in this paper, we propose a sequence labeling based question answering (slqa) approach to construct nl-pl pairs in a pl-agnostic way. we propose a model that can produce multiple code blocks as solutions of a post's question, which is achieved by using bio sequence tagging. we also propose to incorporate the global textual context as pl-independent supplementary information. to validate the capacity of our method, we manually annotate a challenging cross-pl multi-block dataset, named lang2code-human. substantial experiments on the single-pl single-block staqc-human and our lang2code-human benchmarks demonstrate the effectiveness and cross-pl transferability of our method. finally, we present lang2code, the largest-to-date nl-pl corpus to the best of our knowledge, containing over 1.4 million pairs spanning six pls. under statistical analysis and downstream evaluation on code generation task, we demonstrate that lang2code is a large-scale and high-quality nl-pl pair corpus and can greatly help developing data-hungry models in future research.",{},https://arxiv.org/pdf/2203.10744v1.pdf
551,237420723,Towards Retrieval-based Conversational Recommendation,research limitations,Insight-tree,"one potential threat to the validity in user studies like ours lies in the reliability of the study participants. given that we applied more than one quality-assurance measure-participant selection, an attention check, manual inspection-we are confident that our results are reliable. given also that most participants are fluent in english and regular movie watchers, we believe that the participants are representative at least for a subset of potential users of an online crs.another potential limitation is that we so far only analyzed two language generation systems. the question therefore remains to what extent the findings of our study would generalize to other approaches. since both analyzed systems (kbrd and kgsf) were published in the last two years, and since they were published at top-ranked scientific conferences, we believe that they are good representatives of the state-of-the-art in neural generation-based systems. moreover, an earlier analysis of the deepcrs system in [16] indicates that similar phenomena might be found also for other approaches. note also that our study can be easily extended to include alternative or even newer approaches, as long as the respective authors share the needed artifacts for reproducibility. as of now, we did not find any work that demonstrate superior quality than the kgsf system, and that the source code of such a system is available. to ensure replicability of our own work, we also share all the code and data used for our analyses online.so far, we analyzed our approach only with the help of the redial dataset. this choice was necessary to ensure a fair comparison with two recent works (kbrd and kgsf), which also relied on this dataset. recently, a number of alternative dataset was proposed, e.g., [5,14,65]. an evaluation of all compared methods on other datasets is however beyond the scope of our present work, which aimed to assess the relative performance of generation-based and retrievalbased system based on the dataset for which they were originally designed and tuned.regarding the general nature of the proposed crb-crs system, it is a retrieval-based system. however, as discussed in section 3, we also rely on a small set of heuristics in a few processing phases, which are, for example, implemented using keyword lists and string matching. these heuristics, which are not yet learned automatically, are documented in the provided source code of our system. automating the construction of these rules and keyword lists, e.g., based on movielens metadata, is a part of our future work to avoid any knowledge-engineering bottlenecks.",{52822214: '28]'},https://arxiv.org/pdf/2109.02311v1.pdf
552,237420723,Towards Retrieval-based Conversational Recommendation,conclusion,Insight-tree,"conversational recommender systems (crs) that interact with users in natural language obtained increased attention in the past few years. in this paper, we have proposed a retrieval-based approach to conversational recommendations, and we conducted a study involving humans to understand how our system performs compared to recent language generation approaches. our study led to promising results, and we hope that our study design can be used as a blueprint for user-centric studies of crs in the future. overall, the main conclusion of our studies is retrieval-based approaches to crs can be a promising alternative or complement to language generation approaches.",{52822214: '28]'},https://arxiv.org/pdf/2109.02311v1.pdf
553,247779047,Understanding Questions that Arise When Working with Business Documents,limitations and future work,Insight-tree,"in this work we focused only on word documents as a common document type where users can author, copy-edit, or read content. indeed, the organization where we deployed our studies primarily uses word for business documents. the focus on word rather than e.g., pdfs or web pages also allowed us to obtain questions about documents at various stages of development, not just finalized manuscripts. future work can investigate the types of questions that arise when users interact with other file types such as pdf, excel, and powerpoint. because each file type is used for different purposes (e.g., excel documents for long-term book-keeping [27]) and possibly containing content at different levels of abstraction, the extent to which question answering in these documents can be automated and the kinds of expertise knowledge workers need may be different from the word documents in our study.because all questions had to first pass through the knowledge workers' system, the majority of the responses that participants received from the q&a system had a delay. therefore, it is possible that participants would have asked different questions if the responses had been provided instantaneously. another property that could have conceivably impacted the types of questions users asked is the quality of the answers that they received. to understand if the types of questions by a user changed over time as they gained familiarity with the system, we examined the questions that were posted by the same user both across different documents or on the same document. we observed that some users posted multiple questions in succession and close together in time, e.g., in the span of a few minutes. although these users would realize that the system does not provide answers instantaneously, their cluster of initial questions would not be impacted by the expectation of a long delay, their perceived capabilities of the system, or the quality of the answers. in fact, although we had specified in the consent form that there may be a delay in the responses that participants would receive from the q&a system, some end-of-study survey responses indicated that a number of participants had in fact not noticed this point and had asked their first few questions expecting instantaneous answers-""i was initially confused by the delay of asking the question in the document and then waiting for an email that told me to go back to the document. it seemed a bit redundant to get an email about it vs. just a notification in the word doc itself and telling me it was working on it or something. for a plug-in, however, i would expect less of a delay. "" (p-2-18). therefore, the first few questions from these participants could help with the generalizability of our results.the delay or the quality of answers however, may have influenced those users who submitted questions after receiving answers from the system. upon examination, we found that in many instances, when users received answers to their previous questions, they explored increasingly more sophisticated questions of various types, e.g., content-related, concerning metadata, or seeking external information. for instance, one participant started by asking simple metadata questions (""who is the author?"") and proceeded to ask questions on a scanned document that needed not only complex reasoning, but also optical character recognition (ocr): ""what is the total score?"" on a document where handwritten scores were given to each question. this finding suggests that the answers may have in fact encouraged users to be liberal with the types of questions they wished to subsequently ask. this exploration could be due to users gaining confidence that the system can in fact handle the types of questions with which they need support or could be because they wished to test the limits of its abilities.nevertheless, the characterization that we present in the paper also includes the questions users submitted in the experience sampling phase, where participants could imagine a sophisticated system with any or no delay. the set of questions in that phase of the study could further help with generalizability of our results, e.g., to settings where not all questions necessarily experience a delay.it is conceivable that the types of questions about a document may vary with the document type. in phase 2 where we had access to the documents, we observed that the document distribution was in fact very varied and included project proposals and timelines, value propositions, design specifications, service instructions, management training, protocols, faqs, whitepaper reports, strategy planning, customer feedback, research findings, etc. from various domains. with this diverse set of document types, investigating the relationship between types of questions and types of documents would require collecting far more questions by running the study for a long time.the setup of our q&a system was such that users submitted one-shot questions as there were no affordances for following up on previously asked questions. future work should investigate whether the types of questions that users ask a document q&a system differ if the system provides the users with the means for following up on their previous questions or multi-round conversations with the assistant.another area for future work would be to explore the context of user needs including when users need different types of assistance with their documents, what they do before the seek help, and what they do after they receive answers. to minimize concerns about the confidentiality of the business data, the contextual information that we collected in our study concerned the metadata of the document (e.g., file size, last modified date, etc.) and not the content. for the same reason, we did not track a user's modification of the document before or after the user posted a question; the metadata was collected upon the user's submission of a question. therefore, given the data we collected, we cannot examine the context of user needs. future work can study this question through interviews and user-produced logs.","{86611921: '33,', 211010545: '[55,'}",https://export.arxiv.org/pdf/2203.15073v2.pdf
554,247779047,Understanding Questions that Arise When Working with Business Documents,conclusion,Insight-tree,"we studied users' information needs when working with their business documents as a first step towards building document assistants that can handle a variety of user requests. to understand users' actual needs, it was important to collect their document-centric questions in-situ. therefore, we conducted two user studies. in the first study, we performed experience sampling of users' questions via a microsoft word add-in as users were working with their documents. in the second, users submitted their questions via an add-in and received answers from a human-in-the-loop document q&a system that complemented a question-answering ai with human intelligence. we characterized the distributions of questions and observed that the types of questions do indeed vary by whether the user is an author, a reviewer, or a reader of the document. in addition, the questions gave us insight into what types of request can be automated and whether particular skillsets or roles within the document are needed from human respondents in a document digital assistant that is co-powered by artificial and human intelligence.","{86611921: '33,', 211010545: '[55,'}",https://export.arxiv.org/pdf/2203.15073v2.pdf
555,232135266,Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,conclusion,Insight-tree,"in this work, we proposed rissanen data analysis (rda), a method for examining the characteristics of a dataset. we began by viewing the labels of a dataset as being generated by a program over the inputs, then positing that a capability is helpful if it reduces the length of the shortest label-generating program. instead of evaluating minimum program length directly, we use blockwise prequential coding to upper bound minimum description length (mdl). while the choice of learning algorithm a influences absolute mdl values, we only interpret mdl relative to other mdl values estimated with the same a. in particular, we conduct rda by comparing mdl with or without access to a subroutine with a certain capability, and we say that a capability is useful when invoking the subroutine reduces mdl.",,https://arxiv.org/pdf/2103.03872v1.pdf
556,227230675,Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking,conclusion,Insight-tree,"the lit architecture is a simple yet powerful adaptation of the transformer architecture to learn better cross-document interactions for multi-hop ranking. the structure can be easily integrated with any transformer language model to enable cross-referencing of knowledge statements and improved ranking performance. for example, lit can be a drop-in encoder for other multi-hop question answering datasets such as hotpotqa (yang et al., 2018). when applied to the challenging worldtree v2 dataset, lit achieves competitive performance with current state-of-the-art models despite a smaller footprint. we envision that this architecture can be beneficial to many nlp tasks which require multi-hop reasoning over documents.","{204915921: 'Khot et al., 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2012.14164v1.pdf
557,251135345,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,conclusion,Insight-tree,"we provide insights into an rnn-based iterative memory model that incorporates gate attention on multi-step reasoning over natural language. instead of using the original gru and dotproduct attention, we integrate gate attention to update hidden states. the experiment results show the model with gate attention achieves generally better performance than the original rnnbased iterative-memory model with dot-product attention and other rnn-based models. the performance of our model is comparable or better than the much larger and pretrained roberta-large in some scenarios. furthermore, our model shows better out-of-distribution generalisation performance than the pretained roberta. to address the issue of depth-imbalance in the existing datasets on multi-step reasoning over natural language, we develop a large-scale multi-step reasoning dataset called pararule-plus, with more examples of deep reasoning depths than previous datasets. we find that the performance of the models in our experiments improves when we add pararule-plus in the training, especially on examples that require deeper reasoning depths and extra out-of-distribution examples. table 4 we use glove [16] as the word vector representation. we use pararules with all depths as the training set for all models and then test them on examples with different reasoning depths (d). comparison among our ima-glove-ga, ima-glove, mac-glove, dmn-glove, imasm-glove, lstm-glove, and roberta-large on pararules test sets with different reasoning depths.",{},https://export.arxiv.org/pdf/2207.14000v1.pdf
558,253080620,LittleBird: Efficient Faster & Longer Transformer for Question Answering,conclusion,Insight-tree,"we propose littlebird, which is more efficient in terms of memory and computational time than existing transformer models for long sequences, and its effective way to train. it combines a novel position encoding method, bialibi, and pack & unpack with sliding window attention to achieve high speed and accuracy, particularly in question answering tasks for long documents. the distillation and training method with padding insertion allows the model to be trained by reusing the existing pre-trained language model for short inputs and work well for long inputs even if trained on short inputs. we demonstrated through experiments that the accuracy of question answering improves as the model is fed a longer input, and we achieved state-of-the-art performance in korquad2.0 using littlebird.","{198229624: 'Joshi et al. 2020', 86611921: 'Kwiatkowski et al., 2019', 230433978: 'Ram et al. 2021', 226281978: 'Tay et al., 2020', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al. 2020'}",https://www.aclanthology.org/2022.emnlp-main.352.pdf
559,199552247,Playing log(N)-Questions over Sentences,conclusion,Insight-tree,"we have proposed the game of log(n )-questions over sentences, and introduced an end-to-end system of 2 agents that are able to play the game. while our results show promise, there is work to be done on improving game and sw prediction performance simultaneously, as well as playing the game over larger sentence sets. more generally, we shows agents exhibiting reasoning and information-seeking in a text environment.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/1908.04660v1.pdf
560,235097535,MULTIPROVER: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,conclusion,Insight-tree,"we proposed multilabel-multiprover and iterative-multiprover, two variants of a proof-set generation model where the former performs implicit conditioning between the proofs to generate them in parallel while the latter generates a proof-set through explicit conditioning on the previously generated proofs. both models obtain strong proof f1 improvements on synthetic and humanparaphrased datasets and iterative-multiprover also obtains state-of-the-art proof f1 on a zero-shot dataset with single proofs. multiprover's modeling is fairly generic and similar methods can be used in generating a set of structured explanations for other nlp tasks like multi-hop qa.","{211126663: 'Clark et al., 2020;', 202539540: 'Tafjord et al., 2019;', 52822214: 'Yang et al., 2018;', 218487030: 'Ye et al., 2020'}",https://www.aclweb.org/anthology/2021.naacl-main.287.pdf
561,253018873,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,conclusion,Insight-tree,"this paper stimulates the compositional reasoning process of humans in decision-making, and makes the following hypotheses: (1) the intuitive perception system (system 1) and cognitive reasoning system (system 2) can be decoupled and (2) the complex decision-making can be disentangled into multi-step execution of fundamental reasoning skills. correspondingly, we propose reason-former, a compositional general-purpose reasoning framework. reasonformer decouples the representation module and reasoning modules, which are pre-trained to expert in fundamental reasoning skills. the reasoning modules are dynamically composed in parallel and cascaded manner to form a whole reasoning process. reasonformer is endto-end and unified in solving multiple tasks with one model. extensive experiments on 11 tasks reveal the compositional reasoning ability of reason-former and disentangling of representation and reasoning modules.","{52822214: 'Yang et al., 2018b'}",https://export.arxiv.org/pdf/2210.11265v2.pdf
562,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,conclusion,Insight-tree,"in this paper, we proposed apollo, an adaptive pre-trained language model with logical reasoning abilities. we use a subset of wikipedia sentences for continued pretraining of the model using two self-supervised loss functions. the choice of the training dataset and loss functions are guided by the goal to include more reasoning-related sentences and training signals, respectively. through experiments on two logical reasoning datasets and ablation studies, we demonstrate the effectiveness of our proposed approach. overall, we show that apollo is a generalized solution to improving logical reasoning in language models.","{247518855: 'Li et al., 2022;', 220483148: 'Liu et al., 2021', 222141025: 'Saha et al., 2020;', 247594506: 'Sanyal et al., 2022b', 234335834: 'Wang et al. 2022', 248496003: 'Xu et al., 2022;', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2023.acl-long.347.pdf
563,254854559,APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning,limitation,Insight-tree,"a limitation of this approach is the trade-off between completeness and noise in the training data. while our method using keywords to extract text from wikipedia is effective, implication likely contains redundant sentences that cannot improve the model's logical reasoning capability. a better rule-based or neural model might be able to extract a better corpus with potentially higher computational costs. additionally, using pos tagging limits the application of this approach to languages with well-defined pos taggers. switching to a more universal semantic tagging system (abzianidze and bos, 2017)","{247518855: 'Li et al., 2022;', 220483148: 'Liu et al., 2021', 222141025: 'Saha et al., 2020;', 247594506: 'Sanyal et al., 2022b', 234335834: 'Wang et al. 2022', 248496003: 'Xu et al., 2022;', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2023.acl-long.347.pdf
564,225075843,Differentiable Open-Ended Commonsense Reasoning,conclusion,Insight-tree,"we introduce and study a new task -open-ended commonsense reasoning (opencsr) -which is both realistic and challenging.we construct three opencsr versions of widely used datasets targeting commonsense reasoning with a novel crowdsourced collection of multiple answers, and evaluate a number of baseline methods for this task.","{208267807: 'Asai et al., 2020', 211296452: 'Dhingra et al., 2020', 189927857: 'Feldman and El-Yaniv, 2019', 204915921: 'Khot et al., 2020', 202773198: 'Qi et al., 2019', 128345225: 'Sun et al., 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2010.14439v2.pdf
565,259224595,On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective,conclusion,Insight-tree,"in this paper, we analyzed the out-of-distribution robustness of several representative generative and dense retrieval models on the kilt benchmark. specifically, we proposed three perspectives to define the out-of-distribution robustness. the results showed that generative retrieval models expose significant vulnerabilities in ood robustness. more research efforts are needed to develop robust generative retrieval models.","{248366293: '[1,', 258418300: '[3,', 251594672: '[5]', 222125277: '7]', 232147859: '[9]', 221507798: '[24]', 233296016: '29,', 52822214: '[33]'}",https://export.arxiv.org/pdf/2306.12756v1.pdf
566,252819353,Question Generation Based on Grammar Knowledge and Fine-grained Classification,conclusion,Insight-tree,"to solve the problem of mismatch between question types and answers in question generation, this paper constructs a question type classifier and a question generator. we classify questions into finegrained classification and integrate grammar knowledge into question type classifier to improve the accuracy of question types, then, the prediction results of the classifier are fused into the question generator to improve the performance of question generation. to verify the effectiveness of our model, we perform an upper bound analysis on it, we integrate grammar knowledge into the question generator to provide accurate question types to guide the model to generate questions. the final experimental results show that the method proposed in this paper not only improves the accuracy of question words in the generated question, but also improves the quality of the generated question.","{227231411: 'Wang et al., 2020b', 226236844: 'Xie et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.562.pdf
567,259833782,Can In-context Learners Learn a Reasoning Concept from Demonstrations?,conclusion,Insight-tree,this work introduces a task of conceptual few-shot learning that reflects on in-context learners' ability to learn to apply a specific reasoning concept that can be informative for prediction. we assess a set of recent in-context learners for this ability over a set of concepts extracted from human explanations.,"{233296924: 'Bartolo et al., 2021', 218486753: 'Inoue et al. 2020', 240288835: 'Min et al. 2022a', 232035689: 'Wiegreffe and MarasoviÄ, 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.nlrse-1.8.pdf
568,259924447,CodeQueries: A Dataset of Semantic Queries over Code,conclusions and future work,Insight-tree,"we presented the codequeries dataset to test the ability of neural models to understand code semantics on the proposed problem of answering semantic queries over code. it requires a model to perform single-or multi-hop reasoning, understand structure and semantics of code, distinguish between positive and negative examples, and accurately identify answer and supporting-fact spans. our evaluation shows that codequeries is challenging for the best-in-class generative and embedding approaches under different prompting or fine-tuning settings. we are considering extensions to our dataset to include more semantic queries and more programming languages.","{250390665: 'Lee et al., 2022', 52822214: 'Yang et al. 2018'}",https://export.arxiv.org/pdf/2209.08372v2.pdf
569,212657414,TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages,conclusion,Insight-tree,"confidently making progress on multilingual models requires challenging, trustworthy evaluations. we have argued that question answering is well-suited for this purpose and that by targeting a typologically diverse set of languages, progress on the tydi qa dataset is more likely to generalize on the breadth of linguistic phenomena found throughout the world's languages. by avoiding data collection procedures reliant on translation and multilingual modeling, we greatly mitigate the risk of sampling bias. we look forward to the many ways the research community finds to improve the quality of multilingual models. 25 because we believe mt may be a fruitful research direction for tydi qa, we do not release any automatic translations. in the past, this seems to have stymied innovation around translation as applied to multilingual datasets. 26 we will happily share our annotation protocol on request.","{165163607: 'Clark et al., 2019', 67855846: 'Dua et al., 2019'}",https://arxiv.org/pdf/2003.05002v1.pdf
570,244954717,Does Structure Matter? Leveraging Data-to-Text Generation for Answering Complex Information Needs,conclusion,Insight-tree,"traditionally, ir approaches solving complex information needs focused on leveraging multi-turn interactions to provide optimal rankings of candidate documents at each turn. in this paper we have suggested alternative retrieval models that do not rely on the interactive updating of queries and document rankings as answers. we suggest one such alternative approach can be found using datato-text generation models to generate in a single-turn, a natural language and structured answer. experimental evaluation of a planning-based dtt model using the trec car dataset shows the potential of our intuition. we believe that our work opens up novel areas of investigation including answer generation and explanation in conversational systems for ir.","{208267807: '[2,', 158046817: '17,', 52822214: '[22]'}",https://arxiv.org/pdf/2112.04344v1.pdf
571,258865495,Few-shot Unified Question Answering: Tuning Models or Prompts?,conclusion,Insight-tree,"in this work, we explore the viability of prompttuning as a solution to unified qa and conduct a thorough analysis of its promise, effectiveness, and trade-offs compared with the model-tuning paradigm on a set of 16 qa datasets, focusing particularly on several few-shot scenarios. as a result, we obtain several key findings and insights that hopefully will inform which paradigm to prefer under which scenarios. prompt tuning is quite competitive with model-tuning in the lower extreme of the few-shot scenarios, given a good initialization. while parameter-sharing leads to superior performance in the few-shot setting, the trends flip in the full-shot setting, a simple knowledge transfer approach (i.e., an average of relevant prompts) is as effective as complex methods without introducing additional parameters. pre-training the backbone model on the source tasks significantly benefits prompt tuning. while initializing from a strong prior is very helpful for prompt tuning, its benefit is not as substantial when using a larger backbone model, especially when the number of training examples exceeds a certain threshold.","{248572452: 'Zhong et al., 2022a'}",https://export.arxiv.org/pdf/2305.14569v1.pdf
572,258715048,Pre-Training to Learn in Context,conclusion,Insight-tree,"this paper presents picl, a framework that exploits the in-context learning ability of plms by pre-training models on concatenations of text paragraphs sharing the same ""intrinsic tasks"" gathered from the large-scale general corpus. in picl, models learn to perform various intrinsic tasks conditioning on their context while preserving their generalization due to the little bias of the pre-training data. extensive experiments show that picl improves the icl performance on various datasets against several baselines, enabling a 770 m model to outperform a larger model with about 4x parameters while maintaining good generalization across a wide range of tasks. for future work, we would like to consider adding human instructions to our pre-training framework to enhance more abilities of plms like zero-shot instruction following.",{},https://www.aclanthology.org/2023.acl-long.267.pdf
573,259859005,Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we have presented ci4mrc, a novel causal interventional paradigm to address name bias in mrc: the pre-trained knowledge concerning names is a confounder limiting the robust performance.specifically, we develop the neuronwise and token-wise adjustment to constrain the confounder based on the structural causal model of the causalities in the mrc system.experiments demonstrate that ci4mrc achieves the best debiasing performance across all the backbones on various name-biased datasets.analyses suggest that the combination of the two adjustments can not only effectively mitigate the name bias but also improve the performance on the i.i.d evaluation.","{67855846: 'Dua et al., 2019', 245219235: 'Guan et al., 2022', 247292369: 'Wang et al., 2022c', 52822214: 'Yang et al., 2018;'}",https://aclanthology.org/2023.findings-acl.812.pdf
574,214802134,Graph Sequential Network for Reasoning over Sequences,conclusion,Insight-tree,"this paper proposes graph sequential network as a novel neural architecture to facilitate reasoning over graphs with sequential data on the nodes. we develop a new message passing algorithm based on co-attention between two sequences on graph nodes. the scheme avoids the information loss inherent in the pooling based early summarization of existing gnn-based models, and improve the reasoning ability on sentence level. through experiments on hotpotqa and fever, both of which require the model to perform multi-hop reasoning, we show that our proposed gsn attains better performance than existing gnns on different types of tasks. for future work we would like to apply gsn to other applications in nlp that require complex reasoning.","{158046817: 'Tu et al. 2019b', 155100120: 'Xiao et al., 2019;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2004.02001v1.pdf
575,232320760,Mitigating False-Negative Contexts in Multi-Document Question Answering with Retrieval Marginalization,conclusion,Insight-tree,we proposed a new probabilistic model for retrieving set-valued contexts for multi-document qa and show that training the qa model with marginalization over this set can help mitigate the false negatives in evidence annotations. experiments on iirc and hotpotqa fullwiki show that our proposed framework can learn to retrieve unlabeled alternative contexts and improves qa f1 by 5.5 on iirc and 8.9 on hotpotqa.,"{208267807: 'Asai et al. 2020', 212657414: 'Clark et al., 2020', 207853300: 'Fang et al., 2020', 226262208: 'Ferguson et al. 2020', 215768725: 'Groeneveld et al., 2020', 209202200: 'Gupta et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2021.emnlp-main.497.pdf
576,232290456,HOPPER: MULTI-HOP TRANSFORMER FOR SPATIOTEMPORAL REASONING,conclusion and future work,Insight-tree,"this work presents hopper with a novel multi-hop transformer to address object permanence in videos. hopper achieves 73.2% top-1 accuracy at just 1 fps on cater, and demonstrates the benefits of multi-hop reasoning. in addition, the proposed multi-hop transformer uses an iterative attention mechanism and produces a step-by-step reasoning chain that improves interpretability. multi-hop models are often difficult to train without supervision for the middle hops. we propose several training methods that can be applied to other tasks to address the problem of lacking a ground truth reasoning chain. in the future, we plan to experiment on real-world video datasets and extend our methods to deal with other complex tasks (such as video qa).","{211296452: 'Dhingra et al., 2020;'}",https://arxiv.org/pdf/2103.10574v2.pdf
577,210860748,A STUDY OF THE TASKS AND MODELS IN MACHINE READING COMPREHENSION,conclusion,Insight-tree,"this report is a survey on the existing tasks and models in mrc. for mrc tasks, some representative simple-reasoning and complex-reasoning tasks are introduced,",{},https://arxiv.org/pdf/2001.08635v1.pdf
578,218974030,Project PIAF: Building a Native French Question-Answering Dataset,conclusion,Insight-tree,"motivated by the scarcity of non-english data, we described our ongoing effort towards gathering native qa samples for the french language, using a participatory approach. rather than a transactional approach to data collection, as usually adopted in crowd-sourcing efforts, we experiment with a comparatively slower and more engaging process, focusing on quality over quantity. amongst desirable sideeffects of our approach, we highlight the educational aspects, e.g. introducing a wider audience to ai concepts and methodologies during our annothathons.  table 3: n = 191 randomly sampled triplets were manually assigned into one or more of the above categories. words relevant to the corresponding reasoning type are in bold, and the annotated answer is underlined.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.lrec-1.673.pdf
579,215238846,Multi-Step Inference for Reasoning Over Paragraphs,conclusion,Insight-tree,"we propose a multi-step reading comprehension model that performs chained inference over natural language text. we have demonstrated that our model substantially outperforms prior work on ropes, a challenging new reading comprehension dataset. we have additionally presented some analysis of ropes that should inform future work on this dataset. while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets.","{208267807: 'Asai et al., 2019', 67855846: 'Dua et al., 2019', 189927857: 'Feldman and El-Yaniv, 2019', 209202200: 'Gupta et al., 2020', 201058633: 'Lin et al., 2019', 174801080: 'Min et al., 2019;', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2004.02995v2.pdf
580,254366618,Text Embeddings by Weakly-Supervised Contrastive Pre-training,conclusion,Insight-tree,"in this work, we train a general-purpose text embedding model e5 from weak supervision signals. we adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. e5 offers strong off-the-shelf performance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. when further customized for downstream tasks, e5 achieves superior fine-tuned performance compared to existing embedding models with 40Ã more parameters on the large, 56-task mteb benchmark datasets. table 9: details for each data source after filtering. the ""others"" category includes ""sim-plewiki"", ""gooaq"", ""wikihow"", ""yahoo answers"" from https://huggingface.co/datasets/ sentence-transformers/embedding-training-data.","{245218527: '[28]', 86611921: '[32,', 246996947: '[39]', 245144556: '[43,', 233296016: '[53]', 220302524: '[61]', 247411106: '[62]'}",https://export.arxiv.org/pdf/2212.03533v1.pdf
581,222208994,SRLGRN: Semantic Role Labeling Graph Reasoning Network,conclusion,Insight-tree,"we proposed a novel semantic role labeling graph reasoning network (srlgrn) to deal with multihop qa. the backbone graph of our proposed graph convolutional network (gcn) is created based on the semantic structure of the sentences. in creating the edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts. the cross paragraph argument-predicate structure of the sentences expressed in the graph provides an explicit representation of the reasoning path and helps in both finding and explaining the multiple hops of reasoning that lead to the final answer. srlgrn exceeds most of the sota results on the hotpotqa benchmark. moreover, we evaluate the model (excluding the paragraph selection module) on other reading comprehension benchmarks. our approach achieves competitive performance on squad v1.1 and v2.0.","{203836061: 'Chen et al., 2019;', 207853300: 'Fang et al., 2019', 174801080: 'Min et al., 2019', 207870753: 'Tu et al., 2019', 155100120: 'Xiao et al., 2019'}",https://arxiv.org/pdf/2010.03604v1.pdf
582,235186933,Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting,conclusion,Insight-tree,"we explored the task of difficulty-controllable question generation, with question difficulty redefined as the inference steps required to answer it. a step-by-step generation framework was proposed to accomplish this objective, with an input sampler to extract the reasoning chain, a question generator to produce a simple question, and a question rewriter to further adapt it into a more complex one. a dataset was automatically constructed based on hotpotqa to facilitate the research. extensive evaluations demonstrated that our method can effectively control difficulty of the generated questions, and keep high question quality at the same time.","{216553210: 'Pan et al. 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.acl-long.465.pdf
583,245219136,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,conclusion,Insight-tree,"augmenting pre-trained generation models with retrievers has shown to be effective in many knowledge-intensive tasks; however, they often rely on spurious cues or generate hallucinations during inference. we introduce a multi-task learning objective the combines answer generation and evidentiality prediction. we propose task-agnostic data mining techniques to obtain silver evidentiality labels to enable this auxiliary training. our experiments across five datasets show large performance improvements over baselines and our evidentialityguided generator advances the state-of-the-art performance on faviq-ambig, fever and wow. our analysis shows that multi-task learning and silver evidentiality mining both contribute to the performance improvements by helping the model learn to focus on and generate answers from more relevant passages. ",{},https://www.aclanthology.org/2022.naacl-main.162.pdf
584,258378176,Combining Parameter-efficient Modules for Task-level Generalisation,conclusions,Insight-tree,"we argued that a modular design is crucial to ensure that neural networks can learn from a few examples and generalise robustly across tasks by recombining autonomous facets of knowledge. to this end, we proposed a model where a subset of latent, discrete skills from a fixed inventory is allocated to each task in an end-to-end fashion. the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters. we evaluate the sample efficiency of our model on multitask instruction following through reinforcement learning and its few-shot adaptability on multitask text-to-text generation through supervised learning. in both experiments, we surpass competitive baselines such as conditional parameter generation (hyperformer) and mixture of experts (task-moe). finally, we show that modularity helps interpret multi-task models by inferring explicit relationships between tasks according to the skills they share.","{221819379: 'Pilault et al. 2021', 233296709: 'Ye et al. 2021'}",https://www.aclanthology.org/2023.eacl-main.49.pdf
585,248228026,StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,conclusion,Insight-tree,"in this paper, we proposed a new dataset named stepgame that requires a robust multi-hop spatial reasoning ability to be solved and mitigates the issues observed in the babi dataset. then, we introduced tp-mann, a tensor productbased memory-augmented neural network architecture that achieves state-of-the-art performance on both datasets. further analysis also demonstrated the importance of a recurrent memory module for multi-hop reasoning.","{233219660: 'Mirzaee et al. 2021', 202558795: 'Tversky 2019', 52822214: 'Yang et al. 2018'}",https://arxiv.org/pdf/2204.08292v1.pdf
586,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,conclusions,Insight-tree,"this work aims to propose a sensible assessment framework for multiple choice question generation in order to encourage the question generation community to consider more appropriate methods of assessing and benchmarking developed systems to reflect the qualities of interest rather than arbitrary n-gram based approaches. here, the first fully automated end-to-end multiple-choice question generation system is proposed for generating a question, the correct answer and distractor options for an input context without relying on explicit phrase extraction based techniques.",,https://export.arxiv.org/pdf/2209.11830v1.pdf
587,252532175,Multiple-Choice Question Generation: Towards an Automated Assessment Framework,limitations,Insight-tree,"here, the limitations of the current approaches are discussed. first, both measures of unanswerability and question complexity are model and corpusspecific. hence, it is not clear about the applicability of these metrics beyond the race++ dataset. specifically, the unanswerability measure runs the risk of conflating answerability with the failure of a reading comprehension question. the validity of such metrics is centred on in-domain data, which may be diminishingly effective at discriminating the performance of generative models on shifted data (e.g the nature of questions in reclor is more logical-based and requires higher inference than questions in race++, which might make measures trained on race++ struggle at assessment on questions generated in the reclor style). no human evaluation is performed of whether the assessment metrics correlate explicitly with human notions of answerability and complexity. hence, further work should invest resources to comprehensively establish the validity of the proposed measures. finally, the question complexity system is trained specifically on the meaning of complexity as described for race++. however, question complexity has several definitions and hence further work should establish whether this interpretation of complexity may align with other views of complexity. .",,https://export.arxiv.org/pdf/2209.11830v1.pdf
588,221005883,Interpretable Multi-Step Reasoning with Knowledge Extraction on Complex Healthcare Question Answering,conclusions,Insight-tree,"in this paper, we present a system murke that answers healthcare exam questions by using knowledge extraction and multi-step reasoning. to get a relevant document for each question, murke retrieves supporting documents from a large, noisy corpus on the basis of keywords extracted from the original question and semantic retrieval. murke proposes the multi-step iterative method to solve complex healthcare qa, which uses information selected by combining iterative question reformulation and textual entailment. our neural architecture uses a sequence of token-level attention mechanisms to extract relevant evidence from the selected documents in order to update the latent representation of the question, which shows the interpretability of the reasoning path. through empirical results and case study, we demonstrate that our proposed system is able to outperform several strong baselines on the headqa dataset.",{},https://arxiv.org/pdf/2008.02434v1.pdf
589,243756815,Extracting a Knowledge Base of COVID-19 Events from Social Media,conclusion,Insight-tree,"in this paper, we presented a corpus of 10,000 tweets annotated with 5 types of events and 28 slots. we showed that our corpus supports automatic extraction of covid-19 events using supervised learning. by aggregating extractions over millions of tweets, our approach can accurately answer a range of structured queries about events that are publicly reported in real-time on twitter. our knowledge base could be a useful tool for epidemiologists, journalists and policymakers to more efficiently track the spread of this new disease. this work also presents a case-study on how an information extraction system can be rapidly developed for a new domain in response to an emerging crisis. for example, our methodology could be applied to develop knowledge bases for natural disasters (spiliopoulou et al., 2020) or future disease outbreaks.",{52822214: 'Yang et al. 2018'},https://export.arxiv.org/pdf/2006.02567v4.pdf
590,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,conclusion,Insight-tree,"we propose a new parameterization framework, taser, for improving the efficiency and robustness of dense retrieval for odqa. it interleaves shared encoder blocks with specialized ones in a single encoder where some sub-networks are task-specific. as the specialized sub-networks are sparsely activated, taser can provide better parameter efficiency with almost no additional computation cost. experiments show that taser substantially outperforms existing fully supervised biencoder dense retrievers on both in-domain and out-of-domain generalization.",{},https://www.aclanthology.org/2023.acl-short.159.pdf
591,252816088,Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,limitations,Insight-tree,"in this section, we point out several limitations in this work.first, our in-domain evaluation experiments focus on passage retrieval for odqa. while the dense retriever is mostly successful in odqa, it can be also used in other types of retrieval tasks which may have different input and output format. for example, the kilt benchmark (petroni et al., 2021) provides several knowledge-intensive tasks other than odqa. the performance of taser models trained on such retrieval tasks remain unknown.second, compared with traditional sparse vector models like tf-idf and bm25, the cost of training is an inherent issue of dense retrievers. although taser significantly reduce the number of model parameters, the training cost is still high.third, in our experiments, we show that the learned routing does not outperform the deterministic routing. this may suggest a better architecture and/or training algorithms for learned routing is needed to fully unleash the power of moe.last, as observed in Â§4.2, there is still a gap between taser and bm25 in out-of-domain evaluation. therefore, how to close this gap will remain a critical topic for future work on dense retrievers.",{},https://www.aclanthology.org/2023.acl-short.159.pdf
592,232290492,Controllable Generation from Pre-trained Language Models via Inverse Prompting,conclusion,Insight-tree,"in this paper, we present a new method, inverse prompting for text generation. inverse prompting offers a new option for controllable generation using language models by exploiting the inverse form of natural languages.",{},https://arxiv.org/pdf/2103.10685v3.pdf
593,218973757,Why Attention is Not Explanation: Surgical Intervention and Causal Reasoning about Neural Models,limitations,Insight-tree,"while our study of explanation is based in philosophy, our contributions are based in epistemology, not in ethics. many adjacent subfields of philosophy of science exist and only occasional interactions between computer scientists and philosophers have taken place to date (miller, 2018;zerilli et al., 2018). in this work, we have examined whether researchers or users are being given a true explanation. but a good explanation does not mean that an algorithm has made a good decision. explainability research frequently studies tasks with high stakes, including notoriously biased tasks like recidivism prediction, financial risk modeling, and facial recognition for surveillance. our work does not absolve researchers from a broader social responsibility: the presence of a successful explanation will not help if a loan is denied because of race (fuster et al., 2018), if an accused criminal is wrongly identified because of their gender presentation (buolamwini and gebru, 2018), or if algorithms persecute ethnic groups (wang et al., 2016) or misdiagnose mental health (bennett and keyes, 2019).to build algorithmic decision-making in a truly socially responsible way, our work must be a component piece, incorporated into a broader foundation that accounts not only for explanation but also for ethical software development. this work provides a vocabulary for computer scientists struggling to unify the informal language that proliferates across research today, and will allow nlp researchers to improve the quality and rigor of their explanations, and provide a sure footing for the field.","{52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/2020.lrec-1.220.pdf
594,260611249,RETROFORMER: RETROSPECTIVE LARGE LANGUAGE AGENTS WITH POLICY GRADIENT OPTIMIZATION,conclusion,Insight-tree,"in this study, we present retroformer, an elegant framework for iteratively improving large language agents by learning a plug-in retrospective model. this model, through the process of policy optimization, automatically refines the prompts provided to the language agent with environmental feedback. through extensive evaluations on real-world datasets such as hotpotqa, the method has been proven to effectively improve the performances of large language agents over time both in terms of learning speed and final task completion performances.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2308.02151v1.pdf
595,259316419,Meta-training with Demonstration Retrieval for Efficient Few-shot Learning,conclusions,Insight-tree,"we have proposed a meta-training method ( Â§3.2) that retrieves ( Â§3.1) semantically similar demonstrations from a diverse demonstration bank ( Â§3.3). our method achieves higher performance on average across many tasks than other strong parameterefficient few-shot baselines ( Â§5). in future work, one could explore a mixture of demonstration retrieval and passage retrieval for improved performance on a wider variety of tasks-including knowledge-intensive tasks.","{231718729: 'Aghajanyan et al., 2021;', 204823992: 'Fisch et al., 2019', 240288835: 'Min et al. 2022a', 230433978: 'Ram et al., 2021', 233296709: 'Ye et al., 2021;'}",https://export.arxiv.org/pdf/2307.00119v1.pdf
596,256461186,Graph-Induced Transformers for Efficient Multi-Hop Question Answering,conclusions,Insight-tree,"this work presented git, the graph-induced transformer that drastically improved mhqa models' sample efficiency and replaces graphs in the models while retaining their performance. our empirical evidences demonstrated that models can enjoy the benefits of connective inductive bias of graphs without additional graph modules in place. the design of git also allowed us to reuse the parameters of plm while incorporating the graph information. future directions of our work may include using git in downstream nlp applications where the graph inductive bias is necessary and dataset is scarce.","{236771976: 'Trivedi et al., 2022', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.702.pdf
597,261030563,Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering,conclusion,Insight-tree,"we present beam retrieval, a general end-to-end retrieval framework for multi-hop qa. this approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. experimental results on three benchmark datasets prove the effectiveness of beam retrieval and demonstrate it could substantially improve the qa performance of downstream reader. in general, beam retrieval establishes a strong baseline for complex multi-hop qa, where we hope that future work could explore more advanced solutions.","{123758373: 'Chen and Durrett, 2019;', 207853300: 'Fang et al., 2020', 239885904: 'Fu et al., 2021', 256827065: 'Ho et al., 2023', 174801764: 'Min et al., 2019;', 155100120: 'Qiu et al., 2019', 221749191: 'Trivedi et al., 2020', 236771976: 'Trivedi et al., 2022', 254877499: 'Trivedi et al., 2023', 207870753: 'Tu et al., 2020', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2308.08973v1.pdf
598,261049149,Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning,conclusion,Insight-tree,"to theoretically and empirically advance the numerical reasoning research in the llms era, in this paper, we employ equations as imrs to solve the numerical reasoning task by addressing two problems: (1) theoretically, how to prove that the equation is an imr with higher generation accuracy than programs; (2) empirically, how to improve the generation accuracy of equations with llms. for the first problem, we present and prove a proposition to compare the generation accuracy of different","{235399966: '[8]', 174801080: '30]', 247595263: '[37]', 52822214: '[41,'}",https://export.arxiv.org/pdf/2308.10585v1.pdf
599,119104197,Transient dynamics of perturbations in astrophysical disks,conclusion,Insight-tree,"this review is devoted to the transient dynamics of perturbations, which is of special interest in theory of astrophysical disks, in particular accretion disks. exponentially growing perturbations do not exist in a homogeneous inviscid keplerian flow provided that there are no conditions for the magneto-rotational instability. nevertheless, observations suggest that also in this case angular momentum should be somehow transported outwards. at least, this implies that there should be some mechanism of energy transfer from the regular rotational motion to hydrodynamical perturbations. in spectrally stable flows the transient growth mechanism is responsible for this. here it was introduced by a simple example of two-dimensional vortices and it was discussed that the reason for their growth is the shortening of the length of leading spirals by the differential rotation of the flow (see fig. 2 and 3). nonwithstanding their seeming simplicity, those (quasi-)columnar structures exhibit the strongest ability to extract energy from the spectrally stable differentially rotating flows (see [54] about it). physically, the energy growth of vortices takes place due to their own angular momentum conservation, which in the local limit is expressed by the conservation of their potential vorticity and the existence of the invariant i (see section 2.2). here we considered both small-scale (k y â« 1) and large-scale (k y âª 1) vortices and compared their optimal growth with account for non-zero effective viscosity in the disk (see fig. 4). importantly, the transient growth of large-scale vortices strongly increases for a super-keplerian rotation, which can be significant in relativistic disks where q > 3/2. in this paper, special attention was paid to mathematical aspects of non-modal analysis and to methods of optimal perturbations computation. we have discussed in detail that the transient growth is a consequence of non-normality of the governing dynamical operator of the problem and non-orthogonality of its eigenvectors, i.e. modes of perturbations (see fig. 5 and 6). therefore, the growth of arbitrary perturbations can be adequately studied by calculating not eigenvectors but singular vectors of this operator. we have considered two methods: a matrix and variational one and applied them to the particular problems (see the corresponding results in fig. 7 and 11). the matrix method requires a discrete representation of the dynamical operator, for example, in the basis of its eigenvectors. the variational method is reduced to iterative integration of the system of direct and adjoint equations forward and backward in time, respectively. we have emphasized that the variational method is more universal and can be applied to study of non-modal dynamics of perturbations in non-stationary flows, as well as to non-linear problems.",{},https://arxiv.org/pdf/1512.08897v1.pdf
600,249062872,Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts,conclusions,Insight-tree,"in this paper, based on the observation that transformer models trained by massive tasks have better ability to generalize to unseen tasks, we hope to provide a new sight on exploring how this cross-task generalization ability is achieved and reused. inspired by the way that humans sparsely recall learned skills to solve new tasks, we explicitly model this process by resorting to a task-level mixture-of-expert model, where each expert represent different skills and tasks are routed by a router network based on the task property. we empirically investigate several importance design choices, i.e., routing models, expert selection strategies, task representations to exploring their influence on final model. secondly, by conducting a detailed analysis on the final routing decisions, we find it has a strongly correlation with human-defined task ontology (e.g., classification) and task characteristics (e.g., extractive, linguistic) even without any prior knowledge. we believe the result is valuable and promising in understanding the skills learned behind the black-box transformer models. model's parameters as the task embedding. the fim provides a measure of the information a particular parameter learns about the loss corresponding to the probe model, so it is disable to represent task (wang et al., 2021,vu et al., 2020b. given probe model m Î¸ , for example bart-base, we first calculate the loglikelihood with respect to the model parameters(Î¸) as: p Î¸ = log m Î¸ (y | x), then fim is calculated as the covariance of gradients of the loglikelihood:","{218487733: 'Vu et al., 2020b', 233296709: 'Ye et al. 2021'}",https://arxiv.org/pdf/2205.12701v1.pdf
601,6855746,RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response,attack models and limitations,Insight-tree,"we consider three types of attackers with different capabilities for collecting rappor reports.the least powerful attacker has access to a single report from each user and is limited by one-time differential privacy level 1 on how much knowledge gain is possible. this attacker corresponds to an eavesdropper that has temporary ability to snoop on the users' reports.a windowed attacker is presumed to have access to one client's data over a well-defined period of time. this attacker, depending on the sophistication of her learning model, could learn more information about a user than the attacker of the first type. nevertheless, the improvement in her ability to violate privacy is strictly bounded by the longitudinal differential privacy guarantee of â. this more powerful attacker may correspond to an adversary such as a malicious cloud service employee, who may have temporary access to reports, or access to a time-bounded log of reports. the third type of attacker is assumed to have unlimited collection capabilities and can learn the permanent randomized response b with absolute certainty. because of the randomization performed to obtain b from b, she is also bounded by the privacy guarantee of â and cannot improve upon this bound with more data collection. this corresponds to a worst-case adversary, but still one that doesn't have direct access to the true data values on the client.despite envisioning a completely local privacy model, one where users themselves release data in a privacy-preserving fashion, operators of rappor collections, however, can easily manipulate the process to learn more information than warranted by the nominal â. soliciting users to participate more than once in a particular collection results in multiple permanent randomized responses for each user and partially defeats the benefits of memoization. in the webcentric world, users use multiple accounts and multiple devices and can unknowingly participate multiple times, releasing more information than what they expected. this problem could be mitigated to some extent by running collections per account and sharing a common permanent randomized response. notice the role of the operator to ensure that such processes are in place and the required or assumed trust on the part of the user.it is likely that some attackers will aim to target specific users by isolating and analyzing reports from that user, or a small group of users that includes them. even so, some randomly-chosen users need not fear such attacks at all: with probability 1 2 f h , clients will generate a permanent randomized response b with all 0s at the positions of set bloom filter bits. since these clients are not contributing any useful information to the collection process, targeting them individually by an attacker is counter-productive. an attacker has nothing to learn about this particular user. also, for all users, at all times, there is plausible deniability proportional to the fraction of clients providing no information.in one particular attack scenario, imagine an attacker that is interested in learning whether a given client has a particular value v, whose population frequency is known to be fv. the strongest evidence in support of v comes in the form of both bloom filter bits for v being set in the client's report (if two hash functions are used). the attacker can formulate its target set by selecting all reports with these two bits set. however, this set will miss some clients with v and include other clients who did not report v. false discovery rate (fdr) is the proportion of clients in the target set who reported a value different from v. figure 7 shows fdr as a function of fv, the frequency of the string v. notably, for relatively rare values, most clients in the target set will, in fact, have a value that is different from v, which will hopefully deter any would-be attackers.the main reason for the high fdr rate at low frequencies fv stems from the limited evidence provided by the observed bits in support of v. this is clearly illustrated by figure 8 where the probability that v was reported (1) or not reported (0) by the client is plotted as a function of fv. for relatively rare strings (those with less than 10% frequency), even when both bits corresponding to v are set in the report, the probability of v being reported is much smaller than of value v given the two bits observed in a rappor report s corresponding to the two bits set by string v. for rare strings, even when both bits are set to 1 (green lines), it is still much more likely that the client did not report v, but some other value.it not being reported. because the prior probability fv is so small, a single client's reports cannot provide sufficient evidence in favor of v.",{},https://arxiv.org/pdf/1407.6981v2.pdf
602,250729995,ScienceQA: a novel resource for question answering on scholarly articles,conclusion and future work,Insight-tree,"in this paper, we present scienceqa, a novel dataset for benchmark evaluation of methods in the mrc (qa and qg in particular) task on scholarly articles. the dataset is created semi-automatically, consisting of over 100k triples of context-question-answer. the developed qa system could provide valuable evidence in managing the vast number of scholarly submissions. we offer a baseline and two more models, viz., (i). vanilla bert, (ii). science bert (i.e., scib-ert), and (iii). combination of bert and bi-daf. our proposed models are competitive compared to the existing state-of-the-art models. our future works would include:",{52822214: '[60]'},NaN
603,264439639,EpiK-Eval: Evaluation for Language Models as Epistemic Models,conclusion,Insight-tree,"in this paper, we presented the epik-eval benchmark, a tool designed specifically to evaluate the proficiency of lms in consolidating their knowledge for problem-solving tasks.our findings underscore the limitations of current lms, which appear to mostly maintain a disjoint knowledge state of training observations.further, we note a significant performance gap and an increased rate of hallucinations for models trained on segmented narratives compared to those trained on unsegmented ones.we attribute these discrepancies to the training objectives of the models, which underscores the need to more effectively model the dependencies within the training corpus.by highlighting current limitations and opportunities for improving lms, these results delineate paths for future research, hopefully enabling the growth of language models beyond simple knowledge bases.","{246652372: 'Ji et al., 2023'}",https://export.arxiv.org/pdf/2310.15372v1.pdf
604,238259354,FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks,conclusion and future directions,Insight-tree,"our key contribution is providing a thorough and insightful empirical analysis of existing federated learning algorithms in the context of nlp models. notably, we compare typical fl methods for four nlp task formulations under multiple non-iid data partitions. our findings reveal both promise and the challenges of fl for nlp. in addition, we also provide a suite of resources to support future research in fl for nlp (e.g., a unifying framework for connecting transformer models with popular fl methods and different non-iid partition strategies). thus, we believe our wellmaintained open-source codebase to support future work in this area.",{},https://arxiv.org/pdf/2104.08815v3.pdf
605,202573071,CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION,conclusion,Insight-tree,"with 1.63 billion parameters, ctrl is the largest publicly released language model to date. it is trained with control codes so that text generation can be more easily controlled by human users. these codes allow users to explicitly specify domain, subdomain, entities, relationships between entities, dates, and task-specific behavior. we hope that the release of this model at https://github.com/salesforce/ctrl pushes towards more controllable, general models for natural language processing, and we encourage future discussion about artificial generation with our team by emailing ctrl-monitoring@salesforce.com.   table 3). for all the reddit data, the secondary code can be title: or text:, which is the title and text of the article, respectively.","{86611921: 'Kwiatkowski et al., 2019'}",https://arxiv.org/pdf/1909.05858v2.pdf
606,254408974,Successive Prompting for Decomposing Complex Questions,conclusion,Insight-tree,"we present a way to successively decompose complex questions into simple qa pairs, which allows for modular qd and qa systems that can be trained and queried independently. when performing in-context learning, we showed that successive prompting yields an improvement of 4.6 f1 over chain-of-thought prompting. when replacing just the in-context qa module with a fine-tuned one, which is adept at handling list type questions, we further improve the overall performance by 9.5 f1.","{209202200: 'Gupta et al., 2020;', 221448158: 'Khot et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.81.pdf
607,53116244,Re Co CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,conclusion,Insight-tree,"we introduced record, a large-scale reading comprehension dataset requiring commonsense reasoning. unlike existing machine reading comprehension (mrc) datasets, record contains a large portion of queries that require commonsense reasoning to be answered. our baselines, including top performers on existing mrc datasets, are no match for human competence on record. we hope that record will spur more research in mrc with commonsense reasoning.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/1810.12885v1.pdf
608,211572557,TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing,conclusion and future work,Insight-tree,"in this paper, we present textbrewer, a flexible pytorch-based distillation toolkit for nlp research and applications. textbrewer provides rich customization options for users to compare different distillation methods and build their strategies. we have conducted a series of experiments. the results show that the distilled models can achieve state-of-the-art results with simple settings. textbrewer also has its limitations. for example, its usability in generation tasks such as machine translation has not been tested. we will keep adding more examples and tests to expand textbrewer's scope of application.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-demos.2.pdf
609,160009340,Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction,conclusion,Insight-tree,"we consider that the main contributions of our study are (1) the proposed qfe model that is based on a summarization model for the explainable multi-hop qa, (2) the dependency among the evidence and the coverage of the question due to the usage of the summarization model, and (3) the state-of-the-art performance in evidence extraction in both rc and rte tasks.",{52822214: 'Yang et al. 2018'},https://arxiv.org/pdf/1905.08511v1.pdf
610,247450518,Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,conclusion and future work,Insight-tree,"we systematically compare the extractive and generative readers for qa tasks. two sets of experiments are designed to control the effects of different prlms and the size of models. by conducting experiments on 12 qa datasets, our findings provide guidelines on how to choose extractive or generative readers given their strength and weakness. while current work investigates the pros and cons of extractive and generative models systematically, there are some hyperparameters that might affect the model performance. for example, it is known that different prompts in the input effect generative model performance (mishra et al., 2021b,a). also, it is worth studying the ood performance of models deeply. gokhale et al. (2022) compares multiple ways to improve the ood performance of an extractive model on qa task, and how these methods affect generative models have not been well-studied yet. meanwhile, most of the work including this work evaluate ood performance by averaging the performance across multiple dataset, but as mentioned in (mishra et al., 2020), the evaluation should be more carefully designed. also, diagnosing the performance on each ood dataset can provide more insights. for example, why models perform better on bioasq dataset than most other datasets (see table 4",,https://www.aclanthology.org/2022.spanlp-1.2.pdf
611,173188058,MULTIQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension,conclusions,Insight-tree,"in this work we performed a thorough empirical investigation of generalization and transfer over 10 rc datasets. we characterized the factors affecting generalization and obtained several state-ofthe-art results by training on 375k examples from 5 rc datasets. we open source our infrastructure for easily performing experiments on multiple rc datasets, for the benefit of the community. we highlight several practical take-aways: â¢ pre-training on multiple source rc datasets consistently improves performance on a target rc dataset , even in the presence of bert representations. it also leads to substantial reduction in the number of necessary training examples for a fixed performance. â¢ training the high-capacity bert-large representations over multiple rc datasets leads to good performance on all of the trained datasets without having to fine-tune on each dataset separately. â¢ bert representations improve generalization, but their effect is moderate when the source of the context is web snippets compared to wikipedia and newswire. â¢ performance over an rc dataset can be improved by retrieving web snippets for all questions and adding them as examples (context augmentation).",{153312687: 'Ding et al. 2019'},https://arxiv.org/pdf/1905.13453v1.pdf
612,218581117,A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering,conclusion,Insight-tree,"we introduce a resource of law statutes, a dataset of hand-curated rules and cases in natural language, and a symbolic solver able to represent these rules and solve the challenge task. our handbuilt solver contrasts with our baselines based on current nlp approaches, even when we adapt them to the legal domain.",{52822214: '[60]'},https://arxiv.org/pdf/2005.05257v2.pdf
613,220045416,Low-Resource Generation of Multi-hop Reasoning Questions,conclusions and future works,Insight-tree,"we have proposed an approach to generate the questions required multi-hop reasoning in low-resource conditions. we first built a multi-hop qg model and guided it to satisfy the logical rationality by the reasoning chain extracted from a given text. in order to tackle the labeled data shortage problem, we learned the structural patterns from the unlabeled data by the hidden semi-markov model. with the patterns as a prior, we transferred this fundamental knowledge into the generation model to produce the optimal results. experimental results on the hotpotqa data set demonstrated the effectiveness of our approach. moreover, we explored the generated results to facilitate the real-world application of machine reading comprehension. we will investigate the robustness and scalability of the model.","{67855846: 'Dua et al., 2019', 155100120: 'Qiu et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.601.pdf
614,252626893,How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,conclusion,Insight-tree,"we proposed a new multi-hop rc dataset for comprehensively evaluating the ability of existing models to understand date information. we evaluated the top-performing models on our dataset. the results revealed that the models may not possess the ability to subtract two dates even when fine-tuned on our dataset. we also found that our probing questions could help to improve qa performance, and can be used for data augmentation. for future work, we will use the hierarchical manner in our dataset to apply to other types of questions such as numerical reasoning questions in drop. ",{},https://export.arxiv.org/pdf/2210.05208v1.pdf
615,233241188,TWEAC: Transformer with Extendable QA Agent Classifiers,conclusion,Insight-tree,"we analyzed how to automatically select suitable qa agents specializing in different questions, as an alternative to a single qa system that tries to cover all possible questions. we presented a scalable meta-qa system that allows for a flexible extension with different qa agents. for newly posed questions, we rank agents by their ability to answer them and select the most suitable ones.","{173188058: 'Talmor and Berant, 2019;'}",https://arxiv.org/pdf/2104.07081v2.pdf
616,229923177,ERNIE-DOC: A Retrospective Long-Document Modeling Transformer,conclusion,Insight-tree,"in this paper, we proposed ernie-doc, a document-level language pretraining model based on the recurrence transformers paradigm. two well-designed mechanisms, namely the retrospective feed mechanism and the enhanced recurrent mechanism, enable ernie-doc, which theoretically has the longest possible dependency, to model bidirectional contextual information of a complete document. additionally, ernie-doc is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context. experiments on various downstream tasks demonstrate that ernie-doc outperforms existing strong pretraining models such as roberta, longformer, and bigbird and achieves sota results on several language modeling and language understanding benchmarks.","{221845203: 'Ainslie et al., 2020'}",https://www.aclanthology.org/2021.acl-long.227.pdf
617,247187611,"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",conclusion,Insight-tree,"selective prediction ability is crucial for nlp systems to be reliably deployed in real-world applications and we presented the most systematic study of existing selective prediction approaches. our study involved experiments in iid, ood, and adv settings with 17 datasets across several nlp tasks. we showed that despite leveraging additional resources (held-out data/computation), existing approaches fail to consistently and considerably outperform the simplest baseline (maxprob",{},https://www.aclanthology.org/2022.findings-acl.158.pdf
618,233296709,CROSSFIT : A Few-shot Learning Challenge for Cross-task Generalization in NLP,conclusion and future work,Insight-tree,"in this paper, we study the problem of building better few-shot learners via acquiring cross-task generalization ability from diverse nlp tasks. towards our goal, we introduce the crossfit challenge, an task setup that standardizes the training pipeline, data access and evaluation protocol. we also present the nlp few-shot gym, a repository of 160 diverse few-shot nlp tasks, to support crossfit learning in different scenarios. we empirically demonstrated that cross-task generalization can be acquired via multi-task learning and meta-learning; confirmed that the selection of seen tasks would influence the few-shot performance on unseen tasks.",{},https://www.aclanthology.org/2021.emnlp-main.572.pdf
619,259317090,Single Sequence Prediction over Reasoning Graphs for Multi-hop QA,conclusion,Insight-tree,"in this paper, we propose seqgraph, an approach that utilizes the structured relationship between passages in the context of multi-hop questions to reduce disconnected reasoning. we construct a localized entity-passage graph using wikipedia hyperlinks, encode it using a gnn, and fuse the structured representations with the text encoder for predicting a reasoning path. our approach results in strong performance gains in terms of both answer and support em/f1 on hotpot-qa and reduces disconnected reasoning measured using dire score. we also obtain state-of-the-art performance on the more challenging musique benchmark with a 17-point improvement in answer f1 over the current best end-to-end(e2e) model. experimenting with sophisticated methods of encoding the graph structure and fusing the text and graph representations can be explored in future work.","{208267807: 'Asai et al., 2020', 202583433: 'Das et al., 2019', 226262208: 'Ferguson et al., 2020', 189927896: 'Jiang and Bansal, 2019;', 235755349: 'Lee et al., 2021;', 249017531: 'Li et al., 2023', 155100120: 'Qiu et al., 2019', 221749191: 'Trivedi et al., 2020', 236771976: 'Trivedi et al., 2022b', 207870753: 'Tu et al., 2019;', 52822214: 'Yang et al., 2018', 237498988: 'Ye et al., 2021'}",https://www.aclanthology.org/2023.acl-long.642.pdf
620,233296201,Generative Context Pair Selection for Multi-hop Question Answering,conclusion,Insight-tree,"we have presented a generative formulation of context pair selection in multi-hop question answering models. by encouraging the context selection model to explain the entire question, it is less susceptible to bias, performing substantially better on adversarial data than existing methods that use discriminative selection. our proposed model is simple to implement and can be used with any existing (or future) answering model; we will release code to support this integration. since context pair selection scales quadratically with the number of contexts, it is not ideal for scenarios that involve a large number of possible contexts. however, it allows for deeper inter-document interaction as compared to other approaches that use summarized document representations. with more reasoning steps, selecting relevant documents given only the question becomes challenging, increasing the need for inter-document interaction. this paper focuses on biases found in question answering models that make its reasoning capabilities brittle. it uses an existing method of testing model performance on adversarial held-out set as an evaluation metric. this work does not deal with any social impacts of biases in natural language processing systems.","{220045477: 'Dua et al., 2020;', 86611921: 'Kwiatkowski et al., 2019', 174801764: 'Min et al., 2019', 207870753: 'Tu et al., 2020;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2104.08744v1.pdf
621,246863799,MuLD: The Multitask Long Document Benchmark,conclusion,Insight-tree,"to enable the evaluation of long document models, we introduce muld: a benchmark of varied nlp tasks where each document consists of more than 10,000 tokens. the six tasks in our benchmark are created by filtering, extending, or modifying existing nlp tasks and are designed to require a long context for high performance. we evaluate simple chunking-based baselines, and find that the longformer model is able to outperform the t5 model suggesting our benchmark is a good test for the ability of models to make use of longer contexts. we believe that the technique explored in this work of augmenting and extending existing 'short document' datasets, can be applied to many other nlp tasks. as the performance of efficient transformers improves, we anticipate the need to update this benchmark with more challenging tasks. while we are focused on creating a benchmark which tests a model's ability to solve real-world long document tasks, we also expect improvements in the efficiencies of the models themselves which may make datasets with more than 100,000 tokens necessary which may require a fundamentally different approach to creating long document datasets. we leave for future work both the development of improved chunking methods, and more efficient transformers which make such methods unnecessary. we hope that the muld benchmark will encourage this further research into efficient models for long document nlp. to this end we provide the data, baseline models, and other code at www.github.com/ ghomashudson/muld.","{207847640: 'Qiu et al., 2020', 226281978: 'Tay et al., 2021'}",https://arxiv.org/pdf/2202.07362v1.pdf
622,258546701,Query Expansion by Prompting Large Language Models,limitations & future work,Insight-tree,"there are a number of limitations in our work: first, we only study sparse retrieval (bm25) which is where query expansion is important. dense retrieval systems (e.g. dual encoders) are less prone to the vocabulary gap and, as a result, are less likely to benefit from a query expansion. wang et al. [31] has already studied this setting in more detail and we leave the analysis of our prompts for a dense retrieval setting as future work. second, our work focuses on flan [32] instruction-finetuned language models. we chose these models due to their ability to follow instructions and the fact that these models are open-source. our work can naturally be extended to other language models [3,5,9,28] and we leave the study of such models as a topic for future research. third, we study specific prompt templates (see appendix a) and there may be other ways to formulate the different prompts. finally, the computational cost of llms may be prohibitive to deploy llm-based query expansions in practice. it may be possible to distill the output of the large model into a smaller servable model. how to productionize llm-based query expansions is left as an open problem.",{233296016: '[27]'},https://export.arxiv.org/pdf/2305.03653v1.pdf
623,258546701,Query Expansion by Prompting Large Language Models,conclusion,Insight-tree,"in this paper we study llm-based query expansions. in contrast to traditional prf-based query expansion, llms are not restricted to the initial retrieved set of documents and may be able to generate expansion terms not covered by traditional methods. our proposed method is simple: we prompt a large language model and provide it a query, then we use the model's output to expand the original query with new terms that help during document retrieval.",{233296016: '[27]'},https://export.arxiv.org/pdf/2305.03653v1.pdf
624,249191335,Learning Open Domain Multi-hop Search Using Reinforcement Learning,conclusions,Insight-tree,"we proposed a focused reading methodology to automatically learn how to direct search in large corpora while iteratively building a knowledge base. the knowledge base is modeled as a graph, which in turn is used to focus the search toward documents that appear relevant. our methodology complements existing information retrieval and machine tools. we evaluated focused reading on a set of search problems extracted from english wikipedia and demonstrated that reinforcement learning with a state representation based on features about dynamics of the search process and the properties of the corpus is more effective and efficient than heuristic baselines. in this methodology, inference in a knowledge graph acquired during the search process is agnostic of the semantics of the concepts and their relations. their quality depends on the machine reading components used to extract them.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.suki-1.4.pdf
625,236459821,AND does not mean OR: Using Formal Languages to Study Language Models' Representations,conclusion,Insight-tree,"using propositional logic corpora to simulate a controlled language modeling setting, we ask: 1) do properties of the training corpus affect lms' abilities to differentiate the meanings of logical operators? and 2) do any training corpora lead to models that differentiate these meanings to a satisfactory degree? our results imply a positive answer to (1): models trained on corpora generated with different constraints appear to perform differently at the task of separating â§ from â¨. however, these differences are a function of both data and model. for example, the transformer architecture seems better able to learn from weaker signal (corpora generated only with a truthfulness constraint), while lstms require more explicit signal (direct access to truth values). on question (2), our results are largely negative for the syntactically similar operators. even the most semantically transparent training data did not enable models to separate the representations of symbols with similar form but different meaning. only the small transformer trained on the explicit grounding condition can perfectly differentiate â§ from â¨ at the lexical level, despite the task's controlled nature. however, every model did separate Â¬ from both â§ and â¨, illustrating how syntactic differences can support differentiation of meaning.","{52822214: 'Yang et al., 2018'}",https://aclanthology.org/2021.acl-short.21.pdf
626,222208820,Cross-Thought for Sentence Encoder Pre-training,conclusion,Insight-tree,"we propose a novel approach, cross-thought, to pre-train sentence encoder. experiments demonstrate that using cross-thought trained with short sequences can effectively improve sentence embedding. our pre-trained sentence encoder with further finetuning can beat several strong baselines on many nlp tasks.","{208267807: 'Asai et al., 2020', 153312687: 'Ding et al., 2019', 202660724: 'Nie et al., 2019'}",https://arxiv.org/pdf/2010.03652v1.pdf
627,259095666,Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks,conclusion,Insight-tree,"we show that composition of memorized world knowledge can be triggered in lms with up to 11b parameters (t5-xxl) to a desirable extent by leveraging training signal from random walks over structured knowledge using approaches based on prompt-tuning (lester et al., 2021). doing so leads to substantial improvements in the lms' ability to answer 2-hop questions, even beyond standard, full model fine-tuning.",{},https://export.arxiv.org/pdf/2306.04009v1.pdf
628,247158054,'Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities,conclusion,Insight-tree,"we introduce wes, an 11m example semantic entity similarity dataset for training question answering evaluation models. wes is generated by treating wikipedia link texts and target article titles as synonyms then filtering for quality. wes is targeted to question answering evaluation, independent of human annotators, and consistent with human judgment. we hope that future questionanswering datasets will implement semantic evaluation metrics in their leaderboards to encourage the development of more free-form models. in future works, link-mining similarity datasets like wes can be made more challenging by generating negative examples adversarially as described at the end of section 3, more consistent by unioning semantic clusters according to wikipedia's internal redirect pages, and more comprehensive by leveraging link-to-link pairwise synonymy within semantic clusters.","{52822214: ', Yang et al., 2018'}",https://arxiv.org/pdf/2202.13581v1.pdf
629,247518855,AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension,conclusion,Insight-tree,"to meet the challenge of reasoning-based mrc, we presented a neural-symbolic approach where neural and symbolic reasoning mutually and iteratively reinforce each other via our new adalogn model. we also enhanced graph-based neural reasoning with a novel subgraph-to-node message passing mechanism. since these ideas are quite general, we believe they have great potential for a variety of applications beyond mrc, e.g., link prediction.","{67855846: 'Dua et al., 2019', 232380161: 'Huang et al., 2021a', 220483148: 'Liu et al., 2020', 155100120: 'Qiu et al., 2019;', 207870753: 'Tu et al., 2020'}",https://www.aclanthology.org/2022.acl-long.494.pdf
630,14758721,Computational Design of a DNA-and Fc-Binding Fusion Protein,conclusion,Insight-tree,"we have applied multi-objective optimization guided by directed evolution to combine the myod dna-binding motif into the z domain conserving the scaffolds structure. simulations showed that the optimization of the sequences based on hydrophobicity, molecular weight, and secondary structure predictions improved structural stability while maintaining protein functionality. the use of simple fitness functions reduces the optimization complexity, and thus allows to optimize more individuals over more generations resulting in a better sampling of the sequence space.",{},NaN
631,246638887,E M E R G I N G T R E N D S Emerging Trends: SOTA-Chasing,conclusions/recommendations,Insight-tree,"many papers are sota-chasing, and more will do so in the future. sota-chasing comes with many costs. we discussed three costs:",{204823992: 'Fillmore 1968'},NaN
632,258587884,Automatic Evaluation of Attribution by Large Language Models,conclusion,Insight-tree,"in this paper, we investigate the important problem of automatically evaluating attribution given by llms.we begin by defining different types of attribution errors and then explore two approaches for automatic evaluation: prompting llms and fine-tuning smaller lms.we experiment with both simulated test examples and manually curated test examples from a real-life generative search engine.","{246652372: 'Ji et al., 2023', 86611921: 'Kwiatkowski et al., 2019', 207756753: 'Nie et al., 2020, and', 254877499: 'Trivedi et al., 2022;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.06311v2.pdf
633,258841488,Question Answering as Programming for Solving Time-Sensitive Questions,conclusion,Insight-tree,"in this work we propose a novel approach, qaap (question answering as programming), to tackle the challenges posed by time-sensitive factual questions.by leveraging llms' exceptional abilities in natural language understanding and programming, qaap can transform diversely expressed text into well-structured codes, enabling llms to capture both the desired knowledge and the underlying constraints, particularly the temporal aspects.experiments demonstrate that existing llms face significant difficulty in effectively comprehending the temporal constraint stated in the question.while our approach consistently demonstrates superior performance over strong baselines with llms.we hope this work can shed light on the future research direction on enhancing llms' reasoning ability to tackle real-world questions with various constraints and developing more efficient methods to reduce the hallucinations llms frequently encounter.","{237048095: 'Chen et al., 2021', 230799347: 'Geva et al., 2021', 246652372: 'Ji et al., 2023;', 235313508: 'Saxena et al., 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.14221v3.pdf
634,258436694,Explainable Conversational Question Answering over Heterogeneous Sources via Iterative Graph Neural Networks,conclusion,Insight-tree,"there are three main takeaways for a broader audience from this work. first, at a time when large language models (llms) like chat-gpt are used as a one-stop shop for most nlp tasks including convqa, our method explaignn stands out by providing traceable provenance of its answer predictions. next, explainability for graph neural networks is an unsolved concern: we propose an iterative model that sequentially reduces the graph size as a medium for offering causal insights into the prediction process. finally, for several systems in ir and nlp, performance, efficiency, and explainability are seen as trade-offs. through our highly configurable solution, we show that in certain use cases, it is actually possible to find configurations that lie at the sweet spots of all the three factors. a natural future work would be to generalize these insights to other problems, like graph-based neural recommendation.","{224803601: '[4,', 160009340: '[34,', 235399987: '[35]', 128345225: '53,', 52822214: '62]', 233219869: '63]'}",https://export.arxiv.org/pdf/2305.01548v2.pdf
635,258959010,Answering Unanswered Questions through Semantic Reformulations in Spoken QA,conclusion,Insight-tree,"we tackled the problem of improving spoken qa, and analyzed questions from live data to identify key challenges that could be addressed with reformulation. based on this we proposed surf with novel linguistically-motivated reformulation operators to solve the identified challenges. offline experiments show the effectiveness of our novel root transformation and generalization operations, with up to 24% of unanswered questions being answered via reformulations with high answer relevance. live deployment in a leading voice assistant has positively impacted millions of requests.","{248780313: 'Byrd and Srivastava, 2022;'}",https://www.aclanthology.org/2023.acl-industry.70.pdf
636,247187956,Semantic Sentence Composition Reasoning for Multi-Hop Question Answering,conclusion,Insight-tree,"to improve the retrieval results and multi-hop reasoning in question answering, we present a multistage semantic matching module and a factual sentences composition module, respectively. experimental results fully demonstrate our modules outperform the standard ir system and two-step ir method. we first combine the semantic sentence composition module with multi-stage semantic matching module, and it is suitable for any open-domain qa architecture. further exploration will be done on the other question answering datasets with our proposed modules.","{202712552: '[9]', 204915921: '[14]'}",https://arxiv.org/pdf/2203.00160v1.pdf
637,202558815,A Discrete Hard EM Approach for Weakly Supervised Question Answering,conclusion,Insight-tree,"in this paper, we demonstrated that, for many qa tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. we showed that this approach significantly outperforms previous approaches on six qa tasks including reading comprehension, opendomain qa, discrete reasoning task and semantic parsing, achieving absolute gains of 2-10% and setting the new state-of-the-art on five wellstudied datasets.","{173188058: 'Talmor and Berant, 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/D19-1284.pdf
638,201698166,Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations,conclusion,Insight-tree,"in this paper, we propose a meta-learning based model named meta-kgr for multi-hop reasoning over few-shot relations of knowledge graphs.meta-kgr uses training triples with highfrequency relations to find well-initialized parameters and fast adapt to few-shot relations.the meta information learned from high-frequency relations is helpful for few-shot relations.in experiments, our models achieve good performance on few-shot relations and outperform previous work in most cases.some empirical analysis also demonstrates that our models are robust and generalized to different types of knowledge graphs.","{153312687: 'Ding et al., 2019', 155100120: 'Xiao et al., 2019', 52822214: 'Yang et al. 2018'}",https://arxiv.org/pdf/1908.11513v1.pdf
639,264555286,Knowledge Corpus Error in Question Answering,conclusion,Insight-tree,"in this work, we demonstrate that generated contexts may be more helpful than retrieved contexts in open-domain question answering.by revisiting the formulation of question answering, we identify a gap where retriever inevitably ignores potentially helpful contexts outside of the corpus.we call this knowledge corpus error, and design an experiment in order to observe knowledge corpus error empirically.paraphrasing the human-annotated gold contexts with llms led to increased reader performance in 3 out of 4 qa benchmarks, implying the existence of knowledge corpus error.","{230799347: 'Geva et al., 2021', 204915921: 'Khot et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 264555175: 'Oh and Thorne, 2023', 252692968: 'Sun et al., 2023;', 247595263: 'Wang et al., 2023', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.18076v1.pdf
640,15017837,A DNA Network as an Information Processing System,conclusions,Insight-tree,"we have investigated the implementation, in a synthetic dna reaction network, of computing paradigms abstracted from two different cellular biochemical reaction networks, the yeast galactose network [34] and the type-1 incoherent feed-forward loop network motif [27]. the results of chemical kinetics simulations show that the proposed dna network can be programmed to implement transient pulse generation with a steady-state output that can be made robust to changes in network dosage. this network has another interesting property: it can be configured such that the steady-state output is proportional to the initial dose of one of the inputs but insensitive to subsequent additions which generate only transient output pulses.",{},NaN
641,263608847,LARGE LANGUAGE MODELS AS ANALOGICAL REASONERS,conclusion,Insight-tree,"we introduced analogical prompting, a new language model prompting approach that self-generates relevant reasoning exemplars for solving problems. this approach provides detailed, customized exemplars for individual problems without requiring labeled data, effectively addressing the challenges faced by existing 0-shot cot and few-shot cot prompting methods. experimental results show that our approach outperforms 0-shot cot and few-shot cot in various reasoning tasks, including math problem solving, code generation, and other logical/temporal reasoning tasks.",{},https://export.arxiv.org/pdf/2310.01714v2.pdf
642,263608847,LARGE LANGUAGE MODELS AS ANALOGICAL REASONERS,limitations and future research,Insight-tree,"one limitation of our approach is increased inference computation, as our approach generates more tokens than vanilla 0-shot and 0-shot cot prompting. compared to few-shot cot, we use fewer input tokens and more output tokens, as exemplars are counted as input in few-shot cot and as output in our approach.another limitation is that self-generation can fail if the llm lacks sufficient strength or has not learned relevant knowledge to the new problems to solve. conversely, with a stronger llm, it can draw upon relevant prior knowledge to tackle slightly more complex problems. therefore, our approach is better suited for stronger or larger-scale llms.finally, it is known that llm performance can be influenced by specific prompt phrases used to query the model (jiang et al., 2020), and our work is also subject to this prompt sensitivity.recently, with the rise of large language models (llms), prompting them to engage in reasoning has proven effective and gained attention. a common approach is prompting llms to generate intermediate reasoning steps, as demonstrated by the chain-of-thought method kojima et al., 2022;, which assists llms in tackling complex reasoning tasks. several studies have extended this approach with more structured algorithms and search methods (khot et al., 2022;drozdov et al., 2022;zelikman et al., 2022;yao et al., 2023;press et al., 2022;khattab et al., 2022;jung et al., 2022), as well as longer-horizon action and planning hao et al., 2023;park et al., 2023). another line of work incorporates tools and programs into the prompting process to facilitate reasoning cheng et al., 2022;kim et al., 2023;schick et al., 2023).our work complements these efforts to enhance llm reasoning and is the first to draw inspiration from human analogical reasoning to improve llm prompting.",{},https://export.arxiv.org/pdf/2310.01714v2.pdf
643,231627885,COARSE-GRAINED DECOMPOSITION AND FINE-GRAINED INTERACTION FOR MULTI-HOP QUESTION ANSWERING,conclusion and future work,Insight-tree,"in this paper, we propose a mutli-hop question answering model, that contains a coarse-grained decomposition strategy to divide a complex query into multiple single-hop simple queries and a fine- grained interaction strategy to better represent each word in the document and help the model find the sentences needed to answer the question. in the experiments, we show that our models significantly and consistently outperform the baseline model.","{174801080: '[15]', 202565945: '[22]', 155100120: '[30]', 158046817: '[32]', 214071925: '[35]'}",https://arxiv.org/pdf/2101.05988v1.pdf
644,264490789,"Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks",limitations,Insight-tree,"if we put our new more robust models to use, human adversaries may adapt to them as well.checking whether crowd-sourcing fresh attacks is indeed more difficult on the new models is beyond the scope of this work.also, we benefit from having a fair number of human adversarial examples (16.9k in anli, and 10k per round in hate speech).our methods may be less successful in a scenario with very fewer examples (â¼ 10).on the flip side, we have also not evaluated these methods on classifiers with access to even larger real adversarial datasets.finally, our methods work on datasets with the notion of an original example, and a perturbed adversarial example as is the norm for adversarial robustness literature (madry et al., 2017).in the new paradigm of larger more capable nlp models, adversarial datasets may increasingly not involve a perturbation (ganguli et al., 2022b).risks: our techniques can enhance robustness given a set of observed adversarial examples.the new classifier we trained with generated data from di and ice may still be vulnerayble to future human attacks that are able to adapt to the new model (in this paper future attack rounds are known a priori from past model-in-the-loop work).this would require extensive crowd-sourcing efforts to evaluate.we also run the risk of over-fitting to the new human generated adversarial data.this may come at the cost of lower performance on future attacks generated by a different mechanism (say textfooler instead of future anli rounds), and comes at the cost of degrading accuracy on original tasks such as mnli and snli (table 19 in appendix).as a separate concern, any technique that betters generative text modeling brings the risk that humans may struggle to distinguish machine generated text.this can have negative consequences for disinformation and misinformation, which is an active area of research (pu et al., 2023).","{233444226: 'Kiela et al., 2021'}",https://export.arxiv.org/pdf/2310.16955v1.pdf
645,264490789,"Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks",conclusion,Insight-tree,"we  9 this attack makes small modifications to the hypothesis, explaining the high level of distributional similarity to anli rounds 1 and 2. unlike textfooler, however, bertattack makes more sophisticated contextual phrase substitutions.for example in row 2, ""partly recruited from brussels"" is replaced with ""not a volunteers units.""","{233444226: 'Kiela et al., 2021'}",https://export.arxiv.org/pdf/2310.16955v1.pdf
646,253384010,NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering,conclusion,Insight-tree,"hybrid tabular-textual question answering (qa) requires reasoning from heterogeneous information, and numerical reasoning is its key challenge compared to extractive qa.to address the severe exposure bias issue of current autoregressive methods when program generation performance is far from good, we present a non-autoregressive program generation (napg) framework for numerical reasoning, which facilitates program generation in parallel.our framework independently generates complete program tuples containing both the operator and its operands.compared to previous autoregressive decoding methods, napg does not suffer from exposure bias, and can significantly boost program generation speed.","{215785913: 'Chen et al., 2020b;', 67855846: 'Dua et al., 2019;', 235421967: 'Shao et al., 2021;', 52822214: 'Yang et al., 2018;', 237593105: 'Zhang et al., 2021', 248780469: 'Zhao et al., 2022', 234741852: 'Zhu et al., 2021;'}",https://export.arxiv.org/pdf/2211.03462v2.pdf
647,216867332,Robust Question Answering Through Sub-part Alignment,conclusion,Insight-tree,"in this work, we presented a model for doing question answering through sub-part alignment.by having our model structured around an explicit alignment scoring process, we show that our approach can to generalize better to other domains.having alignments also makes it possible to filtering out bad model predictions (by treating the scores as confidence values) and interpreting the model's behavior (by examining the alignments and scores directly).","{202565945: 'Jiang and Bansal, 2019', 198229624: 'Joshi et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 174801080: 'Min et al., 2019;', 211003735: 'Wolfson et al. 2020'}",https://arxiv.org/pdf/2004.14648v2.pdf
648,237940861,Paradigm Shift in Natural Language Processing,conclusion,Insight-tree,"recently, prompt-based tuning, which is to formulate some nlp task into a (m)lm task, has exploded in popularity. they can achieve considerable performance with much less training data. in contrast, other potential unified paradigms, i.e. matching, mrc, and seq2seq, are underexplored in the context of pre-training. one of the main reasons is that these paradigms require large-scale annotated data to conduct pre-training, especially seq2seq is notorious for data hungry.","{52822214: 'Yang et al., 2018b'}",https://arxiv.org/pdf/2109.12575v2.pdf
649,104292014,Quizbowl: The Case for Incremental Question Answering,conclusion,Insight-tree,"this article introduces and argues for quizbowl: an incremental question answering task. solving quizbowl questions requires sophisticated nlp such as resolving complex coreference, multi-hop reasoning, and understanding the relationships between a gigantic menagerie of entities that could be answers. fundamental to answering quizbowl questions is that the questions are incremental; this is both fun and good for research. it is fun because it allows for live, engaging competitions between humans and computers. this format-the product of refining human question answering competitions over decades-is also good for research because it allows for fair, comprehensive comparison of systems and iterative improvement as systems answer questions earlier and earlier.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/1904.04792v1.pdf
650,231698419,WebSRC: A Dataset for Web-Based Structural Reading Comprehension,conclusion,Insight-tree,"in this paper, we introduce websrc, a multi-modal dataset for web-based structural reading comprehension with both html documents and screenshots. the task is to answer questions about the web pages. we evaluate several baselines on our dataset, and the results showed that incorporating layout features with textual contents is crucial to web understanding, but how to utilize such structural information requires further investigation. we hope this work can push the research on web-based structural reading comprehension forward. in the future, we will go beyond web pages to explore more structural reading comprehension tasks.","{52822214: 'Yang et al., 2018;', 219965751: 'Zeng et al., 2020'}",https://www.aclanthology.org/2021.emnlp-main.343.pdf
651,258947821,Fact-driven Logical Reasoning for Machine Reading Comprehension,conclusions,Insight-tree,"in this work, we propose extracting a general form called ""fact unit"" to cover both commonsense and temporary knowledge units for logical reasoning. our proposed focal reasoner not only better uncovers the logical structures within the context but also better captures the logical interactions between context and options. experimental results verify the effectiveness of our method.","{207756753: 'Nie et al., 2020', 208201969: 'Sugawara et al., 2020', 52822214: 'Yang et al., 2018', 233219869: 'Yasunaga et al., 2021;'}",https://export.arxiv.org/pdf/2105.10334v2.pdf
652,231847375,Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, to tackle the multi-hop information retrieval challenge, we introduce an architecture that models a set of paragraphs as sequential data and iteratively identifies them. specifically, we propose gated memory flow to iterative read and memorize reasoning required information without noise information interference. we evaluate our method on both full wiki and distractor settings on the hotpotqa dataset and the method outperforms previous works by a large margin. in the future, we will attempt to design a more complicated model to improve retrieval performance and explore more about the effect of training data with different data distribution for multi-hop information retrieval.","{208267807: 'Asai et al., 2020', 153312687: 'Ding et al., 2019', 204915921: 'Khot et al., 2020;', 202660724: 'Nie et al., 2019', 160009340: 'Nishida et al., 2019', 202773198: 'Qi et al., 2019', 155100120: 'Qiu et al., 2019'}",https://arxiv.org/pdf/2102.03741v1.pdf
653,253397574,Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps,conclusion,Insight-tree,"this paper presented an effective unsupervised domain adaptation method, cai. we showed that the combination of splade with cai and the lexical approach gave a state-of-the-art performance on datasets with a large vocabulary and wordfrequency gap. in addition, cai outperformed gpl and was robust enough to show high accuracy even when bow representations were used for query expression. finally, our analysis showed that splade with cai addressed the problem of the exact matching of low-frequency words in training data. we believe that cai works on smaller mlms by distilling adalm because yao et al. (2021) showed that a distilled adalm achieved higher performance than bert on nlp tasks and formal et al. (2021) showed that the results of splade initialized with distilbert-base 14 was competitive on ms marco with other ir models initialized with bert.","{238857091: 'Xin et al., 2022', 220302524: 'Xiong et al., 2021', 247411106: 'Xu et al., 2022'}",https://www.aclanthology.org/2022.aacl-main.57.pdf
654,218470415,Self-supervised Knowledge Triplet Learning for Zero-shot Question Answering,conclusion,Insight-tree,"in this work, we propose a new framework of knowledge triplet learning over knowledge graphs. we show learning all three possible functions, f r ,f h , and f t helps the model to perform zero-shot multiple-choice question answering. we learn from the atomic knowledge graph and evaluate our framework on the socialiqa dataset. our framework achieves state-of-the-art in the zero-shot question answering task and sets a strong baseline in the few-shot question answering task. ","{86611921: 'Kwiatkowski et al., 2019'}",https://arxiv.org/pdf/2005.00316v1.pdf
655,244800755,Case-Based Abductive Natural Language Inference,limitations,Insight-tree,"the adopted model of explanatory power relies on the availability of human-annotated explanations with specific features (e.g., explanatory facts reused across different training instances). however, these resources might not be available in real-world scenarios and are generally costly to develop. moreover, since the explanatory power model relies on similarity measures and indicator functions, the model's ability to generalise might be sensitive to the incompleteness of the knowledge bases and the availability of representative explanations. we believe these limitations can be potentially alleviated by exploring the role of more abstract sentence representations within the cbr paradigm (bergmann and wilke, 1996).in the current implementation of cb-anli, the refine phase adopts specific assumptions to model the abstraction process required for explanation generation. this process, in fact, is performed by assuming that abstraction at the concept level translates in a correct mapping between hypotheses and central explanatory sentences. however, contextual linguistic elements can still affect the overall meaning of the specific concept being abstracted, inducing the inclusion of spurious links between sentences. while contextual elements are considered during the precedent phases through the use of contextualised embeddings and similar cases, additional work is still required to guarantee the correctness of the abstraction process.","{67855846: 'Dua et al., 2019;', 208089867: 'Jansen and Ustalov, 2019;', 202565945: 'Jiang and Bansal 2019', 215238846: 'Liu et al., 2020;', 235097600: 'Thayaparan et al., 2021b', 231883811: 'Valentino et al., 2021', 237258250: 'Wiegreffe and Marasovic, 2021;', 237433880: 'Xu et al., 2021;', 202785879: 'Yadav et al., 2019b', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.134.pdf
656,244800755,Case-Based Abductive Natural Language Inference,conclusion,Insight-tree,"this paper presented cb-anli, a model that integrates multi-hop and case-based reasoning (cbr) in a unified framework. we demonstrated the efficacy of the framework in complex abstractive and multi-hop nli tasks. we believe this work can open new lines of research on hybrid neurosymbolic models for explanation-based nli, and plan to investigate the efficacy of the framework on architectures that adopt richer symbolic representations in combination with neural models, further exploring the role of abstraction in case-based reasoning for improving robustness, generalisation, and explainability in nli.","{67855846: 'Dua et al., 2019;', 208089867: 'Jansen and Ustalov, 2019;', 202565945: 'Jiang and Bansal 2019', 215238846: 'Liu et al., 2020;', 235097600: 'Thayaparan et al., 2021b', 231883811: 'Valentino et al., 2021', 237258250: 'Wiegreffe and Marasovic, 2021;', 237433880: 'Xu et al., 2021;', 202785879: 'Yadav et al., 2019b', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.134.pdf
657,218487733,Exploring and Predicting Transferability across NLP Tasks,conclusion,Insight-tree,"in this work, we conduct a large-scale empirical study of the transferability between 33 nlp tasks across three broad classes of problems, encompassing classification, question answering, and sequence labeling. we show that the benefits of transfer learning are more pronounced than previously thought, especially when target training data is limited, and we develop methods that learn vector representations of tasks that can be used to reason about the relationships between them. these task embeddings allow us to predict source tasks that will positively transfer to a given target task. our analysis suggests that data size, the similarity between the source and target tasks and domains, and task complexity are crucial for effective transfer, particularly in data-constrained regimes.",{},https://arxiv.org/pdf/2005.00770v1.pdf
658,227231636,Towards building a Robust Industry-scale Question Answering System,conclusion,Insight-tree,"although large pre-trained language models have shown super-human performance on benchmark datasets like squad, we show that there is plenty of room to make improvements on top of bert qa . specifically, we outline prior strategies that do not work on a real benchmark consisting of ""natural questions"" showing the difficulty of the dataset and need for better algorithms. we introduce gaama and outline several strategies that are broadly classified under attention and data augmentation and show how effective it can be to attain competitive performance on nq compared to other industry baselines. we also outline gaama's ootb zero-shot transfer on two unseen datsets and show optimistic performance. our future work will involve adding larger pre-trained language models like t5 and also exploring multi-lingual qa.","{202558815: 'Min et al., 2019', 202572810: 'Zhang and Bansal, 2019'}",https://www.aclweb.org/anthology/2020.coling-industry.9.pdf
659,258615193,Evaluating Open-Domain Question Answering in the Era of Large Language Models,conclusion,Insight-tree,"despite the simplicity and ubiquity of lexical matching as an evaluation metric in open-domain qa, it is unnecessarily rigid because plausible candidate answers are likely not to appear in the list of gold answers. this flaw has been long known, but the efforts to circumvent it have been mostly artisanal. in this paper, we report a systematic study of lexical matching by manually judging answers generated by several prominent open-domain qa models. we found that llms achieve stateof-the-art on nq-open. the accuracy of models is severely underestimated, with most em failure cases stemming from syntactical variations of answers. moreover, a zero-shot prompting method can be a reasonable substitute for human evaluation although it cannot detect unattributability in long-form answers. our insights and analysis in this paper will hopefully underpin the development of solid evaluation techniques in open-domain qa.","{245219136: 'Asai et al., 2022', 208267807: 'Asai et al. 2020;', 67855846: 'Dua et al., 2019', 249097975: 'Izacard et al., 2022', 201058633: 'Lin et al., 2019', 236447339: 'Rogers et al., 2022', 237491981: 'Si et al., 2021', 220302524: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.acl-long.307.pdf
660,236447339,QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension,conclusion,Insight-tree,"the number of qa/rc datasets produced by the nlp community is large and growing rapidly. we have presented the most extensive survey of the field to date, identifying the key dimensions along which the current datasets vary.","{225040142: '12', 207847581: '[44]', 215785913: '[54]', 231698419: '[55]', 235399966: '[56]', 165163607: '[65]', 234093776: '[76]', 67855846: '[83]', 218487111: '[85]', 231709697: '[87,', 196170479: '[92]', 226262208: '96]', 202572622: '[139]', 218974030: '144]', 250390687: '[164,', 201058633: '[173]', 220483148: '[177]', 233219660: '[199]', 245218982: '[217]', 237552879: '[242]', 213474484: '249]', 212644640: '259]', 233189637: '[266]', 208201969: '[269]', 238744031: '272,', 202539540: '[277]', 233219849: '[280]', 233296016: '[282]', 234679223: '[308]', 52822214: '[314]', 52895001: '315]', 234741852: '[328]'}",https://export.arxiv.org/pdf/2107.12708v2.pdf
661,259212487,CompMix: A Benchmark for Heterogeneous Question Answering,conclusion,Insight-tree,"we release compmix, a benchmark for heterogeneous qa that inherently requires the usage of multiple sources. answering questions in compmix requires systems to work consistently well for intents spread across five domains, and deal with a wide variety of challenging human formulations asking about rare entities. thus, our hope is that this resource can help facilitate progress in developing more robust qa models that can appropriately exploit complementary and potentially redundant sources of information. a promising direction for improvement would be to include questions that need answers of a different flavor of heterogeneity: sentences, passages, or longer lists.","{258436694: 'Christmann et al. 2023', 86611921: 'Kwiatkowski et al. 2019;', 235399987: 'Oguz et al. 2022', 128345225: 'Sun et al.  , 2019', 52822214: 'Yang et al. 2018', 234778323: 'Zhang et al. 2021'}",https://export.arxiv.org/pdf/2306.12235v3.pdf
662,254564419,In Defense of Cross-Encoders for Zero-Shot Retrieval,conclusion,Insight-tree,"in this work we study how parameter count influences the zero-shot effectiveness of neural retrievers. we begin by showing that in-domain effectiveness, i.e., when retrievers are fine-tuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al.","{248665596: '12]', 244799249: '[43]'}",https://export.arxiv.org/pdf/2212.06121v1.pdf
663,246240382,Towards Collaborative Question Answering: A Preliminary Study,conclusion,Insight-tree,"the fact that knowledge are not shared gives rise to individual diversity and motivates collaboration. we believe natural-language based collaboration system is a domain that has practical implication and holds scientific values. the collabqa task and dataset we proposed in this paper is a small step towards that direction.  figure 6 shows the structure and examples in our proposed knowledge graphs. g 1 contains a list of person entities. the value of a property of the entity is randomly generated within a reasonable range. for example, the value of a person's height is randomly sampled in the range [160cm, 200cm]. we add a series of constraints to make the kgs more realistic, such as a person who doesn't have job gets no annual income; a person cannot be a mayor and be an employee in some company at the same time; the largest company of a city must be located in that city, and so on.","{221448158: 'Khot et al. 2021a', 211003735: 'Wolfson et al. 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2201.09708v1.pdf
664,245218982,"QuALITY: Question Answering with Long Input Texts, Yes!",conclusion,Insight-tree,"we introduce the long-document qa dataset quality. this dataset was crowdsourced and validated by humans to ensure that the questions are answerable, unambiguous, and challenging. the quality-hard subset, comprising half the dataset, consists of questions that are unanswerable by annotators working under tight time constraints, helping ensure that skimming and simple search do not yield high performance.",,https://www.aclanthology.org/2022.naacl-main.391.pdf
665,248562643,The Unreliability of Explanations in Few-Shot In-Context Learning PREPRINT,conclusion,Insight-tree,"we have explored the capabilities of gpt-3 in using explanation in in-context learning for textual reasoning. through our experiments on two qa datasets and an nli dataset, we find that simply including explanations in the prompt does not always improve the performance of in-context learning.","{139103297: 'Chen and Durrett, 2019', 220045477: 'Dua et al., 2020;', 237513496: 'Garg and Moschitti, 2021;', 230799347: 'Geva et al., 2021', 189927896: 'Jiang and Bansal, 2019', 219721462: 'Kamath et al., 2020;', 233297024: 'Shin et al., 2021;', 225068329: 'Wiegreffe et al., 2021;', 52822214: 'Yang et al., 2018;', 238856959: 'Ye and Durrett, 2022'}",https://arxiv.org/pdf/2205.03401v1.pdf
666,259224614,Resources and Evaluations for Multi-Distribution Dense Information Retrieval,conclusion,Insight-tree,"in this work, we formalize an underexplored information retrieval task where the retriever applied to several different distributions at inference time, some of which are unseen during training. we created benchmarks for this task and evaluated several simple retrieval methods and models on these benchmarks. we show that these simple methods work well obtaining up to 8 points improvement in recall@10 over baselines and an average of 3.8 points improvement.","{247593883: '[1]', 237048095: '[8,', 215785913: '[9]', 238419458: '[16]', 233296016: '[26]'}",https://export.arxiv.org/pdf/2306.12601v1.pdf
667,248780551,CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation,conclusion,Insight-tree,"the mqg task is more challenging and worthy of exploration compared with conventional shallow qg. to address the complexity control problem of mqg, we propose a simple control framework cqg, which consists of a gat-based key entity extractor and a controlled generated. cqg greatly improves the performance and we hope our model will help researchers to study the mqg task.","{155100120: 'Qiu et al., 2019'}",https://www.aclanthology.org/2022.acl-long.475.pdf
668,237415234,CDLM: Cross-Document Language Modeling,conclusion,Insight-tree,"we presented a novel pretraining strategy and technique for cross-document language modeling, providing better encoding for cross-document (cd) downstream tasks. our contributions include the idea of leveraging clusters of related documents for pretraining, via cross-document masking, along with a new long-range attention pattern, together driving the model to learn to encode cd relationships. this was achieved by extending the global attention mechanism of the longformer model to apply already in pretraining, creating encodings that attend to long-range information across and within documents. our experiments assess that our crossdocument language model yields new state-of-theart results over several cd benchmarks, while, in fact, employing substantially smaller models. our analysis showed that cdlm implicitly learns to recover long-distance cd relations via the attention mechanism. we propose future research to extend this framework to train larger models, and to develop cross-document sequence-to-sequence models, which would support cd tasks that involve a generation phase.","{208267807: 'Asai et al., 2020', 174799117: 'Barhom et al. 2019', 226281978: 'Tay et al., 2021', 52822214: 'Yang et al. 2018', 220831004: 'Zaheer et al., 2020'}",https://arxiv.org/pdf/2101.00406v2.pdf
669,261884440,Bridging Dense and Sparse Maximum Inner Product Search,discussion and conclusion,Insight-tree,"we began this research with a simple question: can we apply dense mips algorithms to sparse vectors? that led us to investigate different dimensionality reduction techniques for sparse vectors as a way to contain the curse of dimensionality. we showed, for example, that the jl transform and sinnamon behave differently on sparse vectors and can preserve inner product to different degrees. we also thoroughly evaluated the effect of clustering on sparse mips in the context of an ivf-based retrieval system. coupling dimensionality reduction with clustering realized an effective ivf system for sparse vectors, summarized in algorithms 1 and 2.","{248665596: '[24]', 233296016: '[66]'}",https://export.arxiv.org/pdf/2309.09013v1.pdf
670,260899983,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,conclusion,Insight-tree,"this survey provides a comprehensive review of various literature for the evaluation of llms. we aggregate different works with their intended competencies. some of the competencies(reasoning, knowl-edge) already have holistic evaluation benchmarks, while others(planning, coding) still face disparate challenges. the goal of this paper is to comb the numerous work concerning llms' evaluation through the lens of the core competencies test. lighten the cognitive load for assimilating numerous evaluation works due to the various functions of llms. in doing so, we have also identified the challenge faced by each competency, looking forward to alleviating it in the future.","{215785913: 'Chen et al., 2020b', 211126663: 'Clark et al., 2020', 230799347: 'Geva et al., 2021', 155100120: 'Qiu et al., 2019', 233219849: 'Talmor et al., 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2308.07902v1.pdf
671,237563234,Slot Filling for Biomedical Information Extraction,conclusions and future work,Insight-tree,"in this work we formulated the task of biomedical information extraction as a slot filling problem. this approach aims to forgo the need for entity and relation type specific training data, which is scarce and costly to annotate in the biomedical domain. additionally, this formulation allows to deal with the addition of new relation types, without needing to re-train the relevant models. additionally, we have introduced a new biomedical slot filling benchmark and used it to train a biomedical dpr model, a dual bert-based encoder for retrieval, as well as a biomedical slot filling reader based on biobert. in a series of experiments our approach outperforms significantly a number of general domain baselines as well as the simpler bm25 retriever. furthermore, our results illustrate the importance of in-domain, taskspecific training data, in line with findings from recent works (glass et al., 2021;maillard et al., 2021).","{86611921: 'Kwiatkowski et al., 2019;', 221507798: 'Maillard et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.bionlp-1.7.pdf
672,254854493,Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation,discussion and conclusion,Insight-tree,"in this study, a series of scalable augmentation techniques are proposed to produce surrogate queries for training dense retrievers without using any annotated pairs. we achieve state-of-the-art performance on two collections of widely used benchmarks (beir and six odqa datasets), demonstrating that the inductive bias of the synthetic querydoc pairs is effective for training dense retrievers, greatly bridging the gap between unsupervised dense models and bm25 and inspiring us to rethink the necessity of using real queries.","{248780378: 'Cho et al., 2022;', 253397574: 'Iida and Okazaki, 2022', 86611921: 'Kwiatkowski et al., 2019', 233296016: 'Thakur et al., 2021;', 245131402: 'Wang et al., 2022;', 247411106: 'Xu et al., 2022', 247447562: 'Zhou et al., 2022;'}",https://export.arxiv.org/pdf/2212.08841v1.pdf
673,262217474,FURTHEST REASONING WITH PLAN ASSESSMENT: STABLE REASONING PATH WITH RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS,conclusion,Insight-tree,"based on the experiments and results presented above, it is evident that furthest reasoning with plan assessment exhibits exceptional performance in addressing multi-hop questions.firstly, the furthest reasoning framework significantly contributes not only to the reasoning process but also to the direct answering process.in complex and long-term problem-solving scenarios such as multi-hop questions, furepa is designed to generate more effective plans, avoiding the influence of convoluted previous reasoning processes.moreover, the plan assessment component plays a crucial role in selecting the most suitable query from the llm-generated candidates, leading to more relevant evidence retrieval by the information retriever (ir).this enhancement in query selection contributes to a more effective and accurate multi-hop question-solving process.","{236771976: '[29]', 86611921: '[35]'}",https://export.arxiv.org/pdf/2309.12767v1.pdf
674,264406215,Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning,conclusion,Insight-tree,"in this work, we harness the capabilities of llms combined with self-prompted cots to tackle the intricate mhqa task within the open-domain context, termed as odmr.our innovative sp-cot not only sets a new benchmark by surpassing preceding cot prompting techniques but also outclasses the erstwhile sota llm-only methodologies in open-domain question-answering.a distinguishing feature of sp-cot is its proficiency in eliciting high-caliber intermediate reasoning steps, and its universal efficacy across both large and smallscale llms.we anticipate our innovative selfgeneration pipeline for odmr to not just be foundational for sp-cot, but also to pave the way for future research, catalyzing a shift towards leveraging self-generation in llms, by llms, and for llms.","{226236740: 'Ho et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 236771976: 'Trivedi et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.13552v2.pdf
675,256827065,Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering,conclusion,Insight-tree,"we analyze the effectiveness of the underlying reasoning tasks using two multi-hop datasets: 2wiki and hotpotqa-small. the results reveal that the underlying reasoning tasks can improve qa performance. using four debiased sets, we demonstrate that the underlying reasoning tasks can reduce the reasoning shortcuts of the qa task. the results also reveal that the underlying reasoning tasks do not make the models more robust on adversarial examples, such as sub-questions and inverted questions. we encourage future studies to investigate the effectiveness of the entity-level reasoning task in the form of sub-questions.",{},https://www.aclanthology.org/2023.findings-eacl.87.pdf
676,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,conclusion,Insight-tree,"we take a step forward in constructing the explainable method for multi-hop question answering by proposing two effective improvements. the global differentiable learning strategy learns optimal reasoning paths by exploring latent probability space to alleviate the problem of semantic space mismatch and error propagation. the dynamic adaptive reasoner improves generalization to unseen sub-questions.  figure 4: case study. the green font represents the correct predicted answer, and the red font represents the incorrect. our method successfully learns the intermediate reasoning process and shows better interpretability.","{153312687: 'Ding et al., 2019;', 207853300: 'Fang et al., 2020;', 215768725: 'Groeneveld et al., 2020;', 202565945: 'Jiang and Bansal, 2019;', 198229624: 'Joshi et al., 2020;', 221448158: 'Khot et al., 2021;', 174801080: 'Min et al., 2019;', 160009340: 'Nishida et al., 2019;', 211258645: 'Perez et al., 2020;', 240288953: 'Qi et al., 2021', 155100120: 'Qiu et al., 2019;', 211258744: 'Tang et al., 2021', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020;', 237502990: 'Zhu et al., 2021b;'}",https://www.aclanthology.org/2022.emnlp-main.356.pdf
677,256461282,Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning,limitations,Insight-tree,"â¢ question decomposition is the pre-stage of building interpretable models. to the best of our knowledge, there is only one largescale question decomposition dataset (wolfson et al., 2020), and the performance of existing automatic decomposition models is far below human performance. inaccurate question decomposition leads to errors in reasoning. therefore, exploring better question decomposition techniques is a challenging and rewarding direction.â¢ existing interpretable models (min et al., 2019;jiang and bansal, 2019;ding et al., 2019;khot et al., 2021;wolfson et al., 2020), including our approach, focus on solving complex questions, ignoring a simple question with a complex context that requires a deep understanding of the context to reason out the answer.â¢ the dynamic adaptive reasoner introduces a small number of additional parameters in the router, which can increase the computational cost. a more efficient parameter-free routing approach can be explored in the future.","{153312687: 'Ding et al., 2019;', 207853300: 'Fang et al., 2020;', 215768725: 'Groeneveld et al., 2020;', 202565945: 'Jiang and Bansal, 2019;', 198229624: 'Joshi et al., 2020;', 221448158: 'Khot et al., 2021;', 174801080: 'Min et al., 2019;', 160009340: 'Nishida et al., 2019;', 211258645: 'Perez et al., 2020;', 240288953: 'Qi et al., 2021', 155100120: 'Qiu et al., 2019;', 211258744: 'Tang et al., 2021', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020;', 237502990: 'Zhu et al., 2021b;'}",https://www.aclanthology.org/2022.emnlp-main.356.pdf
678,238856959,Can Explanations Be Useful for Calibrating Black Box Models?,discussion & conclusion,Insight-tree,"limitations despite showing promising results in improving model generalization performance, our attribution-based approach does suffer from intensive computation cost. using either lime or shap to generate attributions requires running inference a fair number of perturbations when the input size is large (see appendix for details), which limits our method's applicability. but this doesn't undermine the main contribution of this paper, answering the question in the title, and our approach is still applicable as-is in the scenarios where we pay for access to the model but not per query.","{139103297: 'Chen and Durrett, 2019', 204823992: 'Fisch et al., 2019', 219721462: 'Kamath et al. 2020', 173188058: 'Talmor and Berant 2019', 52822214: 'Yang et al., 2018', 237498988: 'Ye et al., 2021', 235313893: 'Zhang et al., 2021'}",https://www.aclanthology.org/2022.acl-long.429.pdf
679,242075660,Multi-Hop Question Generation Using Hierarchical Encoding-Decoding and Context Switch Mechanism,conclusions,Insight-tree,"in this paper, we propose a novel question generation model incorporating the hierarchical encoding-decoding structure in order to inject the structural information of input documents, and a context switch mechanism for the purpose of stabilizing the decoding and making the generation process more consistent. the automatic metric results in table 1 show our model achieves the best performance against baseline models on rouge-l in automatic metrics evaluation, although our model does not outperform baseline models on the other baseline models. nonetheless, the results in table 1 prove that our proposed context switch mechanism improves the model's performance on automatic metrics. furthermore, the human evaluation results also show our model outperforms all baseline models on four criteria we used. the experimental results of both automatic evaluation and human evaluation support the effectiveness of our proposed approach on the multi-hop qg task. in addition, we also conduct extensive studies analyzing the model's performance on different question types according to both automatic evaluation metrics and human evaluation scores. future work will include incorporating our method into pre-trained language models.","{52822214: '[5]', 202572810: '[9]', 214802355: '[27]', 216553210: '[28]', 226236844: '[29]'}",NaN
680,258587920,Evaluating Embedding APIs for Information Retrieval,conclusion,Insight-tree,"the incredible capabilities of transformer-based language models at scale have attracted a handful of companies to offer access to their proprietary llms via apis. in this paper, we aim to qualitatively and quantitatively examine semantic embedding apis that can be used for information retrieval. our primary focus is to assess existing apis for domain generalization and multilingual retrieval. our findings suggest that re-ranking bm25 results is a suitable and cost-effective option for english; on the beir benchmark, openai ada2 performs the best on average. in multilingual settings, while re-ranking remains a viable technique, a hybrid approach produces the most favorable results. we hope that our insights aid practitioners and researchers in selecting appropriate apis based on their needs in this rapidly growing market.","{233296016: 'Thakur et al. 2021', 220302524: 'Xiong et al., 2021;'}",https://www.aclanthology.org/2023.acl-industry.50.pdf
681,258960507,GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking,conclusions,Insight-tree,"in this paper, we focus on narrowing the gap between retrieval and generation for retrieval-enhanced text generation methods. we propose griprank, a novel approach to improve the passage ranking capability by distilling knowledge from a generative passage estimator to the passage ranker. we evaluate our approach on diverse knowledge-intensive language tasks, including zero-shot slot filling, open-domain question answering, and knowledge-enhanced dialogue generation. experimental results show that the proposed griprank presents advantages over previous state-of-the-art approaches. further analysis demonstrates the effectiveness of our proposed approach in narrowing the gap between passage retrieval and answer generation.","{248366293: '[5]', 196170479: '[12,', 245144556: '[37]', 239009856: '[39]', 221507798: '[41]', 52822214: '58]'}",https://export.arxiv.org/pdf/2305.18144v2.pdf
682,202577996,KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension,conclusion,Insight-tree,"we introduce korquad1.0-korean question answering dataset, a large-scale standard questionanswering dataset and contribute to researchers in multilingual natural language processing.this data is collected on the same basis as the english standard data, squad, and the properties of the data are similar.accordingly, we present korquad1.0as standard data for korean extractive machine reading comprehension task.the data is freely available through the github site.we also launch korquad1.0as a challenge to encourage exploration of models and provide an evaluation of performance between models.we intend to continually create standard data regarding variety of qa research areas such as the task of assessing whether a system can know what it cannot answer, the task of extracting answers by among various documents, and the task of questioning on formatted documents that have structures such as tables [9] or web documents.we also will keep providing fair evaluation among models on standard datasets and contribute to the active multilingual language engineering research.",{},https://arxiv.org/pdf/1909.07005v2.pdf
683,202539031,Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases,conclusion,Insight-tree,"our key contribution is a method of using human knowledge about what methods will not generalize well to improve model robustness to domain-shift. our approach is to train a robust model in an ensemble with a pre-trained naive model, and then use the robust model alone at test time. extensive experiments show that our method works well on two adversarial datasets, and two changing-prior datasets, including a 12 point gain on vqa-cp. future work includes learning to automatically detect dataset bias, which would allow our method to be applicable with less specific prior knowledge.","{139103297: 'Chen and Durrett, 2019;', 174801764: 'Min et al., 2019', 52822214: 'Yang et al., 2018;', 53116244: 'Zhang et al., 2018b;'}",https://www.aclweb.org/anthology/D19-1418.pdf
684,222140777,DaNetQA: a yes/no Question Answering Dataset for the Russian Language,conclusion,Insight-tree,"in this paper, a new question answering dataset, danetqa, is presented.it comprises binary yes/no questions, paired with paragraphs, which should be used to answer the questions.the overall collection procedure follows the design of the boolq dataset, which is a magnitude larger in size than danetqa, partially due to the use of proprietary sources.we establish a straightforward baseline, exploiting fasttext and rubert models and experiment with multiple transfer learning settings.our results show, that on the one hand, the english dataset can be leveraged to improve the results for the russian one.however, we can not confirm, that we can re-use boolq for training the model while keeping danetqa for evaluation only.this brings us to the following conclusion: although the re-creation of english datasets in other languages may seem like a redundant and secondary activity, the current state of the cross-lingual models does not allow for perfect language transfer.it is not enough to train the model on the english data.it seems impossible to gain high-quality results if the model is not trained in the target language.this highlights the need for future development: the development of more advanced cross-lingual contextualized encoders as well as more sophisticated datasets to evaluate cross-lingual tasks.as for danetqa development, we plan to enlarge the dataset with more question-paragraph pairs and to extend the dataset with an unanswerable question, affecting though the task setting.",{86611921: '[16]'},https://export.arxiv.org/pdf/2010.02605v2.pdf
685,119252748,Fokker-Planck equations for time-delayed systems via Markovian Embedding,conclusions,Insight-tree,"in this work, we have discussed the probabilistic description of delayed stochastic systems. as a starting point, we have reviewed an earlier approach based on novikov's theorem, from which a fokker-planck description can be derived in the form of an infinite hierarchy [30][31][32]. the first member is the well-known fpe for the one-time pdf, which contains the two-time pdf and is thus not closed. still, this equation has in the past been shown to be an important tool in the search for exact results [36,37] and a valuable starting point for approximations [31,38]. the main purpose of our work was to shed light onto the higher members of this hierarchy, which have rarely been discussed in earlier literature.",{},https://arxiv.org/pdf/1903.02322v1.pdf
686,260887004,Performance Prediction for Multi-hop Questions,conclusions,Insight-tree,"in this paper, we introduce the task of query performance prediction for multi-hop questions. we present an approach to estimate a difficulty score of a multi-hop question based on the clues in the question. we propose retrieval paths based on overlapping terms between the question and its supporting documents. our experimental evaluation shows significant correlations between the performance of the retrievers used in our evaluation and our estimated difficulty scores, and those correlations are much higher than those obtained by our qpp baselines from the literature. the same trend is observed for the end-to-end models with the performance 7 https://github.com/mhmdsmdi/performance-prediction-for-multihop-qa figure 3: performance, in terms of f1-score, of mdr [37] with the adaptive retriever compared to a constant retriever while k varied, showing that the adaptive retriever achieves a higher performance under the same budget considerably dropped for the questions that are deemed difficult by our model. determining the difficulty of a multi-hop question using a pre-retrieval method can assist the retrievers to have a better chance of retrieving all required documents to answer the question. as a possible future direction, our models may be improved by considering more refined retrieval path types and maybe parameter settings. also analyzing the performance of down-stream tasks using our difficulty score estimation is another possible direction.","{208267807: '[3,', 202583433: '[9,', 153312687: '[11]', 226236740: '[17,', 229923812: '20,', 202660724: '24]', 202773198: '[26]', 236771976: '32,', 52822214: '[42]', 234778323: '44]'}",https://export.arxiv.org/pdf/2308.06431v1.pdf
687,256846551,STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK,conclusion,Insight-tree,"we aim to enable machines to perform multi-step reasoning while explaining their answers. we believe that teaching machines how to manipulate premises and reach conclusions can be an important step towards true language understanding. with that in mind, we introduce street, a new multi-task reasoning and explanation resource covering various forms of reasoning in the context of questionanswering. we hope this benchmark will allow for a more systematic evaluation of the reasoning capabilities of natural language systems. future avenues of research include exploring the reasoning capabilities and knowledge retrieval and using supervised models trained on multi-step reasoning data to bootstrap unsupervised learning for multi-step reasoning.","{67855846: 'Dua et al., 2019;', 230799347: 'Geva et al., 2021;', 222178328: 'Jhamtani & Clark 2020', 221970302: 'Xiong et al., 2021;'}",https://export.arxiv.org/pdf/2302.06729v1.pdf
688,222141025,PROVER: Proof Generation for Interpretable Reasoning over Rules,conclusion,Insight-tree,"we introduce prover, an interpretable joint model that answers binary questions over natural language rule-bases and generates corresponding proofs. the proofs are generated through the node and edge modules of the model in the presence of multiple global constraints during training and ilp inference. our model improves state-of-theart qa accuracy in the zero-shot scenario by 6% and generates proofs accurately. prover also generalizes much better to higher depth questions with up to 15% absolute improvement in qa performance over ruletakers. prover's modeling is relatively generic, and similar proof generation methods can be explored in traditional multi-hop qa tasks. prover can also be a helpful aid to formal reasoners in scenarios where rules are fuzzy and creating rule-bases in a formal language is tedious or infeasible.","{202539540: 'Tafjord et al., 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2010.02830v1.pdf
689,252846009,PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers,conclusion,Insight-tree,"in the half decade since (poliak et al., 2018) single sentence relation leakage bias has proven to remain a difficult issue. efforts to debias nli have led to datasets that merely exhibit different kinds of bias than those shown before, or less saturated benchmarks that continue to exhibit cheating features. future work must prioritize reducing observable bias directly using a model-driven approach.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.eacl-main.223.pdf
690,195065231,Molecular Sciences Translation Control of HAC1 by Regulation of Splicing in Saccharomyces cerevisiae,conclusions,Insight-tree,"(ire1p+hac1p)-mediated upr signaling represents a beautiful translation control mechanism created by nature. however, there are still many questions unanswered. what recognizes 3'be of hac1 pre -mrna and carries hac1 pre-mrna to ire1p foci to be spliced? how is translation regulated in hac1 homologues that do not have introns in fungal species? is ire1p oligomerization a consequence of ire1p phosphorylation or the opposite? how does ire1p dephosphorylation impact ire1p oligomerization and hac1 splicing activity? how do cells exit upr after its induction? how does this signaling system evolve and diverge in different lineages? i hope that this review will serve as an anchor for addressing all these questions in coming years.",{},NaN
691,232163651,Multinomial Logit Model Building via TreeNet and Association Rules Analysis: An Application via a Thyroid Dataset,discussion and conclusions,Insight-tree,"our model-building framework advances the multinomial logit model by generating variables and interactions as candidate predictors. we demonstrate that the integration of three techniques-treenet, asa, and the multinomial logit model-constitutes a powerful and practical innovation for data analysis and also paves the way for additional innovative strategies. we have shown via our application example that these newly generated variables and interactions make a significant contribution to improving the multinomial logit model. our selected model from the thyroid dataset outperforms all of the 21 methods previously applied to the test data.",{},https://web.archive.org/web/20210212085253/https:/res.mdpi.com/d_attachment/symmetry/symmetry-13-00287/article_deploy/symmetry-13-00287.pdf
692,252819056,Evaluating Diversity of Multiword Expressions in Annotated Text,conclusions and future works,Insight-tree,"in this paper, we borrowed the formalization of the notion of diversity from the literature in ecology. we focused on two out of the three main aspects of diversity, namely variety and balance. our contribution is to apply these measures to assess intralinguistic diversity, focusing on the particular phenomenon of multiword expressions. we not only formalize variety and balance measures in this context but we also put forward methods for selecting those variants of these measure which fit the nature of the mwe phenomenon. this validation methodology is based on corpus sampling with variable sample size. as a result, we retain richness and the e 2,1 evenness as the optimal variety and balance measures for mwes (among those studied by us). we apply these measures to the corpora and system results in the parseme shared task on automatic identification of mwes. the results show that richness of the correct annotations produced by the systems is roughly consistent with their f-measure performances. however, their balance is much less correlated with more traditional measures. we also display the limits of the richness and balance measures, when calculated on automatically annotated data, due to incorrect approximation of types under improper lemmatization in a morphologically rich language.","{52822214: 'Yang et al., 2018'}",NaN
693,260334522,HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution,conclusion,Insight-tree,"generative search with the ability to cite supporting sources has gained a lot of traction lately. however, the absence of accessible high-quality data inhibits progress in building open-source information-seeking models. in this paper, we seek to bridge this gap in the community by introducing hagrid, a new dataset for building endto-end generative retrieval models. our dataset is collected via a human-machine collaboration that starts with generating explanatory answers to  information-seeking queries from gpt-3.5, followed by a human assessment of correctness and attributability of the generated answers. hagrid facilitates the development of open-source models for information-seeking scenarios. our human study has shed light on the room for improvement, i.e. around 40% of gpt-3.5 generated answers are not informative and over 20% fail to demonstrate attribution to the quotes. moving forward, future research endeavors may focus on building more accurate models, aimed at mitigating the errors commonly encountered in current llms.","{67855846: 'Dua et al., 2019', 196170479: 'Fan et al., 2019', 230799347: 'Geva et al., 2021;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2307.16883v1.pdf
694,261908732,Conversations with Search Engines: SERP-based Conversational Response Generation,conclusion and future work,Insight-tree,"in this paper, we propose conversations with search engines as task for the community to consider and we contribute two types of result: first, we release a new test set, saac, which is more suitable and challenging for this research than existing resources.second, we propose an end-to-end neural model, case, to advance the state-of-the-art.we implement state-of-the-art methods from related tasks and conduct extensive experiments to show that: (1) the proposeed case model can achieve state-of-the-art performance; (2) the proposed sti and ppg modules can bring large improvements; and (3) saac is a more challenging dataset than previously introduced ones, leaving significant room for further improvements.","{57721315: '[34]', 52822214: '63]'}",https://arxiv.org/pdf/2004.14162v2.pdf
695,233296016,BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models,conclusions and future work,Insight-tree,"in this work, we presented beir: a heterogeneous benchmark for information retrieval. we provided a broader selection of target tasks ranging from narrow expert domains to open domain datasets. we included nine different retrieval tasks spanning 18 diverse datasets.","{86611921: '[34]', 52822214: '[76]', 221971009: '[79]'}",https://arxiv.org/pdf/2104.08663v4.pdf
696,258865847,Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering,conclusions,Insight-tree,"in this work, we present a novel pre-training scheme for multi-document tasks. first, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. second, we generate high-quality large-scale qa pre-training data using a controlled generation approach, in which each qa pair originates from a salient sentence in one of the documents in the set.","{226281978: 'Tay et al., 2021', 226262229: 'Xu and Lapata, 2020;', 52822214: 'Yang et al. 2018', 247793456: 'Yasunaga et al., 2022', 220831004: 'Zaheer et al., 2020'}",https://www.aclanthology.org/2023.acl-long.110.pdf
697,259501422,A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation,conclusion,Insight-tree,"in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models. through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average. we also demonstrated the individual efficacy of our detection and mitigation techniques. specifically, our detection technique achieves a high recall and the mitigation technique successfully mitigates a large fraction of the correctly detected hallucinations. notably, the mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. we further demonstrated the effectiveness and wide applicability of our approach and presented several interesting studies including evaluation with another llm (vicuna) and answering multi-hop and false premise questions. overall, our work addresses the llms' hallucination problem and thus contributes to improving their reliability and trustworthiness, a crucial step en route to enabling their widespread adoption in real-world applications.  table 6 shows the instructional prompts used for different steps of our approach. we note that these techniques are the preferred techniques as they do not require calling an external task-specific tool to achieve the corresponding objectives.","{219721462: 'Kamath et al., 2020;', 258461053: 'Varshney and Baral, 2023;', 247187611: 'Varshney et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2307.03987v2.pdf
698,254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,conclusion,Insight-tree,"in this study, we focused on the multi-hop mrc approaches. in this regard, after presenting the multi-hop mrc problem definition, the multi-hop mrc techniques had been explained based on 31 studies from 2018 to 2022. in addition to categorize the approaches based on the main technique, they also were reviewed in detail including the architecture, superiority, and motivations. in the following, a fine-grain comprehension of the approaches and techniques was prepared, and finally, some open issues in this field were discussed.","{52822214: '[11]', 231627885: '[12]', 202565945: '[18]', 219965751: '[24]', 160009340: '[30]', 214071925: '[31]', 186206745: '[33]', 203836061: '[38]', 155100120: '[54]', 153312687: '[44]', 158046817: '[46]', 207870753: '[47]', 207853300: '[48]'}",https://export.arxiv.org/pdf/2212.04072v1.pdf
699,247748606,UKP-SQUARE: An Online Platform for Question Answering Research,conclusion and future work,Insight-tree,"we introduce the ukp-square platform that enables researchers and developers to study and compare qa pipelines, i.e., skills, that comprises a selection of datastores, retrieval mechanisms and reader models. the platform enables querying ex-isting public skills, as well as implementing custom ones using ukp-square's microservices and utility functions that support a large collection of model types and datastores. furthermore, users can simultaneously query multiple skills, and analyze them through integrated behavioural tests. our architecture is scalable and flexible to incorporate most of the latest developments in the qa domain. future versions will include automated deployment of custom models and datastores, automated skill selection by incorporating previous works (puerto et al., 2021;geigle et al., 2021) and increasing the number of supported datastores (e.g., wikidata, vrandeÄiÄ and krÃ¶tzsch, 2014). we also plan to incorporate specialized models (e.g., using graph encoders, ribeiro et al., 2021), structured reasoning approaches (yasunaga et al., 2021) and interpretability techniques such as saliency maps .","{67855846: 'Dua et al., 2019', 213474484: 'Rogers et al., 2020', 202539540: 'Tafjord et al., 2019', 220302524: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018', 233219869: 'Yasunaga et al., 2021'}",https://www.aclanthology.org/2022.acl-demo.2.pdf
700,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,adapting existing benchmarks to privacy-preserving qa and limitations,Insight-tree,"we first adapt the widely used benchmark, hotpotqa [yang et al., 2018], to study our problem. hotpotqa contains multi-hop questions, which are each answerable by multiple wikipedia passages. we create hotpotqa-pair by splitting the wikipedia corpus into d g and d p by randomly assigning wikipedia articles to one or the other. this results in questions entirely reliant on p â d p , entirely reliant on d â d g , or reliant on a mix of one private and one public document, allowing us to evaluate performance under the pair constraints.ultimately however, d p and d g come from a single wikipedia distribution in hopotqa-pair. while it is possible that public and private data come from the same distribution (e.g., organizations routinely develop internal wikis in the style of public wikipedia), private and public data will intuitively often reflect different linguistic styles, structures, and topics, that further evolve over time [hawking, 2004]. we observe all existing textual multi-hop benchmarks focus on retrieving from a single distribution (table 1). additionally, we cannot combine existing benchmarks over two different corpora because this will not yield questions requiring one passage from each domain. methodologically, in the pair setting we likely will not have access to training data from all downstream (private) domains. to evaluate with a realistically private set of information and pair set up, we create a new benchmark concurrentqa. 6 following from the simple security property and *-property in the blp model. 7 single-hop can also avoid performance degradations arising from using two enclaves. recall that a non-private system retrieves the top k overall passages, so if for example kp = k 2 and kg = k 2 , such that kp + kg = k, the system may not retrieve the optimal k passages that the non-private system would have retrieved (e.g., consider when the overall top k passages for a question are in dg). however letting kp â [0..k], kg â [0..k] circumvents this challenge, at the cost of retrieving a few more passages per hop.","{189927857: ', Feldman and El-Yaniv, 2019', 52822214: 'Yang et al. [2018]', 174801764: 'Min et al. [2019a]', 139103297: 'Chen and Durrett [2019]', 67855846: '[Dua et al., 2019]', 226236740: '[Ho et al., 2020]', 174801080: '[Min et al., 2019b', 211258645: ', Perez et al., 2020', 218516694: ', Guoa et al., 2021', 221507798: '[Petroni et al., 2021]'}",https://arxiv.org/pdf/2203.11027v1.pdf
701,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,benchmark limitations,Insight-tree,"concurrentqa, like hotpotqa, faces the limitation that crowdworkers see the gold supporting passages when creating questions, which can result in textual overlap between substrings in the questions and passages [trivedi et al., 2020]. we mitigate these effects through our validation task, and by limiting the allowable degree of overlap between passage pairs and questions through the frontend interface during the generation stage. further, our questions are not organic user searches as in kwiatkowski et al. [2019], however search logs do not contain questions over public and private data, and existing dialogue systems have not considered retrieval from a private corpus to our knowledge.additionally, enron was a major public corporation and many entities discussed in enron emails are public entities, so it is possible that public websites and news articles encountered during retriever and reader model pretraining, impact the distinction between public and private questions. we investigate the impact of dataset leakage further in section 6.","{189927857: ', Feldman and El-Yaniv, 2019', 52822214: 'Yang et al. [2018]', 174801764: 'Min et al. [2019a]', 139103297: 'Chen and Durrett [2019]', 67855846: '[Dua et al., 2019]', 226236740: '[Ho et al., 2020]', 174801080: '[Min et al., 2019b', 211258645: ', Perez et al., 2020', 218516694: ', Guoa et al., 2021', 221507798: '[Petroni et al., 2021]'}",https://arxiv.org/pdf/2203.11027v1.pdf
702,247593883,REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS,conclusion,Insight-tree,"this work asks how to personalize retrieval-based systems in a privacy-preserving way and identifies that arbitrary autoregressive retrieval over public and private data poses a privacy concern. in summary, we define the pair privacy framework, present a new multi-domain multi-hop benchmark called concurrentqa for the novel retrieval setting, and demonstrate the privacy-performance tradeoffs faced by existing open-domain systems. we finally investigate two challenges towards realizing the potential of public-private retrieval systems: using selective prediction to manage the privacy-performance tradeoff and concurrently retrieving over multiple distributions. we hope this work inspires new privacy-preserving solutions for personalized retrieval-based systems.","{189927857: ', Feldman and El-Yaniv, 2019', 52822214: 'Yang et al. [2018]', 174801764: 'Min et al. [2019a]', 139103297: 'Chen and Durrett [2019]', 67855846: '[Dua et al., 2019]', 226236740: '[Ho et al., 2020]', 174801080: '[Min et al., 2019b', 211258645: ', Perez et al., 2020', 218516694: ', Guoa et al., 2021', 221507798: '[Petroni et al., 2021]'}",https://arxiv.org/pdf/2203.11027v1.pdf
703,247762238,Lite Unified Modeling for Discriminative Reading Comprehension,conclusion,Insight-tree,"in this work, we propose pos-enhanced iterative co-attention network (poi-net), as a lightweight unified modeling for multiple subcategories of discriminative mrc. poi-net utilizes pos embedding to encode pos attributes for the preciseness of answer boundary, and iterative co-attention mechanism with integration strategy is employed to highlight and integrate critical information at decoding aspect, with almost no additional parameter. as the first effective and unified modeling with pertinence for different types of discriminative mrc, evaluation results on four extractive and multi-choice mrc benchmarks consistently indicate the general effectiveness and applicability of our model.","{52822214: 'Yang et al., 2018', 218595722: 'Zheng et al., 2020;'}",https://www.aclanthology.org/2022.acl-long.594.pdf
704,248022214,QAGAN: Adversarial Approach To Learning Domain Invariant Language Features,conclusion,Insight-tree,we presented a method for training a question-answering language model in an adversarial fashion and showed through various experiments that it helps the model generalize well to out-of-domain dataset.,"{204800552: '[11]', 86611921: '[15]', 204823992: '[27]'}",https://arxiv.org/pdf/2206.12388v1.pdf
705,258841328,QLORA: Efficient Finetuning of Quantized LLMs,limitations and discussion,Insight-tree,"we have shown evidence that our method, qlora, can replicate 16-bit full finetuning performance with a 4-bit base model and low-rank adapters (lora). despite this evidence, we did not establish that qlora can match full 16-bit finetuning performance at 33b and 65b scales. due to the immense resource costs, we leave this study to future work.another limitation is the evaluation of instruction finetuning models. while we provide evaluations on mmlu, the vicuna benchmark, and the oa benchmark, we did not evaluate on other benchmarks such as bigbench, raft, and helm, and it is not ensured that our evaluations generalize to these benchmarks. on the other hand, we perform a very broad study on mmlu and develop new methods for evaluating chatbots.from the evidence presented, it appears that the performance of these benchmarks likely depends how similar the finetuning data is to the benchmark dataset. for example, flan v2 is similar to mmlu, but dissimilar to chatbot benchmarks and vice versa for the chip2 dataset and both models score accordingly on the mmlu and vicuna benchmarks. this highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place. do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? maybe something else? because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction. we should ensure as a community that the benchmarks measure what we care about.while we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible ai evaluation of guanaco. we evaluate the likelihood of guanaco-65b to generate a socially biased sequence of tokens compared to other models in table 8. we see that the average score in guanaco-65b is much lower than other raw pretrained models. as such, it seems that finetuning on the oasst1 dataset reduces the bias of the llama base model. while these results are encouraging, it is unclear if guanaco does also well when assessed on other types of biases. we leave further evaluation of analyzing biases in guanaco and similar chatbots to future work.an additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base models, or different adapter methods. besides lora, there is also a wide variety parameter efficient finetuning (peft) methods that have been shown to work well. however, it is unclear if these methods scale to large models. we used lora as many results established its robustness but other adapters might yield better performance. since finetuning after quantization seems to recover most of the information that is lost during quantization this might enable much more aggressive quantization. for example, 3-bit gptq quantization of the basemodel with lora might also yield 16-bit full finetuning performance after finetuning.",{52822214: '[68]'},https://export.arxiv.org/pdf/2305.14314v1.pdf
706,259145189,Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard,conclusions and future work,Insight-tree,the beir benchmark provides an important instrument for evaluating the cross-domain robustness of retrieval models and has gained traction due to the growing recognition of retrieval as a form of representation learning. the efforts described in this paper address the two shortcomings that we have identified with beir: challenges in reproducibility and in the sharing of results. reproducible reference implementations in the pyserini ir toolkit tackle the first challenge. an official self-service leaderboard and best practices for sharing results target the second challenge.,"{252212320: '6,', 248665596: '10,', 233296016: '[34]', 220302524: '37]', 251320151: '[42]'}",https://export.arxiv.org/pdf/2306.07471v1.pdf
707,243860761,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",conclusion,Insight-tree,"in this work, we presented k2r: a modular approach for knowledge-based dialogue models. we showed that by decomposing the knowledge step and response generation into explicit sequence-tosequence subtasks, we could improve dialogue systems by incorporating knowledge or turning short qa model answers into an appropriate conversational form. in detailed experiments, we showed that this modular system helps with hallucination in knowledge-grounded dialogue, is rated by humans as more knowledgeable and engaging when answering questions, and improves generation metrics on open-domain dialogue. furthermore, it allows for more interpretable results and supports knowledge injection. future work should continue to investigate methods with modular reasoning steps to help in difficult language tasks. a genius is a person who displays exceptional intellectual ability, creative productivity, universality in genres or originality, typically to a degree that is associated with the achievement of new advances in a domain of knowledge. gold response universality in genres or originality typically to a degree that is associated with achievements bart a genius has a high iq.",{},https://arxiv.org/pdf/2111.05204v1.pdf
708,252918165,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,conclusion and future work,Insight-tree,"in this work, we investigate performing it with unlabeled data for zero-shot cross-task generalization. we first empirically find that the it performance is largely restricted by the number of distinct tasks, instructions, and training samples in data-scarce tasks. then, we propose udit to take better advantage of the instructions by constructing pseudo-labeled data from the unlabeled plain texts. through udit, it is possible to perform it with unlabeled data when there are few or no humanannotated samples, which offers a better way to incorporate unlabeled data compared with other approaches. through comprehensive analysis, we find that the domain diversity and the matching between the pseudo-labeled data and corresponding instructions are essential for udit. in contrast, noises in individual task clusters and colossal data amount are less influential. there are three directions for future work: (1) designing automatic and generalizable methods to construct pseudo-labeled data for instruction tuning.","{233296709: 'Ye et al., 2021'}",https://www.aclanthology.org/2022.emnlp-main.105.pdf
709,259991357,Thrust: Adaptively Propels Large Language Models with External Knowledge,conclusion,Insight-tree,"in this work, we propose instance-level adaptive propulsion of external knowledge (iapek) as a solution to propel model performance with external knowledge. accordingly, we propose a simple and effective instance-level metric, thrust, to perform the adaptive knowledge injection. extensive experiments show that thrust is a good indicator of models' knowledgeability and can improve the performance of utilizing external knowledge under various settings. understanding the delicate usage of potentially noisy knowledge for ptlms can further enable the models to conduct inference beyond the limitation of internal knowledge.","{165163607: '[6]', 230799347: '[11]', 86611921: '[25]', 239009856: '[35]', 232135266: '[36]', 244799249: '[41]', 52822214: '[52]', 246904646: '[53]'}",https://export.arxiv.org/pdf/2307.10442v1.pdf
710,261048776,ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval,conclusion,Insight-tree,"in this paper, we present controlretriever, a generic and efficient approach capable of controlling retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions. with a parameter-isolated architecture, con-trolretriever effectively preserves the original capability of the retrieval model, meanwhile efficiently empowering it with a new facet of controllable retrieval conditioned on task-specific instructions. furthermore, we also propose a novel llm-guided instruction synthesizing and iterative training (list) strategy, which iteratively trains controlretriever based on extensive automatically-generated retrieval data with diverse instructions, capitalizing the advancement of large language models. extensive experiments demonstrate the superior zero-shot transferability of controlretriever on various retrieval tasks.","{252519173: 'Dai et al. 2023', 249431623: 'Rosa et al. 2022', 244799249: 'Santhanam et al. 2022', 233296016: 'Thakur et al. 2021', 245131402: 'Wang et al. 2022', 220302524: 'Xiong et al. 2021', 253157773: 'Yu et al. 2022'}",https://export.arxiv.org/pdf/2308.10025v1.pdf
711,256631017,Answer Quality Aware Aggregation for Extractive QA Crowdsourcing,conclusion,Insight-tree,"in this paper, we propose a novel answer annotation aggregation method for eqa crowdsourcing.we show that without any fine-tuning, our methods can achieve comparable performance with the trained qa and nli model using  4: an example from newsqa dataset.there are 7 different answer annotations for the question.some of the answers are overlapped.for each answer we report its ranking scores with ac af sms acaf-sms .","{86611921: 'Kwiatkowski et al., 2019'}",https://aclanthology.org/2022.findings-emnlp.457.pdf
712,256631017,Answer Quality Aware Aggregation for Extractive QA Crowdsourcing,limitations,Insight-tree,"while many automatic answer aggregation methods take crowd worker's reliability into consideration (tian and zhu, 2015;li and fukumoto, 2019), to keep the proposed framework simple and concise, we focus on the influence of answer quality and ignore the worker reliability.moreover, we only use newsqa to evaluate the proposed method.although it is possible to consider more real or simulated datasets, as shown by the experiments on squad and natural questions in appendix a.3, newsqa is the only large extractive qa dataset that provides all actual annotations to the best of our knowledge.besides, this paper assumes there is only one correct answer for each question, while it is possible that there are multiple correct answers in some applications.we will explore crowd worker reliability aware answer aggregation methods and extend our work to multi-answer settings in future research.table 7: performance of answer agreement on primary-nc and test-nc using the bert-base-uncased model in terms of exact match (em) and f1.","{86611921: 'Kwiatkowski et al., 2019'}",https://aclanthology.org/2022.findings-emnlp.457.pdf
713,250390947,Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering,conclusion and future work,Insight-tree,"in this paper, we apply explicit graph reasoning to extracted knowledge and contextual information for multi-hop reasoning. we extract clues at multiple levels of granularity relating entity nodes, and construct a semantic graph from these clues. we then combine a masked attention mechanism and two-stage graph reasoning to perform interpretable inference over the semantic graph. experimental results on hotpotqa dataset show the effectiveness of our model. in future work, we hope to extend the range and precision of the entity relations used, and we hope to extend our model to accommodate more complex multi-hop questions with unknown number of hops and non-linear reasoning.","{153312687: 'Ding et al., 2019', 207853300: 'Fang et al., 2020', 174801080: 'Min et al., 2019', 237620311: 'Nishida et al., 2021', 211258645: 'Perez et al., 2020', 155100120: 'Qiu et al., 2019', 207870753: 'Tu et al., 2020', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018b'}",https://www.aclanthology.org/2022.dlg4nlp-1.8.pdf
714,259833830,Enhancing text comprehension for Question Answering with Contrastive Learning,conclusion,Insight-tree,"in this work, we proposed a novel contrastiveqa to alleviate the problem of confusion in answer extraction using contrastive learning. the proposed model contrastiveqa consists of three tasks: 1) candidate answer extraction, 2) positive and negative sampling, 3) contrastiveqa using contrastive learning. we demonstrated the effectiveness of the proposed model by outperforming the performance of the baseline models on four benchmark datasets. in the future, we would like to adopt a method to specify the number of samples required in contrastive learning dynamically. this approach created an adaptive mechanism to use the number of samples for each input. this method could improve the overall performance of the model.","{248563058: 'Caciularu et al., , 2022', 234093776: 'Dasigi et al., 2021', 207853300: 'Fang et al., 2020', 251224058: 'Ivgi et al., 2023', 198229624: 'Joshi et al., 2020', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020'}",https://www.aclanthology.org/2023.repl4nlp-1.7.pdf
715,259833830,Enhancing text comprehension for Question Answering with Contrastive Learning,limitations,Insight-tree,"we obtain notable qa performance through experiments. however, we conduct many experiments to find the optimal candidate for contrastiveqa. many of these experiments inevitably consume a lot of time and energy, and we have to heuristically determine the number of candidate sets through experiments in a limited environment. we intend to alleviate the current problems by adding a module that can solve these problems in our future research.  question what is the year of the event that occured first, making today a perfect day was produced, or frozen was produced? table a1: example of answer extraction of baseline and contrastiveqa.in table a.1, we classify into positive and negative samples using the information of the given correct answer and train them with contrastive loss to derive a result that is closer to the original correct answer when compared with the baseline.","{248563058: 'Caciularu et al., , 2022', 234093776: 'Dasigi et al., 2021', 207853300: 'Fang et al., 2020', 251224058: 'Ivgi et al., 2023', 198229624: 'Joshi et al., 2020', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020'}",https://www.aclanthology.org/2023.repl4nlp-1.7.pdf
716,252693442,"Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",conclusions,Insight-tree,"in this paper, we introduce mintaka, an end-toend question answering dataset linked to wikidata. mintaka addresses an important gap in qa datasets by being large-scale, complex, naturally-elicited, and multilingual. our baselines show that there is room for improvement in existing methods to handle complex questions, especially in all languages. with the release of mintaka, we hope to encourage researchers to continue pushing the boundaries of question answering to handle more complex questions in more languages.",{},https://www.aclanthology.org/2022.coling-1.138.pdf
717,236486104,Team JARS: DialDoc Subtask 1 -Improved Knowledge Identification with Supervised Out-of-Domain Pretraining,conclusion,Insight-tree,"our submission to the dialdoc subtask 1 performs continual pretraining of a transformer-based encoder on out-of-domain qa datasets. experiments with different qa datasets suggest that conversational qa datasets like coqa and quac are highly beneficial as their setup is substantially similar to doc2dial, the downstream dataset of interest. our final submission ensembles two albert-xl models independently pretrained on coqa and quac and achieves an f1-score of 70.9% and em-score of 53.5% on the competition test-set.","{204823992: 'Fisch et al., 2019'}",https://www.aclanthology.org/2021.dialdoc-1.13.pdf
718,237439283,"On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings",conclusion,Insight-tree,"relevance performance is still undercounted: while the expert-generated relevance ratings produced in this work provide more accurate estimates of performance compared to single gold explanations when used in fully-automatic evaluations, these automatic estimates still undercount overall model performance. in our experiments we show the expert ratings primarily provide a vehicle for training better models, but that automatically evaluating relevance performance still remains a challenge, even with a large targeted increase in relevance annotation. further, annotators reported that determining relevance of single facts in isolation is challenging because it lacks the broader compositional context of the rest of the candidate explanation, suggesting ultimate limits to the utility of exhaustive annotation.","{218486753: 'Inoue et al., 2020', 226283753: 'Jansen, 2020', 208089867: 'Jansen and Ustalov, 2019;', 222178328: 'Jhamtani and Clark, 2020'}",https://www.aclanthology.org/2021.emnlp-main.596.pdf
719,258686160,Parallel Context Windows for Large Language Models,conclusion and future work,Insight-tree,"in recent years, a multitude of successful approaches have been proposed for allowing transformer-based language models to leverage large amounts of text during inference, leading to a variety of dedicated architectures. in parallel, however, the mainstream llm production line of new models with ""regular""-up to several thousand tokens-context window sizes enjoys faster progress in the form of scaling, innovation, and data updating. this paper introduced parallel context windows (pcw): a simple approach for allowing any offthe-shelf llm to broaden the scope of text it can access during inference. we showed the effectiveness of pcw in the framework of in-context learning, where access to a context that is larger by a factor of b implies learning from b times more training examples. our results show that pcw is more effective than the vanilla single context window approach for in-context learning over a broad set of multi-class classification tasks, suggesting that pcw could improve in-context learning in tasks with diverse input or output spaces. we also showed promising signals for applying pcw for multiple retrieved document reading.",{},https://www.aclanthology.org/2023.acl-long.352.pdf
720,256827305,Improving Out-of-Distribution Generalization of Neural Rerankers with Contextualized Late Interaction,conclusion,Insight-tree,"in this work, we presented our finding that adding late interaction to existing rerankers brings visible improvement to out-of-distribution capacity without any degradation on in-domain effectiveness, even though the reranker already processes the token interaction via the attention mechanism at previous layers. extensive experiments on different model sizes and first-stage retrievers show that this improvement is consistent, and according to our analysis, the improvement is more prominent on longer queries. our findings suggest that boiling all information into the [cls] token may not be the optimal choice for neural rerankers, and more studies are required to better explore its capacity.",{233296016: '[18]'},https://export.arxiv.org/pdf/2302.06589v1.pdf
721,263831032,LEMUR: HARMONIZING NATURAL LANGUAGE AND CODE FOR LANGUAGE AGENTS Lemur-70B-chat (ours) Llama-2-70B-chat CodeLlama-34B-Instruct (a) Foundational Language and Code Abilities Tool Augmentation Self-debugging Following NL Feedback on Reasoning Following NL Feedback on Coding Exploring Digital Environment Exploring Physical Environment,conclusion,Insight-tree,"in conclusion, this research underscores the pivotal role of harmonizing natural and programming language proficiencies in the evolution of language models to sophisticated language agents. through the development of lemur and lemur-chat, we demonstrated that the meticulous amalgamation of these competencies allows for elevated performance in diverse environments and applications, narrowing the existent capability divide between open-source and proprietary models. we open-sourced both models with the intention of fostering further research in the field of language models for agents. we performed pre-training on the top of llama-2. the detailed statistics of the pre-training data corpus are presented below in table 8. we train our model on a tpuv4-512 pod. our codebase is based on jax and easylm (geng, 2023). following the pretraining methodology of llama 2 (touvron et al., 2023), we used a batch size of 4m tokens. to improve training efficiency, we packed multiple shorter sequences into each batch entry when possible, an approach known as sequential packing (raffel et al., 2020).",{},https://export.arxiv.org/pdf/2310.06830v1.pdf
722,72940739,Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring,conclusion,Insight-tree,"we formulated a dialogue comprehension task motivated by the need in telehealth settings to extract key clinical information from spoken conver-sations between nurses and patients. we analyzed linguistic characteristics of real-world humanhuman symptom checking dialogues, constructed a simulated dataset based on linguistically inspired and clinically validated templates, and prototyped a qa system. the model works effectively on a simulated test set using symptoms excluded during training and on real-world conversations between nurses and patients. we are currently improving the model's dialogue comprehension capability in complex reasoning and context understanding and also applying the qa model to summarization and virtual nurse applications.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/1903.03530v2.pdf
723,238744411,Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model,conclusion and future work,Insight-tree,"we propose a novel encoder-decoder model incorporating soft templates and moe to address the problem of complexity-controllable question generation. as most domains do not have training data for ccqg models, we propose a simple and effective cross-domain estimator to predict the missing complexity levels of questions. in the extensive experiments of both ccqg and complexity assessment tasks, our models achieve superior performance over the competitive baselines across all experimental settings. in the future, we will consider anaphora resolution and numerical reasoning in complexity estimator, and explore the performance of our model in different applications, such as examination and assisting qa systems.","{211258645: 'Perez et al., 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2110.06560v1.pdf
724,207852944,Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering,conclusion,Insight-tree,"we proposed a general approach for open-domain question answering (qa) that models interactions between paragraphs using structural information from a knowledge base. unlike standard approaches where a model retrieves and reads a set of passages, we integrate graph structure at every stage to construct, retrieve and read a graph of passages. our approach consistently outperforms competitive baselines in three open-domain qa datasets, webquestions, natural ques-tions and triviaqa, and we also include a detailed qualitative analysis to illustrate where the cross paragraph reading contributes the most to the overall system performance.","{202583433: 'Godbole et al., 2019', 174801764: 'Min et al., 2019b;', 128345225: 'Sun et al., , 2019', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/1911.03868v2.pdf
725,119407066,On the Structure of the Yang-Mills Vacuum,conclusions,Insight-tree,"the results i have presented here are a rst step towards a microscopic understanding of the yang-mills vacuum. many of the intuitive ideas we have developed in the last years seem to prove to be correct nally. so, most likely, we are on the right track.",{},https://export.arxiv.org/pdf/hep-lat/9506033v1.pdf
726,248447769,Molecular Characterization of Salmonella Typhimurium and E. coli O157:H7 Isolated from Ready-to-eat Chicken Meat,conclusion,Insight-tree,this study has indicated presence of sdia and fimh genes in salmonella typhimurium and e. coli o157:h7 respectively isolated from ready to eat chicken meats from public eateries in ibadan.,{},https://journaljpri.com/index.php/JPRI/article/download/36132/68318
727,225040142,Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval,conclusion,Insight-tree,"we provide the first in-depth analysis on information-seeking qa datasets to inspect where unanswerability arises and quantify the remaining modeling challenges. our controlled experiments identifies two remaining headrooms, answerability prediction and paragraph selection. observing a large percentage of questions are unanswerable, we provide manual analysis studying why questions are unanswerable and make suggestions to improve answer coverage: (1) going beyond wikipedia textual information as the only source of information, (2) addressing ambiguous queries instead of simply marking and leaving the questions as is, (3) enable accessing multiple documents and introducing abstractive answers for non-factoid questions. together, our work shed light on future work for information-seeking qa, both for modeling and dataset design.","{139103297: 'Chen and Durrett, 2019', 224803601: 'Chen et al., 2021', 215785913: 'Chen et al., 2020b;', 67855846: 'Dua et al., 2019', 210859295: 'Hannan et al., 2020', 219721462: 'Kamath et al. 2020', 233219849: 'Talmor et al., 2021;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.acl-long.118.pdf
728,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,conclusion,Insight-tree,"as most pre-trained language models fail to attach enough importance to sentence-level representation learning, it usually leads to unsatisfactory performance in downstream tasks when good sentence representation is right indispensable. based on investigating the intra-sentence relationship between components of sentences (important phrases) and the whole sentence representations, we propose a generative objective to align these phrases with their corresponding sentence representations. this idea leads to paser, a phrase-aware sentence representation model. as an effective alternative in sentence representation learning, our paser achieves comparable performance with strong contrastive learning baselines on sts tasks, and better performance on the downstream semantic retrieval and reranking tasks on datasets including qqp and askubuntu.",{},https://www.aclanthology.org/2022.emnlp-main.221.pdf
729,252918345,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,limitations,Insight-tree,"we think our paser has the following limitations, and leave them for future work.â¢ the combination of decoding signals is empirically designed. hyperparameters m and n are selected by grid search and lack technical analysis.  table 8: ablation for finding best m and n in the unsupervised setting.",{},https://www.aclanthology.org/2022.emnlp-main.221.pdf
730,213474484,Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks,conclusion,Insight-tree,"we presented quail 5 , the first multi-domain text comprehension challenge that is balanced and annotated for 9 types of verbal reasoning. quail aims to show the extent to which current models can generalize over different domains and reasoning strategies and handle questions that can be answered with the information in a given text, unanswerable questions and questions that require extra world knowledge. we hope that quail will stimulate efforts to develop generalist systems tackling different kinds of verbal reasoning, and that it will be useful in diagnostics and qualitative analysis for new qa systems.","{67855846: 'Dua et al. 2019', 204823992: 'Fisch et al. 2019'}",https://ojs.aaai.org/index.php/AAAI/article/download/6398/6254
731,165163607,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,conclusion,Insight-tree,"we have introduced boolq, a new reading comprehension dataset of naturally occurring yes/no questions. we have shown these questions are challenging and require a wide range of inference abilities to solve. we have also studied how transfer learning performs on this task, and found crowd-sourced entailment datasets can be leveraged to boost performance even on top of language model pre-training. future work could include building a document-level version of this task, which would increase its difficulty and its correspondence to an end-user application. embeddings h 1 , h 2 , ... . then pool these embeddings by computing attention scores a i = w Â· h i , p = sof tmax(a), and then the sum v * = i p i h i . likewise we compute p * from the premise.",{},https://www.aclweb.org/anthology/N19-1300.pdf
732,233297051,Explaining Answers with Entailment Trees,summary and conclusion,Insight-tree,"our goal is to enable machines to generate richer, more systematic explanations. to this end, we have developed a novel formulation of explanations as multistep entailment trees, and created entail-mentbank, the first large dataset of such trees.","{230799347: 'Geva et al., 2021', 218486753: 'Inoue et al., 2020', 222178328: 'Jhamtani and Clark, 2020', 222141025: 'Saha et al., 2020', 52822214: 'Yang et al., 2018', 218487030: 'Ye et al., 2020;'}",https://www.aclanthology.org/2021.emnlp-main.585.pdf
733,252090148,Interactive Question Answering Systems: Literature Review,conclusion,Insight-tree,"in conclusion, we have reviewed a substantial collection of interactive question answering systems (iqass)-related literature published during the past decade. we discovered the literature to be diverse, beginning with adopted methodologies for addressing multiple qa tasks and concluding with a vast array of diverse resources (i.e. knowledge source, and datasets) that are typically utilized to create and evaluate question answering systems (qass). despite the fact that the state-of-the-art is defined by several types of qa solutions, we were able to determine the characteristics shared by the suggested systems that constitute a shared framework. to the best of our knowledge, we are the first to present a unified and comprehensive design that emphasizes the fundamental components and functions of iqass.","{204915921: '[53]', 209063721: '[85]', 202558795: '[123]'}",https://export.arxiv.org/pdf/2209.01621v1.pdf
734,244896105,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,conclusions,Insight-tree,"in this work, we propose an alternative to multidataset models for multi-skill qa. we propose to combine expert agents to create a collaborative system for question answering (qa) called metaqa. it considers questions, answer predictions, and confidence scores from the agents to select the best answer to a question. through quantitative experiments, we show that our model avoids the limitations of multi-dataset models and outperforms the baselines thanks to the agent collaboration established. additionally, since metaqa learns to match questions with answers instead of end-to-end qa, it is highly data-efficient to train. we leave as future work: i) combining partially correct answer predictions to generate a better one, ii) adding new agents without retraining metaqa by fixing most of the weights and only training the weights of the new agent selection network, and iii) identifying a priori agents that are likely to give an incorrect answer to skip them at run-time.","{233296016: 'Thakur et al., 2021'}",https://www.aclanthology.org/2023.eacl-main.259.pdf
735,240354203,Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge,conclusion,Insight-tree,"in this paper, we proposed an unsupervised mcqa method, which exploits the pseudo labels generated by some basic rules or external non-mcqa datasets. the proposed method significantly outperforms the baseline approaches on race and is even comparable with the supervised learning performance on mc500. we hope this paper sheds light on unsupervised learning in nlp tasks.","{86611921: 'Kwiatkowski et al., 2019;', 202558815: 'Min et al. 2019', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2021.mrqa-1.12.pdf
736,232478685,FeTaQA: Free-form Table Question Answering,conclusion,Insight-tree,"in this paper, we introduced the task of generative table question answering with fetaqa, a table qa dataset consisting of complex questions that require free-form, elaborate answers. we also proposed two modeling approaches: (1) a pipeline model that incorporates a table semantic parser and a data-to-text generator, and (2) an end-to-end model that integrates query comprehension, reasoning and text generation. our experimental results indicate that the end-to-end model with a simple table encoding strategy achieves much higher scores than the pipeline model that requires table semantic parsing. furthermore, we show that fetaqa reveals the challenging nature of the table question answering task and calls for innovative model designs in the future.","{224803601: 'Chen et al., 2021', 196170479: 'Fan et al., 2019', 221507798: 'Petroni et al., 2021;', 233219849: 'Talmor et al., 2021'}",https://www.aclanthology.org/2022.tacl-1.3.pdf
737,215768766,Coreferential Reasoning Learning for Language Representation,conclusion and future work,Insight-tree,"in this paper, we present a language representation model named corefbert, which is trained on a novel task, mention reference prediction, for strengthening the coreferential reasoning ability of bert. experimental results on several downstream nlp tasks show that our corefbert significantly outperforms bert by considering the coreference information within the text. in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mention reference prediction training task. it is a feasible approach to introducing the coreferential signal to language representation models, but the automatic labeling mechanism inevitably accompanies with the wrong labeling problem. until now, mitigating noise in ds data is still an open question.","{173188058: 'Talmor and Berant, 2019;', 52822214: 'Yang et al., 2018', 189898081: 'Yao et al., 2019'}",https://arxiv.org/pdf/2004.06870v1.pdf
738,247595263,SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS,conclusion and discussion,Insight-tree,"we introduced a simple yet effective method called self-consistency, and observed that it significantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. beyond accuracy gains, self-consistency is also useful for collecting rationales when performing reasoning tasks with language models, and for providing uncertainty estimates and improved calibration of language model outputs.","{165163607: 'Clark et al., 2019', 237433880: 'Xu et al., 2021a', 52822214: 'Yang et al., 2018', 252873674: 'Ye & Durrett, 2022'}",https://export.arxiv.org/pdf/2203.11171v4.pdf
739,234470046,Encoding Explanatory Knowledge for Zero-shot Science Question Answering,conclusion,Insight-tree,"in this paper, we proposed a neural encoding mechanism for explanatory knowledge acquisition and transfer, n-xkt. we evaluated the impact of the encoding mechanism on downstream science qa. the proposed model delivers better generalisation and accuracy for qa tasks that require multi-hop and explanatory inference. the proposed encoding mechanism can be used to deliver zero-shot inference capabilities, providing comparable performance when compared to supervised models on qa. these results supports the hypothesis that pretraining tasks targeting abstract and explanatory knowledge acquisition can constitute and impor- ","{202785879: 'Yadav et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.iwcs-1.5.pdf
740,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,conclusion,Insight-tree,"we present quest, a new benchmark of queries which contain implicit set operations with corresponding sets of relevant entity documents. our experiments indicate that such queries present a challenge for modern retrieval systems. future work could consider approaches that have better inductive biases for handling set operations in natural language expressions (for example, vilnis et al. (2018)). the attributions in quest can be leveraged for building systems that can provide finegrained attributions at inference time. the potential of pretrained generative lms and multi-evidence aggregation methods to answer set-seeking selective queries, while providing attribution to sources, can also be investigated.","{249062559: 'Amouyal et al., 2022', 86611921: 'Kwiatkowski et al., 2019', 128345225: 'Sun et al., 2019', 233296016: 'Thakur et al. 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.acl-long.784.pdf
741,258822815,QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations,limitations,Insight-tree,"naturalness. since our dataset relies on the wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations. further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios. however, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on quest should correlate with improvements on at least a fraction of natural search engine queries with set operations.recall. we also note that because wikipedia categories have imperfect recall of all relevant entities (that contain sufficient evidence in their documents), systems may be incorrectly penalised for predicted relevant entities assessed as false positive. we quantify this in section 5. we have also limited the trusted source for an entity to its wikipedia document but entities with insufficient textual evidence in their documents may still be relevant. ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions. romqa (zhong et al., 2022) takes a step in this latter direction although the evidence attribution is not manually verified.answer set sizes. to ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers. however, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible.on one hand, this is realistic for a search scenario because users may only be interested in a limited set of results. on the other hand, our dataset does not model a scenario where the answer set sizes are much larger.","{249062559: 'Amouyal et al., 2022', 86611921: 'Kwiatkowski et al., 2019', 128345225: 'Sun et al., 2019', 233296016: 'Thakur et al. 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.acl-long.784.pdf
742,237450610,Flexible Generation of Natural Language Deductions,conclusion,Insight-tree,"building systems that use natural language as a medium for reasoning will require operations to logically combine and transform natural language statements. in this work, we present parapattern, a method for creating such models with minimal manual effort by finetuning pretrained sequence-to-sequence language models on data generated through a three-step process of syntactic retrieval, template expansion, and automatic paraphrasing. our experimental results show that parapattern yields operation models capable of generating consistent logical transformations over a diverse range of natural language inputs, matching the performance of models trained with in-domain human supervision.","{139103297: 'Chen and Durrett, 2019;', 218581117: 'Holzenberger et al., 2020', 204915921: 'Khot et al., 2020;', 221448158: 'Khot et al. 2021', 174801764: 'Min et al., 2019', 222141025: 'Saha et al., 2020', 219573621: 'Talmor et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.506.pdf
743,226278305,Answer Span Correction in Machine Reading Comprehension,conclusion,Insight-tree,"we describe a novel method for answer span correction in machine reading comprehension. the proposed method operates by marking an original, possibly incorrect, answer prediction in context and then making a new prediction using a corrector model. we show that this method corrects the predictions of a state-of-the-art english-language reader in different error categories. in our experiments, the approach also generalizes well to multilingual and cross-lingual mrc in seven languages. future work will explore joint answer span cor-","{86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2011.03435v1.pdf
744,222090978,A Survey on Explainability in Machine Reading Comprehension,conclusion and open research questions,Insight-tree,"this survey has proposed a systematic categorisation of benchmarks and approaches for explainability in mrc. lastly, we outline a set of open research questions for future work:","{139103297: 'Chen and Durrett, 2019;', 67855846: 'Dua et al., 2019', 220045477: 'Dua et al., 2020;', 218486753: 'Inoue et al., 2020', 208089867: 'Jansen and Ustalov, 2019', 189927896: 'Jiang and Bansal, 2019a', 204915921: 'Khot et al., 2020', 174801764: 'Min et al., 2019a;', 174801080: 'Min et al. 2019b', 207756678: 'Wang et al., 2019c'}",https://arxiv.org/pdf/2010.00389v1.pdf
745,231632353,ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks,conclusion,Insight-tree,"in this paper, we study the compositional question answering where the answer is composed of discontiguous segments in the document. we present a large scale chinese comqa dataset containing more than 120k human-labeled questions. the data construction process has undergone rigid inspections to ensure high quality. to solve the comqa problem, we propose a hierarchical graph neural networks that incorporate document graph structure to the model. we also devise two novel tasks, i.e., question selection and node selection, to pre-train the model. the proposed methods achieve significant improvement over previous methods. we also conduct several ablation studies to demonstrate the superiority of the proposed pre-training tasks and the graph structure. however, there is still a large gap between our model with human performance, suggesting that there is still room for improvement in comqa. figure 8: a snapshot of the annotation interface. each row is a single node that could be served as the final answer component. the question is the page title in the top. note that the image or table could also be selected.","{67855846: '[8]', 86611921: '24,', 52822214: '[47]'}",https://arxiv.org/pdf/2101.06400v1.pdf
746,218470472,Diverse Visuo-Lingustic Question Answering (DVLQA) Challenge,conclusion,Insight-tree,"in this work, we introduced the diverse visuo-linguistic question answering (dvlqa) challenge that we believe has the potential to open new research avenues in areas of joint vision & language. our experiments show that a system equipped with state-of-the-art vision-language models does not perform well on the task that requires joint visionlanguage inference. our future work would include extending this dataset to support more diverse visuo-linguistic tasks for future research on building generic ai models that can learn novel visual concepts through small set of examples. ",{},https://arxiv.org/pdf/2005.00330v1.pdf
747,237364113,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,conclusion,Insight-tree,"this work contributes a novel framework for domain adaptation of qa systems in settings with limited text corpora. we develop caqa in which we combine techniques from from question generation and domain-invariant learning to answer out-of-domain questions. different from existing works in question answering, we achieve this by proposing a contrastive adaptation loss. extensive experiments show that caqa is superior to other state-of-the-art approaches by achieving a substantially better performance on out-of-domain data. qagen-t5: we apply lm-filtering as in (shakeri et al., 2020) and select qa pairs with highest scores for each context paragraph. qagen-t5 models are trained similarly to aqgen and qagen, we separately keep the best qg and qa models according to validation performance on the squad dev set. hyperparameter search: in our experiments, we empirically search for hyperparameters Î² and Ï in the contrastive adaptation loss through additional experiments. we experiment with different values of Î² in the range [10 â1 , 10 â2 , 10 â3 ] and gaussian noise n (0, Ï) applied on all token embeddings with standard deviation Ï ranging from 0 to 10 â2 . the best combination of Î² and Ï as per the training set is then selected, these numbers can be found in table 4.  all parameters that have not been mentioned explicitly above were used as reported in their original paper b additional results",{},https://www.aclanthology.org/2021.emnlp-main.754.pdf
748,237365386,Topic Knowledge Acquisition and Utilization for Machine Reading Comprehension in Social Media Domain,conclusion,Insight-tree,"in this paper, we focus on machine reading comprehension in social media domain. we propose a novel method to address the problem of lacking in background knowledge in this task. utilizing the nature of clustering of social media, we retrieve and refine topic knowledge from the relevant messages, and then integrate the knowledge into an mrc model, tkr. experimental results show that our proposed method outperforms the recently proposed models and the bert-based baselines, which proves the method effective overall. by introducing different amount of topic knowledge, we demonstrate the effectiveness of our refined knowledge. moreover, the ablation study further validates the contribution of the key modules of tkr for utilizing the knowledge.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.ccl-1.88.pdf
749,255440689,InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval,conclusion,Insight-tree,"in this work, we presented inpars-v2, an improved version of inpars [1] that uses a publicly available language model to generate queries and a better query-document pair selection process. our results show that we achieve effectiveness on par with the state of the art on beir. the synthetic data and finetuned models were publicly released.",{},https://export.arxiv.org/pdf/2301.01820v4.pdf
750,249395505,On the Advance of Making Language Models Better Reasoners,conclusion and future work,Insight-tree,"we introduce diverse, an effective and general method to make large language models better reasoners. as a continuation of the line of research that prompting language models using multi-step reasoning paths, the key insights of diverse are three-fold: diverse prompts, voting verifier, and step-level correctness. experimental results clearly show that diverse can bring significant and consistent improvements. for example, with codedavinci-002, diverse achieves new state-of-theart results in most of the reasoning tasks, outperforming the 540b palm model combined with previous prompting approaches.","{216035859: 'Asai and Hajishirzi 2020', 237485084: 'Deng et al. 2021', 153312687: 'Ding et al., 2019;', 67855846: 'Dua et al., 2019;', 230799347: 'Geva et al., 2021', 234335834: 'Wang et al., 2022a', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2206.02336v2.pdf
751,247476296,Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach,conclusion & future work,Insight-tree,"we explore an iterative prompting framework towards driving a ""train of thought"" from plms for multi-step reasoning tasks. we show the superiority of this iterative scheme, and also effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme. in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors. in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities. the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely a human can track along the plm's train of thought and make edits and corrections at different steps, which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process. we leave these investigations as future work. ","{226236740: 'Ho et al., 2020', 218486753: 'Inoue et al., 2020', 230437663: 'Khattab et al., 2021', 202773198: 'Qi et al., 2019;', 219573621: 'Talmor et al., 2020b', 221970302: 'Xiong et al., 2021;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2203.08383v2.pdf
752,258865162,Why Does ChatGPT Fall Short in Providing Truthful Answers?,conclusion,Insight-tree,"our paper investigates the common failures of chat-gpt in complex open-domain question answering. we identify four types of errors: comprehension, factualness, specificity, and inference. we also examine the key abilities knowledge memorization, knowledge recall, and knowledge reasoning, which are critical to these failures. additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on recall, and the effect of decomposition on reasoning. finally, we suggest several techniques to help users more effectively use chatgpt as a question-answering tool and enable system builders to develop better qa systems. our research contributes to the understanding of what influencing the truthfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more reliable llms.","{165163607: 'Clark et al., 2019', 252715485: 'Khot et al., 2023;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2304.10513v2.pdf
753,216562779,MAVEN: A Massive General Domain Event Detection Dataset,conclusion and future work,Insight-tree,"in this paper, we present a massive general domain event detection dataset (maven), which significantly alleviates the data scarcity and low coverage problems of existing datasets. we conduct a thorough evaluation of the state-of-the-art ed models on maven. the results indicate that general domain ed is still challenging and maven may facilitate further research. we also explore some promising directions with analytic experiments, including modeling multiple event correlations (sec-tion 5.3), utilizing the hierarchical event schema to distinguish close types (section 5.6), and improving other ed tasks with transfer learning (section 5.5). in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc.","{189898081: 'Yao et al., 2019'}",https://www.aclweb.org/anthology/2020.emnlp-main.129.pdf
754,256868909,How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,conclusion,Insight-tree,"we present dragon, dense retriever trained with diverse augmentation. we propose a unified framework of data augmentation (da) to understand the recent progress of training dense retrievers. based on the framework, we extensively study how to improve dense retrieval training through query and relevance label augmentation. our experiments uncover some insights into training a dense retriever, which contradicts common wisdom that cross encoder is the most effective teacher and human-like queries are the most suitable training data for dense retrieval. instead, we propose a diverse data augmentation recipe, query augmentation with the mixture of sentence cropping and generative queries, and progressive relevance label augmentation with multiple teachers. with our recipe of da, we are the first to demonstrate that a single bert-base-sized dense retriever can achieve state-of-the-art effectiveness in both supervised and zero-shot retrieval tasks. we believe that dragon can serve as a strong foundation retrieval model for domain adaptation retrieval tasks (wang et al., 2022;dai et al., 2022) or the existing retrieval augmented language models (izacard et al., 2022;shi et al., 2023;mallen et al., 2022).","{248665596: 'Formal et al., 2022;', 238857091: 'Xin et al., 2022;', 253157773: 'Yu et al., 2022'}",https://export.arxiv.org/pdf/2302.07452v1.pdf
755,246015349,Natural Language Deduction through Search over Statement Compositions,discussion and conclusion,Insight-tree,"in this work, we propose a system that performs natural language reasoning through generative deduction and heuristic-guided search.","{237450610: 'Bostrom et al., 2021', 230799347: 'Geva et al., 2021', 174801080: 'Min et al. 2019;', 160009340: 'Nishida et al. 2019', 222141025: 'Saha et al., 2020', 219573621: 'Talmor et al., 2020', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2201.06028v2.pdf
756,246015349,Natural Language Deduction through Search over Statement Compositions,limitations,Insight-tree,"the baseline approach we consider in this work, end-to-end modeling of entailment tree generation, enjoys the convenience of simple inference and quadratic complexity. however, the computational overhead of sequence-to-sequence models places a hard limit on the tree size and premise count that can be handled in the end-to-end setting; moreover, recent results call into question how well end-to-end transformers can generalize this type of reasoning (zhang et al., 2022). our structured approach allows arbitrarily large premise sets and step counts. however, by discretizing the reasoning in the scsearch procedure, we do face a runtime theoretically exponential in proof size to do exhaustive search. in practice, we limit our search to a finite horizon and find that this suffices to provide a practical wall clock runtime, never exceeding 5 seconds for any single example. future work on higher tree depths may have to reckon with the theoretical limitations of this procedure, possibly through the use of better heuristics.our experiments are conducted exclusively on english datasets. while we hypothesize that our approach would work equally well for another language given a pretrained sequence-to-sequence model for that language with equivalent capacity, such models are not available universally across languages, representing an obstacle for transferring our results to languages beyond english.furthermore, the entailmentbank dataset on which we train and evaluate targets the elementary science domain, raising a question of domain specificity. in future work, we plan to evaluate deduction models on additional datasets with different style, conceptual content, and types of reasoning in order to verify that the factored approach is equally applicable across diverse settings.","{237450610: 'Bostrom et al., 2021', 230799347: 'Geva et al., 2021', 174801080: 'Min et al. 2019;', 160009340: 'Nishida et al. 2019', 222141025: 'Saha et al., 2020', 219573621: 'Talmor et al., 2020', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2201.06028v2.pdf
757,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,limitations of reflexion,Insight-tree,"reflexion relies on the emergent property of self-reflection that is present in several large language models. in this study, we used gpt-3.0 and gpt 3.5 to power a react agent (yao et al., 2023) to learn from its past mistakes. while reflexion enabled the agent to discover new problem-solving techniques in alfworld decision-making tasks and hotpotqa knowledge-intensive tasks, we observed a shortcoming in its ability to improve on its baseline performance in a third benchmark, webshop (yao et al., rint).webshop is a text-based problem-solving benchmark that tests natural language agents to navigate an e-commerce website to find and purchase products given requests from a client. we tested the agent in 100 environments, giving the agent two few-shot examples of successful webshop trajectories using the react problem-solving technique. however, after only 4 trials, we terminated the baseline and reflexion runs as the agent did not show improvement in accuracy (fig. 6) and was not generating helpful, intuitive self-reflections. the agent achieved a 33% â 34% accuracy improvement in the baseline run and a mere 33% â 35% accuracy improvement in the reflexion run, which suggests that the agent only successfully completed 1 additional task relative to the baseline agent's performance.however, after analyzing the failed trajectories, we noted that the chance of a successful item purchase for an agent in a webshop environment was not necessarily dependent on the agent's ability to plan and execute a correct sequence of actions, but rather on the quality of the webshop search engine's results. this observation may not be a direct limitation of the reflexion approach, but it highlights the ability of a reflexion agent to optimize reasoning trace and action execution but not complete awareness of the quality of the tools that it may be using.",{},https://export.arxiv.org/pdf/2303.11366v1.pdf
758,257636839,Reflexion: an autonomous agent with dynamic memory and self-reflection,conclusion,Insight-tree,"we proposed an approach that allows natural language agents to learn from past mistakes and redirect future decisions in planning sequences which removes the human trainer in a human-in-the-middle approach. we demonstrated learning curves on the alfworld and hotpotqa benchmarks that significantly outperform base react agents. in addition, we include an inconclusive attempt to improve performance on the webshop benchmark and provide a discussion that highlights a few limitations of this approach. reflexion is a highly applicable method to improve performance between trials on decision-making and knowledge-intensive tasks due to its sole dependence on a binary reward model. in the alfworld and hotpotqa experiments, we constrained the reward model to imitate environments in which informative reward models may be difficult to design or compute. we encourage others to apply reflexion to more complex tasks in which the agent must learn to develop new ideas, explore larger unseen state spaces, and form more accurate plans of action through its experiences in past environments. ",{},https://export.arxiv.org/pdf/2303.11366v1.pdf
759,264128412,SEARCH-ADAPTOR: TEXT EMBEDDING CUSTOMIZA-TION FOR INFORMATION RETRIEVAL,conclusions,Insight-tree,"pre-trained llms have shown great potential in a variety of downstream tasks.in this paper, we focus on pushing the capabilities of llms for information retrieval and search.we propose a canonical efficient adaptation method, search-adaptor, that can also be applied to llms available only via apis.we demonstrate that search-adaptor significantly and consistently improves retrieval performance across diverse regimes of training data size, encoder type, and corpus set.important future directions include generalizing the adaptation method to include partial tuning of the embedding models, as well as extensions to multi-modal data.",{},https://export.arxiv.org/pdf/2310.08750v1.pdf
760,258866004,Machine Reading Comprehension using Case-based Reasoning,conclusion,Insight-tree,"we present cbr-mrc, a semi-parametric model for machine reading comprehension that is simple, accurate, and interpretable.our model stores a collection of cases, retrieves the most relevant cases for a given test question, and then explicitly reuses the reasoning patterns encoded in the embeddings of these cases to predict an answer.we show that our model performs well for both extracting answers and identifying supporting evidence on several mrc tasks compared to fully-parametric baselines.we also demonstrate the ability of our model to transfer to new domains with limited labeled data.finally, we analyze our model under varying conditions of lexical diversity and find that it is robust to high lexical diversity, whereas fullyparametric models show a drop in performance.","{204823992: 'Fisch et al., 2019', 247292088: 'Iyer et al., 2023', 198229624: 'Joshi et al., 2020', 226254596: 'Seonwoo et al., 2020b', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2305.14815v2.pdf
761,258866004,Machine Reading Comprehension using Case-based Reasoning,limitations,Insight-tree,"cbr-mrc, and the cbr framework more generally, relies on the existence of past cases to make predictions for a new problem.this can pose a challenge for composite questions, which require multi-hop reasoning, since the likelihood of enumerating each algebraic combination of reasoning patterns in the casebase is impractical.models, thus, may only be able to match a portion of the question, resulting in partially correct reasoning.to address these limitations, future work could explore methods to explicitly encourage such compositional generalization, such as question decomposition with a recursive application of cbr.","{204823992: 'Fisch et al., 2019', 247292088: 'Iyer et al., 2023', 198229624: 'Joshi et al., 2020', 226254596: 'Seonwoo et al., 2020b', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2305.14815v2.pdf
762,186206745,"Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension",conclusion,Insight-tree,"we presented an interpretable 3-module, multihop, reading-comprehension system 'epar' which constructs a 'reasoning tree', proposes an answer candidate for every root-to-leaf chain, and merges key information from all reasoning chains to make the final prediction. on wikihop, our system outperforms all published models on the dev set, and achieves results competitive with the current stateof-the-art on the test set. on medhop, our system outperforms all previously published models on the leaderboard test set. we also presented multiple reasoning-chain recovery tests for the explainability of our system's reasoning capabilities.",{},https://arxiv.org/pdf/1906.05210v1.pdf
763,257038341,Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey,conclusion,Insight-tree,"we give a comprehensive review of large-scale multi-modal pre-trained models (mm-ptms) in this paper.firstly, we introduce the background of mm-ptms, with a focus on conventional deep learning, and pre-training in nlp, cv, and speech.then, the task definition, key challenges, and benefits of mm-ptms are discussed.after that, we dive into the reviews of mm-ptms and discuss the pre-training data, objectives, networks, knowledge enhanced pre-training, etc.we review the downstream tasks including generative, classification, and regression tasks, and also give an overview of model parameters of mm-ptms and hardware for the pre-training.experimental results of several representative tasks are also discussed and visualized.finally, we point out some research directions that are worth to be focused on.we summarize this paper and hope our survey can provide some useful insights for the mm-ptms.","{212747830: '[53]', 52822214: '[228]', 165163607: '[229]'}",https://export.arxiv.org/pdf/2302.10035v2.pdf
764,259096157,"Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",conclusion,Insight-tree,"we revisit ood robustness research in nlp, identifying deficiencies in benchmarks and evaluation.correspondingly, a benchmark construction protocol and an ood robustness evaluation suite are proposed to facilitate future research.the correlation between ood and id performance, the effectiveness of existing methods, and the challenges faced by llms are investigated.","{211010520: '[4]', 202539031: '16,', 234742165: '[26]', 204823992: '[30]', 86611921: '[49]', 216867120: '[67]', 207756753: '73,', 248228026: '[82]', 52822214: '[108]', 256631130: '109,'}",https://export.arxiv.org/pdf/2306.04618v2.pdf
765,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"limited by the size of knowledge bases, some knowledge gaps cannot be filled effectively in practice. we are studying a creative reasoning mechanism to solve this problem. in addition, fact verification is a very open and challenging task. it needs not only the support of linguistic features and background knowledge, but also the support of more complex multi-dimension information, such as social content and spatiotemporal information. for example, claims evolve over time, and what was fake yesterday is true today. ","{202712552: '[14]', 208089867: '[19,', 155100120: '[42]'}",https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
766,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,limitation of the study,Insight-tree,"limited by the size of knowledge bases, some knowledge gaps cannot be filled effectively in practice. we are studying a creative reasoning mechanism to solve this problem. in addition, fact verification is a very open and challenging task. it needs not only the support of linguistic features and background knowledge, but also the support of more complex multi-dimension information, such as social content and spatiotemporal information. for example, claims evolve over time, and what was fake yesterday is true today.","{202712552: '[14]', 208089867: '[19,', 155100120: '[42]'}",https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
767,225311514,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,conclusions,Insight-tree,"in this study, a novel graph-based reasoning framework was proposed for complex fact verification (fv), which can dynamically supplement useful knowledge in the case of knowledge gaps. the framework retrieves and fills the knowledge gaps between the given claim and evidence to construct the collaborative graph before propagating and aggregating sequential information. experiments have shown that dkar can effectively solve the ""not enough information"" mislabeling problem in the fv task and outperform other baselines. in addition, our approach shows outstanding advantages in a small sample and heterogeneous web text sources. our research first illustrates that dynamic knowledge supplementation plays an important role in complex fv tasks, which contributes to the study of reasoning methods driven by data and knowledge for fact verification. it is expected that our first exploration encourages others to expand upon our work, and to further shed light on the broader and more challenging goal of complex and practical fv tasks with joint data and knowledge.","{202712552: '[14]', 208089867: '[19,', 155100120: '[42]'}",https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf
768,256459309,Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data,conclusion,Insight-tree,"recall the desiderata for our algorithm, expressed in the introduction: our algorithm should (1) make no assumptions on the available auxiliary data a-priori, (2) scale well with the number of auxiliary datasets, and (3) add minimal memory and computational overhead.(1) when designing our algorithm, we purposefully formulate the problem as a multi-armed bandit.mab algorithms, in general, make no assumptions on the quality of rewards and, in particular, exp3 even assumes that the auxiliary datasets will play an adversarial role when returning rewards.(2) as previously mentioned, our algorithms have a single-turn computational complexity that is independent of the number of auxiliary datasets.(3) finally, our method adds minimal computational overhead beyond usual training computations.every gradient that we utilize for our reward functions are also used to update the model, adding no additional computations.the only computational overhead is to compute gradient alignment (three vector dot products, two scalar square roots, and two scalar multiplications) or magnitude similarity (four vector dot products, two scalar square roots, three scalar multiplications, and one scalar addition).additionally, our method adds a small amount of memory overhead, used to store gradients between model updates.our rewards consider only the gradient w.r.t the language modelling head and, in practice, require 0.25gb per auxiliary gradient to store, slightly increasing the space complexity above standard fine-tuning.","{218487733: '9,', 231718729: '14,', 244478674: '15,', 207756753: '[49]'}",https://export.arxiv.org/pdf/2302.00674v4.pdf
769,237592852,Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering,conclusion,Insight-tree,"in this work, we provided insights on the performance of state-of-the-art dense retrieval for multihop questions. we showed that rerank+dpr 2 (our hybrid model) outperforms mdr (the state-of-theart multi-hop dense retrieval model) in the low re-source setting, and it is competitive with mdr in the setting where mdr uses considerably more computational resources. finally, we highlighted that fully dense retrieval models get harmed when using limited computational resources. for future work, we plan to build on our insights to improve the performance of multi-hop models by combining the strengths of lexical and dense retrieval. also, we aim to develop less computationally expensive multi-hop retrieval models.","{208267807: 'Asai et al., 2020', 202660724: 'Nie et al., 2019;', 202773198: 'Qi et al., 2019', 221970302: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2021.sustainlp-1.7.pdf
770,237532313,Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings,conclusion,Insight-tree,"in this paper, we addressed three research questions: whether integrating external knowledge can improve explainability for nli, how effective knowledge implicitly stored in language models is for reasoning, and how humans perceive explanation quality of state-of-the-art natural language inference models. to answer these questions, we proposed different methods of integrating various knowledge sources into deep learning models. we found that fine-tuned language models reach the highest performance on e-snli as well as the highest average accuracy within the nli stress test evaluation. however, their performance can break down on numerical reasoning and negations. in addition to automatic evaluation, we conducted a large-scale human crowdsourcing evaluation and found that high differences in accuracy, bleu or bleurt scores do not reflect in significant differences in human ratings of explanation correctness, commonsense inclusion, grammar or label correctness. this highlights an alarming disconnect between automatic evaluation scores and human ratings, that puts the real-world utility of recent model improvements into question and requires to re-think automatic evaluation across the field of explainable ai.  b study interface figure 4 shows an example of the study interface used to collect human ratings as discussed in section 5. table 4 lists the annotation guidelines used to decide on low/high levels of required external knowledge as discussed in section 5.3. table 5 shows example annotations.   the entailment can be decided by matching identical parts in the premise and the hypothesis. premise: a water scene with a sunset in the background.","{222310757: 'Schuff et al. 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf
771,248085193,Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering,conclusion,Insight-tree,"in this paper, we propose a more accurate and efficient architecture to utilize qa-pairs as representation units of knowledge. our proposed model qamat outperforms repaq significantly, while leveraging our less expensive training procedure. furthermore, we show how a qa-backed model can perform compositional reasoning and address more complex queries. in the future, we hope to further close the gap with state-of-the-art documentbased retrieve-and-read models and extend this approach to a broader set of tasks.","{211296452: 'Dhingra et al., 2019;', 86611921: 'Kwiatkowski et al., 2019', 225066758: 'Pan et al. 2021', 233296016: 'Thakur et al., 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2204.04581v3.pdf
772,235399966,FINQA: A Dataset of Numerical Reasoning over Financial Data,conclusion and future work,Insight-tree,"this paper introduces finqa, a new expertannotated qa dataset that aims to tackle numerical reasoning over real-world financial data. the questions in finqa pose great challenge for existing models to resolve domain-specific knowledge, as well as to acquire complex numerical reasoning abilities. we propose baseline frameworks and con-duct comprehensive experiments and analysis. the results show that current large pre-trained models still fall far behind the human expert performance. this encourages potential future work on developing pre-training tasks for such realistic, complex application domains. we believe finqa should serve as a valuable resource for the research community.","{215785913: 'Chen et al., 2020c', 67855846: 'Dua et al., 2019;'}",https://www.aclanthology.org/2021.emnlp-main.300.pdf
773,219133337,Corporate distress prediction using random forest and tree net for india,conclusion,Insight-tree,"the random forest classification is a popular machine learning algorithm to predict bankruptcy (distress) of firms or business that has been considered in this study and evaluated against tree net algorithm that is also an extensively applied machine learning algorithm not only in bankruptcy prediction but also in information technology and other fields. random forest orders firms according to their propensity to default or to become distressed. the relative superiority of the different approaches has been verified in this study employing exhaustive information set from corporate india. the firms covering varied sectors like manufacturing, services etc. from 2006 to 2015 have been chosen for the purpose. on comparison amongst the two, it is observed that the tree net methodology has been producing better 'in-sample' classification accuracy in contrast to random forest methodology translating in estimation gain of around 3%. furthermore, tree net is showing superior predictive performance in contrast to the random forest consistently. the analysis provides useful insights for possible tools that may be used by management, regulators and researchers alike to forecast and ascertain the financial health of firms.",{},http://jms.eleyon.org/index.php/jms/article/download/380/366
774,248476204,QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance,conclusion,Insight-tree,"existing evaluation metrics for question generation are still reference-based and ignore the crucial input context of generation, lacking a deep understanding of the relevance between the generated questions and context. to address these issues, we propose qrelscore, which measures the word-and sentence-level relevance through the off-the-shelf language models. extensive experiments demonstrate that qrelscore achieves start-of-the-art correlation with human judgments and makes up for the shortcomings of existing reference-based metrics.",{},https://www.aclanthology.org/2022.emnlp-main.37.pdf
775,235446913,A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections,conclusions and future work,Insight-tree,our contributions can be summarized as follows:,{},https://www.aclanthology.org/2021.acl-long.301.pdf
776,233240823,TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph,conclusions,Insight-tree,"we proposed transfernet, an effective and transparent framework for multi-hop qa over knowledge graph or text-formed relation graph.it achieved 100% accuracy on 2-hop and 3-hop questions of label-formed metaqa, nearly solving the dataset.on the more challenging webqsp, compwebq and text-formed metaqa, it also outperforms other state-of-the-art models significantly.qualitative analysis shows the good interpretability of trans-fernet.","{153312687: 'Ding et al., 2019;', 67855846: 'Dua et al., 2019;', 128345225: 'Sun et al., , 2019', 207870753: 'Tu et al., 2020;', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2104.07302v2.pdf
777,186346769,The Connection Commonalities in the Mathematical Content of Lesson Sequences,conclusion,Insight-tree,"the comparisons of the individual teachers' practices revealed how one particular teacher was noteworthy. according to the interview data collected from all four teachers, notably, only x-t2 mentioned the importance of making the relevant connections in his mathematics teaching. from the interview, x-t2 stressed that it was essential for all his students to see the connections of what reallife examples can bring to the topic of statistics. at the same time, he wanted his students to enjoy and learn mathematics from the enthusiasm he brought into his mathematics teaching. from the observations made, the same level of technique (the technique used to implement making connections problems) made by x-t2 was not detected in the lessons of the other three brunei teachers. then again, why is the least mathematically equipped teacher doing the most innovative teaching? a possible explanation that can be suggested is that, perhaps, since he did not have the relevant mathematical education background, he may have not seen the connection himself. we need to encourage teachers who are mathematical experts, not take for granted the connections that are obvious to them (and possibly, will be obvious to their students), and to include what x-t2 was doing, and to actually, very deliberately try to create the connections within and across their own lessons.",{},NaN
778,214071925,Translucent Answer Predictions in Multi-Hop Reading Comprehension,conclusions,Insight-tree,"tap is a novel architecture for the multi-hop reasoning based rcqa task. core to this system is logix, a new approach that effectively addresses the challenges of local context and global interactions present in multi-passage, multihop qa. we have shown that tap advances the state-of-theart on hotpotqa dataset, reaching rank-1 and rank-2 in its ensemble and single model variants at the time of submission. finally, by restricting the input of the ap to logix's selected supporting facts, tap admits interpretability that can be used to debug the model for performance enhancement purposes prior to its deployment.","{153312687: 'Ding et al. 2019', 174801080: 'Min et al. 2019', 160009340: 'Nishida et al. 2019', 155100120: 'Qiu et al. 2019', 52822214: 'Yang et al. 2018'}",https://ojs.aaai.org/index.php/AAAI/article/download/6272/6128
779,221662105,CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management,conclusion,Insight-tree,"in this paper, we propose a general system, caire-covid, with open-domain qa and query focused multi-document summarization techniques for efficiently mining scientific literature given a query. the system has shown its efficiency on the kaggle cord-19 challenge, which was evaluated by medical researchers, and a series of experimental results also proved the effectiveness of our proposed methods and the competency of each module. the system is also easy to be generalized to general domain-agnostic literature information mining, especially for possible future pandemics. we have launched our website 2 for real-time interactions and released our code 3 for broader use.","{208000835: 'Su et al., 2019'}",NaN
780,250264121,QA Is the New KR: Question-Answer Pairs as Knowledge Bases,conclusion,Insight-tree,"symbolic kbs organize information into small modular components (e.g., entities, kg triples, wikidata statements) that can be combined compositionally to answer complex queries. while many recent papers have focused on tasks like open qa, where questions are answered from text without using a kb, broad-coverage symbolic kbs continue to be widely used in practice, and, despite recent progress in methods for ""multi-hop"" qa, are still the only computationally efficient way of answering questions that combine information for multiple documents. however, the broad-coverage kbs that are currently in wide use are largely collections of information that easily collected and integrated, and need not reflect the actual infor-mation needs of users. in this position paper, we advocate for a new approach to constructing kbs, and in particular, an approach to collecting modular, compositionally-combinable knowledge components from text, driven by a sample of user's questions and answers.","{86611921: '[Kwiatkowski et al., 2019]', 211003735: 'Wolfson et al. [2020]'}",https://arxiv.org/pdf/2207.00630v1.pdf
781,256231203,Understanding and Improving Deep Graph Neural Networks: A Probabilistic Graphical Model Perspective,conclusion and future work,Insight-tree,"in this paper, we developed a unified theoretical framework to understand the gnn baselines as well as various deep  gnn models using a graphical model representation. specifically, we obtained an iterative solution of variational inference on markov random fields, and the propagation operation of gnns can be represented as approximate forms of it. we also proposed a theoretically motivated and powerful gnn which performs well on both shallow and deep network layers. an interesting direction for future work is to establish the connection between the approximate sampling methods, and the graph neural network to pursue a faster and more powerful sample-based gnn hamilton et al., 2017;hasanzadeh et al., 2020]. to complete the picture, understanding and improving the general gnn with the help of other variational methods would also be interesting.",{},https://export.arxiv.org/pdf/2301.10536v1.pdf
782,226743717,Development of SW Interface between Healthcare Standards-DASTA and HL7,conclusions,Insight-tree,"the paper described the interface created between dasta and hl7 standards using a visual simulation presentable via a web interface that very simply simulates the process of a doctor's prescribing a drug in the ward and its automated preparation in a hospital pharmacy. the newly-created interface between dasta and hl7 (which has a working name abbreviated as idh) allows the selection of medication in a very simple simulation of the registration of a patient's medical record and treatment plan in his, a statement of the data sentence generated in the dasta standard, a statement of the data sentence in the hl7 standard from the dasta message, and showing the patient's prepared medication in a simple graphical output, simulating a robotically prepared ""circuit"" of the patient's personalized drug therapy with accompanying information about the patient and his or her medications for the next 24 h.",{},https://web.archive.org/web/20201103205833/https:/res.mdpi.com/d_attachment/sustainability/sustainability-12-07649/article_deploy/sustainability-12-07649-v2.pdf
783,247475874,E-KAR : A Benchmark for Rationalizing Natural Language Analogical Reasoning,conclusion and discussion,Insight-tree,"in this work, we propose a first-of-its-kind benchmark e-kar (in both chinese and english) for explainable analogical reasoning, which sets a concrete playground and evaluation benchmark to boost the development of human-like analogical reasoning algorithms. the e-kar benchmark is featured by its rich coverage in knowledge and welldesigned free-text explanations to rationalize the analogical reasoning process. preliminary experiments show that this benchmark provides a rather difficult challenge for prevailing language models.","{236459873: 'Aggarwal et al., 2021', 222178328: 'Jhamtani and Clark, 2020;', 204915921: 'Khot et al., 2020;', 86611921: 'Kwiatkowski et al., 2019', 220483148: 'Liu et al., 2020b', 236478211: 'Tan et al., 2021', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.findings-acl.311.pdf
784,247594506,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,conclusion,Insight-tree,"in this paper, we proposed fairr, a faithful and robust deductive reasoning model based on three modular components: rule selection, fact selection, and knowledge composition. fairr ensures causality from proof generation to entailment prediction by design. we established the effectiveness of our approach through experiments on testing robustness to language variations and demonstrating the interpretability of the errors made by our model. we also show that fairr is faster and more precise at deductive reasoning than prior baselines.  ","{211126663: 'Clark et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.acl-long.77.pdf
785,235352906,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,conclusion,Insight-tree,"in this paper, we aimed to solve the qualitative reasoning task in an interpretable manner. inspired by human cognition, we first summarized the questions into two categories, prediction and comparison. then an end-to-end trained reasoning component that contains two reasoning chains was designed. both reasoning chains contained multiple neural modules that provide transparent intermediate predictions for the understanding and reasoning process. the experimental results showed the effectiveness of our approach, and the analysis of each module and case study demonstrated the superior interpretability compared with the ""blackbox"" model. moreover, we found that some questions could be solved by both reasoning chains, thus increasing the default tolerance and generalization capability. furthermore, a human evaluation was conducted to validate the function of the synthetic text and provide an additional explanation for the superior performance achieved by our method. however, the error analysis showed the inadequacy under complicated scenarios. therefore, our future work will focus on applying interpretable reasoning on complex reasoning tasks. the annotated data and models are shared publicly 3 .","{222133899: 'Ren et al., 2020', 202539540: 'Tafjord et al., 2019b', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2106.02399v1.pdf
786,227334750,KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning,conclusion,Insight-tree,"we have proposed a pre-training method by cooperatively modeling the generative and discriminative knowledge injecting approaches. our model can be easily extended to larger pre-training corpus and does not introduce any modifications for downstream tasks during finetuning. experiments show our model consistently outperforms all base models on a variety of question answering datasets, demonstrating that our kgplm is a preferred choice for the knowledge intensive nlp tasks.  ","{204823992: 'Fisch et al. 2019', 198229624: 'Joshi et al. 2020a'}",https://arxiv.org/pdf/2012.03551v1.pdf
787,235368003,Adversarial Training for Machine Reading Comprehension with Virtual Embeddings,conclusion,Insight-tree,"we have applied adversarial training on a wide range of mrc tasks, including span-based extractive rc and multiple-choice rc. especially, we have proposed a novel adversarial training method pqat, which uses virtual p/q-embedding matrices to generate global and role-aware perturbations that consider the characteristics of mrc tasks. our experiments demonstrate that adversarial training improves the mrc model performance universally and consistently, even over the strong pre-trained model baseline. the pqat method further improves the model performance over the standard at on both normal datasets and adversarial datasets.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.starsem-1.30.pdf
788,251718892,Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering,conclusion,Insight-tree,"in this paper, we study the task of multi-hop question answering and propose to stepwise locate the single-hop supporting sentences and generate more fact-grounded single-hop questions for better interpretable multi-hop reasoning. we present a stepwise reasoning framework to incorporate both single-hop supporting sentence identification and the corresponding single-hop question generation for each intermediate step until inferring a final result. it employs a pre-trained simple question generator and takes the identified single-hop supporting sentences as base to generate the single-hop question, which obviates the necessity of constructed supervision and helps generate more fact-based single-hop questions. it utilizes a unified reader to jointly learn both intermediate hop reasoning and final hop inference for better fault tolerance. experimental results validate the general effectiveness and interpretability of our stepreasoner.","{207853300: 'Fang et al., 2019', 248780551: 'Fei et al., 2022', 239885904: 'Fu et al., 2021', 202542881: 'Glass et al., 2019', 226236740: 'Ho et al., 2020;', 202565945: 'Jiang and Bansal 2019', 221448158: 'Khot et al., 2020', 174801080: 'Min et al., 2019', 211258645: 'Perez et al., 2020', 215238353: 'Shao et al., 2020', 207870753: 'Tu et al., 2020', 158046817: 'Tu et al., 2019', 220831004: 'Zaheer et al., 2020'}",https://www.aclanthology.org/2022.coling-1.142.pdf
789,247594811,Simulating Bandit Learning from User Feedback for Extractive Question Answering,conclusion,Insight-tree,"we present a simulation study of learning from user feedback for extractive qa. we formulate the problem as contextual bandit learning. we conduct experiments to show the effectiveness of such feedback, the robustness to feedback noise, the impact of initial model performance, the trade-offs between online and offline learning, and the potential for domain adaptation. our study design emphasizes the potential for reducing annotation costs by annotating few examples or by utilizing existing datasets for new domains.","{222177817: 'Kratzwald et al. 2020', 204800552: 'Lee et al., 2019', 237364113: 'Yue et al., 2021', 244119766: 'Zhu et al., 2021'}",https://www.aclanthology.org/2022.acl-long.355.pdf
790,237347226,"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning",conclusion,Insight-tree,"we present the continual learning of few-shot learners (clif) challenge to simulate the scenario where a learner continually accumulate (generalizable) knowledge over a sequence of nlp tasks, while retaining its performance on the seen tasks. we propose evaluation protocols to study the performance of existing continual learning algorithm, and present our method bihnet-reg. we demonstrate the potentials of building a nlp system that, through continual training, can perform more tasks and also become more efficient in mastering new tasks. future works include extending our work to task agnostic scenarios where the distribution of data may shift continuously and studying algorithms for continual refinement of large-scale pre-trained models with emerging unlabeled data.   ","{102353837: 'Xu et al., 2019;'}",https://export.arxiv.org/pdf/2104.08808v4.pdf
791,96340195,Mixed-ligand complexes of copper(II) ions with L-glutamic acid in the systems with triamines and non-covalent interaction between bioligands in aqueous solution,conclusions,Insight-tree,"the main centres of interactions in the adducts forming in the systems of glu-triamine are the oxygen atoms from carboxyl groups and the nitrogen atom from the amine group of glu and the nitrogen atoms from the amine groups of triamine. moreover, these centres are also potential sites of metal ions coordination. at lower ph in the metal-free systems, the terminal amine groups of the two triamines studied, the oxygen atoms from -c (5) ooand the amine group from glu are not engaged in the weak noncovalent interactions between ligands, in contrast to the situation in the system asp-triamine [43]). thus, not only the length of the polyamine carbon chain [42][43][44] but also the length of the amino acid carbon chain influences the interactions between the bioligands. in the adducts forming above ph 7, all available active centres of the ligands are involved in the interaction and the inversion effect is observed, similarly as in the system of asp-triamine [43]). the amine groups of the polyamine could act either as positive or negative centres of interaction, depending on ph (the character of interaction depends on the degree of protonation). in the system cu(ii)-glu-triamine the following species are formed: mlhl', mll' and mll'oh (where l = glu, l' = triamine). in contrast to the situation in the systems with a shorter chain asp [42,43], no formation of ml â¦ l' type molecular complexes was observed, with polyamine in the outer coordination sphere engaged in noncovalent interactions with the anchoring binary complex ml. in the monoprotonated species, the coordination of 3,3-tri and spd to copper(ii) ions is of the same character, but in disparity to the 3,3-tri species, in the complex cu(glu)h(spd) the oxygen atoms from -c (5) ooof the amino acid are not engaged in the metallation.",{},NaN
792,258959022,Plug-and-Play Document Modules for Pre-trained Models 5 Beijing Key Laboratory of Big Data Management and Analysis Methods,conclusion,Insight-tree,"in this paper, we explore a new paradigm, which aims to represent documents as pluggable modules for ptms. in this setting, we can get rid of encoding the same document multiple times for different tasks. the extensive experiments prove that our proposed plugd can significantly reduce the computational cost and effectively inject document knowledge into ptms to improve performance. in the future, we will explore more effective plugin learning tasks and further attempt to represent knowledge graphs, and figures as plugins to provide knowledge for ptms.","{52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020;'}",https://www.aclanthology.org/2023.acl-long.875.pdf
793,236771976,MuSiQue: Multihop Questions via Single-hop Question Composition,conclusion,Insight-tree,"constructing multihop datasets is a tricky process. it can introduce shortcuts and artifacts that models can exploit to circumvent the need for multihop reasoning. a bottom-up process of constructing multihop from single-hop questions allows systematic exploration of a large space of multihop candidates and greater control over which questions we compose. we showed how to use such a carefully controlled process to create a challenging dataset that, by design, requires connected reasoning by reducing potential reasoning shortcuts, minimizing train-test leakage, and including harder distractor contexts. empirical results show that -ans has a substantially higher human-model gap and is significantly less cheatable via disconnected reasoning than previous datasets. the dataset also comes with unanswerable questions, and question decompositions which we hope spurs further work in developing models that get right answers for the right reasons.","{139103297: 'Chen and Durrett 2019', 226262208: 'Ferguson et al., 2020', 215768725: 'Groeneveld et al., 2020', 226236740: 'Ho et al., 2020', 226278099: 'Jiang et al., 2020', 174801764: 'Min et al. 2019a', 174801080: 'Min et al., 2019b;', 225066758: 'Pan et al., 2021;', 221507798: 'Petroni et al. 2021', 240288953: 'Qi et al., 2021;', 221749191: 'Trivedi et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.tacl-1.31.pdf
794,264818780,DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text,conclusion,Insight-tree,"we introduce the divknowqa, designed to evaluate the proficiency of question-answering systems, especially those enhanced by retrieval tools, in addressing knowledge-intensive questions with a strong emphasis on multi-hop multi-source retrieval.this dataset is constructed through automated data generation and subsequent human verification, minimizing manual effort.our evaluation encompasses both standard llms and llms augmented with retrieval tools.notably, we identify that this task presents a new challenge for state-of-the-art models due to the demand for structured knowledge retrieval and the inherent lack of prior knowledge in this context.to tackle this challenge, we propose the detllm, which incorporates diverse retrieval tools including innovative symbolic query generation for retrieving information from the structured knowledge source.in the future, we are keen on enhancing llms' capabilities in understanding and generating symbolic language, as well as exploring methods to improve performance on knowledge-intensive and complex question-answering tasks.","{224803601: 'Chen et al., 2021a', 215785913: 'Chen et al., 2020', 235399966: 'Chen et al., 2021b', 210859295: 'Hannan et al., 2020', 226236740: 'Ho et al., 2020', 249097975: 'Izacard et al., 2022', 251371732: 'Izacard et al., 2023;', 226278099: 'Jiang et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 233219849: 'Talmor et al., 2021', 236771976: 'Trivedi et al., 2022b;', 52822214: 'Yang et al., 2018;', 248780279: 'Yavuz et al., 2022;'}",https://export.arxiv.org/pdf/2310.20170v1.pdf
795,256662717,Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories,conclusion,Insight-tree,"in this paper we propose a new plug-in mixture-ofmemory mechanism for the retrieval augmented language models to improve their zero-shot ability on the dense retrieval task. to learn the memory mixture we develop a new joint learning approach that trains the augmentation component using the positive signals from the end task, the language model's attention scores, and  wood jr. (october 10, december 10, 1978) was an american filmmaker, actor, writer, producer, and director. hard negatives retrieved from the mixture of augmentation corpora. this leads to our final model moma (t5-ance) and moma (coco) that achieve strong zero-shot accuracy on 18 retrieval tasks included in beir. our analysis shows the importance of augmenting with diverse memory sources and in-domain information for robust generalization. we also share our observations and insights on how the model learns to leverage the augmentation information from multiple corpora during training and testing. we hope our findings and illustrations can inspire more future research in better augmenting language models, to provide other alternatives to achieve generalization ability beyond solely relying on model scale.","{245131402: 'Wang et al., 2022', 238857091: 'Xin et al. 2022'}",https://export.arxiv.org/pdf/2302.03754v1.pdf
796,230433817,Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering,conclusion,Insight-tree,"in this work we presented a comprehensive survey on the latest progress of open-domain qa (openqa) systems. in particular, we first reviewed the development of openqa and illustrated a ""retriever-reader"" architecture. moreover, we reviewed a variety of existing openqa systems as well as their different approaches. finally, we discussed some salient challenges towards openqa followed by a summary of various qa benchmarks, hoping to reveal the research gaps so as to push further progress in this field. based on  our review of prior research, we claim that openqa would continue to be a research hot-spot. in particular, single-step and multi-step neural retrievers will attract increasing attention due to the demand for more accurate retrieval of related documents. also, more end-to-end openqa systems will be developed with the advancement of deep learning techniques. knowledge enhanced openqa is very promising not only because it is helpful to generating the answer but also because it serves as the source for interpreting the obtained answer. however, how to represent and make full use of the knowledge for openqa still needs more research efforts. furthermore, to equip openqa with a dialogue-like interface that enables interaction between human users and the system for information exchange is expected to attract increasing attention, which well aligns with real world application scenarios.  ","{219017420: '[12]', 153312687: '[31]', 202660724: '[32]', 189927857: '[35]', 202773198: '[36]', 211296452: '[75]', 208267807: '[88]', 52822214: '[90]', 67855846: '[103]', 128345225: '[123]', 86611921: '[144]'}",https://arxiv.org/pdf/2101.00774v3.pdf
797,248366293,Association for Computational Linguistics,conclusion,Insight-tree,"in this paper we present seal, a novel retrieval system that combines an autoregressive language model with a compressed full-text substring index. such combination allows to constraint the generation of existing ngrams in a corpus and to jointly retrieve all the documents containing them. empirically, we show an improvement of more than 10 points in average passage-level r-precision on kilt, and establish new state-of-the-art downstream performance on 4 out 7 datasets when paired with a reader model. while our results show that seal could already compete with more established retrieval systems, we believe there is potential in exploring the use of existing (or yet to come) larger autoregressive models.","{52822214: 'Yang et al., 2018;', 221971009: 'Zhao et al., 2021;'}",https://arxiv.org/pdf/2204.10628v1.pdf
798,238198206,Single-dataset Experts for Multi-dataset Question Answering,conclusion,Insight-tree,"made combines the benefits of single-and multidataset training, resulting in better in-domain accuracy and transfer performance than either multidataset models or ensembles of single-dataset models, especially in low resource settings. for future work we plan to explore ensembling methods for better zero-shot prediction and interpolating made weights for better transfer learning. ","{204823992: 'Fisch et al., 2019', 207982228: 'Longpre et al. 2019'}",https://www.aclanthology.org/2021.emnlp-main.495.pdf
799,261245264,"LONGBENCH: A BILINGUAL, MULTITASK BENCH- MARK FOR LONG CONTEXT UNDERSTANDING",conclusion,Insight-tree,"in this paper, we introduce longbench, a multi-task bilingual benchmark tailored for gauging long context understanding abilities of llms. longbench covers six key categories and a total of 21 tasks, with data lengths extending from thousands of tokens up to tens of thousands of tokens. we also develop longbench-e which features a more evenly data length distribution. we conduct extensive experiments on longbench and longbench-e, yielding insightful conclusions about the capabilities of current llms on long context understanding. moreover, our analysis suggests that longbench and longbench-e serve as ideal testbeds for future research in long context modeling. table 6 lists the instantiation of (i, c, a) for each dataset in longbench. table 7 reports the number of data on each task that falls in the length range of 0-4k, 4k-8k, and 8k+ in longbench-e.","{234093776: 'Dasigi et al., 2021', 226236740: 'Ho et al., 2020', 249097975: 'Izacard et al., 2022a', 260440449: 'Tay et al., 2021;', 236771976: 'Trivedi et al., 2022', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020;'}",https://export.arxiv.org/pdf/2308.14508v1.pdf
800,261049520,RALLE: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models,conclusion,Insight-tree,"this paper introduces ralle, an accessible framework for developing and evaluating r-llms.we also report evaluation results of several r-llms built using open-source retrievers and llms on knowledge-intensive tasks.overall, ralle offers a significant advancement in retrieval-augmented generation research, enabling efficient development, evaluation, and improvement of r-llms.we hope that ralle will contribute to the development of best practices for r-llms.","{225040142: 'Asai and Choi, 2021', 86611921: 'Kwiatkowski et al., 2019'}",https://export.arxiv.org/pdf/2308.10633v2.pdf
801,233481241,When to Fold'em: How to answer Unanswerable questions,conclusion,Insight-tree,"our research covered the development of nlp in the past 3 years, when the attention mechanism largely displaced recurrent architectures, using the squad2.0 dataset [1] as the centerpiece of our discussion. through our experiments with bidaf [2], docqa [3] and albert retro-reader [4], we examined the architectural differences and the advantages of the using state-of-the-art pretrained models and then fine-tuning for the question-answering task. our major contribution is our specific method of re-initializing weights in a parameter-shared network. future work can include applying our re-initialization method to electra [10]. as we mentioned before, we divide the bidaf model [2] into 3 part. in the first ""embedding"" part, there are character embedding, word embeddin, and contextual embedding three layers. the words in both context and query are embedded in character-level and word-level respectively.",{},https://arxiv.org/pdf/2105.00328v1.pdf
802,253370208,Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts,conclusions,Insight-tree,"despite large lms' impressive reading abilities and the availability of large scale multi-step qa datasets requiring a rich set of reasoning skills, lmbased qa models do not reliably learn to use such skills for answering complex questions. in this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions.",{},https://www.aclanthology.org/2022.emnlp-main.439.pdf
803,8003056,Applying Length-Dependent Stochastic Context-Free Grammars to RNA Secondary Structure Prediction,conclusions,Insight-tree,we introduced an extension to the concept of stochastic context-free grammars that allows the probabilities of the productions to depend on the length of the generated subword.,{},http://www.mdpi.com/1999-4893/4/4/223/pdf
804,247085284,Deep Understanding based Multi-Document Machine Reading Comprehension,conclusions,Insight-tree,"in this paper, we propose a simple but effective deep understanding based multi-document mrc model. it uses neither any sophisticated technologies nor any pretrained language models. we evaluate our model on dureader and triviaqa web, two widely used benchmark multi-document mrc datasets. experiments show that our model achieves very competitive results on both datasets.","{226281918: 'Long et al. 2020;', 222067190: 'Luo et al. 2020;', 57721315: '[Nishida et al. 2019;', 226262276: '[Tian et al. 2020]'}",https://arxiv.org/pdf/2204.03494v1.pdf
805,258967754,Using contradictions improves question answering systems,limitations,Insight-tree,"our work comes with some limitations. it is uncertain whether our results in two specific settings, multiple choice and extractive qa, would extend to more general settings for nli, although the use of contradictions for factual consistency by laban et al. (2022) suggests that they could. additionally, 3-class nli is not sufficient to capture all the natural language relations that might be needed to verify an answer. as such more challenging datasets in other settings and more granular nli settings should be attempted.another limitation involves answer ranking and the associated computational cost. the main reason we did not test answer ranking in extractive qa is that we did not generate diverse outputs, but another reason is that such a procedure grows prohibitively expensive as the domain becomes more open. in a fully open domain, ranking would require a quadratic evaluation for each context passage against each reformulated answer candidate (schuster et al., 2022). future work should look at comparison approaches that amortize this cost, such as nli-based dense passage retrieval (reimers and gurevych, 2019).  table 1: multiple choice setting. accuracy scores (best per column in bold, second best underlined, statistical significance (pairwise students t-test) is indicated by asterix) after answer ranking with the mnli-large nli model. the top three rows show the accuracy of using only the qa models' confidence score; ""qa"" refers to the scores of the roberta-race model, which was used for calibration. the bottom rows add the entailment and/or contradiction scores to the roberta-race score. for other nli models, and for just e, c, and e+c without calibration with roberta-race, see table 8 in the appendix.",,https://www.aclanthology.org/2023.acl-short.72.pdf
806,258959258,Do Large Language Models Know What They Don't Know?,conclusion,Insight-tree,"this study investigates the self-knowledge of llms by evaluating their ability to identify unanswerable questions. through the introduction of a novel dataset and an automated method for detecting uncertainty in the models' responses, we are able to accurately measure the self-knowledge of llms such as gpt-3, instructgpt and llama. our results reveal that while these models possess a certain degree of self-knowledge, there is still an apparent disparity in comparison to human selfknowledge. this highlights the need for further research in this area to enhance the ability of llms to understand their own limitations on the unknows. such efforts will lead to more accurate and reliable responses from llms, which will have a positive impact on their applications in diverse fields.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.18153v2.pdf
807,153312687,Cognitive Graph for Multi-Hop Reading Comprehension at Scale,discussion and conclusion,Insight-tree,"we present a new framework cogqa to tackle multi-hop machine reading problem at scale.the reasoning process is organized as cognitive graph, reaching unprecedented entity-level explainability.our implementation based on bert and gnn obtains state-of-art results on hotpotqa dataset, which shows the efficacy of our framework.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/1905.05460v2.pdf
808,264146386,EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification,dicussion and conclusion,Insight-tree,"we recognize the challenge of generating highquality explanations in a realistic setting, especially in terms of no post-hoc methods and multi-hop reasoning.however, there is still significant potential for further research in leveraging the capabilities of large language models (llms) for fact-checking systems.overcoming these challenges is essential for advancing fact-checking as a whole, improving reliability, trustworthiness, and facilitating better decision-making across different domains.","{189927896: 'Jiang and Bansal, 2019', 226278099: 'Jiang et al., 2020', 221655732: 'Ostrowski et al., 2021;', 52822214: 'Yang et al. 2018'}",https://export.arxiv.org/pdf/2310.09754v1.pdf
809,248563058,Long Context Question Answering via Supervised Contrastive Learning,conclusion,Insight-tree,"in this work, we proposed an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts. we demonstrate consistent improvement when using our approach on three different models over two different benchmarks. for future work, we propose exploring variations of our proposed supervised loss on other long-context tasks, such as longdocument and multi-document summarization, and integrating our method into information retrieval re-ranker models. depends on the size of the model). in order to determine the temperature hyperparameter Ï , we searched over {0.2, 0.4, 0.6, 0.8, 1.0} per question type (if applicable). we also applied dropout with a rate of p = 0.1 over the linear projections, which consistently improved the results over all the benchmarks. finally, we searched for the best performing Î» hyperparameter over the values of {0.2, 0.4, 0.6, 0.8, 1.0}.","{234093776: 'Dasigi et al. 2021', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020;'}",https://www.aclanthology.org/2022.naacl-main.207.pdf
810,224803601,OPEN QUESTION ANSWERING OVER TABLES AND TEXT,conclusion,Insight-tree,"we focus on the problem of performing open question answering over tables and text in this paper. one interesting question we would like to ask in the future is: can we extend open question answering system to more modalities? some questions can be better answered by images and other resources, but the task can be drastically more challenging by including more modalities, as we have learned from this paper. finally, we believe the techniques we proposed might be useful for other open-qa setting, especially the comparisons between iterative retriever and fusion retriever. ","{215828216: 'Ainslie et al., 2020', 208267807: 'Asai et al., 2019', 211296452: 'Dhingra et al., 2019', 153312687: 'Ding et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 202773198: 'Qi et al., 2019;', 128345225: 'Sun et al., 2019;'}",https://arxiv.org/pdf/2010.10439v1.pdf
811,229923812,HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions,conclusion,Insight-tree,"in this paper, we propose the hopretriever to collect reasoning evidence over wikipedia for multi-hop question answering. both the structured knowledge indicated by hyperlinks and the unstructured knowledge presented as introduc-tory documents in wikipedia, are involved and leveraged together in hopretriever to help the evidence collection. the experiment on the hotpotqa dataset shows that the performance of hopretriever improved observably as a result of combining the structured knowledge with unstructured knowledge, and outperforms all the published models on the leaderboard. moreover, by inspecting the proportion of the two kinds of knowledge in hops, which kind of knowledge leads the retrieving of each evidence piece can be observed directly, which also provides extra intuitive interpretations for the selection of each evidence.","{208267807: 'Asai et al. 2020', 202583433: 'Das et al. 2019b', 211296452: 'Dhingra et al. 2020', 153312687: 'Ding et al. 2019', 189927857: 'Feldman and El-Yaniv 2019;', 174801080: 'Min et al. 2019', 202660724: 'Nie et al. 2019', 211003735: 'Wolfson et al. 2020', 52822214: 'Yang et al. 2018'}",https://arxiv.org/pdf/2012.15534v1.pdf
812,258546861,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,conclusions,Insight-tree,"in this work, we propose a modular model chain-of-skills (cos) that learns five reusable skills for odqa via multi-task learning. to reduce task interference, we design a new parameterization for skill modules. we also show that skills learned by cos can be flexibly chained together to better fit the target task. cos can directly perform superior zero-shot retrieval using multitask self-supervision on wikipedia. when finetuned on multiple datasets, cos achieves sota results across the board. for future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (talmor et al., 2022) and biomedical retrieval (nentidis et al., 2020;zhang et al., 2022b). group at microsoft research for their helpful discussions and anonymous reviewers for their valuable suggestions on this paper.","{220302524: 'Xiong et al., 2021a;', 52822214: 'Yang et al., 2018', 234778323: 'Zhang et al., 2021b', 247447562: 'Zhou et al., 2022', 237502990: 'Zhu et al., 2021'}",https://www.aclanthology.org/2023.acl-long.89.pdf
813,238354350,Transfer Learning for Multi-lingual Tasks -a Survey,conclusion,Insight-tree,"this survey provides an comprehensive overview of the existing studies on leveraging transfer learning models to tackle the multi-lingual and cross-lingual tasks. in addition to the models, we also reviewed the main available datasets in the community and investigated different approaches in term of the architectures and applications to identify the existing research challenges in the domain and later we provide few potential future directions.",{},https://arxiv.org/pdf/2110.02052v1.pdf
814,236477336,Latent Reasoning for Low-Resource Question Generation,conclusions,Insight-tree,"we proposed a jointly optimized two-phase model named plar for low-resource question generation. plar effectively utilizes non-parallel singlehop and multi-hop question answering data to perform optimization. we further designed a planning mechanism to guide the generation process of subquestions so that the generation results are valid to compose a multi-hop question. experimental results confirm that plar achieves better performance compared with the state-of-the-art under various metrics, especially in a question answering based evaluation. for future work, we will explore the heterogeneous multi-hop qg task that requires reasoning beyond plain texts, e.g., tables.","{211296452: 'Dhingra et al., 2020', 174801080: 'Min et al., 2019;', 216553210: 'Pan et al., 2020', 211258645: 'Perez et al. 2020', 211003735: 'Wolfson et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.findings-acl.265.pdf
815,247158553,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",conclusion,Insight-tree,"this paper introduces kmir benchmark for evaluating knowledge memorization, identification, and reasoning abilities. kmir includes a systematic evaluation syllabus to summarize the knowledgerelated abilities of plms, and has 184,348 questions involving 4 types of questions covering 3 types of knowledge. we also find many interesting phenomena through extensive experiments: 1) the memorization ability of plms depends more on the number of parameters than training schemes. 2) current plms are struggling to robustly remember the facts. 3) model compression technology retains the amount of knowledge well, but hurts the identification and reasoning ability, etc.","{153312687: 'Ding et al., 2019;', 220483148: 'Liu et al., 2020'}",https://arxiv.org/pdf/2202.13529v1.pdf
816,252819049,To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning,conclusions and future work,Insight-tree,"in this work, we present a novel framework, which can diagnose the correlation between the nlu dataset and a specific skill and we probe a fundamental reasoning skill, logical reasoning, on 11 nlu datasets. our framework involves a logical probe to conduct diagnosis and defines a qualitative process and a quantitative process to calculate two indicators. from the results, we observe that 1) most nli datasets have a relatively strong correlation with logical reasoning. 2) the correlations between type 1 mrc datasets and logical reasoning are moderate because logical reasoning is not the only dominant skill in these datasets. 3) the dependences of type 2 mrc datasets are not always exactly consistent with their intended purpose. based on the analysis, although there are several limitations in our proposed method, this work is still a reasonable attempt to deeply understand the relationship between the dataset and a specific nlu skill. in future works, we will focus on: 1) exploring the solution to the limitations of the proposed method; 2) build associations for different datasets that require the same nlu capabilities.","{67855846: 'Dua et al., 2019', 204915921: 'Khot et al., 2020', 208201969: 'Sugawara et al. 2020', 202539540: 'Tafjord et al., 2019', 243865235: 'Tian et al., 2021', 52822214: 'Yang et al., 2018;', 52895001: 'Yatskar, 2019', 219965751: 'Zeng et al., 2020'}",https://www.aclanthology.org/2022.coling-1.147.pdf
817,226262367,EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering,conclusion and future work,Insight-tree,"we presented eÏÎ±Âµs, a new challenging crosslingual and multilingual benchmark for science qa in 16 languages and 24 subjects from high school examinations.","{52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2011.03080v1.pdf
818,235790370,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,contemporary limitations and challenges,Insight-tree,"environment complexity is limited, and it's currently difficult to author complex worlds. two competing needs are currently at odds: the desire for complex environments to learn complex skills, and the desire for environment variation to encourage robustness in models. current tooling emphasizes creating varied procedural environments, but those environments have limited complexity, and require agents to complete straightforward tasks. economically creating complex, interactive environments that simulate a significant fraction of real world interactions is still well beyond current simulators or libraries -but required for higher-fidelity interactive worlds that have multiple meaningful paths toward achieving task goals. generating these environments semi-automatically (e.g. ammanabrolu et al., 2020a) may offer a partial solution. independent of tooling, libraries and other middleware offer near-term solutions to more complex environment modeling, much in the same way 3d game engines are regularly coupled with physics engine middleware to dramatically reduce the time required to implement forces, collisions, lighting, and other physics-based modeling. currently, few analogs exist for text worlds. the addition of a chemistry engine that knows ice warmed above the freezing point will change to liquid water, or a generator engine that knows the sun is a source of sunlight during sunny days, or an observation engine that knows tools (like microscopes or thermometers) can change the observation model of a pomdp -may offer tractability in the form of modularization. efforts using large-scale crowdsourcing to construct knowledge bases of commonsense knowledge (e.g., atomic, sap et al., 2019) may be required to support these efforts.current planning languages offer a partial solution for environment modelling. while simulators partially implement facilities for world modeling, some (e.g. cÃ´tÃ© et al., 2018;shridhar et al., 2020b) suggest using mature planning languages like strips (fikes and nilsson, 1971) or pddl (mcdermott et al., 1998) for more full-featured modeling. this would not be without significant development effort -existing implementations of planning languages typically assume full-world observability (in conflict with pomdp modelling), and primarily agent-directed state-space changes, making complex world modeling with partial ob-servability, and complex environment processes (such as plants that require water and light to survive, or a sun that rises and sets causing different items to be observable in day versus night) outside the space of being easily implemented with off-the-shelf solutions. in the near-term, it is likely that a domain-specific language specific to complex text world modeling would be required to address these needs while simultaneously reducing the time investment and barrier-to-entry for end users.analyses of environment complexity can inform agent design and evaluation. text world articles frequently emphasize agent modeling contributions over environment, methodological, or analysis contributions -but these contributions are critical, especially in the early stages of this subfield. agent performance in easy environments has increased incrementally, while medium-to-hard environments have seen comparatively modest improvements. agent performance is typically reported as a distribution over a large number of environments, and the methodological groundwork required to understand when different models exceed others in time or performance over these environment distributions is critical to making forward progress. transfer learning in the form of training on one set of environments and testing on others has become a standard feature of benchmarks (e.g. hausknecht et al., 2020), but focused contributions that work to precisely characterize the limits of what can be learned from (for example) omniquest and transferred to zork, and what capacities must be learned elsewhere, will help inform research programs in agent modeling and environment design.transfer learning between text world and 3d environments. tasks learned at a high-level in text worlds help speed learning when those same models are transferred to more complex 3d environments (shridhar et al., 2020b). this framing of transfer learning may resemble how humans can converse about plans for future actions in locations remote from those eventual actions (as when we apply knowledge learned in classrooms to the real world). as such, text-plus-3d environment rendering shows promise as a manner of controlling for different sources of complexity in multi-modal task learning (from high-level task-specific knowledge to low-level perceptual knowledge), and appears a promising research methodology for imparting complex task knowledge on agents that are able to navigate high-fidelity virtual environments.","{52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.wordplay-1.1.pdf
819,248827723,A STEP towards Interpretable Multi-Hop Reasoning: Bridge Phrase Identification and Query Expansion,conclusion,Insight-tree,"we proposed an unsupervised approach for the identification of bridge phrases in multi-hop question answering. our method constructs a graph of noun phrases from the question and the available context, and applies the steiner tree algorithm to identify the minimal subgraph that connects all question phrases. we extract as bridge phrases nodes in this graph that are not any of the question phrases. our method can be coupled with any downstream qa component, i.e., it can be used as query expansion for evidence retrieval; it can be used to generate enhanced context for answer prediction; and it can be used to generate post-hoc explanations for given answers. using the hotpotqa dataset, we demonstrate that our method yields improved results in all these scenarios, for multiple types of downstream components. 5. ravi sethi: he is best known as one of three authors of the classic computer science textbook """", also known as the ""dragon book"".",{},NaN
820,245769925,Does Entity Abstraction Help Generative Transformers Rea- son?,conclusion,Insight-tree,"conclusion. we presented various ways to incorporate abstract knowledge into transformer language models. focusing on entity types, this work evaluated model performance on reasoning tasks requiring compositional generalization and multi-hop reasoning. overall our results demonstrate three things: (i) incorporating abstract knowledge significantly improves reasoning and compositional generalization in both interpolation and extrapolation when the environment is formally defined in a logical reasoning setting; (ii) different ways to incorporate abstraction yields different performance boosts: enc-sum and dec-loss are generally performing better than others; (iii) abstraction is not beneficial when the task at hand is more natural, less procedural, and not requiring long reasoning chains. this last result is due to the noisy entity tagging from ""off-the-shelf"" taggers, and due to the nature of the task at hand.","{160009340: 'Nishida et al., 2019', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2201.01787v2.pdf
821,225067214,READONCE Transformers: Reusable Representations of Text for Transformers,conclusion,Insight-tree,"this work introduced readonce transformers, a novel approach for using large scale transformerbased language models to both build and consume reusable document representations. akin to humans' ability to read a document and extract useful information without knowing the enduse, readonce representations are compact, information-capturing document representations that can be pre-computed once, in a task-and example-independent fashion.","{215768725: 'Groeneveld et al., 2020', 222208820: 'Wang et al., 2020', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020'}",https://www.aclanthology.org/2021.acl-long.554.pdf
822,238856994,Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,conclusion and future work,Insight-tree,"in this work, we present elue, which is a public benchmark and platform for efficient models, and elasticbert, which is a strong baseline (backbone) for efficient static (dynamic) models. both of the two main contributions are aimed to build the pareto frontier for nlu tasks, such that the position of existing work can be clearly recognized, and future work can be easily and fairly measured. our future work is mainly in four aspects: (1) including more baselines in elue, (2) supporting the evaluation for more frameworks such as tensor-flow (abadi et al., 2016)","{52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.naacl-main.240.pdf
823,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,conclusion,Insight-tree,"we conduct an extensive evaluation of the robustness of different model and adaptation methods on 15 distribution shifts in question answering. our in-depth analysis suggests several concrete directions for future work: improving the in-distribution performance of icl methods and understanding why different few-shot fine-tuning methods yield the squadshifts wiki dataset is derived from the same data source (wikipedia) as squad. as a result, models lie closer to the y = x diagonal than on other distribution shifts. (middle) progress on squad is a weaker indicator for progress on searchqa for fully fine-tuned models and few-shot fine-tuned models. we find that zero-shot and icl models are less robust than fine-tuned and few-shot models with the exception of larger language models. (right) on the squadâdrop distribution shift, we observe that progress beyond 70 f1 on squad yields quick progress on drop for fine-tuned models.","{216867120: 'Miller et al. 2020', 230433978: 'Ram et al. 2021b', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2210.12517v1.pdf
824,253098080,Exploring The Landscape of Distributional Robustness for Question Answering Models,limitations,Insight-tree,"experimenting with different in-distribution datasets. we choose squad as a representative in-distribution dataset since it is one of the largest and most popular qa datasets. one limitation of squad is that the training set is mainly collected from wikipedia articles which may not be optimal for building a qa model that generalizes to many domains. future work could explore the robustness of models trained on datasets from other domains for increased coverage.specialized modeling methods. our work does not evaluate models with task or data specific components. as an example andor et al. (2019) improved performance on drop (dua et al., 2019) by using arithmetic programs to improve a model's mathematical reasoning. evaluating the robustness of methods like these are an exciting area for future investigations.few-shot gpt evaluations. our results indicate that large gpt models fine-tuned on a smaller number of samples are more robust to distribution shifts compared to other few-shot fine-tuned models that use a prompt or span prediction. however, gpt-2 xl and gpt-neo, which both have more than one billion parameters, are larger than all few-shot models we evaluate. future work could examine the impact of architecture on this trend by evaluating other models with more than a billion parameters like t5.multiple fine-tuning runs. for fine-tuned models we include a single data-point for each model. however, previous work (phang et al., 2018;dodge et al., 2020) has shown that different data ordering and weight initialization can lead to large variance in model performance. in figure 11 we evaluate the robustness of roberta large models fine-tuned with different data ordering and initialization for the span prediction head (devlin et al., 2019). we find that on average the robustness of these models does not differ substantially. further investigation into the effect of random seeds on robustness would improve our understanding of the robustness of individual data points. as at most other universities, notre dame's students run a number of news media outlets. the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. question: how many student news papers are found at notre dame? answer: three figure 9: a sample from squad with the input formatting used for fine-tuning decoder-only models, incontext learning, and zero-shot inference. 14 figure 10: a sample from squad with the input formatting used for fine-tuning decoder-only models, incontext learning, and zero-shot inference.","{216867120: 'Miller et al. 2020', 230433978: 'Ram et al. 2021b', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2210.12517v1.pdf
825,246441887,Active Learning Over Multiple Domains in Natural Language Tasks,conclusion,Insight-tree,"we examine a challenging variant of active learning where target data is scarce, and multiple shifted domains operate as the source set of unlabeled data. for practitioners facing multi-domain active learning, we benchmark 18 acquisition functions, demonstrating the h-divergence family of methods and our proposed variant dal-e achieve the best results. our analysis shows the importance of example selection in existing methods, and also the surprising potential of domain budget allocation strategies.",{},https://arxiv.org/pdf/2202.00254v2.pdf
826,251280129,SPANDROP: Simple and Effective Counterfactual Learning for Long Sequences,conclusion,Insight-tree,"in this paper, we presented spandrop, a simple and effective method for learning from long sequences, which ablates parts of the sequence at random to generate counterfactual data to distill the sparse supervision signal that is predictive of the desired output. we show via theoretical analysis and carefully designed synthetic datasets that spandrop and its variant based on the beta-bernoulli distribution, beta-spandrop, help models achieve competitive performance with a fraction of the data by introducing diverse augmented training examples, and generalize better to previously unseen data. our experiments on four real-world nlp datasets confirm these theoretical findings, and demonstrate span-drop's efficacy on strong neural models even when data is abundant.","{226281978: 'Tay et al., 2020', 207870753: 'Tu et al., 2020', 189898081: 'Yao et al., 2019', 220831004: 'Zaheer et al., 2020'}",https://export.arxiv.org/pdf/2208.02169v1.pdf
827,258823156,Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval,conclusion and future work,Insight-tree,"we demonstrate that query representations can be improved using feedback from a cross-encoder reranker at inference time for better performance of dual-encoder retrieval. this work proposes for distillation using relevance feedback from the re-ranker as a better and faster alternative to the traditional strategy of re-ranking a larger pool of candidates for improving recall. our proposed distillation process is lightweight and im-proves retrieval accuracy across different domains, languages and modalities over a state-of-the-art retrieve-and-rerank pipeline with comparable latency. future work will explore relevance feedback for token-level query representations as well as disentangling term importance scores from query representations for better interpretability.","{233296016: 'Thakur et al., 2021', 220302524: 'Xiong et al., 2020'}",https://export.arxiv.org/pdf/2305.11744v1.pdf
828,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,probe considerations and limitations.,Insight-tree,"the continued growth of probing-based papers has also led to recent work examining best practices for probes, and how to interpret their results. hewitt and liang [66] considered how to ensure that a probe is truly reflective of the underlying information present in a model, and proposed the use of a control task, a randomised version of a probe task in which high performance is only possible by memorisation of inputs. hence, a faithful probe should perform well on a probe task and poorly on a corresponding control task if the underlying model does indeed contain the information being probed for. the authors found that most probes (including linear classifiers)are over-parameterised, and discuss methods for constraining complex probes (e.g. multilayer perceptrons) to improve faithfulness while still allowing them to achieve similar results.while most papers we have discussed above follow the intuition that probes should avoid complex probes to prevent memorisation, pimentel et al. [127] suggest that instead the probe with the best score on a given task should be chosen as the tightest estimate, since simpler models may simply be unable to extract the linguistic information present in a model, and such linguistic information cannot be 'added' by more complex probes (since their only input are hidden representations). in addition, the authors argue that memorisation is an important part of linguistic competence, and as such probes should not be artificially punished (via control tasks) for doing this. recent work has also presented methods that avoid making assumptions about probe complexity, such as mdl probing [101,170], which directly measures 'amount of effort' needed to achieve some extraction task, or directprobe [198], which directly examines intermediate representations of models to avoid having to deal with additional classifiers.finally, hall maudslay et al. [59] compared the structural probe [67] with a lightweight dependency parser (both given the same inputs), and demonstrate that the parser is generally able to extract more syntactic information from bert embedding. in contrast, the probe performs better with a different metric, showing that the choice of metric is important for probes: when testing for evidence of linguistic information, one should not only consider the nature of the probe, but also the metric used to evaluate it. furthermore, the significance of well-performing probes is not clear: models may encode linguistic information not actually used by the end-task [138], showing that the presence of linguistic information does not imply it is being used for prediction. more causal approaches such as amnesiac probing [50], which directly intervene in the underlying model's representations, may better distinguish between these cases.","{236459873: '3', 237502773: '71]', 207870753: '165]', 237258250: '[177]', 225068329: '[178]'}",https://export.arxiv.org/pdf/2103.11072v2.pdf
829,232307813,Local Interpretations for Explainable Natural Language Processing: A Survey,discussion and conclusion,Insight-tree,"this paper focused on the local interpretable methods commonly used for natural language processing models. in this survey, we have divided these methods into three different categories based on their underlying characteristics: 1) explaining the model's outputs from the input features, where these features could be identified through rationale extraction, perturbing inputs, traditional attribution methods, and attention weight extraction; 2) generating the natural language explanations corresponding to each inputs; 3) using diagnostic classifiers to analyse the hidden information stored within a model. for each method type, we have also outlined the common datasets used for different nlp tasks and different evaluation methods for examining the validity and efficacy of the explanations provided.","{236459873: '3', 237502773: '71]', 207870753: '165]', 237258250: '[177]', 225068329: '[178]'}",https://export.arxiv.org/pdf/2103.11072v2.pdf
830,207847382,Ask to Learn: A Study on Curiosity-driven Question Generation,conclusions,Insight-tree,"asking inquisitive questions allows humans to learn from each other and increase their knowledge. we thus proposed a new task: curiosity-driven question generation, which attempts to address such a key component for several human-machine interaction scenarios. in absence of data directly usable for this task, we proposed an automatic method to derive it from conversational qa datasets. further, recognizing that the great majority of qa datasets are not conversational, we also extended the method to standard qa data. our experiments, which include learning strategies such as pretraining and reinforcement, show promising results under both automatic and human evaluation. in future works, we plan to extend the approach to conditional generation of curiosity-driven questions.","{67855846: 'Dua et al., 2019', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/2020.coling-main.202.pdf
831,229923926,Coreference Reasoning in Machine Reading Comprehension,conclusions,Insight-tree,"we show that the high performance of recent models on the quoref dataset does not necessarily indicate that they are adept at performing coreference reasoning, and that qa based on coreference reasoning is a greater challenge than current scores suggest. we then propose a methodology for creating a dataset that better presents the coreference reasoning challenge for mrc. we provide our methodology to an annotator and create a sample dataset. our analysis shows that our dataset contains fewer biases compared to quoref, and the performance of state-of-the-art quoref models drops considerably on this evaluation set.","{202539031: 'Clark et al., 2019a;', 207756753: 'Nie et al., 2020;', 208201969: 'Sugawara et al., 2020', 219978758: 'Wu et al., 2020a;', 52822214: 'Yang et al., 2018a;'}",https://www.aclanthology.org/2021.acl-long.448.pdf
832,235694196,Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems,conclusion,Insight-tree,"the common paradigm in the natural language processing community to develop models for a new benchmark is to either train a new model or fine-tune a pre-trained one. in addition to their high computational costs and environmental effects [8], their accuracies drop significantly for a new data distribution [5]. in this paper, we investigated    the effect of light-weight ensemble-based approach on the generalization of machine reading comprehension models to out-of-distribution data. the experiments were conducted using eight different datasets, six mrc models, and three settings including heterogeneous (different base models with the same training dataset), homogeneous (the same base model trained on different datasets), and hybrid.","{173188058: '[5]', 233990699: '[9]', 52822214: '[15]', 67855846: '[16]', 153312687: '22,', 203610341: '23,', 57721315: '26,', 158046817: '29,', 207870753: '30,', 208000835: '34,', 202539031: '[43]', 204823992: '[44]'}",https://arxiv.org/pdf/2107.00368v2.pdf
833,253510370,World Knowledge in Multiple Choice Reading Comprehension,limitations,Insight-tree,"we propose an approach that can automatically flag questions that can be answered without contextual information. however, the remaining questions are not necessarily high-quality questions, since many other aspects make up question quality. second, the experiments are conducted using only the electra model, though it is expected similar trends will be picked up by alternative transformer-based language models. further, exams might be aimed at a level where a lack of specific knowledge may be assumed. our work does not consider variable candidate knowledge levels, and our evaluation was only done by highly educated (we'd like to think) graduate students. finally, we acknowledge that our human evaluation was limited in size and questions, however it is clearly demonstrated that for low 'shortcut entropy' questions, comprehension is not necessarily required.","{248779897: 'Raina and Gales, 2022', 208201969: 'Sugawara et al., 2020;', 234335834: 'Wang et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2211.07040v2.pdf
834,260154895,GPT-3 MODELS ARE FEW-SHOT FINANCIAL REASONERS,conclusion,Insight-tree,"financial analysis is an important method for assessing the performance of firms. advanced quantitative analyses are used by practitioners to answer financial queries on reports and make lucrative investment decisions. as a result, financial question answering (qa) is an important question-answering task requiring in-depth numerical reasoning. a retriever must extract critical details about the financial issue from the text, and a generator must construct a legitimate computational tree and a final response. large language models like gpt-3 have recently achieved the state-of-the-art performance on similar tasks. however, due to the nature of financial inquiries and the extensive information held in financial documents, we come up with a financial qa system that requires a retriever and program generator. our work produces highly promising results in terms of answers to detailed numerical financial questions that may contribute as a supporting tool to the finance industry.",{},https://export.arxiv.org/pdf/2307.13617v2.pdf
835,234334701,REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training,conclusion and future work,Insight-tree,"in this paper, we present a novel pre-training approach, rept, to bridge the gap between pretrained language models and machine reading comprehension through retrieval-based pre-training. specifically, we design two retrieval-based pretraining tasks equipped with self-supervised learning, namely surrounding sentences prediction (ssp) and retreval based masked language modeling (rmlm), to enhance plms with the capability of evidence extraction for mrc. the experiments over five different datasets validate the effectiveness of our proposed method. in the future, we plan to extend the proposed pre-training approach to the more challenging open-domain settings.","{208267807: 'Asai et al. 2020', 67855846: 'Dua et al., 2019', 198229624: 'Joshi et al., 2020', 52822214: 'Yang et al., 2018', 215768766: 'Ye et al., 2020'}",https://arxiv.org/pdf/2105.04201v2.pdf
836,248780469,MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,limitations and future work,Insight-tree,"although the proposed mt2net model outperforms other baseline models, it still performs significantly worse than human experts, which reflects the challenge of multihiertt. primarily, we find that models do not perform well on certain types of questions: 1) questions requiring reasoning across multiple tables; 2) questions requiring multi-step reasoning; 3) questions requiring reasoning over tables with complex hierarchical structures; and 4) questions requiring external financial knowledge.to deal with these challenges, we believe that four main directions of work may be workable: 1) designing a specialized module to handle multitable reasoning; 2) decomposing a complex question requiring multi-step reasoning into several simpler sub-questions that qa models can handle (perez et al., 2020;chen et al., 2020); 3) applying a more advanced table-encoding method. for example, a pre-trained model with specialized table structure-aware mechanisms cheng et al., 2021a;yang et al., 2022) can be utilized in the facts retrieving module to better understand hierarchical tables; and 4) leveraging structured knowledge (xie et al., 2022) to inject external financial knowledge to models.","{202539031: 'Clark et al., 2019;', 67855846: 'Dua et al., 2019', 189927896: 'Jiang and Bansal, 2019;', 232478685: 'Nan et al., 2022', 211258645: 'Perez et al., 2020;'}",https://www.aclanthology.org/2022.acl-long.454.pdf
837,248780469,MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,conclusion,Insight-tree,"we have proposed multihiertt, a new largescale qa dataset that aims to solve complicated qa tasks that require numerical reasoning over documents containing multiple hierarchical tables and paragraphs. to address the challenge of multi-hiertt, we introduce a baseline framework named mt2net. the framework first retrieves supporting facts from financial reports and then generates executable reasoning programs to answer the question. the results of comprehensive experiments showed that current qa models (best f 1 : 38.43%) still lag far behind the human expert performance (f 1 : 87.03%). this motivates further research on developing qa models for such complex hybrid data with multiple hierarchical tables.","{202539031: 'Clark et al., 2019;', 67855846: 'Dua et al., 2019', 189927896: 'Jiang and Bansal, 2019;', 232478685: 'Nan et al., 2022', 211258645: 'Perez et al., 2020;'}",https://www.aclanthology.org/2022.acl-long.454.pdf
838,225066758,Unsupervised Multi-hop Question Answering by Question Generation,conclusion and future works,Insight-tree,"in this work, we study unsupervised multi-hop qa and propose a novel framework mqa-qg to generate multi-hop questions via composing reasoning graphs built upon basic operators. the experiments show that our model can generate human-like questions that help to train a well-performing multi-hop qa model in both the unsupervised and the fewshot learning setting. further work is required to include more flexible paraphrasing at the fusion stage. we can also design more reasoning graphs and operators to generate more complex questions and support more input modalities.","{215785913: 'Chen et al. 2020b', 207853300: 'Fang et al., 2020', 204823992: 'Fisch et al., 2019', 198229624: 'Joshi et al., 2020', 211258645: 'Perez et al., 2020', 211258652: 'Puri et al., 2020', 155100120: 'Qiu et al., 2019;', 211003735: 'Wolfson et al. 2020', 202583429: 'Xiong et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2021.naacl-main.469.pdf
839,260865960,BOLAA: BENCHMARKING AND ORCHESTRATING LLM-AUGMENTED AUTONOMOUS AGENTS,conclusion and future work,Insight-tree,"in this paper, we systematically investigate the performances of various laa architecture paired with different llm backbones. we also provide one novel orchestrating method for multiple agents, i.e. bolaa. the benchmarking results provide experimental justification for the laa investigation and verify the potential benefits of bolaa architecture. during the investigation, we also identify the challenge of designing bolaa architecture for environments with compounding actions. in the future, we will explore whether we can harness llms in the controller such that selection and communication with labor agents is also fully autonomous. we will continue developing more laa architectures and include more llms and environments for evaluations.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2308.05960v1.pdf
840,218486765,Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset,conclusion,Insight-tree,"we study the clinical reading comprehension (clinirc) task with the recently created emrqa dataset. our qualitative and quantitative analysis as well as exploration of the two desired aspects of clinirc systems show that future clinical qa datasets should not only be large-scale but also less noisy and more diverse. moreover, questions that involve complex relations and are across different domains should be included, and then more advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated.  ","{202572622: 'Jin et al. 2019b', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.410.pdf
841,261341823,Learning on Structured Documents for Conditional Question Answering,conclusion and limitations,Insight-tree,"in this paper, we present learning on structured documents (lsd), a self-supervised learning method for conditional question answering. lsd uses a conditional question generation method to leverage massive structured documents while improving conciseness, and applies contrastive learning to learn effective semantic representations from complex documents. we further propose a pipeline that could generate multiple answers and conditions to better handle the cqa task. we verify the effectiveness of the proposed method on the conditionalqa dataset. for future work, we plan to investigate how to better generate conditional questions and improve our model's performance in providing correct answers. despite the effectiveness of lsd in utilizing the structure of massive unsupervised data, there are still some potential points for improvement. one issue is that the state generator is only trained on answerable questions, leading to a distribution bias that there might be unanswerable questions. in addition, our pipeline can still not handle the position where a sentence has more than one answer, which limits our model's performance for broader scenarios. we will resolve these issues in future work. table 7: statistics of our scraped dataset. we present document count, average document length measured by word (avg. w) and sentences (avg. s), average sentence length (avg w/s) and tag distribution (h:p:li/tr).","{86611921: 'Kwiatkowski et al., 2019', 238744031: 'Sun et al., 2022a', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.ccl-1.51.pdf
842,258556908,VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens,conclusions,Insight-tree,"we propose a vip-token centric sequence compression method to compress/decompress the input/output sequences of transformer layers thereby reducing the complexity dependency on the sequence length n without sacrificing the model accuracy. specifically, we design the compression our empirical evaluation shows that our method can be directly incorporated into existing pretrained models with some additional training. also, it often has much higher efficiency compared to baselines with the same sequence length while offering better or competitive model accuracy. for future work, we believe that extending our method to the decoder of the encoder-decoder models will further boost the efficiency of transformers while maintaining similar model performance.    we provide a table 4 of notations that are used for more than once so that the readers can refer to their definition easily.","{52822214: '[34]', 220831004: '[35]'}",https://export.arxiv.org/pdf/2305.04241v2.pdf
843,253098174,Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge,conclusion,Insight-tree,"in this paper, we present a new framework, core, for odqa over heterogeneous knowledge sources.","{221845203: 'Ainslie et al., 2020', 215785913: 'Chen et al., 2020b', 153312687: 'Ding et al., 2019;', 189927857: 'Feldman and El-Yaniv, 2019;', 230799347: 'Geva et al., 2021', 202660724: 'Nie et al., 2019;', 240288953: 'Qi et al., , 2021', 202773198: 'Qi et al., 2019', 128345225: 'Sun et al., 2019', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2210.12338v1.pdf
844,245218668,Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering,conclusion,Insight-tree,"in this work, we propose an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts. we demonstrate consistent improvement when using our approach on three different models over two different benchmarks. in future work, we would like to explore variations of our proposed supervised loss on other long-context tasks, such as long document summarization.","{234093776: 'Dasigi et al. 2021', 226281978: 'Pang et al., 2021', 52822214: 'Yang et al., 2018b', 220831004: 'Zaheer et al., 2020;'}",https://arxiv.org/pdf/2112.08777v1.pdf
845,201668869,Propagate-Selector: Detecting Supporting Sentences for Question Answering via Graph Neural Networks,conclusion,Insight-tree,"in this paper, we propose a graph neural network that finds the sentences crucial for answering a question. the experiments demonstrate that the model correctly classifies supporting sentences by iteratively propagating the necessary information through its novel architecture. we believe that our approach will play an important role in building a qa pipeline in combination with other mrqa models trained in an end-to-end manner.",{},https://www.aclweb.org/anthology/2020.lrec-1.664.pdf
846,257771900,Explicit Planning Helps Language Models in Logical Reasoning,conclusion,Insight-tree,"in this paper, we presented leap, an lm-based logical reasoning system that integrates explicit planning into the inference method.we also proposed a method that learns to prevent the explicit planning from being misguided.our proposed methods exhibit intriguing technical connections to other reasoning systems and can be likened to the deliberative system 2 in ""dual process"" theories of reasoning.in our experiments, our planning-based system outperforms strong baseline methods including the selection-inference method and chain-of-thought prompting.we will discuss several exciting avenues for further improvements in appendix a.","{246015349: 'Bostrom et al. 2022', 237450610: 'Bostrom et al. 2021', 123758373: 'Chen and Durrett, 2019;', 211126663: 'Clark et al., 2020', 233297051: 'Dalvi et al., 2021', 174801764: 'Min et al., 2019', 222141025: 'Saha et al. 2020', 219573621: 'Talmor et al., 2020', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2303.15714v3.pdf
847,264590451,PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter,conclusion,Insight-tree,"in conclusion, this research successfully introduces a prca-based paradigm for reqa tasks, tackling the inherent challenges of fine-tuning llms in the retrieval-enhancement framework, especially given their vast parameter size and closed-source natures.prca innovatively distills retrieved documents via generator rewards, leading to a marked improvement in the reqa task's performance.experimental outcomes consistently demonstrate the robustness and effectiveness of prca when paired with various retrievers and generators, indicating its potential to be widely deployed as an adapter on the reqa task.","{249097975: 'Izacard et al., 2022a', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.18347v1.pdf
848,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,conclusions,Insight-tree,"in this paper we investigate whether meta-learning applied to multi-task instructional learning (mtil) can boost the generalizability of lms to unseen tasks in a zero-shot setting. specifically, we eval-  figure 6: % differences from standard training with other models with bart-large. hnet has the best performance for the strong generalization set.",{},https://www.aclanthology.org/2022.emnlp-main.456.pdf
849,253080893,Boosting Natural Language Generation from Instructions with Meta-Learning,limitations,Insight-tree,"there are several limitations of the proposed metalearning based approaches in its present form.â¢ computation and memory overhead: metalearning approaches have higher resource requirements which can limit the usage specially for larger models. for example with bartlarge, the hnet-maml model on a single gpu is inefficient to train since we have to use small batch sizes which leads to lower performance.â¢ regressions with easy tasks: we see some regression in metrics for the easy tasks. further analysis and research is needed to understand the factors and improve the models such that model enhancements are uniform across tasks.â¢ hyper-parameter tuning: meta learning models have more hyper-parameters and thus might be more difficult to tune than the standard training approach.â¢ overall zero-shot performance: the zero-shot performance even with the best meta-learning approaches is quite far from state-of-the-art results. it will be interesting to see at what point (e.g. with k-shot learning) the performance can match a fully supervised model.",{},https://www.aclanthology.org/2022.emnlp-main.456.pdf
850,264935229,IMPLICIT CHAIN OF THOUGHT REASONING VIA KNOWLEDGE DISTILLATION,limitations,Insight-tree,"lack of transparency and interpretability one of the main advantages of explicit cot is its inherent transparency: the intermediate steps allow for easy interpretation of the model's reasoning process.in contrast, implicit cot, by virtue of its internal processing within hidden states, lacks this transparency.while it achieves compactness and efficiency in generation, it sacrifices human interpretability, making it challenging to understand how the model arrives at its conclusions.reliance on the teacher's thought process our current three-step strategy is, at a high level, trying to distill the teacher model's horizontal reasoning process into the vertical reasoning process of the student and the emulator.while the ultimate goal of implicit reasoning is to allow models to develop their own unique trajectories of reasoning, our initial approach still relies heavily on the teacher's thought processes for a starting point.performance discrepancies our current results of implicit cot still lag behind the performance of explicit cot.however, this work is just a first step towards building implicit cot, and there exists ample room for further optimization.","{247595263: 'Wang et al., 2023;', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2311.01460v1.pdf
851,264935229,IMPLICIT CHAIN OF THOUGHT REASONING VIA KNOWLEDGE DISTILLATION,conclusion and future work,Insight-tree,"in this work, we proposed the concept of implicit chain of thought reasoning for transformer-based language models, where reasoning is performed ""vertically"" among the transformer hidden states, instead of being performed ""horizontally"" in the form of generating intermediate tokens.this concept potentially enables the model to break away from the human-like reasoning process and develop its own internal reasoning process.","{247595263: 'Wang et al., 2023;', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2311.01460v1.pdf
852,238744031,ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers,conclusion,Insight-tree,"we propose a challenging dataset conditionalqa that contains questions with conditional answers.  table 4: em/f1 w/ conditions on the subset of questions with conditional answers. ""best overall"" uses the best checkpoints/hyper-parameters on the full dataset, while ""best conditional"" uses the best ones on the subset of questions.",{},https://www.aclanthology.org/2022.acl-long.253.pdf
853,216035859,Logic-Guided Data Augmentation and Regularization for Consistent Question Answering,conclusion,Insight-tree,"we introduce a logic guided data augmentation and consistency-based regularization framework for accurate and globally consistent qa, especially under limited training data setting. our approach significantly improves the state-of-the-art models across three substantially different qa datasets. notably, our approach advances the state-of-the-art on quarel and wiqa, two standard benchmarks requiring rich logical and language understanding. we further show that our approach can effectively learn from extremely limited training data.","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.acl-main.499.pdf
854,258841368,What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?,discussion and conclusion,Insight-tree,"large general-purpose language models, such as gpt family of models [brown et al. 2020;openai 2023] , lamda [thoppilan et al. 2022], palm [anil et al. 2023;chowdhery et al. 2022], and others, have propagated into informationseeking workflows of a general audience. a vast host of existing and ongoing work in nlp examine the deficiencies of these language models, ranging from hallucinated generations  with background with background and highlights fig. 4. users' subjective rating of the system for the usefulness of highlights, background, their confidence in the ai system, selfconfidence, and satisfaction with ai. users rate self-confidence marginally higher in the condition with background than the condition without (left). however, users rate their satisfaction with ai in the condition with background slightly lower than without. users' satisfaction rating is slightly lower even after introducing highlights with the background (right), with a slightly higher rating of background utility in the condition with highlights than without. however, there are no other discernible differences in ratings in the background condition with or without highlight.","{139103297: '[Chen and Durrett 2019;', 174801764: 'Min et al. 2019;', 221749191: 'Trivedi et al. 2020]', 248227734: 'Xie et al. 2022]', 52822214: '[Yang et al. 2018]'}",https://export.arxiv.org/pdf/2305.14331v1.pdf
855,258378227,MAFiD: Moving Average Equipped Fusion-in-Decoder for Question Answering over Tabular and Textual Data,conclusion,Insight-tree,"in this paper, we address long range-reasoning for the multi-hop table-and-text qa and propose mafid, which extends fid by equipping ema and the gated cross-attention layer for the encoder and decoder parts, respectively, to design an effective way of combining various types of encoded representations. the experimental results on hy-bridqa showed that the proposed mafid achieved state-of-the-art performances in both the development and blind test sets. in future work, we will extend mafid to open-domain table-and-text qa and explore a unified approach that integrates single-row and multi-row reasoning.","{221845203: 'Ainslie et al., 2020', 67855846: 'Dua et al., 2019;', 248426967: 'Nakamura et al., 2022', 233219849: 'Talmor et al., 2021', 52822214: 'Yang et al., 2018;', 234741852: 'Zhu et al., 2021a'}",https://www.aclanthology.org/2023.findings-eacl.177.pdf
856,256274941,Causal Reasoning About Entities and Events in Procedural Texts,conclusion and future work,Insight-tree,"we present crepe, a benchmark for causal reasoning about events and entities in procedural texts. we show that mainstream llms such as gpt-3 perform close to chance on crepe, while using code-like event representation as a prompt to code language model codex greatly improves the performance. further, we experiment with various ways to encode entity information into this representation and find that eliciting chain-of-thought reasoning from codex further improves performance while existing cot approaches with gpt-3 are ineffective. we clearly show that llms benefit from lower-level entity information when making predictions about higher-level events. future work should explore related tasks such as next-event prediction, event temporal ordering, etc., by injecting relevant information about entities into our representation. our code-representation of events allows more powerful expressions than simply entailment and negation considered in this work. future work may explore other forms of code chain-of-thought such as first-order logic. these expressions generated by llms can be computed objectively, thus ameliorating llms' hallucinations and improving the interpretability and faithfulness of predictions.","{174801080: 'Min et al., 2019;', 155100120: 'Qiu et al., 2019;', 203610361: 'Thayaparan et al., 2019;', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2023.findings-eacl.31.pdf
857,256274941,Causal Reasoning About Entities and Events in Procedural Texts,limitations,Insight-tree,"despite our best efforts, our crepe dataset has inherent limitations. first, the choice of studying procedure texts, despite many discussed advantages, limits the domain, writing style, and other semantic features of the texts. as a result, porting our methods and findings to other text styles such as stories or news might require domain adaptation. second, we prioritize quality over quantity when creating this benchmark, which suffers from small size and contains biases from the annotators, even though we address the latter by having different annotators label a test set.when annotating the hypothetical events, our intention is that they represent a wild variety that doers of the procedures, humans or machines, would care about. however, we also have to ensure these events are unambiguously bound to some entities in order to challenge models for their causal reasoning ability. while we do our utmost to balance these two conflicting objectives, the issue might still persist.in crepe, each event likelihood change is caused by exactly one entity state change. this is an over-simplification made to facilitate evaluation. in real life, many complex events require many entity states to be reasoned about, which in turn may have complex logical relations among them. we leave this for future work.while we intend our representation of events and entities to be a general and effective one, we have only shown that it works well empirically using codex, which is one of the only code language models at present. whether the idea of our structured representation applies to other models remains to be explored. in section 4 and 5, we have discussed our bestperforming prompts for gpt-3 and codex. here, we elaborate on inferior codex prompts and shed light on why they do not work well empirically.best prompt as discussed, our best-performing prompt represents procedures as classes and steps as functions.class wash_sneakers: # init # remove shoelaces # rinse def __init__(self, event0): self.event0 = event0 # my feet get wet by wearing the sneakers. def remove_shoelaces(self): self.event0.change = ""equally likely"" # my feet get wet by wearing the sneakers. def rinse(self):self.event0.change = ""more likely"" # my feet get wet by wearing the sneakers.nested functions instead of representing procedures as classes as in our best-performing prompt, we can also represent them as nested functions.def wash_sneakers(event0): # init # remove shoelaces # rinse event0 = event0 # my feet get wet by wearing the sneakers. def remove_shoelaces(self): event0.change = ""equally likely"" # my feet get wet by wearing the sneakers. def rinse(self): event0.change = ""more likely"" # my feet get wet by wearing the sneakers.no step comments the comments displaying the steps immediately after the class declaration are removed.class wash_sneakers: def __init__(self, event0): self.event0 = event0 # my feet get wet by wearing the sneakers. def remove_shoelaces(self): self.event0.change = ""equally likely"" # my feet get wet by wearing the sneakers. def rinse(self):self.event0.change = ""more likely"" # my feet get wet by wearing the sneakers.","{174801080: 'Min et al., 2019;', 155100120: 'Qiu et al., 2019;', 203610361: 'Thayaparan et al., 2019;', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2023.findings-eacl.31.pdf
858,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,conclusion,Insight-tree,"in this paper, we propose a novel self-supervised framework called qada for qa domain adaptation. qada introduces: (1) hidden space augmentation tailored for qa data to enrich target training corpora; and (2) an attention-based contrastive adaptation to learn domain-invariant features that generalize across source and target domain. our experiments demonstrate the effectiveness of qada: it achieves a superior performance over state-ofthe-art baselines in qa domain adaptation.","{204823992: 'Fisch et al., 2019', 219721462: 'Kamath et al., 2020', 222177817: 'Kratzwald et al., 2020;', 52822214: 'Yang et al., 2018', 237364113: 'Yue et al., 2021b', 252199900: 'Yue et al., , 2022d'}",https://www.aclanthology.org/2022.emnlp-main.147.pdf
859,253018834,QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation,limitations,Insight-tree,"despite having introduced hidden space augmentation in qada, we have not discussed different choices of Î± values for multi-hop synonyms to exploit the potential benefits of the dirichlet distribution. for context cutoff, dropping multiple context spans in each qa example may bring additional benefits to improve context understanding and the answer extraction process of the qa model. combined with additional question value estimation in pseudo labeling, we plan to explore such directions in adaptive qa systems as our future work. a combination of question augmentation ratio Î¶ and context cutoff ratio Ï. specifically, we empirically search for the best combination in the range of [0.1, 0.2, 0.3, 0.4] for both Î¶ and Ï. eventually, the best hyperparameter combination is selected.","{204823992: 'Fisch et al., 2019', 219721462: 'Kamath et al., 2020', 222177817: 'Kratzwald et al., 2020;', 52822214: 'Yang et al., 2018', 237364113: 'Yue et al., 2021b', 252199900: 'Yue et al., , 2022d'}",https://www.aclanthology.org/2022.emnlp-main.147.pdf
860,235678938,Controllable Open-ended Question Generation with A New Question Type Ontology,conclusion,Insight-tree,"we present a new question type ontology which better captures the nuances of questions to support the study of open-ended question generation. we further annotate a new dataset with 4,959 questions based on the proposed ontology. we describe a joint question focus detection and question generation framework with a novel semantic graphaugmented representation, which is directly built on large pre-trained models. based on this framework, we also enhance the controllability and diversity of generated questions by employing template exemplars or automatically generated templates. experiments on two large datasets show that questions generated by our models have better quality and higher diversity than non-trivial comparisons, with similar results rated by human judges.","{216553210: 'Pan et al., 2020', 52822214: 'Yang et al., 2018', 202572810: 'Zhang and Bansal, 2019'}",https://www.aclanthology.org/2021.acl-long.502.pdf
861,263830473,MINPROMPT: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering,conclusion,Insight-tree,"in this paper, we present minprompt, a robust data augmentation framework that leverages a graph-based algorithm and unsupervised question generation to extract minimally meaningful qa training samples from raw text.our contributions reside in the application of minimal data augmentation, enhancing computational efficiency and model performance while mitigating overfitting.","{237420912: 'Chada and Natarajan, 2021', 233206583: 'Esteva et al., 2021', 204823992: 'Fisch et al., 2019', 198229624: 'Joshi et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 211258652: 'Puri et al., 2020;', 230433978: 'Ram et al., 2021;', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.05007v1.pdf
862,211572791,DC-BERT: DECOUPLING QUESTION AND DOCUMENT FOR EFFICIENT CONTEXTUAL ENCODING,conclusion,Insight-tree,"this paper introduces dc-bert to decouple question and document for efficient contextual encoding. dc-bert has been successfully applied to document retrieval, a key component in opendomain qa, achieving 10x speedup while retaining most of the qa performance. with the capability of processing high-throughput of questions each with a large collection of retrieved documents, dc-bert brings open-domain qa one step closer to serving real-world applications.","{208267807: 'Asai et al., 2020', 202565945: 'Jiang & Bansal, 2019;', 86611921: 'Kwiatkowski et al., 2019', 202558815: 'Min et al. 2019', 202660724: 'Nie et al., 2019;', 202773198: 'Qi et al., 2019;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2002.12591v1.pdf
863,257410167,MiR-1281 is involved in depression disorder and the antidepressant effects of Kai-Xin-San by targeting ADCY1 and DVL1,conclusions,Insight-tree,"collectively, based on the discovery of clinical-level mirna expression profiles, the biological function of mir-1281 was elucidated in vitro from the perspective of posttranscriptional regulation. we found that the target genes of mir-1281 (adcy1 and dvl1) were closely related to the occurrence and development of depression. it was demonstrated in vitro that kxs may activate the camp/pka/ erk/creb and wnt/Î²-catenin signal transduction pathways by downregulating mir-1281, which targets adcy1 and dvl1, to achieve its role in neuronal cell protection. these results provide a novel way of perceiving depression disorder, shedding light on the development of new therapeutic approaches.",{},NaN
864,219965751,"A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets",conclusions,Insight-tree,"we conducted a comprehensive survey of recent efforts on the tasks, evaluation metrics and benchmark datasets of machine reading comprehension (mrc). we discussed the definition and taxonomy of mrc tasks, and proposed a new classification method for mrc tasks. the computing methods of different mrc evaluation metrics have been introduced with their usage in each type of mrc tasks also analyzed. we also introduced attributes and characteristics of mrc datasets, with 47 mrc datasets described in detail. finally, we discussed the open issues for future research of mrc and we argued that high-quality multi-modal mrc datasets and the research findings of cognitive neuroscience may help us find better ways to construct more challenging datasets and develop related mrc algorithms to achieve the ultimate goal of human-level machine reading comprehension.","{67855846: '[23]', 86611921: '[50]', 52822214: '[109]'}",https://arxiv.org/pdf/2006.11880v1.pdf
865,232335785,Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2,conclusion and future work,Insight-tree,"in this paper, we introduce chainruler, a dataset for multi-hop deductive argumentation, and assess gpt-2's zero-shot ability both to solve the inference tasks and to generate effective problem elaborations, i.e., texts which -once added to the context -improve performance. our main findings are:","{218470415: ', Banerjee and Baral, 2020', 203836061: 'Chen et al. [2019]', 52822214: '[Yang et al., 2018]'}",https://arxiv.org/pdf/2103.13033v1.pdf
866,3937062,Using Automatic Refactoring to Improve Energy Efficiency of Android Apps,conclusion,Insight-tree,"our work shows the potential of using automatic refactoring tools to improve energy efficiency of mobile applications. we have analyzed 140 foss android apps and as an outcome we have fixed 222 energy related anti-patterns. in total, we improved the energy footprint of 45 apps.",{},https://arxiv.org/pdf/1803.05889v1.pdf
867,246652372,Survey of Hallucination in Natural Language Generation,conclusion,Insight-tree,"in this survey, we provide the first comprehensive overview of the hallucination problem in nlg, summarizing existing evaluation metrics, mitigation methods, and the remaining challenges for future research. hallucination is an artifact of neural-based nlg and is of concern because they appear fluent and can therefore be misleading to users. in some scenarios and tasks, hallucination can cause harm. we survey various contributors to hallucination, ranging from noisy data, erroneous parametric knowledge, incorrect attention mechanism, inappropriate training strategy, to inference exposure bias, etc. we show that there are two categories of hallucinations, namely intrinsic hallucination and extrinsic hallucination, and they need to be treated differently with diverse mitigation strategies. hallucination is relatively easy to detect in abstractive summarization and in nmt against the evidence in the source. for dialogue systems, it is important to balance diversity vs consistency in dialogue responses. hallucination in gqa and vl tasks is detrimental to the performance, but research on mitigation methods is still very preliminary in these areas. for datato-text generation, hallucination arises from the discrepancy between the input and output format. most methods to mitigate hallucinations in nmt either aim to reduce dataset noise or alleviate exposure bias. in the vl domain, models also generate unfaithful output given the visual scene, and recent works have mainly focused on the object hallucination problem. there remain many challenges ahead in identifying and mitigating hallucinations in nlg, and we hope research in this area can benefit from this survey.","{196170479: '[47,', 86611921: '[92]', 52822214: '[218]', 220831004: '223,'}",https://export.arxiv.org/pdf/2202.03629v5.pdf
868,259252098,SciMRC: Multi-perspective Scientific Machine Reading Comprehension,conclusion,Insight-tree,"in this paper, we proposed a novel multiperspective scientific machine (smrc) reading comprehension dataset, called scimrc, with different perspectives of readers, including begin-ners, students and experts. extensive experimental results suggest the inner relations and differences among different perspectives, suggesting the importance of analyzing perspectives. the extensive results suggest that scimrc could serve as a testbed for evaluating smrc research.","{234093776: 'Dasigi et al., 2021', 226262208: 'Ferguson et al., 2020', 202572622: 'Jin et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2306.14149v1.pdf
869,257427664,Challenges in Explanation Quality Evaluation,conclusion,Insight-tree,"this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face. we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate the discussed characteristics. we support our arguments with empirical evidence of a crowdsourced case study that we conducted for the example of explainable question answering systems from the hotpotqa leaderboard.","{174801080: 'Min et al., 2019', 237620311: 'Nishida et al., 2021', 222310757: 'Schuff et al., 2020', 237532313: 'Schuff et al., , 2021', 207870753: 'Tu et al., 2020', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2210.07126v2.pdf
870,257901155,UKP-SQuARE v3: A Platform for Multi-Agent QA Research,conclusions and discussions,Insight-tree,"in this work, we have extended ukp-square to support multi-agent models. in particular, we deployed a routing system, tweac (geigle et al., 2021), a method to combine adapter weights, made (friedman et al., 2021), and a model that combines the prediction of multiple skills, metaqa (puerto et al., 2023). we have conducted experiments on these three models and unifiedqa (khashabi et al., 2020), a multi-dataset system, to analyze the trade-off between the performance, efficiency, and flexibility of these systems. we showed that in scenarios where new domains or expertise are often needed, metaqa provides the best tradeoff since its performance is close to the best model, it is compatible with any qa format, cheap to train, and its inference runtime is close to tweac and made using the parallel engine provided by ukp-square. however, when simple deployment is needed or the model is not expected to be updated, made and unifiedqa might be more appropriate.","{247748606: 'BaumgÃ¤rtner et al., 2022b;', 238198206: 'Friedman et al., 2021', 244896105: 'Puerto et al., 2023', 236447339: 'Rogers et al., 2023', 173188058: 'Talmor and Berant, 2019;', 248572452: 'Zhong et al., 2022'}",https://www.aclanthology.org/2023.acl-demo.55.pdf
871,252873674,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,discussion & conclusion,Insight-tree,"caveats and risks of explanations from large language models our analysis suggests that llms' internal ""reasoning"" does not always align with explanations that it generates, as shown by our consistency results. more concerning, the explanations might not be factually grounded in the provided prompt. this shortcoming should caution against any deployment of this technology in practice: because the explanations are grammatical english and look very convincing, they may deceive users into believing the system's responses even when those responses are incorrect. section 6 of bender et al. (2021) discusses these risks in additional detail. the fact that language models can hallucinate explanations is also found in other work (zhou and tan, 2021). this result is unsurprising in some sense: without sufficient supervision or grounding, language models do not learn meaning as distinct from form (bender and koller, 2020), so we should not expect their explanations to be strongly grounded.","{139103297: 'Chen and Durrett, 2019', 220045477: 'Dua et al., 2020;', 237513496: 'Garg and Moschitti, 2021;', 230799347: 'Geva et al., 2021', 189927896: 'Jiang and Bansal, 2019', 219721462: 'Kamath et al., 2020;', 233297024: 'Shin et al., 2021;', 225068329: 'Wiegreffe et al., 2021;', 52822214: 'Yang et al., 2018;', 238856959: 'Ye and Durrett, 2022'}",https://export.arxiv.org/pdf/2205.03401v2.pdf
872,258179336,Tool Learning with Foundation Models,conclusion,Insight-tree,"this paper studies the paradigm of tool learning with foundation models. we first recapitulate the cognitive origins of tool use in human history and categorize tools from the perspective of the user interface. then we review the ai paradigm shift brought about by foundation models and discuss the complementary roles of tools and foundation models. we perform a comprehensive literature review for existing exploration in tool learning and start with formulating a general tool learning framework. then we highlight core research problems such as bridging user intents with appropriate tools, better planning by leveraging the reasoning abilities of foundation models, training strategies for tool learning, and how to facilitate generalization for tool learning. finally, we discuss important research topics, including safe and trustworthy tool learning, tool learning for large complex systems, ai tool creation, personalized tool learning, embodied tool learning, knowledge conflict issue in tool augmentation, etc. in general, this paper serves as a systematic investigation of tool learning. we hope this paper could facilitate research in integrating tools with foundation models in the future.","{52822214: 'Yang et al., 2018b'}",https://export.arxiv.org/pdf/2304.08354v2.pdf
873,243865235,Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI,conclusion,Insight-tree,"in this paper, we propose a diagnostic method to diagnose lms' fol reasoning ability. this method introduces a novel proposed benchmark, logicnli, that disentangles the fol reasoning from commonsense inference. specifically, it includes four evaluations to measure the fol reasoning ability from different perspectives. results on three lms show that although some lms (roberta) own a certain interpretable fol reasoning ability, they still cannot make sensible fol reasoning like humans. detailed analysis motivates us to enhance specific reasoning abilities or explore new methods to make neural models understand more refined logic.","{211126663: 'Clark et al., 2020', 67855846: 'Dua et al., 2019;', 189927896: 'Jiang and Bansal, 2019;', 220483148: 'Liu et al., 2020', 208201969: 'Sugawara et al., 2020;', 202539540: 'Tafjord et al., 2019;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.303.pdf
874,247218120,SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs,conclusion,Insight-tree,"in this work, we re-examine the use and effect of supervision in the entity alignment problem, which targets aligning entities with identical meanings across different knowledge graphs.based on the three insights we derive-uni-space learning, relative similarity metric, and self-negative sampling, we develop a self-supervised entity alignment algorithm-selfkg-to automatically align entities without training labels.the experiments on two widely-used benchmarks dwy100k and dbp15k show that selfkg is able to beat or match most of the supervised alignment methods which leverage the 100% of the training datasets.our discovery indicates a huge potential to get rid of supervision in the entity alignment problem, and more studies are expected for a deeper understanding of self-supervised learning.",{},https://export.arxiv.org/pdf/2203.01044v1.pdf
875,258841502,Towards Graph-hop Retrieval and Reasoning in Complex Question Answering over Textual Database,conclusion,Insight-tree,"our study introduces the reasongraphqa dataset, the first textual database qa dataset with an explanation graph, which provides complex structured retrieval assistance for graph retrieval systems.we have tested various traditional evidence retrieval methods on the reasongraphqa dataset and evaluated them manually.additionally, we propose the graph-hop retrieval paradigm and develop a bidirectional graph retrieval model, which significantly improves the evidence retrieval and graph construction capabilities of complex question answering by reconstructing reasoning paths in different directions.future research utilizing the rea-songraphqa dataset can enable fine-grained analysis of the explanation graph output from models, leading to further advancements in real and complex qa environments.while the current methods have several limitations, this presents opportunities for future research to improve upon them.",{},https://export.arxiv.org/pdf/2305.14211v1.pdf
876,220403560,KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base,conclusions,Insight-tree,"we introduce kqa pro, a new benchmark of complex kbqa with following features: 1) with explicit reasoning process, including sparqls and programs; 2) large-scale, with about 120k natural questions; 3) with rich kinds of knowledge, including relational, literal, and high-level. we create a unified codebase to implement the baselines and state-of-the-arts of complex kbqa. extensive experiments reveal a huge gap between machines and humans, demonstrating that kqa pro is very challenging. kqa pro is the first kbqa benchmark that provides the explicit reasoning process for complex questions. we hope that these additional information can help machines develop the compositional reasoning ability and learn to tackle complex questions like a human.",{67855846: '[10]'},https://arxiv.org/pdf/2007.03875v1.pdf
877,261891487,Cross-Lingual Knowledge Editing in Large Language Models,conclusion,Insight-tree,"in this paper, we first explore the cross-lingual effect of knowledge editing. to achieve that, we automatically construct bi-zsre dataset by translating the previous zsre dataset from english to chinese. based on bi-zsre, we conduct experiments on various knowledge editing methods and multilingual llms, and study the cross-lingual effect from english to chinese and vice versa. our results indicate that (1) the language modeling gaps of different languages might influence the efficiency of knowledge editing in different languages; (2) it is still hard for existing knowledge editing methods to transfer the edited knowledge from one language to another in a multi-lingual llm; (3) when editing llms in a language, the locality in the other languages could also be influenced. we also analyze the inconsistent behaviors of the edited models and discuss their specific challenges to provide a deeper understanding of the cross-lingual effect in knowledge editing.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2309.08952v1.pdf
878,250390665,CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course,conclusion,Insight-tree,"in this paper, we present cs1qa, a dataset for codebased question answering in introductory programming course. cs1qa's crowdsourced data from a programming course provide rich information that code understanding models need to consider to correctly answer the given questions. we introduce three tasks for cs1qa, whose output can help students debug and reduce workloads for the teaching staff. results from the baseline models indicate that tasks for cs1qa are challenging for current language understanding models. cs1qa promotes further research to better represent and understand source code for code-based question answering.","{207847581: 'Castelli et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.naacl-main.148.pdf
879,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",conclusion,Insight-tree,"large language models struggle with hallucination, or generating incorrect information, despite the large amount of factual pre-training data they were trained on. to help alleviate this problem, we proposed according-to prompts, asking language models to ground their output to their pretraining corpus. to quantify the extent to which models achieve this goal, we introduced a new metric, quip-score, that efficiently and quickly measures the percent of the model's generation that exists as exact quotes in the pre-training corpus. we showed that prompting models with grounding prompts greatly improves the quip-score while anti-grounding prompts reduces the quip-score. our analysis also shows that quip-score increases with instruction-tuning, popularity of the entity in the question, and model size. we hope that this work brings more attention to the positive aspects of llm memorization and encourages more work into understanding how and when language model output is grounded to its pre-training data.","{196170479: 'Fan et al., 2019', 221507798: 'Petroni et al., 2021', 247188085: 'Su et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.13252v1.pdf
880,258832937,"""According to . . . "" Prompting Language Models Improves Quoting from Pre-Training Data",limitations,Insight-tree,"our proposed metric only accounts for exact lexical match and will miss other types of grounded statements -thus we view quip-score as a lower bound on grounding where grounding is defined only by quoting from source material.we also recognize the possibility of a discrepancy between the pre-training data of private models like chatgpt and the wikipedia version we use for analysis, due to limited information on their pretraining. however, this might not be a significant concern, as although wikipedia is not completely static, a substantial part of the information in this knowledge source remains consistent over a short span of years. furthermore, our results with chat-gpt are similar compared with models for which we do have the exact pre-training data (like gpt-j).","{196170479: 'Fan et al., 2019', 221507798: 'Petroni et al., 2021', 247188085: 'Su et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.13252v1.pdf
881,215238360,Query Focused Multi-Document Summarization with Distant Supervision,conclusions,Insight-tree,"in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization. we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments. experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more relevant and less redundant. we have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query. in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019;chu and liu, 2019) and investigate how recent advances in open domain qa qi et al., 2019) can be adapted for query focused summarization.","{202773198: 'Qi et al., 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2004.03027v1.pdf
882,204915921,QASC: A Dataset for Question Answering via Sentence Composition,conclusion,Insight-tree,"we present qasc, the first qa dataset for multi-hop reasoning beyond a single paragraph where two facts needed to answer a question are annotated for training, but questions cannot be easily syntactically decomposed into these facts. instead, models must learn to retrieve and compose candidate pieces of knowledge. qasc is generated via a crowdsourcing process, and further enhanced via multi-adversary distractor choice selection. state-of-the-art bert models, even with massive fine-tuning on over 100k questions from previous relevant datasets and using our proposed two-step retrieval, leave a large margin to human performance levels, thus making qasc a new challenge for the community.","{67855846: 'Dua et al. 2019', 202712552: 'Khot, Sabharwal, and Clark 2019'}",https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175
883,235125947,Fact-driven Logical Reasoning,conclusion,Insight-tree,"in this work, we propose a novel method named focal reasoner for logical reasoning in the machine reading comprehension task. our method not only better uncovers the logical structures within the context, which can be a general method for other sophisticated reasoning tasks but also better captures the logical interactions between context and options. the experimental results verify the effectiveness of our method. in the future, we intend to design more elaborate mechanisms to cope with different question types and logical types as well as combine the symbolic and neural approaches.","{220483148: '[1]', 233219869: '4', 232380161: '6', 155100120: '[10,', 153312687: '11,', 67855846: '[25]', 52822214: '[26]', 207756753: '29]'}",https://arxiv.org/pdf/2105.10334v1.pdf
884,54585587,Predicting tbx22 Zebrafish Protein Structure Using Multi-Level Prediction Tools and Demonstration of Conserved Structural Domains in Relation to Orthologous tbx22 Proteins in Humans,conclusion,Insight-tree,"protein prediction methods have allowed biologists to identify protein structure, functionality, orientation and other important general information.available knowledge of existing proteins can guide us with a template to approach a solution to unknown structures and function.uncharacterized proteins can now be defined with the advantage of multiple public databases, with increased set of established data sources combined with bioinformatics tools.as more tools are accessed computational biologists can unravel mysteries of unidentified/unpredicted biological molecules to solve known problems leading to advancements and new ideas.the goal of this paper was to show the sequence similarity between t-box22 of 2 different species and show how they are sequentially and structurally similar to each other.supporting documentation available from databases and the usage of the right tools has shown enough evidence to prove that the zf tbx22 tf shares functional homology to the human tbx22 tf.",{},NaN
885,248779897,Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension,conclusion,Insight-tree,"this paper addresses answer uncertainty and unanswerability in multiple-choice mrc. measures of answer uncertainty are required to identify examples that the system may struggle to get correct and hence should abstain from answering such questions. unanswerability detection is required for when the answer cannot be deduced using the information provided. an electra prlm achieve competitive results on the default reclor dataset, achieving up to 67.1% accuracy on the evaluation split. ensemble-based predictive uncertainty measures are explored for both modes of operation: answer uncertainty for negative marking schemes and the presence of unanswerability. it is shown that uncertainty in the prediction such as expected entropy is correlated with the error rate of the mrc system allowing better than vanilla performance with an aggressive negative marking scheme for reclor and race. interestingly, it is found that expected entropy from the predictions of an implicitly trained system is competitive at unanswerability detection and is able to out-compete map decoding from an explicitly trained system that has been trained with unanswerable examples for reclor.",{},https://www.aclanthology.org/2022.findings-acl.82.pdf
886,231883811,Unification-based Reconstruction of Multi-hop Explanations for Science Questions,conclusion,Insight-tree,this paper proposed a novel framework for multihop explanation reconstruction based on explanatory unification. an extensive evaluation on the worldtree corpus led to the following conclusions:,"{208089867: 'Jansen and Ustalov, 2019', 203610361: 'Thayaparan et al., 2019', 202785879: 'Yadav et al., 2019', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/2021.eacl-main.15.pdf
887,208089867,TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration *,conclusion,Insight-tree,"the textgraphs 2019 shared task on multi-hop inference for explanation regeneration received four team submissions that exceeded the performance of the baseline system. the systems used a variety of methods from additional knowledge resources (such as conceptnet or framenet) to directly training language models to perform multihop inference by predicting chains of facts. the top-performing system increased baseline performance by nearly a factor of two on this task, achieving a new state-of-the-art.","{139103297: 'Chen and Durrett, 2019;', 174801764: 'Min et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/D19-5309.pdf
888,266163790,Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering,conclusion,Insight-tree,"in this paper, we attempt to improve the robustness of qa models by testing time adaptation (tta) but find that tta causes the models collapse.we thoroughly investigate why previous tta methods cause the model collapse and find that the imbalanced label distribution is the main reason.we address this problem by adding constraints between the source and adapted model during the tta process.we also design an efficient side block to speed up the inference time.sufficient experimental results show that our proposed method is effective and efficient, making tta a big step closer to being applied in real-world scenarios.","{204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://aclanthology.org/2023.emnlp-main.803.pdf
889,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"we introduce halueval, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing hallucinations.to automatically generate large-scale samples, we propose a two-step approach, i.e., sampling-then-filtering.we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one.besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.we find that, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated content.finally, we suggest several strategies to help llms recognize hallucinations.our benchmark can facilitate research in understanding what types of content and to which extent llms tend to hallucinate, ultimately paving the way for building more effective and reliable llms in the future.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.11747v3.pdf
890,258832847,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,limitations,Insight-tree,"in our approach, we leverage a llm, i.e., chatgpt, to automatically generate the hallucinated samples.therefore, the quality of our hallucinated samples is limited by the capacity of chatgpt in following the complex instruction of hallucination sampling.although we design the high-quality hallucination filtering process, it is still necessary to apply quality control to the generation of hallucinated samples.besides, our benchmark focuses on evaluating the ability of llms in recognizing the hallucinations in text but does not investigate the underlying reasons behind the appearance of hallucinations like prior work (zheng et al., 2023;das et al., 2023).as for the potential issue, since the hallucinated samples in our benchmark looks highly similar to the ground-truth samples, which might be misused for an unexpected purpose than we planned.to alleviate this issue, we should monitor and regulate the spread and usage of our benchmark.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.11747v3.pdf
891,256662612,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",conclusion and discussion,Insight-tree,"multitask, multilingual, multimodalchatgpt outperforms multiple state-of-the-art zero-shot llms on various tasks and even surpasses fine-tuned models on some tasks. although chatgpt performs well in most of the tasks, there are still some failure cases on each task ( Â§3.1). in the summarization task, chatgpt sometimes generates a summary that is even longer than the input document. in the machine translation task, chatgpt sometimes produces an incorrect translation for some words, making the meaning slightly shifted. therefore, dealing with these special cases is a complex but important task.",{},https://export.arxiv.org/pdf/2302.04023v2.pdf
892,249062748,Generating Natural Language Proofs with Verifier-Guided Search,conclusion,Insight-tree,"we have introduced nlproofs for stepwise proof generation in natural language. it learns to generate relevant proof steps conditioning on the hypothesis. to prevent hallucination, nlproofs searches for proofs that maximize a validity score judged by a verifier. our method has achieved state-of-the-art performance on en-tailmentbank and ruletaker, demonstrating the promise of stepwise proof generation for human-authored proofs. in the future, we hope to see increasing applications of verifiers and proof search in various reasoning tasks.","{237450610: 'Bostrom et al. 2021', 247594506: 'Sanyal et al. 2022', 235742855: 'Chen et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.7.pdf
893,233189637,NLQuAD: A Non-Factoid Long Question Answering Data Set,conclusion,Insight-tree,"we introduce nlquad, a non-factoid long question answering data set from bbc news articles. nlquad's question types and the long lengths of its context documents as well as answers, make it a challenging real-world task. we propose to use intersection over union (iou) as an evaluation metric for long question answering. to establish a baseline performance, we experimented with the bert, roberta, and longformer question answering models. longformer outperforms the other methods with an iou of 73.57%, but the results show that the performance of state-of-the-art question answering systems is far from perfect. we hope nlquad will inspire more research in the area of document-level language understanding and question answering.","{67855846: 'Dua et al., 2019', 196170479: 'Fan et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2021.eacl-main.106.pdf
894,218469981,On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections,conclusion,Insight-tree,"this work is the first attempt to develop a formal framework for understanding the behavior of complex natural language reasoning in the presence of linguistic noise. the importance of this work is two-fold. first, it proposes a novel graph-theoretic paradigm for studying reasoning, inspired by the symbol-meaning problem in the presence of redundancy, ambiguity, incompleteness, and inaccuracy of language. second, it shows how to use this framework to analyze a class of reasoning algorithms. we expect our findings, as well as those from future extensions to other classes of reasoning algorithms, to have important implications on how we study problems in language comprehension.",{},https://arxiv.org/pdf/1901.02522v3.pdf
895,233219392,Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval,conclusion,Insight-tree,"we introduce a simple yet effective multi-step dense retrieval method, beamdr. by conducting beam search and globally refreshing negative chains during training, beamdr finds reasoning chains in dense space. beamdr is competitive to more complex sota systems albeit not using semi-structured information.","{208267807: 'Asai et al. 2020', 211296452: 'Dhingra et al. 2020', 202660724: 'Nie et al., 2019;', 202773198: 'Qi et al. 2019', 220302524: 'Xiong et al. 2021a', 221970302: 'Xiong et al., 2021b', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2021.naacl-main.368.pdf
896,249888962,A Dense Representation Framework for Lexical and Semantic Matching,conclusions and future work,Insight-tree,"we present a simple yet effective approach to densifying lexical representations for passage retrieval. this work introduces a dense representation framework and proposes a new scoring function to compute relevance scores between dense lexical representations (dlrs) derived from queries and passages. using our framework, we can combine lexical and semantic representations into dense hybrid representations (dhrs) for hybrid retrieval. our experiments show that dlrs can accurately approximate any ""off-the-shelf"" lexical model. furthermore, when combined with other semantic representations (as dhrs), the resulting models can achieve comparable effectiveness to existing state-of-the-art hybrid retrieval methods.","{233296016: '[48]', 220302524: '[50]'}",https://export.arxiv.org/pdf/2206.09912v2.pdf
897,263605962,RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUC-TION TUNING,conclusion,Insight-tree,"in this paper, we propose ra-dit, a lightweight retrieval-augmented dual instruction tuning framework that can effectively retrofit any pre-trained llm with retrieval capabilities.ra-dit updates the llm with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information.it also fine-tunes the retriever with supervision from the llm to retrieve texts that can better help the llm generate correct outputs.ra-dit achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context ralm approaches such as replug and compete effectively against methods that require extensive pre-training such as atlas.","{196170479: 'Fan et al., 2019', 249097975: 'Izacard et al., 2022a', 86611921: 'Kwiatkowski et al., 2019', 253708231: 'Li et al., 2023', 221507798: 'Petroni et al., 2021', 220302524: 'Xiong et al., 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.01352v2.pdf
898,202660724,Revealing the Importance of Semantic Retrieval for Machine Reading at Scale,conclusion,Insight-tree,"we proposed a simple yet effective hierarchical pipeline system that achieves state-of-the-art results on two mrs tasks. ablation studies demonstrate the importance of semantic retrieval at both paragraph and sentence levels in the mrs system. the work can give general guidelines on mrs modeling and inspire future research on the relationship between semantic retrieval and downstream comprehension in a joint setting. table 6: hyper-parameter selection for the full pipeline system. h and k are the retrieval filtering hyperparameters mentioned in the main paper. p-level and s-level indicate paragraph-level and sentence-level respectively. ""{}"" means values enumerated from a set. ""[]"" means values enumerated from a range with inter-val=0.1 ""bs.""=batch size ""# e.""=number of epochs","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/D19-1258.pdf
899,263830898,RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS,conclusion,Insight-tree,"in this paper, we present itrg, which is an iterative retrievalgeneration synergy framework, containing two important steps: generation-augmented retrieval and retrieval-augmented generation.they form a closed loop, and can improve each other via multiple iterations.we propose a simple and effective generation-augmented retrieval strategy and two retrieval-augmented generation strategies.empirical results show our approach significantly exceeds several strong baselines, including gpt 3.5, on four open domain question answering datasets, which indicates that our method can significantly improve the reasoning ability of large language models.","{86611921: '[15]', 226236740: '[17]', 52822214: '[18]'}",https://export.arxiv.org/pdf/2310.05149v1.pdf
900,253098276,Re-Examining Calibration: The Case of Question Answering,conclusion,Insight-tree,"this paper investigates calibration in the realistic application of odqa where users need to decide whether to trust the model prediction based on the confidence scores. although confidence scores produced by existing calibration methods improves the popular ece metric, these confidence scores do not help distinguish correct and wrong predictions. we propose to use the macroce metric to remedy the flaws, and existing calibration methods fail on our macroce metric. we further propose a simple and effective calibration method conscal that leverages training consistency. our human study confirms both the effectiveness of conscal as well as the alignment between macroce and human preference. our work advocates and paves the path for user-centric calibration, and our con-scal method is a promising direction for better calibration. future work can adapt our calibration metric and method to more diverse tasks (such as generative tasks) and explore other ways to further improve user-centric calibration.","{219721462: 'Kamath et al. 2020', 52822214: 'Yang et al., 2018', 238856959: 'Ye and Durrett, 2022', 235313893: 'Zhang et al., 2021;'}",https://export.arxiv.org/pdf/2205.12507v2.pdf
901,246823260,QA4QG: USING QUESTION ANSWERING TO CONSTRAIN MULTI-HOP QUESTION GENERATION,conclusion,Insight-tree,"in this paper, we propose a novel framework, qa4qg, a qaaugmented bart-based framework for mqg. it is the first work to explore large pre-trained language models for mqg and takes advantage of an additional multi-hop qa module to further constrain the question generation. our results on the hotpotqa dataset show that qa4qg outperforms all state-of-the-art models, with an increase of 8 bleu-4 and 8 rouge points compared to the best results previously reported. our work suggests the advantage of introducing pre-trained language models and qa modules for the mqg task.","{52822214: '[3]', 216553210: '[4]', 224705407: '[5]', 227231411: '[7]', 214802355: '[8]', 226236844: '[12]', 207853300: '[13]', 202572810: '[15]', 220045416: '[16]'}",https://arxiv.org/pdf/2202.06538v1.pdf
902,245131402,GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval,conclusion,Insight-tree,"in this work we propose gpl, a novel unsupervised domain adaptation method for dense retrieval models. it generates queries for a target corpus and pseudo labels these with a cross-encoders. pseudolabeling overcomes two important short-comings of previous methods: not all generated queries are of high quality and pseudo-labels efficiently detects those. further, training with mined hard negatives is possible as the pseudo labels performs efficient denoising.",{},https://www.aclanthology.org/2022.naacl-main.168.pdf
903,189898081,DocRED: A Large-Scale Document-Level Relation Extraction Dataset,conclusion,Insight-tree,"to promote re systems from sentence level to document level, we present docred, a large-scale document-level re dataset that features the data size, the requirement for reading and reasoning over multiple sentences, and the distantly supervised data offered for facilitating the development of weakly supervised document-level re. experiments show that human performance is significantly higher than re baseline models, which suggests ample opportunity for future improvement.",{},https://www.aclweb.org/anthology/P19-1074.pdf
904,258053774,Cancer-Specific Survival after Limb Salvage versus Amputation in Children and Adolescents with Osteosarcoma: A Population-Based Analysis with Propensity Score Matching,conclusion,Insight-tree,"to summarize, we established and validated a novel nomogram for os patients of cya, which could serve as concise and practical tools for clinicians to anticipate the 1-, 3-, and 5-years css. lss for patients with os exhibited signifcant beneft on css compared with amputation. while new chemotherapy regimens will be required to increase survivorship in the setting of os, patients with tumor features suitable to lss had a much higher survival rate than those suitable for amputation. [28][29][30].",{},NaN
905,233296243,Learning with Instance Bundles for Reading Comprehension,conclusion,Insight-tree,"we have presented a way to use contrastive estimation in a supervised manner to learn from distinguishing cues between multiple related qa pairs, or instance bundles. our experiments with multiple ce-based loss functions, defined over a joint neighborhood of questions and answers, have shown that these models outperform existing methods on two datasets: ropes and hotpotqa. apart from presenting several ways to create instance bundles, we also explore theoretical connections between unlikelihood training and contrastive estimation, and initial exploration into when instance bundles are likely to be effective with these methods. we believe our results give strong motivation for further work in techniques to both create and use instance bundles in nlp datasets. the code is available at https://github.com/ddua/ contrastive-estimation.","{216035859: 'Asai and Hajishirzi, 2020;', 201058633: 'Lin et al., 2019', 215238846: 'Gardner et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.584.pdf
906,252872925,Improving Question Answering with Generation of NQ-like Questions,conclusion and future work,Insight-tree,"we clearly observe from the results that adding filtered nq-like questions from the qb data has given a boost over using only nq questions. in finer detail, we observe that questions from the last sentence of the qb are of higher quality than from intermediate sentences and therefore provide a higher boost to performance even with less sam-ples. even by simply adding questions generated from last sentence, we increase the exact match accuracy by nearly 2 points. we also observe that the bleu score of answers generated from quality controlled nq like system is 16 points more than the bleu score of the baseline qb system for the rag system and by 13 points for the drqa system. this shows that our algorithm to generate nq-like questions has been effective in improving the quality of the training dataset.","{86611921: 'Kwiatkowski et al., 2019'}",https://export.arxiv.org/pdf/2210.06599v1.pdf
907,222125277,Under review AUTOREGRESSIVE ENTITY RETRIEVAL,conclusions,Insight-tree,"in this work, we propose genre, a novel paradigm to addresses entity retrieval: generate entity names autoregressively. entity names have several properties that might help (even humans) retrieving them, including a compositional structure and a predictable interaction with the context. the autoregressive formulation allows us to directly capture some of these properties, leading to several advantages with respect to current solutions, including an efficient way to cross encode men-tion context and entity candidates, a much smaller memory footprint, and the ability to compute an exact softmax without the need to subsample negative data. we empirically show that these characteristics, combined with constrained decoding strategies, led to state-of-the-art performance on a plethora of entity retrieval datasets, spanning entity disambiguation, end-to-end entity linking, and page-level document retrieval, while resulting in systems with a remarkably contained memory footprint, a space reduction by a factor of twenty on average. we additionally demonstrate that new entities can be effectively considered in our system by simply appending their unambiguous name to the candidate set.","{86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018c'}",https://arxiv.org/pdf/2010.00904v1.pdf
908,250334200,Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,conclusion,Insight-tree,"we proposed a simple yet effective approach for multi-task training of the fid retrieval-augmented generation model on the kilt benchmark. we cleaned (and downsampled were necessary) the training set by removing query-answer pairs with low relevance confidence. we demonstrated that this approach substantially improves two imbalanced tasks, and has a smaller benefit on two of the remaining five tasks. by scaling the model capacity we achieve state-of-the-art results on five kilt tasks evaluated by the leaderboard.","{86611921: 'Kwiatkowski et al., 2019', 221507798: 'Petroni et al. 2021'}",https://arxiv.org/pdf/2207.03030v1.pdf
909,248572452,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,conclusion,Insight-tree,"we introduce proqa, a unified qa paradigm that adopts a single model for solving various qa tasks with the bridge of a structural prompt. structural prompt simultaneously models the common ability required for various tasks and keeps the speciality of each task, through a structurally designed learnable input schema. we further conduct structural prompt-based pre-training, seeking to empower the model with general qa-centric ability and injects the semantic knowledge of the structural prompt into the pre-training model. experimental results on 11 qa benchmarks demonstrate that proqa can significantly boost performance on all settings. further analyses show that our method can better mitigate the catastrophic forgetting issue during continual learning, and our method can be adapted to a newly involved task more quickly, by taking the advantages of the structural prompt. in the future, we hope our analysis could inspire more explorations on the unified qa methods, or the unification of distinct tasks with complex inputs modeling by the structural prompt. we also hope structural prompt can be further utilized into the unification of more tasks with complex inputs.","{165163607: 'Clark et al., 2019', 230433978: 'Ram et al., 2021', 173188058: 'Talmor and Berant, 2019;', 52822214: 'Yang et al., 2018;', 219965751: 'Zeng et al., 2020'}",https://www.aclanthology.org/2022.naacl-main.313.pdf
910,215737171,Longformer: The Long-Document Transformer,conclusion and future work,Insight-tree,"we present longformer, a transformer-based model that is scalable for processing long documents and that makes it easy to perform a wide range of document-level nlp tasks without chunking/shortening the long input and without complex architecture to combine information across these chunks. longformer employs an attention pattern that combines local and global information while also scaling linearly with the sequence length. longformer achieves state-of-the-art results on the character-level language modeling tasks of text8 and enwik8. when pretrained, longformer consistently outperforms roberta on long document tasks and sets new state-of-the-art results on wiki-hop and triviaqa. for future work, we would like to explore other attention patterns that are more efficient by dynamically adapting to the input. we also would like to apply our model to other relevant long document tasks such as summarization.","{207853300: 'Fang et al., 2019', 204823992: 'Fisch et al., 2019', 202542881: 'GlaÃ et al., 2019', 207870753: 'Tu et al., 2019;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2004.05150v1.pdf
911,221340891,RELATION/ENTITY-CENTRIC READING COMPREHENSION,conclusion,Insight-tree,"we presented a large-scale person-centered cloze dataset. the dataset is not anonymized, and each passage is a raw text which is not only natural but also easier to be pre-processed by syntactic and semantic parsers. in the dataset construction, we used baseline suppression,","{52822214: '[24]', 215737171: '[52]', 203836061: '[55]'}",https://arxiv.org/pdf/2008.11940v1.pdf
912,221340891,RELATION/ENTITY-CENTRIC READING COMPREHENSION,conclusion,Insight-tree,"in this work, we claimed and empirically showed that the success of aggregation readers and explicit readers could be explained by equation finally, we proposed one-hop pointer annotation to helps aggregation readers whose performance indicates that these neural networks are benefited from externally provided linguistic features, including externally annotated reference information.","{52822214: '[24]', 215737171: '[52]', 203836061: '[55]'}",https://arxiv.org/pdf/2008.11940v1.pdf
913,221340891,RELATION/ENTITY-CENTRIC READING COMPREHENSION,conclusion,Insight-tree,"we proposed the explicit reference transformer that has a simple sum layer on the top of a pre-trained transformer encoder. the sum layer, called explicit reference structure, performs over contextual token embeddings referring to each candidate answer. our model is simple and efficiently fine-tuned over wikihop, and its performance is significantly better than that of models with the similar parameter size.","{52822214: '[24]', 215737171: '[52]', 203836061: '[55]'}",https://arxiv.org/pdf/2008.11940v1.pdf
914,221340891,RELATION/ENTITY-CENTRIC READING COMPREHENSION,conclusions and contribution,Insight-tree,"in this study, we developed and tested our knowledge extraction and representation system intended to support material design, by representing knowledge as relationships. knowledge was represented as relationships in pspp design charts. we leveraged weakly supervised learning for relation extraction. the end-to-end system proved our concept, and its relation extraction performance was superior to that of other baseline models.","{52822214: '[24]', 215737171: '[52]', 203836061: '[55]'}",https://arxiv.org/pdf/2008.11940v1.pdf
915,254017497,Solving math word problems with process- and outcome-based feedback,limitations to generalizability of our results,Insight-tree,"we generally expect process-based and outcome-based feedback to align more closely for math compared to other domains. for math problems, incorrect traces are typically harmful for reaching correct final answers. this matches our earlier finding that outcome-supervised rms approximate process-based feedback. in contrast, in other domains, undesirable behaviors may be helpful for highly-rated outcomes, e.g., manipulation may increase reported user satisfaction. as a result, we believe optimizing for outcomes (final-answer correctness) for math problems has a stronger effect on inducing a correct process than it would in other domains.","{233297051: 'Dalvi et al., 2021'}",https://export.arxiv.org/pdf/2211.14275v1.pdf
916,254017497,Solving math word problems with process- and outcome-based feedback,conclusion,Insight-tree,"in this work, we run the first comprehensive comparison between process-and outcome-based supervision on a natural language task. we find that both types of supervision lead to similar finalanswer error rates, with our best models improving the state-of-the-art final-answer error on gsm8k from 16.8% to 13.8% when using outcome-based supervision and to 12.9% when using processbased supervision. in contrast, we find that obtaining low trace error requires either process-based supervision, or a reward model that emulates it. a purely process-based approach of sft with prm reranking reduces the state-of-the-art trace error rate from 14.0% to 3.4%, while its outcome-based analogue achieves 12.7% trace error. however, somewhat surprisingly, we find that reward models trained with outcome-based labels result in predictions that agree more closely with the process-based labels than they do with the outcome-based labels themselves. by using this reward model during rl training, we close most of this gap, reducing trace error from 12.7% to 5.5%. while some of these conclusions may be specific to our setting of math word problems, we hope that future work explores the extent to which they generalize to other domains.","{233297051: 'Dalvi et al., 2021'}",https://export.arxiv.org/pdf/2211.14275v1.pdf
917,248512744,On Continual Model Refinement in Out-of-Distribution Data Streams,conclusion & future directions,Insight-tree,"in this paper, we propose a novel continual learning formulation named continual model refinement (cmr). the cmr problem aims to efficiently fix prediction errors when learning in outof-distribution data streams without catastrophically forgetting the acquired knowledge. for studying such a realistic and complex problem, we presented a dedicated evaluation protocol with a general method to create non-stationary, diverse ood data streams for analysis. also, we design multiple evaluation metrics to deliver a comprehensive yet concise measurement of cmr methods. the proposed cmr problem with our comprehensive analysis opens up a range of new opportunities for studying continual learning problems that are closer to real-world applications for the nlp community and beyond. for example, based on our results and analysis about (q3) and (q6), we find that it is promising to study how we can integrate both regularization methods and replay methods for mitigating the forgetting issue while improving the generalization ability. the analysis about (q5) suggests that developing more stable ranking criteria is also important to conditional replay methods (e.g., our simple extension maxloss can outperform mir under specific settings). developing cmr methods of which the configurations can generalize to diverse types of streams is also an important challenge. we release our codebase and processed datasets for supporting the reproducibility of our experiments and future research.","{204823992: 'Fisch et al., 2019', 237347226: 'Jin et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.acl-long.223.pdf
918,256459315,The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models,conclusion,Insight-tree,"in this work, we investigate the effects of training mrc models with unanswerable questions on their robustness against adversarial attacks. we construct adversarial samples from answerable and unanswerable questions in squad 2.0 and evaluate three mrc models fine-tuned on either squad 1.1 (v1 models) or squad 2.0 (v2 models) independently. adversarial attacks on answerable questions reveal that v2 models initially show little improved robustness over v1 models yet possess a latent ability to deal with these attacks that v1 models do not; the correct responses are often hidden as second-best answers, an indicator of the ""hidden robustness"" of v2 models resulting from additional training on unanswerable questions. by eliminating the ""unanswerable"" option and forcing v2 models to output an answer to any answer-  able questions, we leverage this hidden robustness to improve the performance of mrc models to attacks on answerable questions. furthermore, we also show that this robustness translates well to out-of-domain test sets.","{208201969: 'Sugawara et al., , 2020', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2023.eacl-main.113.pdf
919,254564450,Prompting Is Programming: A Query Language for Large Language Models Prompting Is Programming: A Query Language for Large Language Models,conclusion,Insight-tree,"in this work, we introduce the concept of language model programming, a novel way to interact with (large) language models. we presented lmql, a high-level query language, offering a concise and intuitive syntax. lmql implements purpose-designed evaluation semantics, which enable efficient query execution. we have substantiated this claim in a series of case studies, where we demonstrate that complex, state-of-the-art prompting techniques can be implemented as intuitive, concise and efficient lmql programs that reduce (compute) costs by up to 80%.","{233297024: '[24]', 52822214: '[32]'}",https://export.arxiv.org/pdf/2212.06094v3.pdf
920,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,limitations,Insight-tree,"the spin model achieves linear complexity via advanced self-attention mechanisms; this also introduces additional hyper-parameters into the model, potentially increasing tuning time. the resulting architecture remains overparametrized even after with small numbers of inducing points h, f and may overfit.the primary source of expressivity is attention between datapoints, which has a query dimension he.highly expressive models may learn to ignore the training set and operate in a fully-parametric mode; this failure mode is best avoided via regularization and large datasets. interestingly, our approach benefits from big data, while classical non-parametric models work best on small datasets due to their computational complexity. regularization via small h, f , dropout, and feature masking control overfitting; we will explore more compact architectures in future work.","{52822214: '[69]', 220831004: '[70]'}",https://arxiv.org/pdf/2205.11718v1.pdf
921,249018129,Semi-Parametric Deep Neural Networks in Linear Time and Memory,conclusion,Insight-tree,"in this paper, we introduce a domain-agnostic general-purpose architecture, the semi-parametric inducing point network (spin). unlike previous semi-parametric approaches whose computational cost grows quadratically with the size of the dataset, our approach scales linearly in the size and dimensionality of the data by leveraging a cross attention mechanism between datapoints and induced latents, allowing it to scale to large datasets.","{52822214: '[69]', 220831004: '[70]'}",https://arxiv.org/pdf/2205.11718v1.pdf
922,233219449,What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models,conclusions and discussion,Insight-tree,"we show that training multiple heads on top of a pre-trained language model creates a steering effect, where the target head influences the behaviour of another head, steering it towards capabilities beyond its training objective. in three multi-task settings, we find that without any dedicated training, the steered head often outputs explanations for the model predictions. moreover, modifying the input representation based on the outputs of the steered head can lead to predictable changes in the target head predictions.","{67855846: 'Dua et al., 2019', 160009340: 'Nishida et al., 2019;', 222310757: 'Schuff et al., 2020;', 202558795: 'van Aken et al., 2019;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.646.pdf
923,253447227,Using contradictions to improve QA systems,limitations,Insight-tree,"despite the results above, multiple choice qa and extractive qa with a provided context is a limited setting that doesn't indicate the results would extend to other popular settings where nli. given that laban et al. (2022) shows similar results that contradiction is an important signal in factual consistency we are hopeful that it would.","{204823992: 'Fisch et al., 2019', 219721462: 'Kamath et al. 2020', 207756753: 'Nie et al., 2020'}",https://export.arxiv.org/pdf/2211.05598v1.pdf
924,235097600,TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration,conclusion,Insight-tree,"the 2021 edition of the shared task on multi-hop inference for explanation regeneration was a success, with 4 participating teams each substantially improving performance over the baseline model. the best performing team, deepblueai, produced a system that improves absolute performance by 32%, up to 0.820 ndcg, bringing overall state-ofthe-art performance at this relevancy ranking aspect of multi-hop inference to a moderate level. we hope that future systems for many-hop multi-hop inference that aim to build large detailed explanations for question answering will be able to leverage these results to build strong relevancy retrieval subcomponents to augment their compositional inference algorithms. award #1815948, ""explainable natural language inference""). this edition of the shared task would not have been possible without the hard work of a number of relevance annotators, and their generous offer to anonymously use their data while their work is under review. a special thanks to andrÃ© freitas for the helpful discussions. additionally, we would like to thank the computational shared facility of the university of manchester for providing the infrastructure to run our experiments.","{139103297: 'Chen and Durrett, 2019;', 208089867: 'Jansen and Ustalov, 2019', 174801764: 'Min et al., 2019;', 221749191: 'Trivedi et al., 2020', 231883811: 'Valentino et al., 2021', 218487313: 'Yadav et al., 2020', 52822214: 'Yang et al., 2018;'}",NaN
925,258841405,"HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision",conclusion,Insight-tree,"we present hug, a probabilistic, principled approach for explainable multi-hop reasoning without rationale supervision. hug explicitly models multi-hop reasoning by considering the dependency between documents and between sentences within a document. experimental results demonstrate that hug outperforms other state-of-the-art methods that do not rely on rationale labels.","{207853300: 'Fang et al., 2020', 230799347: 'Geva et al., 2021', 215768725: 'Groeneveld et al., 2020;', 198229624: 'Joshi et al., 2020;', 174801764: 'Min et al. 2019', 202773198: 'Qi et al. 2019', 211258744: 'Tang et al., 2021', 236771976: 'Trivedi et al., 2022', 207870753: 'Tu et al., 2020', 158046817: 'Tu et al., 2019;', 237258250: 'Wiegreffe and Marasovic, 2021', 237433880: 'Xu et al. 2021', 202785879: 'Yadav et al. 2019', 218487313: 'Yadav et al. 2020', 52822214: 'Yang et al. 2018', 243865275: 'Zhao et al. 2021'}",https://export.arxiv.org/pdf/2305.14237v1.pdf
926,202558795,How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations,conclusion and future work,Insight-tree,our work reveals important findings about the inner functioning of transformer networks. the impact of these findings and how future work can build upon them is described in the following:,{52822214: '[40]'},https://arxiv.org/pdf/1909.04925v1.pdf
927,264406133,Improving Question Generation with Multi-level Content Planning,conclusion and future works,Insight-tree,"this paper presents multifactor, a novel qg method with multi-level content planning.specifically, multifactor consists of a fa-model, which simultaneously select important phrases and generate an answer-aware summary (a full answer), and q-model, which takes the generated full answer into account for question generation.both fa-model and q-model are formalized as our simple yet effective pet.experiments on hotpotqa and squad 1.1 demonstrate the effectiveness of our method.","{235678938: 'Cao and Wang, 2021;', 248780551: 'Fei et al., , 2022 ', 246823260: 'Su et al., 2022', 224705407: 'Su et al., 2020;', 227231411: 'VeliÄkoviÄ et al., 2018', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.13512v2.pdf
928,264406133,Improving Question Generation with Multi-level Content Planning,limitations,Insight-tree,"our work may have some limitations.first, the experiments are only on english corpus.the effectiveness of multifactor is not verified on the datasets of other languages.second, the context length in sentence-level qg task is not very long as shown in table 8.for particularly long contexts (> 500 or 1000), it needs more explorations.multifactor aims to improve the performance of the answer-aware qg task, especially the complex qg.during our research, we did not collect any other datasets, instead conduct our experiments and construct the corresponding full answer on these previously works.our generation is completely within the scope of the datasets.even the result is incorrect, it is still controllable and harmless, no potential risk.the model is currently english language only, whose practical applications is limited in the real world.","{235678938: 'Cao and Wang, 2021;', 248780551: 'Fei et al., , 2022 ', 246823260: 'Su et al., 2022', 224705407: 'Su et al., 2020;', 227231411: 'VeliÄkoviÄ et al., 2018', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.13512v2.pdf
929,14182996,Integrating miRNA and mRNA Expression Profiling Uncovers miRNAs Underlying Fat Deposition in Sheep,conclusion,Insight-tree,"in conclusion, we identified a number of mirnas that are differentially expressed between the fat-tailed and short-tailed sheep breeds. we further highlighted gene targets of related mirnas that may be involved in regulating fat deposition and adiposeness, in sheep and other livestock. this occurs via the key signaling pathways including focal adhesion, pyruvate metabolism, and the mapk, foxo, and tnf signaling pathway. further studies are needed to verify the correlation between key mirnas and their target genes by in vitro approach and elucidate the functional impacts that mirnas serve during adiposeness. our results also provide evidence for the interaction of mirnas and genes in the regulation of obesity and metabolic syndromes, which suggests that this may serve as an animal model for human' obesity and metabolic syndromes researches.",{},NaN
930,254564418,Momentum Contrastive Pre-training for Question Answering,conclusion,Insight-tree,"this paper presents a novel pre-training method mcross for extractive qa which contains two tasks: 1) contrastive learning and 2) answer term prediction. specifically, mcross adapts moco frameworks to maintain consistency in answering cloze-like and natural questions, enabling pretrained models to have a more comprehensive understanding of supporting passages. the empirical experiments on three public datasets demonstrate that our approach can obtain noticeable improvements in extractive qa tasks in supervised and zero-shot scenarios.","{248563058: 'Caciularu et al., 2022', 204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 230433978: 'Ram et al., 2021', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.emnlp-main.291.pdf
931,254564418,Momentum Contrastive Pre-training for Question Answering,limitations,Insight-tree,"although mcross can already obtain satisfactory qa performance, due to limited time and computational resources, we only use 5 million cloze-like samples for pre-training, which is one-twentieth of the scale of original sspt experiments.","{248563058: 'Caciularu et al., 2022', 204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 230433978: 'Ram et al., 2021', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.emnlp-main.291.pdf
932,258332119,Exploring the Curious Case of Code Prompts,conclusion,Insight-tree,"in this work we investigate whether or not there exists a systematic performance difference between prompting plms with code or with text. we confirm that there are indeed tasks for which code prompting is significantly more effective than text prompting and that this finding holds across different types of models. however, for most tasks, we find that text prompting is still the best method for eliciting few-shot generalization from plms. given this result it seems reasonable to attempt to predict which tasks will benefit from code prompts and which tasks will not. however, we show that making such predictions based on simple heuristics such as domain and task category is difficult and that the larger trends remain unclear. future work should seek to investigate the core mechanism behind what makes code prompting effective for certain tasks.",,https://www.aclanthology.org/2023.nlrse-1.2.pdf
933,259096138,Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation,conclusion and future work,Insight-tree,"in this work, we propose to incorporate the cloze task to improve neural machine question answering with a few training examples. the key idea is to identify and mask the informative entities in the passage and make the model predict them correctly. through empirical experimental studies on various qa benchmarks and different few-shot settings, we show that the cloze task indeed benefits the qa task due to its commonalities. we find different ways of incorporating the cloze task improve the qa task while prompt-tuning brings the most. looking forward, it is of interest to explore qa-dedicated pre-training and ways of pipelining pretraining and prompt-tuning for downstream few-shot qa needs.","{237420912: '[5]', 204823992: '[9]', 198229624: '[13]', 211258652: '[27,', 230433978: '[30]', 52822214: '[44]'}",https://export.arxiv.org/pdf/2306.04101v1.pdf
934,207852656,Meta Answering for Machine Reading,conclusion,Insight-tree,"meta-answering is a framework for qa that attempts to simulate real-world-imperfectinformation-seeking tasks, where humans look for answers in settings mediated by machines, using natural language. human meta-answerers can compete with a bert-based single system with access to full documents, by only looking at a five token window around candidates. a machine meta-answerer built on bert can improve the environment's qa system, thus proving that it is possible to investigate mr in imperfect information settings in high-performance regimes. further, the task brings to the surface, yet again but from a novel perspective, limitations of the current nlu paradigm. mma cannot use the contextual information that is effortlessly exploited by humans. thus, it might prove a suitable framework to advance on these challenges.","{160009340: 'Nishida et al. 2019', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/1911.04156v2.pdf
935,245329570,Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering,conclusion,Insight-tree,"in this work, we propose a reasoning chain based adversarial attack for multi-hop qa. by formulating the multihop reasoning process with a reasoning chain, we can identify different reasoning types and customize adversary design for each type. our method allows to attack any certain hop by identifying different hop spans of the question, and making modification on relational words. the hop-targeted attack can inspect models' error-prone parts during the reasoning process specifically. three qa models under evaluation both exhibit poor performance in face of adversaries, suggesting that they are not robust enough and have limited interpretability of conducting multi-hop reasoning. our adversarial evaluation can be utilized to improve models' performance by adversarial retraining as well as motivate new model development according to the weakness detected.","{153312687: 'Ding, Zhou, Chen, Yang and Tang, 2019', 207853300: 'Fang et al., 2020', 155100120: 'Qiu et al., 2019', 221749191: 'Trivedi, Balasubramanian, Khot and Sabharwal, 2020', 207870753: 'Tu et al., 2020', 158046817: 'Tu, Wang, Huang, Tang, He and Zhou, 2019;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2112.09658v1.pdf
936,218487288,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,conclusion,Insight-tree,"in this paper, we propose a generator of multi-hop knowledge paths, which provides structured evidence for answering commonsense questions. the generator, learned by fine-tuning gpt-2 on random walks sampled from conceptnet, produces a path between each pair of question and answer entities. all generated paths are aggregated into a knowledge embedding and fused with a context embedding given by a text encoder for classification. our qa framework enhanced with this generator outperformes both pre-trained language models and prior kg-augmented methods on two commonsense qa benchmarks. the accuracy gain increases with less training data. furthermore, automatic-and human-based evaluations of the generated paths yield high scores for their validity, novelty, and relevance. future research should investigate how to optimally fuse the knowledge and the context embeddings. it should also address the ambiguity of the entity mentions in the questions, the answers, and the lexical nodes in conceptnet.",{},https://www.aclweb.org/anthology/2020.findings-emnlp.369.pdf
937,237605111,BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles,conclusion,Insight-tree,"in this paper, we introduce birdqa, a large-scale, bilingual multiple-choice question answering dataset to facilitate the development of qa systems capable of solving tricky riddles. the huge gap between the human and machine leaves much room for improvement. in future work, we plan to extend birdqa with riddles in other languages and incorporate figurative language understanding into riddle solving. we hope that birdqa will stir more research for question answering on riddles.","{202767252: 'Jing, Xiong, and Yan 2019', 52822214: 'Yang et al. 2018'}",https://arxiv.org/pdf/2109.11087v2.pdf
938,174801080,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,conclusion,Insight-tree,"we proposed decomprc, a system for multihop rc that decomposes a multi-hop question into simpler, single-hop sub-questions. we recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions. moreover, decomprc achieved further gains from the decomposition scoring step. decomprc achieved the state-of-the-art on hotpotqa distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines. in this section, we describe span annotation collection procedure for bridging and intersection questions.",,https://www.aclweb.org/anthology/P19-1613.pdf
939,207917676,Bend but Don't Break? Multi-Challenge Stress Test for QA Models,conclusion,Insight-tree,we conclude our discussion by presenting suggestions for good future practices when building and presenting new models and datasets. we constructively offer these points and have no intent to criticize authors whose prior work we reference.,"{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/D19-5818.pdf
940,261276895,MEMORY-VQ: Compression for Tractable Internet-Scale Memory,conclusion,Insight-tree,"we introduced memory-vq, a novel approach for reducing the storage requirements of memoryaugmented language models without compromising performance. by employing vq-vae to compress token representations, we obtain a lumen model with 16x compression, denoted as lumen-vq. remarkably, lumen-vq maintains performance close to lumen and fid and benefits from lumen inference speed-ups with sharply reduced storage cost. using memory-vq, memory augmentation is a practical solution for drastic inference speedups with extensive retrieval corpora. fine-tuning during fine-tuning, we utilize the adafactor optimizer (shazeer and stern, 2018) with a constant learning rate of 0.0001, a batch size of 128, and a dropout rate of 0.1 for all tasks. when performing multi-task training, we uniformly sample from the tasks. we allocate 48 and 304 tokens for question and passage inputs, respectively. lumen-vq is using 0.999 as an ema factor for code updates.","{86611921: 'Kwiatkowski et al., 2019', 221507798: 'Petroni et al., 2021', 244799249: 'Santhanam et al., 2022', 52822214: 'Yang et al., 2018', 234334398: 'Zemlyanskiy et al., 2021;'}",https://export.arxiv.org/pdf/2308.14903v1.pdf
941,237490850,"Extract, Integrate, Compete: Towards Verification Style Reading Comprehension",conclusion,Insight-tree,"in this paper, we present a novel verification style reading comprehension dataset named vgaokao from the chinese language tests of gaokao for chinese native speakers, which embed multiple advanced language understanding skills. to address the challenges in vgaokao, we propose a new extract-integrate-compare approach for complementary evidence retrieval/integration and option discrimination. experiments show that our approach outperforms several strong baselines, with additional merits of efficiency and explainability. we believe vgaokao is a challenging test-bed for natural language understanding in chinese and encourage further research in verification style reading comprehensionn.","{86611921: 'Kwiatkowski et al., 2019', 211258645: 'Perez et al., 2020', 202773198: 'Qi et al. 2019', 214233456: 'Sun et al., 2020', 233219392: 'Zhao et al. 2021'}",https://arxiv.org/pdf/2109.05149v1.pdf
942,3520779,ATTac-2000: An Adaptive Autonomous Bidding Agent,conclusion and future work,Insight-tree,"tac-2000 was the rst autonomous bidding agent competition. while it was a very successful event, some minor improvements would increase its interest from a multiagent learning perspective.",{},https://arxiv.org/pdf/1106.0678v1.pdf
943,226262208,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,conclusion,Insight-tree,"we introduced iirc, a new dataset of incompleteinformation reading comprehension questions. these questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways. our baseline model, built on top of state-ofthe-art models for the most closely related existing datasets, performs quite poorly in this setting, even when given oracle retrieval results, and especially when combined with other reading comprehension datasets. iirc both provides a promising new avenue for studying complex reading and retrieval problems and demonstrates that much more research is needed in this area.","{139103297: 'Chen and Durrett, 2019', 212657414: 'Clark et al., 2020', 67855846: 'Dua et al., 2019b', 202712552: 'Khot et al., 2019', 202558815: 'Min et al., 2019a', 174801764: 'Min et al., 2019b', 173188058: 'Talmor and Berant, 2019;', 52822214: 'Yang et al., 2018;', 201657804: 'Yuan et al. 2020'}",https://arxiv.org/pdf/2011.07127v1.pdf
944,261276512,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,limitations of chatgpt's self-attention mechanism in chess gameplay,Insight-tree,"chatgpt's self-attention mechanism plays a crucial role in its performance, especially in chess gameplay. our experiments reveal two critical limitations of transformer-based lms like chatgpt when trained on natural language. the first limitation is related to the increase in imr and rblm over the course of a game. as highlighted by [stÃ¶ckl, 2021], gpt-2 models are found to devote less attention to san notation tokens that are farther away from the latest input. since a complete game memory is paramount for models to accurately track the board state [toshniwal et al., 2022], we postulate that chatgpt's disproportionate attention allocation might be the cause of a significant portion of its mistakes. this limitation is evident across all variations that depend on formal language but don't actively reinforce the game state, which presents a challenge to the effectiveness of llms in tasks requiring extended conversation memory.the second limitation pertains to the tendency of natural language trained llms to neglect formal language where tokens are used in an unconventional manner. maynez et al.[2020] noted that lms typically remain indifferent to noises or artifacts in training data, which we argue may also apply to formal languages like chess notations. this issue is particularly evident in the int-rules variation, where despite the introduction of helpful data, chatgpt's performance dropped substantially. we hypothesize that this may be due to the model shifting its focus towards the rules, thereby reducing the attention allocated to the game board.these identified limitations, while challenging, also provide valuable insights for future research. for instance, addressing the second limitation might involve frequent repe-tition of formal language sequences, potentially leading to more substantial improvements in game performance. our findings is a first step towards investigating techniques such as token repetition's impact on model performance, laying the ground work for future work to explore how we can mitigate the impact of disproportionate attention allocation.",{},https://export.arxiv.org/pdf/2308.15118v1.pdf
945,261276512,Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills,conclusion,Insight-tree,"in summary, our investigation reveals that despite its exceptional capabilities in natural language processing, chatgpt faces considerable challenges with complex reasoning tasks involving formal language, as evidenced by its chess gameplay performance. the model's attention mechanism exhibits limitations in adequately recognizing tokens used in formal language, resulting in a suboptimal understanding of the game board. interestingly, our findings indicate that consistent repetition of relevant information throughout a conversation can partially alleviate this limitation. yet, despite chat-gpt's capacity to learn and internalize rules, the model struggles with self-regulation, which neither in-prompt instructions nor improved board comprehension appear to enhance. additionally, we find that the model's decision-making focus, or ""intent,"" can be strengthened by allowing nl reasoning, providing nl chessboard descriptions or enabling a clearer representation of the game board. future research could examine how this disproportionate attention allocation impacts other tasks that involve formal language and necessitate complex cognitive processing. in conclusion, while chatgpt stands as a remarkable advancement in artificial intelligence, it continues to face significant limitations, especially in nonlinguistic contexts. these findings highlight the necessity for further refinement before chatgpt, and models of its kind, can be considered reliable tools for practical applications requiring complex cognition akin to human abilities.   ",{},https://export.arxiv.org/pdf/2308.15118v1.pdf
946,233933158,Phenotypic Divergence of P Proteins of Australian Bat Lyssavirus Lineages Circulating in Microbats and Flying Foxes,conclusions,Insight-tree,"taken together, our data indicate that the p proteins of the two established lineages of ablv, which are associated with frugivorous and insectivorous bats, differ in immune evasion and nuclear trafficking functions. the effects on immune antagonism and nuclear import are independent, resulting from altered sequences in different regions of the proteins, indicative of distinct mechanisms including altered interactions with stats (see summary in table 1). these data suggest that adaptation to the different bat hosts includes changes in p protein, which has multiple roles in viral infection including in replication and immune evasion. these data are consistent with potential differences in the virus-host interface of different bat species.  figure s1. cos7 cells were transfected to express the indicated proteins or empty vector (ev) and with plasmids for the ifnÎ±/stat1 reporter assay before treatment with or without ifnÎ± and calculation of luciferase activity, as described in the legend of figure 2. **** p < 0.0001. supplementary figure s2. hela cells expressing the indicated proteins were treated with or without lmb before analysis by clsm and calculation of fn/c (mean Â± sem), as described in the legend of figure 2. **** p < 0.0001, n â¥ 40 cells for each condition.",{},https://web.archive.org/web/20210429115151/https:/www.preprints.org/manuscript/202102.0434/v1/download
947,252907685,MTEB: Massive Text Embedding Benchmark,conclusion,Insight-tree,"in this work, we presented the massive text embedding benchmark (mteb). consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, mteb aims to provide reliable embedding performance estimates. by opensourcing mteb alongside a leaderboard, we provide a foundation for further pushing the state-ofthe-art of available text embeddings.",{},https://www.aclanthology.org/2023.eacl-main.148.pdf
948,252907685,MTEB: Massive Text Embedding Benchmark,limitations of mteb,Insight-tree,"while mteb aims to be a diverse benchmark to provide holistic performance reviews, the benchmark has its limitations. we list them here:1. long document datasets mteb covers multiple text lengths (s2s, p2p, s2p), but very long documents are still missing. the longest datasets in mteb have a few hundred words, and longer text sizes could be relevant for use cases like retrieval.2. task imbalance tasks in mteb have a different amount of datasets with summarization consisting of only a single dataset. this means mteb average scores, which are computed over all datasets, are biased towards tasks with many datasets, notably retrieval, classification and clustering. as mteb grows, we hope to add more datasets to currently underrepresented tasks like summarization or pair classification.3. multinguality mteb contains multilingual classification, sts and bitext mining datasets. however, retrieval and clustering are english-only. sgpt-bloom-7b1-msmarco is geared towards multilingual retrieval datasets and due to the lack thereof cannot be comprehensively benchmarked in mteb. further, mteb does not contain any code datasets that could be used to benchmark code models (neelakantan et al., 2022;allal et al., 2023). it should be easy to extend mteb with datasets, such as codesearchnet",{},https://www.aclanthology.org/2023.eacl-main.148.pdf
949,226262229,Coarse-to-Fine Query Focused Multi-Document Summarization,conclusions,Insight-tree,"in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization. we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments. experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more  gpus with 11gb memory. for the answer sentence selection model, bert was fine-tuned with a learning rate of 3 Ã 10 â6 and a batch size of 16 for 3 epochs . for span selection, we adopted a learning rate of 3 Ã 10 â5 and a batch size of 64 for 5 epochs. during inference, the confidence threshold for the relevance estimator was set to Î¸ = 0.75 (kratzwald and feuerriegel, 2018) for both sentence and passage retrieval. for the evidence estimator, k qa was tuned on the development set. we obtained 90 and 110 evidence sentences from the sentence selection and span selection models, respectively. for the centrality estimator, the influence of the query was set to Ï = 0.15 (wan, 2008;wan and zhang, 2014","{52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf
950,252089700,A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension,conclusion,Insight-tree,"we have covered the shortcut identification and mitigation landscape in mrc. the presence of shortcuts can be made clear through a variety of methods, and most researchers are aware of this issue. mitigation methods are varied and have some degree of success, but a lot more work is needed before we can achieve models mostly free of shortcut biases. efforts should be made to improve mrc shortcut debiasing techniques by incorporating those found in other fields such as computer vision and nli, as well as finding methods with lower human and/or computation costs.","{233444226: 'Bartolo et al. 2021', 216868500: 'Ko et al. 2020', 235293903: 'Lai et al. 2021 and', 235755349: 'Lee et al. 2021', 216867120: 'Miller et al. 2020', 174801764: 'Min et al. 2019a', 174801080: 'Min et al., 2019b;', 211258645: 'Perez et al., 2020', 207917676: 'Pugaliya et al., 2019;', 256846551: 'Ribeiro et al. 2023', 236447339: 'Rogers et al. 2023', 212644640: 'Schlegel et al., 2020b;', 231709861: 'Sugawara et al. 2021', 211258744: 'Tang et al. 2021', 221749191: 'Trivedi et al. 2020', 236771976: 'Trivedi et al. 2022', 211003735: 'Wolfson et al. 2020', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2209.01824v2.pdf
951,221970190,What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams,conclusion,Insight-tree,"we present the first open-domain multiple-choice question answering dataset for solving medical problems, medqa, collected from the real-world professional examinations, requiring extensive and advanced domain knowledge to answer questions. this dataset covers three languages: english, simplified chinese, and traditional chinese. together with the question data, we also collect and release a largescale corpus from medical textbooks from which the reading comprehension models can obtain necessary knowledge for answering the questions. we implement several state-of-theart methods as baselines to this dataset by cascading two components: document retrieval and reading comprehension. and experimental results demonstrate that even current best approach cannot achieve good performance on these data. we anticipate more research efforts from the community can be devoted to this dataset so that future openqa models can be strong enough to solve such real-world complex problems.","{215745470: 'Gao et al. 2020', 214233456: 'Sun et al. 2019', 52822214: 'Yang et al. 2018;'}",https://arxiv.org/pdf/2009.13081v1.pdf
952,222225265,Visuo-Linguistic Question Answering (VLQA) Challenge,conclusion,Insight-tree,"in this work, we introduced the visuo-linguistic question answering (vlqa) challenge that we believe has the potential to open new research avenues in areas of joint vision & language. our experiments show that a system equipped with state-of-the-art vision-language pre-training does not perform well on the task that requires joint image-text inference. there is a room for significant improvement in capability of these models to tackle multi-modal contexts. our future work would include further expansion of this dataset and building generic ai models that can learn novel visual concepts from a small set of examples.",{},https://www.aclweb.org/anthology/2020.findings-emnlp.413.pdf
953,257353502,SPARSE MOE AS THE NEW DROPOUT: SCALING DENSE AND SELF-SLIMMABLE TRANSFORMERS,conclusion,Insight-tree,"in this paper, we present a novel plug-and-play smoe-dropout strategy for training overparameterized transformers in full-capacity settings without collapse. we design a fixed and randomly initialized router to assign experts and gradually increase their number along with the training. as a result, our proposal provides an appealing ""self-slimmable"" property to large transformers during inference and downstream fine-tuning, depending on available resources. it implies alleviated representation collapse and delivers an in-situ trade-off between efficiency and performance. extensive experiments across various combinations of network backbone and dataset, consistently demonstrate the significantly improved performance and training time savings from our algorithm. future work includes the extension of other network architectures and tasks like vision recognition. smoe-dropout (k = n 2 ) 82.03 Â± 0.26 smoe-dropout (k = n) 82.32 Â± 0.14 to evaluate the stability of the improvement obtained by our smoe-dropout, we carry out further experiments of transformer-xl on sst-2. the results are reported in table a4, from which we can observe that our smoe-dropout achieves a statistically significant improvement of 0.93% â¼ 1.17% accuracy gains compared with other smoe-variants and the dense network, where there is no overlap between the error bars (one standard deviation). table a5 demonstrates that both random routing policy and progressively increasing the number of activated experts are beneficial for alleviating representation collapse and providing ""selfslimmable"" property, yet not as good as combining both. to be specific, when applying the strategy of progressively enlarging the number of activated experts, the learnable smoes suffer less representation collapse and achieve better performance, i.e., 0.31% higher accuracy. meanwhile, we find that learnable smoe with curriculum learning has the ""self-slimmable"" property only when activating experts from k = 1 to k = 8. however, the performance starts to degrade if using more experts like k = 16. as for our smoe-dropout with a random routing, it enjoys a better ""self-slimmable"" property from k = 1 to k = 16 (full model capacity), with up to 0.87% higher accuracy on sst-2 across all scenarios, compared to its learnable variants. we conduct a further transfer study of the pre-trained bert networks on a multi-hop questionanswering dataset, hotpotqa yang et al. (2018). and we use exact match (em) accuracy to assess networks' performance. following the same metric in press et al. (2022), we calculate the compositionality gap, i.e., the gap of em accuracy between multi-hop question answering and its all single-hop sub-questions , of each network. as shown in table a6, our smoe-dropout is beneficial for reducing the compositionality gap, which achieves the best performance with up to 0.30% higher em score and 0.30% narrower compositionality gap, compared with the learnable smoe and its dense counterpart.",{},https://export.arxiv.org/pdf/2303.01610v1.pdf
954,264426669,Merging Generated and Retrieved Knowledge for Open-Domain QA,conclusion and future work,Insight-tree,"in this work, we study the problem of merging retrieved and llm-generated knowledge for opendomain qa.we tackle the challenge of knowledge conflicts caused by llm's hallucination with a compatibility-oriented knowledge merging framework (combo).specifically, we match llmgenerated and retrieved passages for a given question into pairs based on their compatibility and perform information fusion on the encoder side of the fid-based reader by feeding matched pairs as input.","{246652372: 'Ji et al., 2023;', 86611921: 'Kwiatkowski et al., 2019', 235755349: 'Lee et al., 2021', 235399987: 'Oguz et al., 2022', 221507798: 'Petroni et al., 2021', 221970302: 'Xiong et al. 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.14393v1.pdf
955,251224058,Efficient Long-Text Understanding with Short-Text Models,limitations,Insight-tree,"we present sled as a simple and effective method to extend the capabilities of pretrained short-text models to long-text tasks. despite its impressive empirical performance on scrolls, sled suffers from two disadvantages which may limit its applicability to some long-range tasks.long output to obtain linear complexity, sled assumes the output length k is constant. this is since the decoder uses quadratic selfattention over the output, on top of o(nk) crossattention between the output and input. while most current long-text tasks follow this assumption, future tasks, such as academic reports or script writing, may require long text generation. this limitation is not unique to sled and affects other long-range transformers including longt5 and led. aside from finetuning, this also affects pretraining models on long inputs with selfsupervised losses such as span-corruption (raffel et al., 2020b) or denoising (lewis et al., 2020), which require the decoder to process an output that is linear in the length of the input.co-reference resolution and fact retention an assumption at the heart of sled is the locality of information assumption. when the input text is long, this assumption may break if distant entity resolution or factual knowledge are required. for example, a chapter in a book may mention ""they were walking into the room"" when knowledge of what room or who walked is located a few chapters back. in such cases, the encoder used by sled will not be able to access this information, moving more responsibility to the decoder and reducing the effectiveness of the contextual encoding. similarly, in multi-hop questions (yang et al., 2018), attending to one part of the context is necessary in order to fully understand the question and encode a second piece of information correctly. as the encoder will not have access to the first context that leads to better question understanding, here as well more responsibility is delegated to the decoder.","{189927896: 'Jiang and Bansal, 2019', 226281978: 'Tay et al., 2021', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020b'}",https://export.arxiv.org/pdf/2208.00748v3.pdf
956,251224058,Efficient Long-Text Understanding with Short-Text Models,conclusions,Insight-tree,"in this work we present sled, a simple approach for modeling long texts which slides a pretrained short-range encoder over a long input document and then generates an output by attending to the encoded tokens. we show sled can perform core operations that are important for long text understanding, such as finding relevant pieces of information and fusing them at decoding time, and demonstrate competitive performance on the scrolls benchmark compared to larger models and models that employ a dedicated and expensive pretraining step. one of sled's most attractive features is that it can be readily used with any short-range pretrained lm. thus, any future encoder-decoder model can be flexibly plugged into it to achieve further gains in performance on scrolls, some of its tasks, or any other long-range task.","{189927896: 'Jiang and Bansal, 2019', 226281978: 'Tay et al., 2021', 52822214: 'Yang et al., 2018', 220831004: 'Zaheer et al., 2020b'}",https://export.arxiv.org/pdf/2208.00748v3.pdf
957,248562985,KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering,conclusion,Insight-tree,"to bridge the gap between the pre-training and finetuning objectives, kecp views eqa as an answer generation task. in kecp, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query. the span-level contrastive learning objective is proposed to improve the performance of eqa. experiments on multiple benchmarks in both instance-level and task-level few-shot scenarios show that our framework consistently outperforms the state-of-the-art methods.","{237420912: 'Chada and Natarajan, 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.206.pdf
958,254044526,Dense Text Retrieval based on Pretrained Language Models: A Survey,conclusion,Insight-tree,"in this survey, we thoroughly review the recent progress of dense retrieval based on pretrained language models (plm). as an important evolution of language intelligence techniques, plms empower dense retrieval models with excellent modeling capacities to capture and represent text semantics for relevance matching. our survey has extensively discussed the key issues and the mainstream solutions in four major aspects to develop dense retrieval systems, including architecture, training, indexing and integration. next, we briefly summarize the discussions of this survey and introduce some remaining issues for dense retrieval.","{86611921: '[33]', 236477844: '[37]', 52822214: '[57]', 221507798: '[73]', 220302524: '[98]', 245144556: '[129]', 245218527: '[130]', 251224409: '[147]', 251953412: '[152]', 252519173: '[153]', 248405719: '[220]', 248377036: '[221]', 251224044: '[242]', 245334864: '[269]', 222125277: '[299]', 248366293: '[302]', 251594672: '[307]', 226278099: '[329]', 208267807: '[330]', 230437663: '[331]', 221970302: '[332]'}",https://export.arxiv.org/pdf/2211.14876v1.pdf
959,235794845,Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking,conclusion,Insight-tree,"in this paper we propose a multilingual extractive rc modeling approach named xltt, by a crosslingual transposition rethinking approach for lowresource extractive reading comprehension using multilingual adaptive attention to model existing erc training datasets in a multilingual context.experimental results demonstrate the effectiveness of our multilingual erc modeling approach.","{52822214: 'Yang et al., 2018', 153312687: 'Ding et al., 2019;', 208267807: 'Asai et al., 2020'}",https://arxiv.org/pdf/2107.05002v2.pdf
960,262053695,MINT: EVALUATING LLMS IN MULTI-TURN INTER-ACTION WITH TOOLS AND LANGUAGE FEEDBACK,conclusion,Insight-tree,"in this work, we present mint, an evaluation benchmark designed to evaluate llm's task-solving ability in multi-turn interaction by using tools and leveraging natural language feedback, which we","{258833449: 'Pan et al., 2023;'}",https://export.arxiv.org/pdf/2309.10691v2.pdf
961,232307674,Complementary Evidence Identification in Open-Domain Question Answering,conclusion,Insight-tree,"in the paper, we propose a new problem of complementary evidence identification and define the criterion of complementary evidence in vector space. we further design an algorithm and a loss function to support efficient training and inference for complementary evidence selection. compared to the baseline, our approach improves more than 20% and remains to scale well to the computationally complex cases.","{202660724: 'Nie et al., 2019'}",https://www.aclweb.org/anthology/2021.eacl-main.234.pdf
962,174801764,Compositional Questions Do Not Necessitate Multi-hop Reasoning,conclusions,Insight-tree,"in summary, we demonstrate that question compositionality is not a sufficient condition for multi-hop reasoning. instead, future datasets must carefully consider what evidence they provide in order to ensure multi-hop reasoning is required. there are at least two different ways to achieve this.",,https://arxiv.org/pdf/1906.02900v1.pdf
963,215238741,Knowledge Fusion and Semantic Knowledge Ranking for Open Domain Question Answering,conclusion and future work,Insight-tree,"in this work, we have pushed the current state-ofthe-art by 2.2% on openbookqa and 7.28% on qasc, two tasks that need external knowledge and knowledge composition for question answering. our semantic knowledge ranking and knowledge fusion question answering model over the bertbased language model demonstrably improves the performance on openbookqa and qasc. we also provide a dataset to learn semantic knowledge ranking using the annotations present in qasc, openbookqa, and scitail. we have analyzed the performance of the components in our qa system. our analysis shows the need to further improve knowledge ranking and knowledge composition.","{208089867: 'Jansen and Ustalov, 2019;', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2004.03101v2.pdf
964,253098398,Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning,conclusion,Insight-tree,"in this work, we explore the hypothesis that the adaptations of different delta tuning methods could all be re-parameterized as low-dimensional optimizations in a unified optimization subspace. the empirical results provide strong evidence for our hypothesis. we also extend our analysis to find the connection between fine-tuning and delta tuning.",{},https://export.arxiv.org/pdf/2210.13311v1.pdf
965,239885904,Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable,conclusion and future work,Insight-tree,"we propose a three-stage framework of relation extractor-reader and comparator (rerc), which solves the multi-hop qa task through the idea of complex question decomposition, and obtains the state-of-the-art results in the 2wikimultihopqa dataset, which is close to human performance. our rerc framework can also provide faithful evidence with excellent interpretability.","{153312687: 'Ding et al. 2019', 226236740: 'Ho et al., 2021', 218486753: 'Inoue et al., 2020', 174801080: 'Min et al. 2019', 160009340: 'Nishida et al. 2019', 211258645: 'Perez et al., 2020', 158046817: 'Tu et al., 2019;', 155100120: 'Xiao et al., 2019'}",https://arxiv.org/pdf/2110.13472v1.pdf
966,253553165,Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured Rationales,conclusion,Insight-tree,"in this paper, we propose reasoning circuits, a new framework suited to real-world scenarios where the nlp task at hand requires multiple steps of structured reasoning, with only a limited number of available labelled examples, and a small annotation budget, also only a modest deep learning computational infrastructure/budget is accessible. in this work, we apply this framework to the task of fewshot multi-hop question generation which fits all these criteria. we identify structured multi-step rationales that break down this problem into many discrete reasoning steps. each step in these rationales is treated as a single ""task"" within a mixture of similar ""tasks"". the individual tasks can be categorized into control tasks, which control the flow of information between tasks, and generative tasks, that generate free-form text for successive tasks in the reasoning circuit. the framework is relatively easy to implement, since only a single generative model is fine-tuned with a mixture of all reasoning steps; at inference time, the same model can generate all reasoning steps sequentially. we show that fine-tuning with only around 64 to 128 labelled rationale examples with our approach is enough to improve automatic evaluation metrics compared to a baseline trained without rationales on the hot-potqa dataset. more importantly, with human evaluation, we find that this framework can strongly improve the central objective of multi-hop qg, to generate challenging questions which cannot be answered from reading only a single passage.",{},https://www.aclanthology.org/2023.nlrse-1.6.pdf
967,253553165,Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured Rationales,limitations,Insight-tree,"the proposed reasoning circuits framework intends to replace the need for thousands of annotated examples with a strong inductive bias of structured rationales. there is two issues with this approach at a conceptual level: 1. it may not always be possible to break down a multi-step reasoning problem cleanly into discrete reasoning steps, and another related issue it increasing complexity of the circuit with the complexity of the task. 2. for the design of these reasoning circuits a researcher must develop a thorough understanding of this reasoning task, so that the final circuit design broadly covers all possible types of reasoning problems expected to be solved. an under-or illdesigned reasoning circuit may cause the system to either not support a certain portion of problems or produce non-sensical outputs.essentially, there is trade off between a tighter control over reasoning by investing in a deep understanding of the problem leading to a comprehensive reasoning circuit design and lower annotations budget, versus, less control over logic and depending on a large number of annotations which allow the model to discover this logic on its own at much higher cost of large scale annotations budget. at the implementation and operations level one of the the key limitations our proposed system is the number of inference steps to solve the problem. the number of times model inference may be needed to solve a single example is equal the length of the longest task sequence chain in the reasoning circuit. one possible solution for this could be by training the model to solve the entire problem by generating all the steps of reasoning and the target string in a single inference step and could massively reduce inference time and costs.",{},https://www.aclanthology.org/2023.nlrse-1.6.pdf
968,260735813,Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval,conclusion,Insight-tree,"we have implemented methods for combining explanatory context from two knowledge sources: llmgenerated rationales and retrieved paragraphs from wikipedia.the first method involves training our smaller reasoning model on ratd datasets such that it becomes proficient at reasoning over long, noisy contexts which contain information from both knowledge sources.the second method is to use rationale ranking model scores for each knowledge source as guidance in constructing contexts that may contain information from both, or either knowledge source.we have shown that both methods are individually effective in significantly improving unseen question-answering performance both versus the baselines established by hartill et al. (2023) and versus a baseline that ablates both rr and ratd methods (section 3.4.3).","{226262208: 'Ferguson et al., 2020', 230799347: 'Geva et al., 2021', 260379194: 'Hartill et al. 2023', 249097975: 'Izacard et al. 2022', 226278099: 'Jiang et al., 2020', 230437663: 'Khattab et al., 2021', 236771976: 'Trivedi et al., 2022', 221970302: 'Xiong et al. 2021'}",https://export.arxiv.org/pdf/2308.04711v3.pdf
969,225068329,Measuring Association Between Labels and Free-Text Rationales,conclusion,Insight-tree,"after demonstrating the weaknesses that pipeline models exhibit for free-text rationalization tasks, we propose two measurements of label-rationale association in self-rationalizing models. we find that on three free-text rationalization datasets for commonsenseqa and snli, models based on t5 exhibit high robustness equivalence and feature importance agreement, demonstrating that they pass a necessary sanity check for generating faithful free-text rationales.",{237258250: 'Wiegreffe and MarasoviÄ 2021'},https://www.aclanthology.org/2021.emnlp-main.804.pdf
970,236635315,Talk2Data: High-Level Question Decomposition for Data-Oriented Question and Answering,limitations and future work,Insight-tree,"here, we would like to report and discuss several limitations that was found during our system implementation and evaluation. scalability issue. the current implementation of the prototype system still cannot handle large datasets that contain tens of thousands of data records, where the answer extraction algorithm is the primary bottleneck. it will be more difficult to find out accurate answers from a large dataset within a fixed period of time. there are several approaches that could be applied to address the issue, which will be our future work. first, using parallel searching algorithms [45] will greatly improve the algorithm efficiency. second, using a pre-trained model such as tabert [64], to built a table-based q&a system, will also improve the system's performance. although such a system doesn't exist yet, we believe it is a promising direction, which will be our next plan.accuracy issue. although showing the relevant context is helpful for the answer interpretation, when mistake happens, the irrelevant charts could also be a distraction, which will affect users' judgments. we believe there are two methods that could be used to improve the accuracy of the system. first, we can employ knowledge bases such as wolfra-malpha 5 and knowledge graphs to guide the searching directions so that the answers could be more directly found without checking too many irrelevant candidates in the space. second, again, training a qa system based on tabert [64] could also help improve the accuracy.generalization issue. our training corpus is generated based on 26 tabular data that primarily contain marketing data records such as car sales values, and best selling books. as a result, our model could better handle high-level questions in the marking domain, but may have a lower question decomposition quality when facing a question from other domains. to overcome the issue, more datasets in various domains should be collected and more questions should be prepared to train the model and improve the generalization of the system.","{215785913: '[13,', 153312687: '[17,', 189927857: '19,', 211258645: '42]', 211003735: '[59]', 52822214: '62]'}",https://arxiv.org/pdf/2107.14420v1.pdf
971,236635315,Talk2Data: High-Level Question Decomposition for Data-Oriented Question and Answering,conclusion,Insight-tree,"we present talk2data, a data-oriented online question and answering system that supports answering both low-level and high-level questions. the system employs a novel deep-learning based question decomposition model to resolve a high-level question into a series of relevant low-level questions, and a search algorithm to extract the data facts that are most relevant to each of low level questions. to visualize the data facts, we designed a set of annotated and captioned visualization charts to support interpretation and narration. the proposed technique was evaluated via case studies, performance validation, and a controlled user study. the evaluation showed the power of the talk2data system and revealed several limitations of the current system, which will be addressed in the future.","{215785913: '[13,', 153312687: '[17,', 189927857: '19,', 211258645: '42]', 211003735: '[59]', 52822214: '62]'}",https://arxiv.org/pdf/2107.14420v1.pdf
972,222310757,F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,limitations of current models,Insight-tree,"we manually analyze outputs of the models by qi et al. (2019) and  and identify the following two problems. silent facts. the models make use of facts without including them into their explanations (cf., figure 1). as a result, the predicted answer does not occur in the explanation, leaving the user uninformed about where it came from.unused facts. the models predict facts to be relevant without any relation to the predicted answer. the second fact of the explanation in figure  1 is an example for this. we also found examples where the facts predicted to be relevant do not even contain the entities from the question.","{174801080: 'Min et al., 2019;', 160009340: 'Nishida et al., 2019;', 211258645: 'Perez et al., 2020', 202773198: 'Qi et al. 2019', 207870753: 'Tu et al., 2019;', 207765742: 'Ye et al., 2019;'}",https://arxiv.org/pdf/2010.06283v1.pdf
973,222310757,F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,limitations of current evaluation scores,Insight-tree,"current evaluation of xqa is focused on three scores: (i) answer-f 1 , which is based on the token overlap between the predicted and the ground truth answer, (ii) sp-f 1 , which calculates f 1 based on the overlap of predicted and ground truth relevant (""supporting"") facts and (iii) joint-f 1 , which is based on the definitions of joint precision and joint recall as the products of answer and sp precision and recall as described in . for hotpotqa, models are ranked based on joint-f 1 . we argue that this creates a false incentive that potentially hinders the development of truly usable models for the following reasons.no empirical evidence. there is no empirical evidence that joint-f 1 is related to user performance or experience regarding xqa.rewarding poor explanations. figure 1 shows an example prediction that is rewarded with a joint-f 1 of 0.5 although its explanation provides no value to the user. the reward stems from the overlap of the explanation with the ground truth but does not consider that the predicted answer is not contained in any of the predicted relevant facts.punishing good explanations. consider a model output in which the predicted answer is wrong but the explanation perfectly explains this wrong answer, showing to the user why the model has selected it. standard f 1 -scores compare the model output to the ground truth annotations and will, therefore, score both the answer and the explanation with an f 1 of 0. however, we argue that an explanation should be evaluated with a score higher than 0 if it is able to explain the reasoning process of the model to the user and, thus, lets the user identify the failure of the model.","{174801080: 'Min et al., 2019;', 160009340: 'Nishida et al., 2019;', 211258645: 'Perez et al., 2020', 202773198: 'Qi et al. 2019', 207870753: 'Tu et al., 2019;', 207765742: 'Ye et al., 2019;'}",https://arxiv.org/pdf/2010.06283v1.pdf
974,222310757,F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,conclusion,Insight-tree,"in this paper, we investigated explainable question answering, revealing that existing models lack an explicit coupling of answers and explanations and that evaluation scores used in related work fail to quantify that. this highly impairs their applicability in real-life scenarios with human users. as a remedy, we addressed both modeling and evaluation, proposing a hierarchical neural architecture, a regularization term, as well as two new evaluation scores. our user study showed that our models help the users assess their correctness and that our proposed evaluation scores are better correlated with user experience than standard measures like f 1 .","{174801080: 'Min et al., 2019;', 160009340: 'Nishida et al., 2019;', 211258645: 'Perez et al., 2020', 202773198: 'Qi et al. 2019', 207870753: 'Tu et al., 2019;', 207765742: 'Ye et al., 2019;'}",https://arxiv.org/pdf/2010.06283v1.pdf
975,256461326,Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation,conclusions,Insight-tree,"automatic question generation is an important task in the improvement of artificial intelligent systems. in this work, we propose a novel primal-dual approach for question generation. it integrates question generation with its dual problem question answering into a unified framework. a knowledge distillation module is introduced into the framework to improve model generalization on uncommon word generation. experimental results on two benchmarks demonstrate the effectiveness of the primal-dual modeling.","{52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.4.pdf
976,237502773,Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension,conclusions,Insight-tree,"we have proposed suqa, an rc system augmented with an abstractive explainer component. our experiments have demonstrated that the abstractive explainer can generate more concise explanations than an extractive explainer with limited supervison, while keeping explanations sufficient for qa.","{215768725: 'Groeneveld et al., 2020;', 218486753: 'Inoue et al., 2020', 221749191: 'Trivedi et al., 2020', 207870753: 'Tu et al., 2020', 218487313: 'Yadav et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.490.pdf
977,231632239,"GRID SEARCH HYPERPARAMETER BENCHMARKING OF BERT, ALBERT, AND LONGFORMER ON DUORC A PREPRINT",conclusion and future work,Insight-tree,"we have performed a grid search hyperparameter benchmarking on three models on the duorc dataset. the models we evaluated are the bert, albert, and longformer models which are transformer-based neural network models. the duorc dataset contained two main components for each unique plot. the selfrc has shorter plot lines while the paraphrase has longer plot lines. the dataset was reduced into subsets called the ""span"" and ""full"" where the ""span"" subset is the set of plots where only the relevant sentences to the questions are extracted. the best performing model is the albert model which was pretrained using the squad1 and fine-tuned on the selfrc. the best performing model fine-tuned on the paraphraserc is the longformer model which was pretrained using the selfrc.",{},https://arxiv.org/pdf/2101.06326v1.pdf
978,258418354,"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks",conclusion,Insight-tree,"in this paper, we propose a novel method to improve the accuracy, credibility and traceability of large language models (llm) for complex knowledge-intensive tasks called searchain, which is a framework for deep interaction between llm and information retrieval (ir). in searchain, when llm faces complex questions, it constructs a chain called chain-of-query (coq), each node of the coq is an ir-oriented query and llm-generated answer. ir interacts with each node on the chain, judges whether the answer is correct, and provides llm with its unknown knowledge. llm generates a new coq according to the feedback of ir. ir interacts with llm for multiple rounds, gradually helping llm generate the correct coq, and finally solve the complex question. besides, the contents returned to the user include not only the final answer but also the reasoning process for the question, that is, the coq and the supporting documents retrieved by ir for each node of the coq, which improves the credibility and traceability of the contents generated by llm. experiments on multi-hop question answering datasets show that searchain not only has strong knowledge-reasoning ability when faced with complex questions but also can effectively exploit the interaction with ir to supplement and correct its knowledge, so as to improve the accuracy and credibility of the generated contents. besides, searchain effectively decouples the knowledge of llm and ir, which avoids the misleading of llm and can also accurately explain whether the knowledge involved in solving knowledge-intensive tasks comes from parameters or external ir.","{247595263: '[13]', 52822214: '[22]', 236771976: '[23]', 226236740: '[24]', 230799347: '[25]', 252692968: '[26]', 244799249: '[27]'}",https://export.arxiv.org/pdf/2304.14732v1.pdf
979,249431623,No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval,conclusion,Insight-tree,"in this work we studied how distillation and parameter count influence the zero-shot effectiveness of neural retrievers. we begin by showing that in-domain effectiveness, i.e., when retrievers are finetuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al. [14] and gupta et al. [9] and zhan et al [46]. furthermore, we show that a distilled reranker has better zeroshot effectiveness than much larger non-distilled rerankers, which is an important and desirable feature of deployed models. however, our largest reranker significantly outperforms smaller rerankers and achieves a new state of the art across almost all datasets used in our zero-shot experiments. this suggests that a large number of parameters may play a significant role in the generalization capability of pretrained language models.",{},https://export.arxiv.org/pdf/2206.02873v5.pdf
980,119162516,Adaptive PBDW approach to state estimation: noisy observations; user-defined update spaces,conclusions,Insight-tree,"in this paper, we presented a number of extensions to the pbdw formulation for state estimation. first, we proposed a tikhonov regularization of the original pbdw statement for general linear functionals, which relies on holdout validation, to systematically deal with noisy measurements. second, we proposed user-defined update spaces, which guarantee rapid convergence with respect to the number of measurements m and also might not require the solution to m riesz problems. third, we presented an a priori error analysis that provides insights into the role of the regularization hyper-parameter Î¾ associated with the penalized formulation.",{},https://arxiv.org/pdf/1712.09594v1.pdf
981,235731930,FAVIQ: FAct Verification from Information-seeking Questions,conclusion & future work,Insight-tree,"we introduced faviq, a new fact verification dataset derived from ambiguous information-seeking questions. we incorporate facts that real users were unaware of when posing the question, leading to false claims that are more realistic and challenging to identify without fully understanding the context. our extensive analysis shows that our data contains significantly less lexical bias than previous fact checking datasets, and include refute claims that are challenging and realistic. our experiments showed that the state-of-the-art models are far from solving faviq, and models trained on faviq lead to improvements in professional fact checking. altogether, we believe faviq will serve as a challenging benchmark as well as support future progress in professional fact-checking.","{226278099: 'Jiang et al., 2020', 219721462: 'Kamath et al. 2020', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.acl-long.354.pdf
982,203836061,Multi-hop Question Answering via Reasoning Chains,discussion and conclusion,Insight-tree,"in this work, we learn to extract reasoning chains to answer multi-hop reasoning questions. experimental results show that the chains are as effective as human annotations, and achieve strong performance on two large datasets. however, as remarked in past work (chen and durrett, 2019;min et al., 2019a), there are several aspects of hot-potqa and wikihop which make them require multi-hop reasoning less strongly than they otherwise might. as more challenging qa datasets are built based on lessons learned from these, we feel that reasoning in a more explicit way and properties of chain-like representations will be critical. this work represents a first step towards this goal of improving qa systems in such settings.","{67855846: 'Dua et al., 2019;', 174801764: 'Min et al., 2019a', 174801080: 'Min et al., 2019b', 160009340: 'Nishida et al., 2019', 155100120: 'Qiu et al., 2019', 128344862: 'Trivedi et al., 2019;'}",https://arxiv.org/pdf/1910.02610v2.pdf
983,218487535,Obtaining Faithful Interpretations from Compositional Neural Networks,conclusion,Insight-tree,"we introduce the concept of module-wise faithfulness, a systematic evaluation of faithfulness in neural module networks (nmns) for visual and textual reasoning. we show that naÃ¯ve training of nmns does not produce faithful modules and propose several techniques to improve module-wise faithfulness in nmns. we show how our approach leads to much higher module-wise faithfulness at a low cost to performance. we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models. ",{},https://www.aclweb.org/anthology/2020.acl-main.495.pdf
984,246294995,Reasoning Like Program Executors,conclusion & future work,Insight-tree,"we introduce poet, a new pre-training paradigm for boosting reasoning capability of language models via imitating program executors. experimental results on six datasets demonstrate that poet can significantly boost existing language models on several reasoning skills, including numerical, logical and multi-hop reasoning. our best language model under poet can reach highly competitive performance with previous specialized models. in the future, we hope our work could inspire more transference of reasoning knowledge from program executors to models. and we will also investigate the causes of the reasoning transfer with more insightful experiments, since we still do not know how the reasoning transfer occurs.","{211003735: 'Geva et al., 2020', 52822214: 'Yang et al., 2018', 234741852: 'Zhu et al., 2021'}",https://www.aclanthology.org/2022.emnlp-main.48.pdf
985,262084273,LLM Guided Inductive Inference for Solving Compositional Problems,conclusion,Insight-tree,"we have introduced rebel, a recursive reasoning algorithm designed to use any arbitrary api as an external tool.rebel outperforms the state-of-the-art on questions that require the collection of many facts and those that benefit from the ability to make highly specific queries to outside sources of data, which may be unstructured.rebel also has a demonstrable improvement over the gpt3 llm when answering questions that require multi-step information processing.however, the rebel algorithm tends to over-complicate simple problems, leading to a reduction in accuracy when compared to baseline gpt3 on questions that require minimal compositionality.",{},https://export.arxiv.org/pdf/2309.11688v1.pdf
986,249847776,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,conclusion and future work,Insight-tree,"we have demonstrated use of question decomposition based on the amr semantic representation for multi-hop qa, using an intrinsically interpretable framework to incorporate interpretability directly into the system structure. the complex task of multi-hop question interpretation is delegated to amr parsers. these parsers produce amr graphs to which two segmentation methods are applied, i.e., unknownbased and path-based graph segmentation, to achieve question decomposition. to generate a well-formed sub-question, we perform both amr parsing and amr-to-text generation with the same architecture, which uses a fully graphisomorphic linearization technique to complete the transformation from graph to a sequence of symbols without losing adjacency information. experimental results demonstrate that our qdamr system outperforms baseline question decomposition methods, both in performance of multi-hop qa and in the quality of generated sub-questions. since our proposed graph segmentation methods are based on predicateargument relations and parallel conditions/entities respectively, they could in principle be generalized to an unknown number of hops by identifying multiple predicate nodes or capturing multiple parallel conditions/entities. while, as noted, an aim of the qdamr design is to provide inherent interpretability, the effectiveness with which its outputs serve as explanations for human users remains to be evaluated in future work.","{52822214: '[Yang et al., 2018a]'}",https://export.arxiv.org/pdf/2206.08486v1.pdf
987,261076315,KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases,limitations,Insight-tree,"while knowledgpt enables llms to effectively perform kb operations on external knowledge bases, there remain several limitations in its current form. first, the retrieval process entails a singleround of code generation and execution for efficiency concerns. however, a multi-round mecha-nism may better allow llms to autonomously explore kbs. as llms are not aware of the contents within kbs, they might generate search that appear logical but yield no results. for example, a query like ""who is the voice actor for the heroine in ..."" may require a two-hop searching for the relations heroine and voice actor subsequently in certain kbs, or just a single relation main voice actor in others. in these scenarios, a multi-round mechanism empowers llms to probe and revisit the kbs autonomously, which might yield better results but with increased costs. second, we experiment with knowledgpt on representative yet small datasets, constrained by the expenses of accessing gpt-4 via api. while the results validate the effectiveness of knowledgpt, more comprehensive evaluations on full benchmarks are expected to better compare knowledgpt to related methods. we plan to study fine-tuning llms like llama for knowledgpt in our future work to improve the efficiency and conduct more thorough experiments. finally, it remains a practical issue when llms need to access external kgs, rather than solving problems independently. in this work, we simply let llms make this decision, while better approaches remain to be explored.",{},https://export.arxiv.org/pdf/2308.11761v1.pdf
988,261076315,KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases,conclusion,Insight-tree,"in this paper, we introduce knowledgpt, a comprehensive framework to integrate llms with external knowledge bases, facilitating llms' retrieval and storage on kbs. for retrieval, knowledgpt adopts ""program of thought"" prompting, which retrieves knowledge via code generation and execution. ",{},https://export.arxiv.org/pdf/2308.11761v1.pdf
989,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,limitations and discussions,Insight-tree,"while we highlight some of the major drawbacks of instruction tuning and propose an alternative approach of instead training and retrieving experts in this paper, we do not perform experimental results over mt lms that have more than >11b parameters. for example, mt lms with >11b parameters may be less susceptible to negative task transfer because of increased model capacity. also, during the inference of unseen tasks, our retrieval mechanism assumes batch inference (i.e. having access to 32 samples of the target tasks without labels). finally, when showing the compositional instruction experiments, we assume the two optimal experts could be retrieved from the compositional instruction (concatenation of the two seen instructions) given as the input along with the evaluation instance. this might not necessarily be the case with more complex, compositional instructions, which might require a separate decomposition stage. we instead focus on showing the possibility merging experts can bring and leave developing novel methods of retrieving the optimal experts during inference for future work.",{239009558: 'Vu et al. 2022'},https://export.arxiv.org/pdf/2302.03202v2.pdf
990,256627673,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,conclusion,Insight-tree,"in this work, we provide an interesting finding that expert lms trained on single tasks show strong generalization capability to unseen tasks, even surpassing mt lms trained on multiple tasks (300+) by a non-trivial margin. we leverage this capability and show three main benefits of training and retrieving experts for inference over mt lms, demonstrating that our proposed distributed approach is more robust against negative task transfer, more adapt at learning new tasks, and can perform compositional instructions. to this end, we urge the research community to further explore distributed and collaborative training of experts which may have other future benefits including efficiency, privacy, and personalization not explicitly explored in this paper.",{239009558: 'Vu et al. 2022'},https://export.arxiv.org/pdf/2302.03202v2.pdf
991,247476426,Relation Leakage in Elicited Natural Language Inference Datasets,conclusion,Insight-tree,"we have introduced useful tools and techniques for analyzing elicited sentence relation leakage bias in nli datasets, and applied them to a large, representative set of popular current datasets.","{52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2112.09237v2.pdf
992,250264890,Rationale-Augmented Ensembles in Language Models,conclusion,Insight-tree,"in this paper, we have presented a unified framework for rationale-augmented ensembles, and found that rationale sampling in the output space is a key component for achieving improved performance in natural language processing tasks. by sampling diverse rationales and ensembling the results, we have shown that rational-ensembling methods in the proposed framework can reliably outperform standard prompting and rationale-based few-shot prompting, across a wide range of natural language tasks and alternative language models. overall, rationale-augmented ensembling appears to be a reliable way to shift from the paradigm of (input â output) pairs to (input, rationale â output) pairs to achieve more accurate and interpretable natural language processing.","{208267807: 'Asai et al., 2020;', 165163607: 'Clark et al., 2019', 207756753: 'Nie et al., 2020', 237433880: 'Xu et al., 2021;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2207.00747v1.pdf
993,236486285,Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering,conclusion and future work,Insight-tree,"in this paper, the application of transfer learning by utilizing transformer models like roberta, al-bert and electra for question answering span prediction task was explored. we also experimented with pretrained models on several other datasets prior to fine-tuning it on the dialdoc21 dataset, provided as part of the dialdoc21 shared task. maximum confidence score based ensemble techniques were employed to combine various base transformer models to further boost the per-formance. we plan to extend our approach and experiment with other ensembling techniques for further enhancing the performance and also explore avenues for improved scalability when applied to larger datasets.","{67855846: 'Dua et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.dialdoc-1.9.pdf
994,258822799,HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,conclusion,Insight-tree,"we introduce helma, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing and improving hallucinations. to automatically generate large-scale samples, we propose a chatgpt-based two-step approach, i.e., samplingthen-filtering. we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.","{52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2305.11747v1.pdf
995,238856979,Representation Decoupling for Open-Domain Passage Retrieval,conclusion,Insight-tree,"in this paper, we analyze the contrastive conflicts problem in current contrastive learning framework adopted in the open-domain passage retrieval area. to solve such problem, we propose dense contextual sentence representation (dscr). concretely, we first decouple the original passage representations into contextual sentence-level ones, and then refine the original contrastive learning framework by creating sentence-aware positive and negative samples. our dcsr achieves significant performance gain compared to original dpr baseline, especially on datasets with severe conflicting problem. extensive experiments shows that our dcsr also enjoys better transferability, indicating that dcsr well captures the universality in different datasets.","{220302524: 'Xiong et al. 2020', 52822214: 'Yang et al. 2018'}",https://arxiv.org/pdf/2110.07524v1.pdf
996,253117139,Analyzing Multi-Task Learning for Abstractive Text Summarization,conclusion & future work,Insight-tree,"in this work, we studied the influence of multi-task learning combinations of task families during the pre-finetuning stage for english abstractive text summarization. we trained three different training strategies, six task families composed of 18 tasks, and evaluated two downstream tasks.",,https://www.aclanthology.org/2022.gem-1.5.pdf
997,259991144,FLASK: FINE-GRAINED LANGUAGE MODEL EVALUATION BASED ON ALIGNMENT SKILL SETS,conclusion,Insight-tree,"in this paper, we introduce flask, a fine-grained language skill set evaluation setting for the alignment of language models. we categorize 12 fine-grained skills to evaluate llms and annotate necessary skills, the target domain, and the difficulty level for each instance. flask provides a comprehensive and interpretable analysis of the capabilities of llms by allowing the analysis of the performance depending on different skills, domains, and difficulty levels. also, we observe that applying fine-grained evaluation results in better reliability in terms of correlation between humanbased and model-based evaluation and the robustness of model-based evaluation to stylistic changes.",{},https://export.arxiv.org/pdf/2307.10928v2.pdf
998,15542473,Nonparametric Binary Recursive Partitioning for Deterioration Prediction of Infrastructure Elements,summary and conclusion,Insight-tree,"nonparametric methods possess a number of advantages over the commonly used parametric statistical methods, such as no requirements of specific distribution assumptions, interactions of explanatory variables, and randomness of sample observations.as a convention, bridge deck condition is assessed using a weighted scale from 0 to 9 representing poorest to best conditions.with ranked categorical data in place, the nonparametric deck deterioration modeling can be treated as a classification and decision problem.this paper introduced the brp method for solving the classification and decision problem.the proposed brp method involves four basic steps.the first step consists of tree building, during which a tree is built using recursive splitting of nodes.each resulting node is assigned a predicted class based on the distribution of classes in the learning dataset which would occur in that node and the loss matrix.the assignment of a predicted class to each node occurs whether or not that node is subsequently split into child nodes.the second step consists of stopping the tree building process.at this point a maximal tree has been produced.the third step consists of tree pruning, which results in the creation of a sequence of continuously simpler trees through ""cutting off "" increasingly important nodes.the fourth step consists of optimal tree selection, during which the tree which fits the information in the learning dataset, but does not overfit the information, is selected from among the sequence of pruned trees.the proposed brp method was applied through the cart software program to the indiana bridge inventory data containing information on function adequacy and structural conditions of approximately 5,500 indiana state-maintained highway bridges.the response variable was defined as number of units of deck condition drops after the last inspection.the explanatory variables were consistent with those in [5,6], which mainly included deck age, geographical region, highway class, deck geometric design, deck structure type, average daily traffic, and deck condition rating at the time of inspection.two sets of classification trees were created.the first set used 4 predicted classes for the response variable, including class 1 for no deterioration, class 2 for one unit of deterioration, class 3 for two units of deterioration, and class 4 for three units of deterioration.the significant explanatory variables corresponding to the optimal classification tree were current deck condition rating and deck age.additional variables that appeared to be influential included highway class, traffic, deck width, and wearing surface protection system.the success in correctly predicting deck condition deterioration for the four predicted classes were 91, 62, 69, and 92 percent, respectively, a noted improvement over previous research utilizing parametric methods and the same dataset.wherein, the degrees of successful prediction for classes 2 and 3 were lower than those for classes 1 and 4. thus, the second set of special 2-class tree that only considered no deterioration and one to three units of deterioration was created.the degrees of success in correctly predicting deck condition for the two predicted classes increased to 95 and 84 percentages, correspondingly.with fewer classes used, the special classification tree presented a more general view of deck condition deterioration prediction.the proposed brp method could outperform other known nonparametric methods developed so far for bridge deck condition prediction from 73-75 percent to 92-95 percent, a margin of 20 percent higher in accuracy.",{},http://downloads.hindawi.com/journals/ace/2009/809767.pdf
999,263828698,Empower Nested Boolean Logic via Self-Supervised Curriculum Learning,conclusion,Insight-tree,"this paper provides a quantified analysis on the multi-nested boolean logic.we flag the deficiency in the state-of-the-art language models in terms of such basic capability, which will inevitably cause pitfalls in dealing with more complex reasoning tasks.for this, we propose curriculum logical reasoning, a new self-supervised learning method to empower language models with foundational logical capability.we also show that our idea can act as a cornerstone learning method for general logical reasoning.","{207756753: 'Nie et al., 2020', 249062828: 'Sanyal et al., 2022a', 247594506: 'Sanyal et al., 2022b', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2310.05450v1.pdf
1000,220302524,Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval,conclusion,Insight-tree,"ance fundamentally eliminates the discrepancy between the representation learning of texts and their usages in dense retrieval. our ance trained dense retrieval model, the vanilla bert-siamese, convincingly outperforms all dense retrieval and sparse retrieval baselines in our large scale document retrieval and passage retrieval experiments. it nearly matches the ranking accuracy of the state-of-theart cascade sparse retrieval and bert reranking pipeline. more importantly, all these advantages are achieved with a standard transformer encoder at a 1% online inference latency, using a simple dot-product in the ance-learned representation space.","{52822214: '[3,', 86611921: '[26]'}",https://arxiv.org/pdf/2007.00808v1.pdf
1001,202572810,Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering,conclusion,Insight-tree,"we proposed two semantics-enhanced rewards to regularize a qg model to generate semantically valid questions, and introduced a qa-based evaluation method that directly evaluates a qg model's ability to mimic human annotators in generating qa training data. experiments showed that our qg model achieves new state-of-the-art performances. further, we investigated how to use our qg system to augment qa datasets and conduct semi-supervised qa via two synthetic data generation methods along with a data filter and mixing mini-batch training. experiments showed that our approach improves both bidaf and bert qa baselines even without introducing new articles. qa for qa, we use squadv1.1 (rajpurkar et al., 2016). previous semi-supervised qa works sampled 10% from training set as the testing set dhingra et al., 2018). since we want to use the full training set in semi-supervised qa setup without any data size reduction, we instead split the original development set in half for validation and testing respectively. for semi-supervised qa, first, without introducing new articles, we generate new questions for squad training set by keeping all beam search outputs. second, with introducing new articles, we obtain new paragraphs with pre-extracted answer spans from harvestingqa . without using their provided questions, we use our best qg model to label questions. meanwhile, we investigate the influence of synthetic data size, so we sample 10% to 100% examples from harvest-ingqa, which are denoted as h1-h10 in our experiments.",{},https://www.aclweb.org/anthology/D19-1253.pdf
1002,263699899,A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation,conclusion,Insight-tree,"in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models.through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average.we also demonstrated the individual efficacy of our dete","{246652372: 'Ji et al., 2023', 252715485: 'Khot et al., 2023', 258461053: 'Varshney and Baral, 2023;', 247187611: 'Varshney et al., 2022', 52822214: 'Yang et al., 2018', 219721462: 'Kamath et al., 2020;'}",https://export.arxiv.org/pdf/2307.03987v2.pdf
1003,230770437,SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering,conclusion,Insight-tree,"in conclusion, this paper presents sf-qa, a novel evaluation framework to make open-domain qa research simple and fair. this framework fixes the gap among researchers from different fields, and make the open-domain qa more accessible. we show the robustness of this framework by successfully reproducing several existing models in opendomain qa research. we hope that sf-qa can make the open-domain qa research more accessible and make the evaluation easier. we expect to further improve our framework by including more models in both ranker and reader side, and encourage community contributions to the project as well.","{198229624: 'Joshi et al., 2020'}",https://www.aclweb.org/anthology/2021.eacl-demos.2.pdf
1004,208267807,Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING,conclusion,Insight-tree,"this paper introduces a new graph-based recurrent retrieval approach, which retrieves reasoning paths over the wikipedia graph to answer multi-hop open-domain questions. our retriever model learns to sequentially retrieve evidence paragraphs to form the reasoning path. subsequently, our reader model re-ranks the reasoning paths, and it determines the final answer as the one extracted from the best reasoning path. our experimental results significantly advance the state of the art on hotpotqa by more than 14 points absolute gain on the full wiki setting. our approach also achieves the state-of-the-art performance on squad open and natural questions open without any architectural changes, demonstrating the robustness of our method. our method provides insights into the underlying entity relationships, and the discrete reasoning paths are helpful in interpreting our framework's reasoning process. future work involves end-to-end training of our graph-based recurrent retriever and reader for improving upon our current two-stage training.","{153312687: 'Ding et al., 2019', 189927857: 'Feldman & El-Yaniv, 2019', 202583433: 'Godbole et al., 2019;'}",https://export.arxiv.org/pdf/1911.10470v2.pdf
1005,203610361,Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks,conclusion,Insight-tree,"in this paper, we investigated the role played by interlinked sentence representation for complex, multi-hop question answering under the focus of supporting facts identification, i.e. retrieving the minimum set of facts required to answer a given question. we emphasise that this problem is worth pursuing, showing that the performance of stateof-the-art models substantially deteriorates as the size of the accompanying context increases.",{},https://www.aclweb.org/anthology/D19-5306.pdf
1006,238419458,TOWARDS CONTINUAL KNOWLEDGE LEARNING OF LANGUAGE MODELS,conclusion,Insight-tree,"in this paper, we propose continual knowledge learning (ckl), where we establish benchmark datasets and metrics, and explore methodologies towards continual knowledge learning of an ever-changing lm. we find that parameter-expansion methods show the most robust performance throughout all of the experimental settings, which nevertheless has severe memory inefficiency and that seeing the same data often is a critical cause of forgetting. we also discuss several other interesting results of which we leave further exploration to future studies. to this end, we suggest the community to explore ckl for the better design of an ever-changing lm. ",,https://arxiv.org/pdf/2110.03215v4.pdf
1007,232147859,Semantic Models for the First-stage Retrieval: A Comprehensive Review,conclusion,Insight-tree,"the purpose of this survey is to summarize the current research status on semantic retrieval models, analyze existing methodologies, and gain some insights for future development. it includes a brief review of early semantic retrieval methods, a detailed description of recent neural semantic retrieval methods and the connection between them. specially, we pay attention to neural semantic retrieval methods, and review them from three major paradigms, including sparse retrieval methods, dense retrieval methods and hybrid retrieval methods. we also refer to key topics about neural semantic retrieval models learning, such as loss functions and negative sampling strategies. in addition, we discuss several challenges and promising directions that are important for future researches. we look forward to working with the community on these issues.","{189927857: '[69]', 86611921: '[116]'}",https://arxiv.org/pdf/2103.04831v4.pdf
1008,211126663,Transformers as Soft Reasoners over Language,conclusion,Insight-tree,"just as mccarthy advocated 60 years ago for machines reasoning (""taking advice"") in logic, we have shown (in a restricted setting) that machines can by trained to reason over language. while we have assumed a particular semantics of inference, the methodology we have used is general: characterize the desired behavior in a formal way, synthesize examples, generate linguistic equivalents, and train a model. the result, at least within our experiments, appears to be both nat-ural and robust, in a way distinct from working with the original formalization.",{},https://arxiv.org/pdf/2002.05867v2.pdf
1009,216868500,Look at the First Sentence: Position Bias in Question Answering,conclusion,Insight-tree,"most qa studies frequently utilize start and end positions of answers as training targets without much considerations. our study shows that most qa models fail to generalize over different positions when trained on datasets having answers in a specific position. we introduce several de-biasing methods to make models to ignore the spurious positional cues, and find out that the sentence-level answer prior is very useful. our findings also generalize to different positions and different datasets. one limitation of our approach is that our method and analysis are based on a single paragraph setting which should be extended to a multiple paragraph setting to be more practically useful.","{139103297: 'Chen and Durrett, 2019;', 202539031: 'Clark et al. 2019', 67855846: 'Dua et al., 2019', 204823992: 'Fisch et al., 2019', 174801764: 'Min et al., 2019', 173188058: 'Talmor and Berant, 2019', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2004.14602v1.pdf
1010,247518803,Synthetic Question Value Estimation for Domain Adaptation of Question Answering,conclusion,Insight-tree,"we propose a question value estimator to estimate the usefulness of synthetic questions and select useful ones for improving target-domain qa train- 7 we treat it as a binary classification problem here: if a question is selected, the prediction is 1; 0 otherwise.","{204823992: 'Fisch et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 202565869: 'Nema et al., 2019;', 211258652: 'Puri et al., 2020', 52822214: 'Yang et al., 2018', 237364113: 'Yue et al., 2021', 202572810: 'Zhang and Bansal, 2019;'}",https://www.aclanthology.org/2022.acl-long.95.pdf
1011,244119330,What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension,conclusion,Insight-tree,"this paper aims at addressing the newly discovered difficulty of the boundary ambiguity between sentences and sub-sentences, which exists in many languages to different extents and essentially limits the performance of span extraction mrc models, especially in chinese environment. we apply explicit span-sentence predication (esp) to enhance model's ability of precisely locating sentences containing the target span. our proposed model design is evaluated on chinese span extraction mrc benchmark, cmrc 2018. the experimental results show that our model significantly improves both em and f1 scores compared with strong baselines and helps achieve a new state-of-the-art performance. our method also shows generality and potential in dealing with other languages. this work highlights the research line of further improving challenging mrc by analyzing specific linguistics phenomena.","{207870753: 'Tu et al., 2020', 52822214: 'Yang et al., 2018', 218613640: 'Zhang et al., 2020d'}",https://aclanthology.org/2021.findings-emnlp.202.pdf
1012,247292088,Question-Answer Sentence Graph for Joint Modeling Answer Selection,conclusions,Insight-tree,"to our knowledge, our model is the first graphbased approach for jointly modeling sentence-level semantics of question-answer pairs for as2 as an offline processing application, such as those required by community qa, forums, etc. this is different from previous methods using graphs, e.g., multihop or graph-based qa, which mainly model semantics via entities. our approach builds query-specific small-scale training graphs for offline learning, through (q, a) pairs as nodes, and edges encoding relations between members of pairs to capture both supporting question-question, and answer-answer dependencies. further, we demonstrate that our approach achieves significant performance gains over existing sota models on as2 for metrics of p@1, map, and mrr.","{207853300: 'Fang et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.eacl-main.68.pdf
1013,235293903,Why Machine Reading Comprehension Models Learn Shortcuts?,conclusions,Insight-tree,"in this work, we try to answer why many mrc models learn shortcut tricks while ignoring the pre-designed comprehension challenges that are purposely embedded in many benchmark datasets. we argue that large proportions of shortcut questions in training data push mrc models to rely on shortcut tricks excessively. to properly investigate, we first design two synthetic datasets where each instance has a shortcut version paired with a challenging one which requires paraphrasing, a complex reasoning skill, to answer, rather than performing question word matching or simple matching. with these datasets, we are able to adjust the proportion of shortcut questions in both training and testing, while maintaining other factors relatively steady. we propose two methods to examine the model training process regarding the shortcut questions, which enable us to take a closer look at the learning mechanisms of bidaf and bert under different training settings. we find that learning shortcut questions generally requires less computational resources, and mrc models usually learn the shortcut questions at their early stage of training. our findings reveal that, with larger proportions of shortcut questions for training, mrc models will learn the shortcut tricks quickly while ignoring the designed comprehension challenges, since the remaining truly challenging questions, usually limited in size, may not motivate models to explore sophisticated solutions in the later training stage. ","{211010520: 'Bartolo et al. 2020', 208201969: 'Sugawara et al., 2020', 52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/2106.01024v1.pdf
1014,237940293,Sorting through the noise: Testing robustness of information processing in pre-trained language models,conclusion,Insight-tree,"we have presented results manipulating inputs of pre-trained lms, to test the ability of such models to represent and retain information conveyed by input text. our results show that though models may appear to handle information correctly in simple settings, these correct predictions are easily broken by insertion of distracting material in the context. systematic manipulation of the distracting content further indicates key roles for semantic similarity and relative word position in models' selection of relevant contextual cues for prediction. overall, the results suggest that lm predictions are driven more by coarse-grained superficial cues than by extraction of robust meaning information from context. the results serve as a reality check for considerations of the extent to which lms ""understand"" their input, and lay groundwork to understand the mechanisms that do drive predictions in these models.   ","{207756753: 'Nie et al., 2020', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2021.emnlp-main.119.pdf
1015,231639356,Situation and Behavior Understanding by Trope Detection on Films,conclusion,Insight-tree,"in this work, we presented a brand new task with a new dataset timos for situation and behavior understanding in films. unlike previous tasks and datasets, trope detection requires deep cognitive skills, including consciousness, systematic generalization, and casual and motivational comprehension. modern learning systems including contextual embedding bert, movie tag prediction system, and recurrent relational network reach at most 23.97 f1 score, which is far behind human performance (64.87) and suggests that trope detection is a challenging task. we proposed a multi-level comprehension framework with multi-stream attention and multistep reasoning to tackle this problem and boost performance to 25.00 f1. we carefully analyze the task along with the network's behavior. we adopt human evaluation to verify the answerability of our dataset and discuss potential directions. we believe that our research could pave a new path for future research on deep cognition.","{218487111: '[9]', 52822214: '40]'}",https://arxiv.org/pdf/2101.07632v1.pdf
1016,239009834,MixQG: Neural Question Generation with Mixed Answer Types,conclusion,Insight-tree,"in this paper, we present mixqg, a question generation model pre-trained on a collection of qa datasets with a mix of answer types. we show through experiments that the resulting model is a strong starting point for further fine-tuning which achieves state-of-the-art results on target datasets in commonly-used similarity metrics as well as our designed human evaluation. we release our code and the model checkpoints to facilitate qg research and downstream applications.","{202572810: 'Zhang and Bansal, 2019;'}",https://arxiv.org/pdf/2110.08175v2.pdf
1017,240353952,Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections,conclusion,Insight-tree,"we present dcqa that connects pieces in a document via open-ended questions and full-sentence answers. dcqa is collected via a new paradigm that regards the main purpose of a new sentence as an answer to a free-form question evoked earlier in the context. consequently, this paradigm yields both discourse and semantic links across all sentences in a document. dcqa is introduced with the goal of providing a more scalable data collection paradigm, also as initial resource, for answering open-ended questions for discourse comprehension. our experiments showed that dcqa provides valuable supervision for such tasks.","{235678938: 'Cao and Wang 2021', 218487111: 'Dunietz et al. 2020', 196170479: 'Fan et al., 2019', 221507798: 'Petroni et al. 2021', 233189637: 'Soleimani et al. 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.emnlp-main.806.pdf
1018,254854519,Discovering Language Model Behaviors with Model-Written Evaluations,limitations & future work,Insight-tree,"model capabilities we observed that current lms struggle to generate several categories of examples. lms struggled to generate examples related to concepts they do not understand well (e.g. cryptography and steganography). as discussed in Â§6, we also found that lms struggled to generate examples with many constraints, in particular, those in the bbq dataset (parrish et al., 2022). we expect these limitations to wane as lms grow more capable with scale. lastly, many evaluations related to lm capabilities require the dataset creator to know how to solve the evaluation. we expect that lms will not be able to generate high-quality evaluations of this kind (e.g., to test for factual knowledge they do not yet know). our approach is thus differentially useful for evaluating other properties of models aside from capabilities (e.g., safety-related behaviors).model biases lms learn biases from their training data (sheng et al., 2019;gehman et al., 2020;brown et al., 2020), impacting the generator p g and discriminator p d . for example, generated evaluations may exhibit gender or racial biases and be lower quality for languages under-represented in the lm training data. lms will also be systematically worse at generating evaluations for tasks that are omitted from their training data (e.g., due to copyright, licensing, or privacy issues).example diversity we found limited example diversity for some kinds of evaluations ( Â§3.4) though not all ( Â§5). diversity appears to depends on the kind of evaluation generated, the generation hyperparameters, and the prompt used, and thus sometimes requires e.g. hyperparameter tuning to get right.we found data visualizations to be powerful tools for understanding and debugging data diversity, such as those at evals.anthropic.com/model-written/. qualitatively, we also found that using p d to rank/filter examples limited the diversity, since p d sometimes selected for prototypical examples for testing some behavior (observed qualitatively by workers in Â§3.2). we are excited for future work to explore other methods that achieve similar example quality with higher diversity than our method, such as generating many examples and subsampling for diversity.instructions may be misunderstood lms, similar to crowdworkers, may generate evaluations that are testing something different than intended, especially if the generation instructions are underspecified. for example, using the method in Â§3.1, we generated statements that a person who ""shares beliefs with derek parfit"" (the influential analytic philosopher) would agree or disagree with. the ""disagree"" statements were often ones that many people, not just derek parfit, would disagree with (""i support slavery"" or ""i believe evolution never happend""). in this case, we should have provided more specific instructions to the lm, to have it generate examples that derek parfit would disagree with but that another philosopher would agree with. when feasible, we recommend briefly examining the generated data, to catch salient issues such as the above.sensitivity to instructions our approach allows the dataset developer fairly fine-grained control over the evaluation by using instructions to guide p g . however, the quality of lm outputs is sensitive to text inputs in unintuitive ways (perez et al., 2021;lu et al., 2022), adding hard-to-predict variance to the quality of the resulting evaluation; see appendix Â§a.4 for a possible example of this effect we found. we hope that lm advances such as instruction-tuning (wei et al., 2021;sanh et al., 2022;ouyang et al., 2022) mitigate this issue in the future. for now, it may be possible to use prompt sensitivity to generate more diverse datasets, by generating similar datasets with distinct prompts and combining the results, as we did in Â§3. where prompt sensitivity caused issues, we found it helpful to be able to view example generated outputs in seconds, to quickly iterate and catch salient failures. for the 133 datasets in Â§3, we found a general instruction template that worked well; we did not do dataset-specific tuning to obtain samples rated as high-quality by human evaluators.",,https://export.arxiv.org/pdf/2212.09251v1.pdf
1019,243986045,Recent Advances in Automated Question Answering In Biomedical Domain,conclusion,Insight-tree,"in this review, we focused on the recent advances in biomedical question answering. we provided a review of general domain question answering using knowledge bases, texts or both, before moving on to biomedical question answering systems. we explored current state of the art biomedical qa systems and discussed their limitations. we finally provided an overall analysis of limitations of bqa systems along with ways to overcome them. we finally explore the potential areas of focus for further research.","{86611921: '[109]', 233219869: '206]'}",https://arxiv.org/pdf/2111.05937v1.pdf
1020,247793456,LinkBERT: Pretraining Language Models with Document Links,conclusion,Insight-tree,"we presented linkbert, a new language model (lm) pretraining method that incorporates document link knowledge such as hyperlinks. in both the general domain (pretrained on wikipedia with hyperlinks) and biomedical domain (pretrained on pubmed with citation links), linkbert outperforms previous bert models across a wide range of downstream tasks. the gains are notably large for multi-hop reasoning, multi-document understanding and few-shot question answering, suggesting that linkbert effectively internalizes salient knowledge through document links. our results suggest that linkbert can be a strong pretrained lm to be applied to various knowledge-intensive tasks. ","{204823992: 'Fisch et al., 2019', 221970190: 'Jin et al., 2021', 198229624: 'Joshi et al., 2020', 233219869: 'Yasunaga et al., 2021'}",https://www.aclanthology.org/2022.acl-long.551.pdf
1021,252280663,Prompt-based Conservation Learning for Multi-hop Question Answering,conclusions and future work,Insight-tree,"in this paper, we introduce a novel prompt-based conservation learning framework for multi-hop qa -a framework that retains knowledge from previous component tasks -able to answer questions in a principled way that matches human expectations by answering sub-questions and integrating the answers. by developing soft prompts related to reasoning types during training, we also show that we can condition plms to stimulate and apply the reasoning knowledge required for specific multihop questions. experimental results on multiple multi-hop qa datasets demonstrate the improved performance of pcl over previous multi-hop qa models in multi-hop qa.","{207853300: 'Fang et al., 2020', 155100120: 'Qiu et al., 2019', 211258744: 'Tang et al., 2021', 207870753: 'Tu et al. 2020', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.154.pdf
1022,143604734,Influence of In-Service Teacher Training on Their Opinions about IBSE,conclusions,Insight-tree,"the fact that, after training, polish teachers still have the opinion that curricula are not adjusted to ibse is significant, and informs us that the introduction of ibse into school practice will be a major challenge. on the other hand, the fact that the teachers, after training, reinforced their attitudes that existing curricula can be realized with the use of ibse methods and that carrying out experiments according to the ibse rules does not require more advanced equipment in school laboratories in comparison to the traditional methods, allows for an optimistic look at the future. changes in attitudes are in this case less expensive than changes in external conditions. the stronger agreement that ibse might have a positive influence on the examination results can be the result of a session on new forms of external examinations. the increase in the number of statements saying that the school system and parents are in favor of the traditional/lecturing methods gives teachers some kind of excuse for such rare application of the use of strategy that they found valuable but at the same time very demanding.",{},https://web.archive.org/web/20200307032630/https:/ruj.uj.edu.pl/xmlui/bitstream/handle/item/18748/bernard_maciejowska_krzeczkowska_odrowaz_influence_of_in-service%20_2015.pdf?isAllowed=y&sequence=4
1023,254125744,NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework,conclusion,Insight-tree,"in this paper, we point out that although there are some differences among the various information retrieval tasks, there are still essential matching signals shared by the various tasks, such as exact matching, semantic matching, and inference matching.if the model can capture and exploit these signals, the generalization ability of the model across tasks and domains will be improved.with this intuition, we propose a neural information retrieval training framework called nir-prompt consisting of essential matching module (emm) and matching description module (mdm) based on the idea of decoupling the process of signal capturing and signal combination.mdm uses the method of prompt learning to obtain the description of different tasks in the pre-trained language model.emm is trained on diverse mixed datasets and combined with the guidance from the task descriptions in mdm to capture essential matching signals and adapt these signals to different tasks.based on this, a generalized neural information retrieval pipeline consisting of retrieval and reranking is constructed.the experimental results on eighteen public datasets and a heterogeneous benchmark for testing the generalization ability of retrieval models show that our method yields better in-domain multi-task, out-of-domain multi-task, and new task adaptation performance for dense retrieval, reranking, and the entire neural information retrieval pipeline compared to the traditional fine-tuning paradigm.","{254125751: '[3]', 221507798: '[50]', 173188058: '[67]', 233296016: '[68]', 218487733: '[72]', 238857091: '[78]', 220302524: '[80]', 237502990: '90]'}",https://export.arxiv.org/pdf/2212.00229v3.pdf
1024,198229624,SpanBERT: Improving Pre-training by Representing and Predicting Spans,conclusion,Insight-tree,"we presented a new method for span-based pretraining which extends bert by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. together, our pretraining process yields models that outperform all bert baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.","{204823992: 'Fisch et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.tacl-1.5.pdf
1025,263620134,RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS,conclusion,Insight-tree,"in this work, we systematically study the retrieval-augmentation versus long context extension using the state-of-the-art llms after instruction tuning for various long context qa and querybased summarization tasks.after study, we have the following interesting findings: i) retrieval largely boosts the performance of both 4k short context llm and 16k/32k long context llms.","{234093776: 'Dasigi et al., 2021', 236771976: 'Trivedi et al., 2022'}",https://export.arxiv.org/pdf/2310.03025v1.pdf
1026,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,limitation discussion,Insight-tree,"we provide a new interpretability evaluation benchmark which contains three tasks with both english and chinese annotated data. there are three limitations in our work.â¢ how to evaluate the quality of human-annotated rationales is still open. we have several annotators to perform quality control based on human intuitions and experiences. meanwhile, we compare model behaviors on full inputs and humanannotated rationales to evaluate the sufficiency and comprehensiveness of rationales, as shown in table 4 and table 7. however, this manner has damaged the original input distribution and brings uncontrollable factors on model behaviors. therefore, how to automatically and effectively evaluate the quality of human-annotated rationales should be studied in the future.â¢ we find that the interpretability of model architectures and saliency methods vary with tasks, especially with the input form of the task. thus our benchmark should contain more datasets of each task type ( e.g., single-sentence task, sentencepair similarity task and sentence-pair inference task) to further verify these findings. and we will build evaluation datasets for more tasks in the future.â¢ due to space limitation, there is no analysis of the relationships between metrics, e.g., the relationship between plausibility and accuracy, and the relationship between faithfulness and robustness. we will take these analyses in our future work.finally, we hope more evaluation metrics and analyses are proposed based on our benchmark. and we hope our benchmark can facilitate the research progress of interpertability.","{218487030: 'Ye et al., 2020;'}",https://www.aclanthology.org/2022.conll-1.6.pdf
1027,248987702,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,conclusion,Insight-tree,"we propose a new fine-grained interpretability evaluation benchmark, containing token-level rationales, a new evaluation metric and corresponding perturbed examples for three typical nlp tasks, i.e., sentiment analysis, textual similarity and machine reading comprehension. the rationales in this benchmark meet primary properties that a rationale should satisfy, i.e., sufficiency, compactness and comprehensiveness. the experimental results on three models and three saliency methods prove that our benchmark can be used to evaluate interpretability of both models and saliency methods. we will release this benchmark and hope it can facilitate progress on several directions, such as better interpretability evaluation metrics and causal analysis of nlp models. ","{218487030: 'Ye et al., 2020;'}",https://www.aclanthology.org/2022.conll-1.6.pdf
1028,258841172,IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions,limitations,Insight-tree,"the main limitation of ifqa dataset is that it only covers event-based questions, due to the nature of creating counterfactual presuppositions. therefore, our dataset is not intended for training general opendomain qa models or evaluate their capabilities.for data collection, we relied heavily on human annotators, both for question annotation and verification. despite our efforts to mitigate annotator bias by providing explicit instructions and examples and by sampling annotators from diverse populations, it is not possible to completely remove this bias. in addition, we use heuristic rules to select only a small portion of wikipedia passages and then present them to human annotators (as mentioned in section 3.1.1), which might lead to pattern-oriented bias in the annotated data.for evaluated models, large language models performance on our dataset may preserve biases learned from the web text during pre-training or and make biased judgments as a result.","{250390687: 'Li et al., 2022', 236771976: 'Trivedi et al., 2022', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2305.14010v1.pdf
1029,222133842,Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space,conclusion,Insight-tree,"in this work, we present a novel question data augmentation method, called crqda, for contextrelevant answerable and unanswerable question generation. crqda treats the question data augmentation task as a constrained question rewriting problem. under the guidance of a pre-trained mrc model, the original question is revised in a continuous embedding space with gradient-based optimization and then decoded back to the discrete space as a new question data sample. the experimental results demonstrate that crqda outperforms other strong baselines on squad 2.0. the crqda augmented datasets can improve multiple reading comprehension models. furthermore, crqda can be used to improve the model performance on question generation and question-answering language inference tasks, which achieves a new state-of-theart on the squad 1.1 question generation task. yukun zhu, ryan kiros, rich zemel, ruslan salakhutdinov, raquel urtasun, antonio torralba, and sanja fidler. 2015. aligning books and movies: towards story-like visual explanations by watching movies and reading books. in proceedings of the ieee international conference on computer vision, pages 19-27. figure 3 and figure 4 provide some augmented data samples of each baseline on squad 2.0. we can see that the baseline eda tends to introduce noise which destroys the original sentence structure. the baselines of text vae, backtranslation and ae+noised often change some important words of the original question. this can cause the augmented question to miss the original key information and not to able to infer the original answer. in contrast, it can be observed that the generated answerable questions of crqda still maintain the key information for the original answer inference.",{},https://arxiv.org/pdf/2010.01475v1.pdf
1030,57721315,Multi-Style Generative Reading Comprehension,conclusion,Insight-tree,"this study sheds light on multi-style generative rc. our proposed model, masque, is based on multi-source abstractive summarization and learns multi-style answers together. it achieved stateof-the-art performance on the q&a task and the q&a + nlg task of ms marco 2.1 and the summary task of narrativeqa. the key to its success is transferring the style-independent nlg capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder. in particular, the capability of copying words from the question and passages can be shared among the styles, while the capability of controlling the mixture weights for the generative and copy distributions can be acquired for each style. our future work will involve exploring the potential of our multi-style learning towards natural language understanding.",{},https://arxiv.org/pdf/1901.02262v2.pdf
1031,264426324,PaRaDe: Passage Ranking using Demonstrations with Large Language Models,conclusion,Insight-tree,"in this work we present passage ranking with demonstrations (parade), an extensive study on the topic of using demonstrations to improve reranking performance of llms.we show the challenges of applying demonstrations effectively, and that performance heavily relies on selecting ""good"" demonstrations.we propose a simple yet effective selection method, named difficulty-based selection (dbs), and confirm its effectiveness in both reranking using query likelihood scoring and query generation tasks.for future work, we plan to combine difficulty-based selection with similaritybased selection as an effort to further improve the robustness and effectiveness of the selected demonstrations, and extend dbs to other ranking paradigms (qin et al., 2023;sun et al., 2023).","{252519173: 'Dai et al., 2023;', 240288835: 'Min et al., 2022;', 248218489: 'Sachan et al., 2022', 233296016: 'Thakur et al., 2021'}",https://export.arxiv.org/pdf/2310.14408v1.pdf
1032,263829952,Policy-Gradient Training of Language Models for Ranking,conclusion,Insight-tree,"in this work, we introduce neural pg-rank, a novel training algorithm designed to address challenges associated with training llm-based retrieval models.as a rigorous approach that reduces the dependence on intricate heuristics and directly optimizes relevant ranking metrics, neural pg-rank has demonstrated its effectiveness when training objective aligns with evaluation setup -specifically, in the context of second-stage reranking -by exhibiting remarkable in-domain performance improvement and presenting subtantial out-of-domain generalization to some critical datasets employed in downstream question answering.our work establishes a principled bridge between training objectives and practical utility of the collective set of retrieved results, thereby paving the way for future research endeavors aimed at constructing highly effective retrieval-based llm pipelines that are tailored for practical applications.","{218516694: 'Guo et al., 2022', 245144556: 'Ni et al., 2021', 239009856: 'Paranjape et al., 2022;', 233296016: 'Thakur et al., 2021'}",https://export.arxiv.org/pdf/2310.04407v1.pdf
1033,257687445,Modular Retrieval for Generalization and Interpretation,conclusion,Insight-tree,"this paper formally defines a new information retrieval paradigm, modular retrieval, which aims to bring the benefits of modular learning to information retrieval tasks and increase the interpretability of the retrieval process. based on the proposed retrieval paradigm, we propose a novel modular retrieval method remop, which utilizes three modular operations to flexibly combine retrieval modules to perform zero-shot retrieval tasks, with high interpretability and generalization ability. we explore and verify the effectiveness of the proposed module arithmetic, and experiments on a zero-shot retrieval benchmark show that remop has comparable performance to the fine-tuned model in zero-shot retrieval.",{},https://export.arxiv.org/pdf/2303.13419v1.pdf
1034,234742165,FEW-NERD: A Few-shot Named Entity Recognition Dataset,conclusion and future work,Insight-tree,"we propose few-nerd, a large-scale few-shot ner dataset with fine-grained entity types. this is the first few-shot ner dataset and also one of the largest human-annotated ner dataset. few-nerd provides three unified benchmarks to assess approaches of few-shot ner and could facilitate future research in this area. by implementing state-of-the-art methods, we carry out a series of experiments on few-nerd, demonstrating that few-shot ner remains a challenging problem and worth exploring. in the future, we will extend few-nerd by adding cross-domain annotations, distant annotations, and finer-grained entity types. few-nerd also has the potential to advance the construction of continual knowledge graphs.","{216562779: 'Wang et al., 2020', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2021.acl-long.248.pdf
1035,234479457,"A Multilingual Modeling Method for Span-Extraction Reading Comprehension CCS CONCEPTS â¢ Computing methodologies â Natural language processing; Multilingual modeling KEYWORDS multilingual modeling, span-extraction reading comprehension, multilingual attention, self-adaptive attention",conclusion,Insight-tree,"despite tremendous advances have been made in the field of spanextraction reading comprehension in recent years, large-scale highquality extractive qa datasets in languages other than english remain scarce, and collecting such a sufficient amount of training data for each language is costly, and even impossible, making training reading comprehension systems in other languages challenging. to make full use of all existing extractive training datasets in various languages and to learn the rich hidden semantic knowledge from different language families, we propose xlrc, a multilingual extractive reading comprehension method, to simultaneously model the existing extractive reading comprehension training data in a multilingual environment by using multilingual bert and multilingual attention. experimental results demonstrate the effectiveness of our proposed multilingual modelling by transferring the semantic knowledge learned from various existing datasets in different languages.",{52822214: '[8]'},https://arxiv.org/pdf/2105.14880v1.pdf
1036,258714968,It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance,conclusion,Insight-tree,"we develop a taxonomy of disagreement (based on measurement modeling) which distinguishes between how tasks are conceptualized and how measurements of model performance are operationalized. to provide evidence for our taxonomy, we conduct a survey of practitioners and meta-analysis of relevant literature. based on our taxonomy, we propose a framework for the creation of benchmarks and the documentation of their limitations. future work includes studying task conceptualization via benchmark inter-annotator disagreement.",{},https://export.arxiv.org/pdf/2305.09022v1.pdf
1037,253244513,Natural Language Deduction with Incomplete Information,conclusion,Insight-tree,"in this work, we tackle the generation of missing premise statements in textual reasoning through the use of abduction. we introduce a new system capable of abductive and deductive step generation, which yields inferred missing premises while building a proof showing its reasoning. furthermore, we propose a novel validation method that reduces hallucination and other common failure modes in end-to-end and stepwise searches. future work can improve our system by scaling up the models used, plus using additional notions of validation as discussed in the error analysis. we believe our overall framework can be a promising foundation for future reasoning systems.","{246015349: 'Bostrom et al. 2022', 237450610: 'Bostrom et al. 2021', 233297051: 'Dalvi et al., 2021;', 174801080: 'Min et al., 2019;', 207756753: 'Nie et al., 2020', 160009340: 'Nishida et al., 2019', 248986946: 'Qu et al., 2022', 249062748: 'Yang et al., 2022', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.emnlp-main.564.pdf
1038,253244513,Natural Language Deduction with Incomplete Information,limitations,Insight-tree,"end-to-end models are able to produce a single generation per example reducing the time complexity for sufficiently small sets of premises.step-by-step models like our search procedure in this work are capable of handling sets of any size of premises for the search, but do increase the execution time per example, especially when using validators that require doing generation themselves. nevertheless, validators do reduce the total time required for running a set of examples due to their ability of pruning the search space and thus removing numerous heuristic and generation calls. with better heuristics and validators it may be possible to reduce the time complexity further, but that is left for future work.both the entailmentbank and enwn dataset were written in english and capture relatively limited domains of textual reasoning. different languages might introduce easier lexical patterns for abstraction though and could be a promising path forward. we believe adgv and its variants should work on non-english languages, but testing this was left to future work.enwn draws on everyday ethical scenarios because this was a domain we found fruitful to exhibit the kind of reasoning our system can do. however, we do not follow in the steps of delphi (jiang et al., 2021) in making any claims about its ability to make systems ethical or say anything about ""values"" encoded in pre-trained models. we do not support its use as part of any user-facing system at this time.step signaturestep typex, x â y d deductive x, y d â y d deductive g, x â ya abductive g, y d â ya abductive ya, x â ya abductive ya, y d â ya abductive table 6: a list of possible input statement types each step model can take. x refers to a premise, y d refers to an intermediate deductive conclusion, g refers to the goal, and y a refers to an abductive hypothesis. note that the deductive model can accept inputs in any order but the abductive model cannot, as the abduction operation is not commutative. also note that deductive outputs can be used as inputs to abductive steps, but not the other way around; allowing deductive steps to accept abductive generations could result in vacuous proofs.","{246015349: 'Bostrom et al. 2022', 237450610: 'Bostrom et al. 2021', 233297051: 'Dalvi et al., 2021;', 174801080: 'Min et al., 2019;', 207756753: 'Nie et al., 2020', 160009340: 'Nishida et al., 2019', 248986946: 'Qu et al., 2022', 249062748: 'Yang et al., 2022', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2022.emnlp-main.564.pdf
1039,258575519,Multi-Hop Question Generation with Knowledge Graph-Enhanced Language Model,conclusions,Insight-tree,"the present study proposes a novel approach to question generation, which leverages the benefits of a knowledge graph and mimics human reasoning. the proposed model, referred to as kgel, incorporates an answer-aware graph reasoning module to improve the ability to identify key information from the context. empirical evaluations on the hotpotqa dataset demonstrate the superiority of kgel over baseline models, particularly in terms of the completeness and answerability of generated questions.","{202565869: '[4,', 216553210: '8,', 214802355: '[9]', 224705407: '10', 155100120: '[23]', 52822214: '[28]'}",NaN
1040,264451629,Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs,conclusion,Insight-tree,"in this paper, we propose web-dro, which is an efficient grouplevel data clustering and reweighting strategy for unsupervised dense retrieval model training.we use structures of the web graph to train a link prediction style embedding model for clustering and utilize groupdro to reweight the clusters during training.experiments on ms marco and beir show that web-dro achieves significant improvements over baseline models, especially on difficult tasks.further analysis shows that group weights learned by",{86611921: '[20]'},https://export.arxiv.org/pdf/2310.16605v2.pdf
1041,155100120,Dynamically Fused Graph Network for Multi-hop Reasoning,conclusion,Insight-tree,"we introduce dynamically fused graph network (dfgn) to address multi-hop reasoning. specifically, we propose a dynamic fusion reasoning block based on graph neural networks. different from previous approaches in qa, dfgn is capable of predicting the sub-graphs dynamically at each reasoning step, and the entity-level reasoning is fused with token-level context. we evaluate dfgn on hotpotqa and achieve leading results. besides, our analysis shows dfgn can produce reliable and explainable reasoning chains. in the future, we may incorporate new advances in building entity graphs from texts, and solve harder reasoning problems, e.g. ""comparison"" in hot-potqa.",{},https://arxiv.org/pdf/1905.06933v2.pdf
1042,258685562,Answering Complex Questions over Text by Hybrid Question Parsing and Execution,conclusion,Insight-tree,"we propose hpe for answering complex questions over text, which combines the strengths of neural network approaches and symbolic approaches. we parse the question into h-expressions followed by the hybrid execution to get the final answer. our extensive empirical results demonstrate that hpe has a strong performance on various datasets under supervised, few-shot, and zero-shot settings. moreover, our model has a strong interpretability exposing its underlying reasoning process, which facilitates understanding and possibly fixing its errors. by replacing our text reader with kb or ","{236771976: 'Trivedi et al., 2022b', 211003735: 'Wolfson et al., 2020'}",https://export.arxiv.org/pdf/2305.07789v1.pdf
1043,221569481,The Evolution of Imprinted microRNAs and Their RNA Targets,conclusions,Insight-tree,"oviparous mothers commit resources to offspring before fertilization. therefore, genes expressed in embryos are unable to influence how much an embryo receives because the quantity of yolk in an egg is a fait accompli. by contrast, how much an embryo receives is ""up for grabs"" in placental development because mothers invest substantial resources after fertilization. embryos have evolved to actively solicit or seize maternal resources and to compete with other (contemporary or future) embryos for maternal investment. this fundamental difference between prezygotic and postzygotic provisioning accounts for many distinctive features of mammalian development.",{},NaN
1044,253708231,CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,conclusion,Insight-tree,"this paper proposes a novel multi-vector retrieval method that achieves state-of-the-art performance on several benchmark datasets while being 40Ã faster than colbert-v2 and 17Ã faster than the most efficient multi-vector retrieval library to date, plaid, on gpus. by jointly optimizing for the token index size and load balancing, our new dynamic lexical routing scheme greatly reduces the redundancy in the all-to-all token interaction of col-bert while bridging the word-mismatch problem in coil. experiments on both in-domain and outof-domain datasets demonstrate the effectiveness and efficiency of our model.","{238744204: 'Chen et al., 2022', 249097975: 'Izacard et al., 2022;', 249888962: 'Lin and Lin, 2022;', 253255229: 'Qian et al., 2022', 244799249: 'Santhanam et al., 2022b', 233296016: 'Thakur et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.acl-long.663.pdf
1045,253708231,CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,limitations,Insight-tree,"the limitation of citadel mainly shows in two aspects. first, at the beginning of training, the model needs to route each token vector to multiple activated keys for token interaction, which increases the computation cost compared to coil and col-bert. this results in slower training speed but it gets better when training approaches the end as more tokens are pruned by the â 1 regularization. another drawback lies in the implementation of citadel, or more generally speaking, most multivector retrieval methods. the token-level retrieval and aggregation make them not compatible with established search libraries such as faiss or pyserini. moreover, for time and space efficiency, multi-vector retrieval also requires more engineering efforts and low-level optimization. recently, xtr (lee et al., 2023) provides a solution that constrains the document-level retrieval to be consistent with the token-level retrieval during training, which can be used for streamlining citadel.","{238744204: 'Chen et al., 2022', 249097975: 'Izacard et al., 2022;', 249888962: 'Lin and Lin, 2022;', 253255229: 'Qian et al., 2022', 244799249: 'Santhanam et al., 2022b', 233296016: 'Thakur et al., 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2023.acl-long.663.pdf
1046,211532373,Generating Followup Questions for Interpretable Multi-hop Question Answering,conclusion,Insight-tree,"followup queries are essential to solving the difficult cases of multi-hop qa, and real followup questions are an advance in making this process interpretable. we have shown that pointer generator networks can effectively learn to read partial information and produce a fluent, relevant question about what is not known, which is a complement to their typical role in summarizing what is known. our task poses a novel challenge that tests semantic properties of the generated output.","{174801764: 'Min et al. 2019a', 174801080: 'Min et al. 2019b', 202660724: 'Nie et al., 2019;', 160009340: 'Nishida et al., 2019;', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2002.12344v1.pdf
1047,235313620,Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins,conclusion,Insight-tree,"we demonstrate how seemingly unrelated tasks spanning data integration, search, and recommendation can all be viewed as instantiations of context enrichment. we propose keyless joins as a unifying abstraction that can power a system for general context enrichment, which allows us to view context enrichment as a data management problem. consequently, we developed and applied ember, a first-of-its kind system that performs no-code context enrichment via keyless joins. we evaluate how developing a keyless join enrichment layer empowers a single system to generalize to five downstream applications, with no ml code written by the user.",{},https://arxiv.org/pdf/2106.01501v1.pdf
1048,226283753,COSATA: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences,conclusion,Insight-tree,"cosata is an open-source constraint satisfaction solver for easily expressing and evaluating multifact compositional patterns in semi-structured tables of text, paired with an interpreted language that allows expressing micro-models. the tool, source, examples, and documentation are available at http://www.github.com/clulab/cosata/ .","{207870753: 'Tu et al., 2020', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2020.emnlp-demos.10.pdf
1049,226254024,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,conclusion,Insight-tree,"in this paper, we deal with the multi-choice question answering task which requires background knowledge or commonsense. we propose a novel question-answering method by exploring how to efficiently integrate multiple knowledge sources, i.e. conceptnet, wikipedia and the cambridge dictionary. firstly, we propose a novel graph-based iterative knowledge retrieval module to iteratively retrieve concepts and entities related to a given question and its choices. in addition, we propose an answer choice-aware attention mechanism to fuse all hidden representations encoded by a pre-trained language model. we conducted experiments on the commonsenseqa dataset and the experimental results show that our method significantly outperforms other competitive methods in accuracy. further ablation studies show the effectiveness of graph-based iterative knowledge retrieval module and answer choice-aware attention module in retrieving and synthesizing background knowledge from multiple knowledge sources. in the future, we will extend our method to deal with the open-domain question answering tasks that require the external background knowledge.","{189927857: 'Feldman and El-Yaniv, 2019', 202773198: 'Qi et al., 2019', 52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/2020.coling-main.232.pdf
1050,237771729,In silico Analysis of Human miRNAs in SARS-CoV-2 Genome,conclusion,Insight-tree,"in our current investigation, we identified mirnas for sars-cov-2 in human beings using computational tools.this study was based on an interesting hypothesis of the utilization of host mirna as a potential post-exposure therapy because the current evidence suggests that host mirnas may downregulate the viral gene expression.",{},https://journals.umt.edu.pk/index.php/BSR/article/download/1599/647
1051,236986870,How Optimal is Greedy Decoding for Extractive Question Answering?,conclusions,Insight-tree,"we investigate the optimality of greedy decoding for extractive question answering by comparing it to exact-extract, an optimal decoding algorithm that guarantees both extractiveness and exactness. while the greedy algorithm lags behind exact-extract in the zero-shot setting, training the model on as few as 16 labeled examples shrinks the performance gap substantially. this gap continues to narrow as more examples are available, typically converging to less than 1 point (f1) when training on 1024 examples. overall, our results showcase the impressive ability of pretrained language models to adapt to extractive question answering while relying only on a naive decoding algorithm.","{237420912: ', Chada and Natarajan, 2021', 204823992: '[Fisch et al., 2019]', 230433978: 'Ram et al. [2021]', 52822214: '[Yang et al., 2018'}",https://export.arxiv.org/pdf/2108.05857v2.pdf
1052,235187342,Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering *,conclusion,Insight-tree,"we propose to dynamically construct amr-sg that can reflect the intrinsic relations of relevant facts leveraging amr, a graph annotation. amr-sg combines the advantages of rich textual corpus and graph structure, where we can select useful facts that completely form the reasoning chain and make fact-level modeling. experimental results show that amr-sg can maintain high explainability, and successfully couple with strong pretrained models to achieve significant improvement on openbookqa and arc-challenge over approaches leveraging additional kgs. ","{208267807: 'Asai et al., 2020;', 202660724: 'Nie et al., 2019;', 155100120: 'Qiu et al., 2019;', 207870753: 'Tu et al., 2020;', 202785879: 'Yadav et al. 2019', 218487313: 'Yadav et al. 2020', 52822214: 'Yang et al., 2018;', 211562797: 'Zhao et al., 2020;'}",https://arxiv.org/pdf/2105.11776v1.pdf
1053,259203489,GLIMMER: generalized late-interaction memory reranker,conclusion,Insight-tree,"retrieval-augmented language models are powerful but slow in inference, while pre-computed memory-augmented models are fast at the cost of quality. hybrid late-interaction models such as lu-men present a good quality-compute trade-off. we introduce glimmer, an improved late-interaction model that also incorporates learned end-to-end reranking and multi-task training to achieve an even better trade-off. glimmer achieves strong gains in quality at faster speeds compared to lumen and fid on the kilt benchmark of knowledgeintensive tasks.","{86611921: 'Kwiatkowski et al., 2019', 244799249: 'Santhanam et al., 2022', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2306.10231v1.pdf
1054,255545881,Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text,conclusion and future work,Insight-tree,"to study the zero-shot capability of the logical reasoning models, we propose the first benchmark for the generalized zero-shot logical reasoning, named zslr. it includes six splits sampled with three strategies and two metrics to comprehensively evaluate the performances. also, we propose a model taco to enhance the reasoning type perception through the heuristic input reconstruction and the type-aware contrastive learning. also, we conduct extensive experiments on the zero-shot splits, full-data setting as well as other dataset. superior results illustrate the effectiveness and generalization capability of the proposed modules.","{232380161: '[7]', 248496003: '[9]', 247518855: '[10]', 234335834: '[11]', 220483148: '[16]', 219965751: '[20]', 52822214: '[23]', 202572622: '[27]', 230433978: '[30]'}",https://export.arxiv.org/pdf/2301.02983v1.pdf
1055,257921404,Rethinking the Role of Token Retrieval in Multi-Vector Retrieval,conclusion,Insight-tree,"multi-vector retrieval leverages query and document token representations for effective information retrieval. in this paper, we propose xtr that simplifies the existing three-stage inference of multivector models by improving the initial token retrieval stage. specifically, xtr scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens. as a result, xtr achieves state-of-the-art performances on zero-shot information retrieval benchmarks while greatly reducing the flops of the scoring stage. we further show that indeed our objective encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query.","{249097975: '[Izacard et al., 2022]', 245144556: '[Ni et al., 2021]'}",https://export.arxiv.org/pdf/2304.01982v2.pdf
1056,234093776,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,conclusion,Insight-tree,"we presented qasper, an information-seeking qa dataset over nlp research papers. with natural questions asked as follow-up to titles and abstracts, the task presented by qasper requires evidence from multiple paragraphs and/or figures and tables within the full text of the papers. our empirical quire workers to write questions grounded in those snippets.  table 5: error analysis of our best model (led from row 5 from table 2) on 55 test examples with low f 1 score (excluding those with ""yes,"" ""no,"" or ""unanswerable"" gold answers). ""quotations"" denote extractive gold answers. we note lacks domain knowledge errors are not always solved by better entity type resolution (see â ).","{208267807: 'Asai et al., 2020;', 215737171: 'Beltagy et al., 2020', 67855846: 'Dua et al., 2019', 226262208: 'Ferguson et al., 2020', 202572622: 'Jin et al., 2019', 86611921: 'Kwiatkowski et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/2021.naacl-main.365.pdf
1057,243865561,Neural Natural Logic Inference for Interpretable Question Answering,conclusion,Insight-tree,"in this work, we explore the feasibility of combining natural logic with neural networks for interpretable question answering. we present an end-toend differentiable method for learning the parameters as well as the structure of natural logical rules, which is capable of considering the contextual information while conducting natural logic-based reasoning. experimental results on the regents science exam of the aristo dataset show that our proposed model could bring improvements over baseline methods.","{52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2021.emnlp-main.298.pdf
1058,260379194,Teaching Smaller Language Models To Generalise To Unseen Compositional Questions,conclusion,Insight-tree,"we have argued that an ability to reason over imperfect and incomplete information is a critical skill with which question-answering models must be endowed. to facilitate such ability we create ratd datasets that are designed to impart heuristic reasoning strategies with context of a form similar to that which retrieved contexts for downstream tasks will have. we show that training on ratd datasets improves performance on all unseen evaluation datasets with retrieved contexts. this sometimes comes at a small cost in situations where questions come with gold contexts that are in a form that our model is already good at utilizing (sqa gf , drop, and iirc g ) although we suggest that in practice such gold contexts are the less common case. (r1)","{211010520: 'Bartolo et al., 2020', 165163607: 'Clark et al., 2019a', 249097975: 'Izacard et al., 2022', 202572622: 'Jin et al., 2019', 230437663: 'Khattab et al. 2021', 204915921: 'Khot et al., 2020', 86611921: 'Kwiatkowski et al., 2019', 201058633: 'Lin et al., 2019', 240288953: 'Qi et al., 2021', 213474484: 'Rogers et al., 2020', 237263476: 'Talmor et al., 2021', 233296016: 'Thakur et al. 2021', 236771976: 'Trivedi et al. 2022a', 211003735: 'Geva et al. 2020', 221970302: 'Xiong et al. 2021', 52822214: 'Yang et al., 2018'}",https://export.arxiv.org/pdf/2308.00946v2.pdf
1059,253080447,Efficiently Tuned Parameters are Task Embeddings,conclusion,Insight-tree,"in this paper, we show that efficiently tuned parameters are highly predictive for inter-task transferability and thus can be used as off-the-shelf task embeddings for source task selection in intermediatetask transfer learning. our empirical investigation with three parameter-efficient tuning methods on 22 nlp tasks demonstrates that our approach outperforms prior works on inter-task transferability prediction despite being more efficient.",{},https://www.aclanthology.org/2022.emnlp-main.334.pdf
1060,250390431,Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding,conclusion and future work,Insight-tree,"temporal reading comprehension plays a critical role in natural language understanding. in this paper, we propose a precise question understanding method to tackle the trc problem. specifically, we encode temporal ordering questions into repre-sentations of referred events and concerned temporal relations, based on which candidate answers are evaluated in terms of their temporal relations to the referred events. in addition, a contrastive loss is employed to empower the model to capture essential differences among temporal relations. experimental results based on four pre-trained models verify the effectiveness of our proposed approach. in the future, we will investigate general approaches to handle more diverse temporal relation understanding problems and improve the passage understanding capability for temporal reading comprehension.","{67855846: 'Dua et al., 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.naacl-main.28.pdf
1061,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,limitations & future work,Insight-tree,"as other parameter-efficient adaptation methods (see Â§4) may outperform prompttuning in specific situations, it would be interesting to test whether an approach similar to spot could extend successfully to these methods. at the same time, we believe that prompttuning has its own merit. as pre-trained language models become larger and larger, some advantages of prompttuning over other methods are: (1) among current methods with learnable parameters, prompttuning is the most parameter efficient, requiring less than 0.01% task-specific parameters for most model sizes.(2) prompttuning is simpler than other methods, as it does not modify the internal model architecture (cf. the prefix-tuning method of li and liang (2021), which adds a prefix to each layer of both the transformer encoder and decoder); as such, prompttuning allows mixed-task inference and facilitates transfer learning between tasks.(3) as model capacity increases, prompttuning becomes more competitive with modeltuning; to the best of our knowledge, this has not been shown for other methods. (4) soft prompts could possibly be interpreted as natural language instructions.additionally, since our prompt-based task embedding approach does not capture all of the factors that influence task transferability, we leave further exploration of other task embedding methods to future work.","{233289699: 'Poth et al. 2021', 173188058: 'Talmor and Berant, 2019;', 218487733: 'Vu et al. 2020'}",https://arxiv.org/pdf/2110.07904v2.pdf
1062,239009558,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,conclusion,Insight-tree,"in this paper, we study transfer learning in the context of prompt tuning. we show that scale is not necessary for prompttuning to match the performance of modeltuning. on superglue, our spot approach matches or even exceeds the performance of modeltuning by a large margin across model sizes while being more parameter-efficient. our large-scale study on task transferability indicates that tasks can benefit each other via prompt transfer in various scenarios. finally, we demonstrate that task prompts can be interpreted as task embeddings to formalize the similarity between tasks. we propose a simple yet efficient retrieval approach that measures task similarity to identify which source tasks could confer benefits to a novel target task. taken as a whole, we hope that our work will spur more research into prompt-based transfer learning.   (raffel et al., 2020), i.e., learning rate 0.001, adafactor optimizer with pre-training parameter states restored, and dropout probability 0.1. to improve the model tuning baselines, we perform a sweep over the batch size hyperparameter and select 2 16 tokens per batch, following lester et al. (2021). table 5 shows the performance of our spot xxl superglue submission, along with several strong competitors from the public superglue leaderboard. apart from the human baseline, the top-7 submissions all tune >3b parameters directly on the final tasks. only three previous superglue submissions use parameter efficient adaptation, in the sense of tuning <1m parameters on the final tasks; all other submissions tune >50m parameters. 17 17 the ""ailabs team, transformers"" submission is listed as tuning 3m parameters, but we suspect this is in error, as the our spot submission achieves a score of 89.2, which far exceeds all other parameter-efficient adaptation methods, including gpt-3, which benefits from over 10Ã more frozen parameters (although it uses no tuned parameters). compared to warp (hambardzumyan et al., 2021), our spot approach tunes 16Ã more parameters (410k vs. 25k), and benefits from 50Ã more frozen parameters.","{233289699: 'Poth et al. 2021', 173188058: 'Talmor and Berant, 2019;', 218487733: 'Vu et al. 2020'}",https://arxiv.org/pdf/2110.07904v2.pdf
1063,249922213,Discover Oncology Circ_0000705 facilitates proline metabolism of esophageal squamous cell carcinoma cells by targeting miR-621/PYCR1 axis,conclusion,Insight-tree,"these findings indicated that circ_0000705 could promote proline metabolism and escc progression by targeting mir-621/pycr1 axis, suggest a key role for circ_0000705 in proline metabolism in escc, and imply that circ_0000705 may be a potential biomarker or/and possible therapeutic target for escc. however, whether circ_0000705 is also highly expressed and has a similar mechanism of regulating proline metabolism in other types of tumors deserve further research in the future.",{},NaN
1064,219401765,GMAT: Global Memory Augmentation for Transformers,conclusion,Insight-tree,"in this work, we proposed gmat, a simple extension to the transformer architecture that allows a better trade-off between compute and performance and can be naturally used for sequence compression. our approach can be seamlessly integrated with the increasingly-popular sparse transformer variants.","{174801764: '[14]', 52822214: '[29]'}",https://arxiv.org/pdf/2006.03274v1.pdf
1065,247647041,"Determination of Important Variables in Food Security Classification Using Random Forest 1 EÅtÃ¼rk, Ã. GÄ±da GÃ¼vencesi DÃ¼zeyi SÄ±nÄ±flandÄ±rÄ±lmasÄ±nda KullanÄ±lan Ãnemli GÃ¶stergelerin Random Forest YÃ¶ntemine GÃ¶re Belirlenmesi",conclusions,Insight-tree,"results indicated that seasonal apricot workers suffered from food insecurity. food security was largely dependent on the purchasing power of food. the auc value of the rf classification model (0.846) indicated its utility in the detection of the driving forces and causation patterns behind household food (in)security of the seasonal agricultural workers in turkey and in the world. the findings of this study showed how the families of agricultural workers perceiving food security and necessity of developing different intervention strategies for different populations. thus, public services addressing the chronic food insecurity status of",{},https://dergipark.org.tr/tr/download/article-file/2139537
1066,236478213,Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations,conclusion,Insight-tree,"we conducted user studies to understand whether explanations from a state-of-the-art open-domain qa system help improve error-detectability for endusers. our study showed that for odqa, simple explanations based on evidence snippets can significantly improve error-detectability and beat strong baselines such as communicating model's confidence. we observed this for multiple modalities of interaction: spoken and visual modalities. however, results also indicated that not every explanation type is guaranteed to improve performance over confidence and the best explanation strategy may change with the modality, e.g., due to differences in users' cognitive abilities across modalities. thus, developers and researchers of explainable odqa systems should not take the effectiveness of explanations for granted and should evaluate and tune them on the tasks and modalities where these models will be eventually deployed.",{},https://www.aclanthology.org/2021.findings-acl.95.pdf
1067,102352338,Tracking Discrete and Continuous Entity State for Process Understanding,conclusion,Insight-tree,"in this paper, we present a structured architecture for entity tracking which leverages both the discrete and continuous characterization of the entity evolution. we use a neural crf approach to model our discrete constraints while tracking entities and locations recurrently. our model achieves state of the art results on the propara dataset.","{52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/W19-1502.pdf
1068,264590585,N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics,conclusion,Insight-tree,"we introduced n-critics, an innovative method leveraging feedback from open-sourced llms to iteratively refine model outputs, setting it apart from current self-refinement approaches (also, their underlying models are not free to use).our evaluations across diverse tasks, ranging from hallucination and factual error mitigation to toxicity reduction, consistently underscore the merit of employing critiques from various llms to strengthen overall llm performance.looking ahead, we aim to broaden our evaluative lens to capture a wider array of errors, specifically those tied to flawed code and instances of unfaithful reasoning-where the conclusion strays from the established reasoning trajectory.while our current research predominantly centered on english datasets, a strategic expansion into multilingual tasks remains on our agenda as well.",{},https://export.arxiv.org/pdf/2310.18679v1.pdf
1069,248496232,Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions,conclusions and discussion,Insight-tree,"we identify a prominent source of bias in crowdsourced nlu datasets, called instruction bias, which originates in annotation instructions written by dataset creators. we study this bias in 14 nlu benchmarks, showing that instruction examples used to create nlu benchmarks often exhibit clear patterns that are propagated by annotators to the collected data. in addition, we investigate the effect of instruction bias on model performance, showing that instruction patterns can lead to overestimated performance as well as limit the ability of models to generalize to other task examples.",{},https://www.aclanthology.org/2023.eacl-main.130.pdf
1070,237101158,A Robustly Optimized BERT Pre-training Approach with Post-training,conclusion,Insight-tree,"in the paper, we present a 'pre-training'+'post-training'+'fine-tuning' three-stage paradigm and a language model named ppbert based on the three-stage paradigm, which is a supplementary framework for the standard 'pre-training'+'fine-tuning' two-stage architecture. our proposed three-stage paradigm helps to incorporate task-awareness knowledge and domain knowledge within pre-trained model, also reduce the bias in the training corpus. ppbert can benefits from the regularization effect since it leverages cross-domain or cross-task data, which helps model generalize better with limited data and adapt to new domains or tasks better. with the latest plms as baseline and encoder backbone, ppbert is evaluated on 24 well-known benchmarks, which outperforms strong baseline models and obtains new sota results. we hope this work can encourage further research into the language models training, and the future works involve the choice of other transfer learning sources such as cv etc.","{52822214: 'Yang et al., 2018'}",NaN
1071,248780313,Predicting Difficulty and Discrimination of Natural Language Questions,conclusion,Insight-tree,"in this paper, we explored qa datasets through the lens of item response theory. we have demonstrated a way to build regression models that can describe the difficulty and discrimination of a question. we note that our work is limited in two important ways: firstly, we only use the dfgn model in our artificial crowd, which may have introduced a bias in which some factors that make questions difficult/discriminatory are only applicable to this model. secondly, we only explore the hotpotqa dataset, which may further limit our analysis to only be applicable to hotpotqa or similar datasets. future work could incorporate multiple models and datasets to explore a more easily generalizable difficulty/discrimination prediction pipeline. we also note that our analysis here focused on qa. however, there are many nlp tasks in which the difficulty or discrimination of an item may be important. our work here could naturally extend to these domains. finally, automatically predicting these traits without relying on user responses can engender a host of creative educational applications. future work can also leverage such predictive models to explore more efficient strategies for learning and evaluation.","{155100120: 'Qiu et al. 2019', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.acl-short.15.pdf
1072,204823992,MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension,conclusions,Insight-tree,"we have presented the mrqa 2019 shared task, which focused on testing whether reading comprehension systems can generalize to examples outside of their training domain. many submissions improved significantly over our baseline, and investigated a wide range of techniques.","{67855846: 'Dua et al., 2019', 204800552: 'Lee et al. 2019', 208000835: 'Su et al. 2019', 173188058: 'Talmor and Berant 2019', 52822214: 'Yang et al., 2018'}",https://www.aclweb.org/anthology/D19-5801.pdf
1073,202539540,QUARTZ: An Open-Domain Dataset of Qualitative Relationship Questions,conclusion,Insight-tree,"understanding and applying textual qualitative knowledge is an important skill for questionanswering, but has received limited attention, in part due the lack of a broad-coverage dataset to study the task. quartz aims to fill this gap by providing the first open-domain dataset of qualitative relationship questions, along with the requisite qualitative knowledge and a rich set of annotations. specifically, quartz removes the requirement, present in all previous qualitative reasoning work, that a fixed set of qualitative relationships be formally pre-specified. instead, quartz tests the ability of a system to find and apply an arbitrary relationship on the fly to answer a question, including when simple reasoning (arguments, polarities) is required.","{52822214: 'Yang et al., 2018;'}",https://www.aclweb.org/anthology/D19-1608.pdf
1074,207765742,Multi-Paragraph Reasoning with Knowledge-enhanced Graph Neural Network,conclusion,Insight-tree,"multi-paragraph reasoning is crucial for answering open-domain questions in practice, while it is still not considered in most existing openqa systems. in this work, we propose a novel openqa model kgnn, which performs reasoning over paragraphs via a knowledge enhanced graph neural network. experimental results show that kgnn outperforms strong baselines with a large margin on the hotpotqa dataset, and also has the ability to tackle more informative texts. we hope our work can shed some lights to the combination of knowledge graph and text for openqa.",{52822214: 'Yang et al. 2018'},https://arxiv.org/pdf/1911.02170v1.pdf
1075,239016681,Learning to Solve Complex Tasks by Talking to Agents,conclusion,Insight-tree,"in this work we motivate a new challenge task of solving complex task by communicating with existing ai agents. developing approaches for this challenge, we argue, could allow for more generalizable, privacy-preserving and efficient models. towards this goal, we introduce a new benchmark dataset commaqa which involves multihop questions with three multi-hop reasoning challenges, all solvable by composing four qa agents. each agent has an internal knowledge base (similar to ai assistants or large lms) that can be queried via natural language queries. experiments with state-of-art language models indicated that they struggle to solve commaqa, even when provided with agents' internal knowledge. in contrast, a model that is able to learn to communicate with the agents, albeit using annotated decompositions, is able to solve this task. this indicates the need and potential of such approaches to solve complex tasks. we hope this dataset will enable future work on learning to communicate with agents without relying on this additional supervision. theory 1: what movies have people from the country $1 acted in? a1:select(textqa, _, ""who are from $1?"") a2:project_keys_flat_unique(textqa/tableqa, a1, ""which movies has {} been an actor in?"") theory 2: what movies have the directors from $1 directed? a1:select(textqa, _, ""who is from the country $1?"") a2:project_keys_flat_unique(textqa/tableqa, a1, ""which movies has {} directed?"") theory 3: what awards have movies produced by people born in $1 won? a1:select(textqa, _, ""who were born in the year $1?"") a2:project_keys_flat_unique(textqa/tableqa, a1, ""for which movies was {} the producer?"") a3:project_keys_flat_unique(tableqa, a2, ""which awards did the movie {} win?"") theory 4: what awards have movies written by people born in $1 won? a1:select(textqa, _, ""who were born in the year $1?"") a2:project_keys_flat_unique(textqa/tableqa, a1, ""what movies has {} written?"") a3:project_keys_flat_unique(tableqa, a2, ""which awards were given to {}?"")","{211126663: 'Clark et al., 2020;', 67855846: 'Dua et al., 2019;', 226262208: 'Ferguson et al., 2020', 221749191: 'Trivedi et al., 2020;', 236771976: 'Khot et al., 2021', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2110.08542v1.pdf
1076,128344862,Repurposing Entailment for Multi-Hop Question Answering Tasks,conclusions,Insight-tree,"using entailment for question answering has seen limited success. neural entailment models are designed and trained on tasks defined over sentence pairs, whereas qa often requires reasoning over longer texts spanning multiple sentences. we propose multee, a novel qa model that addresses this mismatch. it uses an existing entailment model to both focus on relevant sentences and aggregate information from these sentences. results on two challenging qa datasets, as well as our ablation study, indicate that entailment based qa can achieve state-of-the-art performance and is a promising direction for further research.","{52822214: 'Yang et al., 2018;'}",https://arxiv.org/pdf/1904.09380v1.pdf
1077,239016730,On the Robustness of Reading Comprehension Models to Entity Renaming,conclusion,Insight-tree,"in this paper, we systematically study the robustness of mrc models to entity name substitution. specifically, we propose a substitution framework along with candidate names of different implications. we experiment with three pretrained language models on five mrc datasets. we find that models trained on distantly-supervised datasets are susceptible to entity name substitution, while models trained on human-annotated datasets are relatively robust, with gpe renaming harder than per and org renaming. the lack of robustness can be further attributed to model's overreliance on entity knowledge and name clues. we also find that spanbert, which is pretrained using span-level objectives, shows better robustness than bert and roberta. leveraging these insights, we study defense approaches based on continual pretraining and demonstrate that entity-based masking policies are beneficial to model's robustness. future works include systematically studying the effect of background knowledge in mrc, and developing more effective methods to improve the robustness of mrc models.","{204823992: 'Fisch et al., 2019;', 198229624: 'Joshi et al. 2020', 235293903: 'Lai et al., 2021', 208201969: 'Sugawara et al., 2020;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.naacl-main.37.pdf
1078,219687051,Self-supervised Learning: Generative or Contrastive,conclusion,Insight-tree,"this survey comprehensively reviews the existing selfsupervised representation learning approaches in natural language processing (nlp), computer vision (cv), graph learning, and beyond. self-supervised learning is the present and future of deep learning due to its supreme ability to utilize web-scale unlabeled data to train feature extractors and context generators efficiently. despite the diversity of algorithms, we categorize all self-supervised methods into three classes: generative, contrastive, and generative contrastive according to their essential training objectives. we introduce typical and representative methods in each category and sub-categories. moreover, we discuss the pros and cons of each category and their unique application scenarios. finally, fundamental problems and future directions of self-supervised learning are listed.",{198229624: '[64]'},https://arxiv.org/pdf/2006.08218v5.pdf
1079,212633743,Natural Language QA Approaches using Reasoning with External Knowledge,conclusion and future directions,Insight-tree,"in this paper we have surveyed 2 recent research on nlqa when external knowledge -beyond what is given in the test part -is needed in correctly answering the questions. we gave several motivating examples, mentioned several datasets, discussed available knowledge repositories and methods used in selecting needed knowledge from larger repositories, and analyzed and grouped several models and architectures of nlqa systems based on how the knowledge is expressed in them and the type of reasoning module used in them. although there have been some related recent surveys, such as [67], none of them focus on how exactly knowledge and reasoning is done in the nlqa systems dealing with datasets that require external knowledge. our survey touched upon knowledge types that include structured knowledge, textual knowledge, knowledge embedded in a neural network, knowledge provided via specially constructed examples and combinations of them. we explored how symbolic, neural and mixed models process and reason with such knowledge. based on our observations for various models following are some questions and future directions. following up on the lifecycleqa dataset where some concepts, such as ""indicates"", were manually defined, several questions -with partial answers -may come to mind. (i) how big is the list of such concepts? if a list of them is made and they are defined then these definitions can be directly used or compiled into neural models. can cyc and the book [27] be starting points in this direction? (ii) can these definitions be learned from data? how? unless one only focuses on specific datasets, the challenge in learning these definitions would be that for each of them specialized examples would have to be created. (iii) when is it easier to just write the definitions in a logical language? when is it easier to learn from data sets? some concepts are easy to define logically but may require lot of examples to teach a system. there are concepts whose definitions took decades for researchers to formalize. an example is the solutions of frame problem. so it is easier to use that formalization rather than learning from scratch. on the other hand the notion of ""cause"" is still being fine tuned. another direction of future work centers around the question of how well neural network can do reasoning; what kind of reasoning they can do well and what are the challenges ?","{67855846: '[20]', 86611921: '[37]', 52822214: '[83]'}",https://arxiv.org/pdf/2003.03446v1.pdf
1080,208201969,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,conclusion,Insight-tree,"existing analysis work in mrc is largely concerned with evaluating the capabilities of systems.by contrast, in this work, we proposed an analysis methodology for the benchmarking capacity of datasets.our methodology consists of input-ablation tests, in which each ablation method is associated with a skill requisite for mrc.we exemplified 12 skills and analyzed 10 datasets.the experimental results suggest that for benchmarking sophisticated nlu, datasets should be more carefully designed to ensure that questions correctly evaluate the intended skills.in future work, we will develop a skill-oriented method for crowdsourcing questions.",{52822214: 'Yang et al. 2018'},https://arxiv.org/pdf/1911.09241v1.pdf
1081,224705407,Multi-hop Question Generation with Graph Convolutional Network,conclusion,Insight-tree,"multi-hop qg task is more challenging and worthy of exploration compared to conventional singlehop qg. to address the additional challenges in multi-hop qg, we propose mulqg, which does multi-hop context encoding with graph convolutional network and encoding fusion via a gated reasoning module. to the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentencelevel information. the model performance on hot-potqa dataset demonstrates its effectiveness on aggregating scattered pieces of evidence across the paragraphs and fusing information effectively to generate multi-hop questions. the strong reasoning ability of the multi-hop encoder in the mulqa model can potentially be leveraged in complex generation tasks for the future work.  table a1: performance comparison between our multqg model and fine-tuning state-of-the-art large pre-trained models on hotpotqa test set.","{52822214: 'Yang et al., 2018', 202572810: 'Zhang and Bansal, 2019'}",https://www.aclweb.org/anthology/2020.findings-emnlp.416.pdf
1082,258564753,DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation,conclusion,Insight-tree,"in this paper we proposed a novel qa domain adaptation framework called domaininv, an unsupervised algorithm which does not require the use of labeled target domain, neither it depends on the synthetic data or pseudo labeled target domain. domaininv uses 1) domain invariant fine tuning which fine tunes the qa model using the target style on the source domain and 2) adversarial label correction which identifies the target distributions which are still far apart from source domain and optimize the feature generator to bring them closer near to source support class wisely. evaluation of domaininv showed that it outperforms all the baselines and achieves the superior performance establishing the new benchmark on qa domain adaptation.","{204823992: 'Fisch et al., 2019b', 219721462: 'Kamath et al., 2020', 222177817: 'Kratzwald et al., 2020;', 86611921: 'Kwiatkowski et al., 2019', 204800552: 'Lee et al., 2019b;', 216867120: 'Miller et al., 2020;', 173188058: 'Talmor and Berant, 2019;', 52822214: 'Yang et al., 2018', 247518803: 'Yue et al., 2022a;', 237364113: 'Yue et al., 2021', 252199900: 'Yue et al., , 2022b'}",https://export.arxiv.org/pdf/2305.05589v1.pdf
1083,202565869,Let's Ask Again: Refine Network for Automatic Question Generation,conclusion and future work,Insight-tree,"in this work, we proposed refine networks (refnet) for question generation to focus on refining and improving the initial version of the generated question. our proposed refnet model consisting of a preliminary decoder and a refinement decoder with dual attention network outperforms the existing state-of-the-art models on the squad, hotpot-qa and drop datasets. along with automated evaluations, we also conducted human evaluations to validate our findings. we further showed that using reward-refnet improves the initial draft on specific aspects like fluency, answerability and originality. as a future work, we would like to extend refnet to have the ability to decide whether a refinement is needed on the generated initial draft.","{67855846: 'Dua et al., 2019'}",https://www.aclweb.org/anthology/D19-1326.pdf
1084,226254596,Context-Aware Answer Extraction in Question Answering,conclusion,Insight-tree,"in this paper, we showed the importance of predicting an answer with the correct context of a given question. we proposed blanc with two novel ideas: context word prediction task and a block attention method that identifies an answer within the context of a given question. the context words prediction task labels latent context words with the labeled answer-span and is used in a multi-task learning manner. block attention models the latent context words with negligible extra parameters and training/inference time. we showed that blanc increases reading comprehension performance, and we verify that the performance gain increases for complex examples (i.e., when the answer occurs two or more times in the passage). also, we showed the generalizability of blanc and its contextaware performance with the zero-shot supporting fact prediction task on the hotpotqa dataset. ","{208267807: 'Asai et al., 2020', 204823992: 'Fisch et al. 2019', 198229624: 'Joshi et al., 2020', 202558815: 'Min et al. 2019a', 174801080: 'Min et al., 2019b;'}",https://arxiv.org/pdf/2011.02687v1.pdf
1085,233297024,Constrained Language Models Yield Few-Shot Semantic Parsers,conclusion,Insight-tree,"we wish to rapidly develop semantic parsers in new domains. to this end, we have demonstrated that constrained decoding of powerful language models can enable the paraphrasing of user utterances into a controlled sublanguage, which may then be mapped to a task-specific representation. with small hundreds of examples we are able to quickly bootstrap models for a variety of datasets, enabling future work that explores human in the loop interactions for iterative model refinement. ",{},https://arxiv.org/pdf/2104.08768v2.pdf
1086,226262339,Generating Fact Checking Briefs,conclusion,Insight-tree,"we propose the concept of fact checking briefs, to be read before performing a fact check. crucially, we develop qabriefer and release the accompanying qabriefdataset, to create qabriefs. we show in extensive empirical studies with crowdworkers and volunteers that qabriefs can improve accuracy and efficiency of fact checking. ",,https://arxiv.org/pdf/2011.05448v1.pdf
1087,259316561,Improving Multitask Retrieval by Promoting Task Specialization,conclusions,Insight-tree,"multitask retrieval has compelling practical advantages such as model simplicity and memory efficiency, but it lags behind task-specific retrieval in the existing literature. we have shown that it is possible to significantly improve the performance of multitask retrieval by promoting task specialization. the key steps are the use of a base model optimized for multitasking with appropriate prompting and a per-parameter adaptive learning technique that upweights the task gradients by the parameters' sensitivity to the task losses. we have achieved strong results on the kilt retrieval benchmark. ","{248227284: 'Leszczynski et al., 2022'}",https://export.arxiv.org/pdf/2307.00342v1.pdf
1088,252907386,Can language representation models think in bets?,conclusion and future work,Insight-tree,"modern lrms, based on transformer neural networks, have rapidly exceeded the previous state-of-theart on a range of natural language understanding tasks, including question answering, text summarization and information extraction [104][105][106]. in this article, we addressed the question of whether such lrms can be adapted for (approximately) rational decision-making and preference elicitation. in the cognitive science literature, such decision-making is often evaluated using bets. given the near human-like performance of lrms on language-based problems, we formulated a set of rqs to specifically test whether: (i) lrms have a distinct preference for high-value items over lowvalue items, especially when the items were not seen during training, and after stratifying by the format of the questions, (ii) lrms can make, or be taught to make, (approximately rational) bets in a generalizable manner, including when an lrm has been fine-tuned on one 'modality' of bet but is evaluated on another modality.",{220831004: '[51]'},https://export.arxiv.org/pdf/2210.07519v1.pdf
1089,245144556,Large Dual Encoders Are Generalizable Retrievers,conclusion,Insight-tree,"this paper presents the generalizable t5 retriever (gtr), a scaled-up dual encoder model with a fixed-size dot-product bottleneck layer. we show that scaling up the model size brings significant improvement on retrieval performance across the board on the beir zero-shot retrieval benchmark, especially for out-of-domain generalization. the gtr-xxl model performs at the level of state-ofthe-art performance on beir, outperforming many models that use earlier interactions between queries and documents. this sheds light on the research direction to continue enhancing the single vector representation model through better backbone encoders. moreover, our in-depth analysis reveals the impact of scaling up under the scenarios of different training stages, pre-training strategies, fine-tuning datasets, and bottleneck sizes, as well as how scaling up influences the retrieved document lengths. our findings can inform future work and is an integral part of the joint effort to improve dual encoder models.","{86611921: 'Kwiatkowski et al., 2019', 233296016: 'Thakur et al., 2021'}",https://www.aclanthology.org/2022.emnlp-main.669.pdf
1090,245144556,Large Dual Encoders Are Generalizable Retrievers,limitations,Insight-tree,"in our work, we focus on standard dual encoder training and have not investigated other techniques such as distillation. there have been shown distillation is a strong recipe to improve the dense retrieval models on out-of-domain performance (santhanam et al., 2021;formal et al., 2021). we hope to investigate whether the scaling effect could also benefit distillation if we scale up the student dual encoders. in addition, we only focus on english-only corpus and we leave the exploration of scaling up dense retrievers for multi-lingual corpus to future work. table 7 shows the comparisons of gtr models and the baselines. note that the best rocketqa model used additional augmented data other than ms marco to improve the model performance while all others do not. our best gtr-xxl models outperforms rocketqa on both mrr and recall.  a.2 recall on beir","{86611921: 'Kwiatkowski et al., 2019', 233296016: 'Thakur et al., 2021'}",https://www.aclanthology.org/2022.emnlp-main.669.pdf
1091,237502990,Adaptive Information Seeking for Open-Domain Question Answering,conclusion and future work,Insight-tree,"this work presents an adaptive informationseeking approach for open-domain question answering, called aiso. it models the open-domain qa task as a pomdp, where the environment contains a large corpus and the agent is asked to sequentially select retrieval function and reformulate query to collect the evidence. aiso achieves stateof-the-art results on two public datasets, which demonstrates the necessity of different retrieval functions for different questions. in the future, we will explore other adaptive retrieval strategies, like directly optimizing various informationseeking metrics by using reinforcement learning techniques.","{208267807: 'Asai et al., 2020', 202583433: 'Das et al., 2019b;', 211296452: 'Dhingra et al., 2020', 153312687: 'Ding et al., 2019', 189927857: 'Feldman and El-Yaniv, 2019', 202660724: 'Nie et al., 2019;', 202773198: 'Qi et al. 2019', 207756678: 'Wang et al., 2019a', 221970302: 'Xiong et al. 2021', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2021.emnlp-main.293.pdf
1092,233189566,Globalizing BERT-based Transformer Architectures for Long Document Summarization,conclusion,Insight-tree,"in this paper, we have introduced a novel transformer-based model for long document summarization based on propagation layers that spread information between multiple transformer windows. this model preserves the architecture of commonly used pre-trained language models, thus allowing the transfer of parameters. an evaluation, conducted on top of the bert model in the context of an extractive summarization task, further revealed its effectiveness in dealing with long documents compared to other adaptations of bert and previously proposed models. in the future, we plan to adapt our model to other tasks that require understanding long documents, as question-answering and document-scale machine translation. a baselines: implementation details bertsumext:",{},https://www.aclweb.org/anthology/2021.eacl-main.154.pdf
1093,247411106,LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval,conclusion and future work,Insight-tree,"in this paper, we introduce laprador, an unsupervised pretrained dense retriever that achieves state-of-the-art performance on the zero-shot text retrieval benchmark beir.we propose iterative contrastive learning (icol) for efficiently training laprador and lexicon-enhanced dense retrieval (ledr) to combine lexical matching with laprador.our experiments verify the effectiveness of both icol and ledr, shedding light on a new paradigm for unsupervised text retrieval.for future work, we plan to extend unsupervised laprador to multilingual and multi-modal retrieval.",{},https://export.arxiv.org/pdf/2203.06169v2.pdf
1094,245616876,Using Bloom's Taxonomy to Classify Question Complexity,conclusion and future work,Insight-tree,"we have shown that bloom's revised taxonomy can be transferred from pedagogy to qa systems. the diagonal of the matrix is a determinant for defining complex questions, ranging from simple questions in the upper left to complex questions on the bottom right. for the proof of concept, we added pos tags to the questions as syntactic information to train a domain-independent classifier for question complexity. we argued that question words also contribute to complexity, so they were not transformed. although the unequal distribution of the training data only allowed a binary classification for two representative classes a1 and d3, the classifier already provides good results for computing question complexity.","{222178328: 'Jhamtani and Clark, 2020;', 52822214: 'Yang et al., 2018;'}",https://www.aclanthology.org/2021.icnlsp-1.34.pdf
1095,254246977,Global memory transformer for processing long documents,conclusion and future work,Insight-tree,this paper presented a study of our proposed model on two new tasks; masked language modeling as pretraining task and question answering task using hot-potqa as finetuning task. experimental results showed good performance of the model on masked language modeling using linear learning rate. the prposed model with chunked input outperformed t5 as baseline. this approves previous results on translation task where the proposed model overcomes the base line.,{},https://export.arxiv.org/pdf/2212.01650v1.pdf
1096,251594672,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,conclusion,Insight-tree,"in this paper, we have proposed corpusbrain, a novel pre-trained generative retrieval model to encode all information about the corpus into its parameters. to train such a strong generative model, we delicately devised a set of pre-training tasks to emphasize different aspects of semantics between queries and documents. the key idea is to sample a context from one document as a pseudo query and generate the document identifiers of source or destination documents based on hyperlinks. corpusbrain just needs to pre-train one model and could be then adapted to improve a diversity of downstream kilt tasks without the need of constructing additional index. through experiments on the kilt benchmark in terms of the retrieval task, corpusbrain achieved significant improvements over strong baseline approaches. we also showed that corpusbrain can achieve strong performance under both the zero-and low-resource settings.","{222125277: '[9]', 196170479: '[12,', 232147859: '[15]', 86611921: '24,', 221507798: '38]', 220302524: '47,', 52822214: '48]'}",https://export.arxiv.org/pdf/2208.07652v1.pdf
1097,237940982,FQuAD2.0: French Question Answering and knowing that you know nothing,conclusion & future work,Insight-tree,"in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions. to the best of our knowledge, this is the first french (and, perhaps most importantly, non-english) adversarial question answering dataset. we trained various baseline models using camembert architectures. our best model, a finetuned camembert large , reaches 83% f1 score and 82.3% f1 no ans , the latter measuring its ability to distinguish answerable questions from unanswerable ones. the study of learning curves with respect to the number of samples used for training such models show that our baseline models would benefit from additional unanswerable questions. in the future, we plan to collect additional samples to expand fquad2.0. for comparison, its english cousin squad2.0 (rajpurkar et al., 2018) contains 53,775 unanswerable questions. such a large-scale dataset would of course enable the acquisition of even better models as the ones presented in sec-tions 5 and 6. as far as data collection is concerned, we could also collect additional answers for each unanswerable question. by following the same procedure as in d' hoffschmidt et al. (2020), this would allow for the computation of human performance, measuring the inherent difficulty of the challenge provided by fquad2.0.","{165163607: 'Clark et al., 2019', 218974030: 'Rachel et al., 2020', 52822214: 'Yang et al., 2018'}",https://arxiv.org/pdf/2109.13209v1.pdf
1098,258547173,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,conclusions,Insight-tree,"in this paper, we introduce a verify-and-edit framework for open-domain question-answering. it is a first attempt to post-edit cot-style reasoning chains for better end-task performance. by combining knowledge retrieval with reasoning, the framework edits cots in a natural and conversational way, which enhances prediction factuality. combined with google search, the framework also shows a promising direction that combines the open-generation ability of state-of-art llms with the updated facts provided by search engines.",{},https://www.aclanthology.org/2023.acl-long.320.pdf
1099,253157773,COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,conclusion,Insight-tree,coco-dr improves zerodr accuracy by combating the distribution shifts using continuous contrastive learning and implicit distributionally robust optimization. coco helps models better capture the sequence representations of target corpora in pretraining. implicit dro improves model robustness by reweighting query clusters in fine-tuning.,"{233296016: 'Thakur et al., 2021', 245131402: 'Wang et al. 2022', 238857091: 'Xin et al., 2022;', 247411106: 'Xu et al., 2022'}",https://export.arxiv.org/pdf/2210.15212v2.pdf
1100,237433880,Exploiting Reasoning Chains for Multi-hop Science Question Answering *,conclusion,Insight-tree,"we propose a novel chain guided retriever-reader framework for multi-hop qa. our modeling for the reasoning chains is effective to find both direct and indirect facts and is less likely to introduce noise. moreover, our framework is corpus-independent and is capable of handling the setting without any ground-truth annotations. further analysis and discussions also elucidate some of the inner workings of our framework while maintaining the explainability at the same time.","{208267807: 'Asai et al., 2020;', 207853300: 'Fang et al., 2020;', 226262208: 'Ferguson et al., 2020', 202660724: 'Nie et al., 2019;', 155100120: 'Qiu et al., 2019;', 207870753: 'Tu et al., 2020', 221970302: 'Xiong et al., 2021', 235187342: 'Xu et al., 2021', 52822214: 'Yang et al., 2018;', 211562797: 'Zhao et al., 2020'}",https://arxiv.org/pdf/2109.02905v1.pdf
1101,261875390,Reward Engineering for Generating Semi-structured Explanation,conclusion,Insight-tree,"in this work, we focused on the semi-structured explanation generation task and proposed to use train a single model with sft+rl to generate both answers and structured explanations.we highlight the inadequacy of sft in performing this complex task, and proposed a carefully designed reward engineering method in rl to better address this problem.we investigated different reward aggregation methods and conduct extensive experiments under different settings to better highlight the dynamic of the rl objective function and reward model choices.our method achieves the new sota results on two seg benchmarks, explagraph and copa-sse.","{258999803: 'Cui et al., 2023', 52822214: 'Yang et al., 2018;'}",https://export.arxiv.org/pdf/2309.08347v1.pdf
1102,225511882,Article Type: Research Article Article Citation,conclusions,Insight-tree,"1) 18 rna fragments of homology equal or more than 80% with human or simian retroviruses have been found in the covid_19 genome. 2) these fragments are 18 to 30 nucleotides long and therefore have the potential to modify the gene expression of covid19. we have named them external informative elements or eie. 3) these eie are not dispersed randomly, but are concentrated in a small part of the covid_19 genome. 4) among this part, a 225-nucleotide long region is unique to covid_19 and bat ratg13 and can discriminate and formally distinguish these 2 genomes. 5) in the decreasing slope of the epidemic, this 225 bases area and the 1770 bases spike region, exhibits an abnormally high rate of mutations/deletions (cases of 44 patients from wa seattle state, original epicenter in usa). 6) in the comparative analysis of both spikes genes of covid_19 and bat ratg13, we note two abnormal facts: â¢ the insertion of 4 contiguous prra amino acids in the middle of spike (then we show that this site was already an optimal cleavage site before this insertion). â¢ an abnormal ratio of synonymous codons / non synonymous codons in the second half of spike.",{},https://www.granthaalayahpublication.org/journals/index.php/granthaalayah/article/download/IJRG20_B07_3568/691
1103,252819325,CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers,conclusion,Insight-tree,"in this paper, we propose a new challenge: conditional question answering with hierarchical multispan answers, which might be widespread in multispan qa in real-world scenarios. moreover, we introduce cmqa, which contains conditional and hierarchical samples to study the new proposed task. data analysis and experimental results show the main characteristics and challenges of cmqa, and the poor model performance demonstrates that the proposed task is challenging for the community to solve. we believe cmqa can serve as a benchmark to study the new proposed task and help build more reliable and sophisticated qa systems. figure 4: the amount of labels in cmqa. c-a refers to condition-answer and c-f refers to coarse-fine.","{215785913: 'Chen et al., 2020', 67855846: 'Dua et al., 2019;', 52822214: 'Yang et al., 2018'}",https://www.aclanthology.org/2022.coling-1.146.pdf
1104,249049412,Examining Single Sentence Label Leakage in Natural Language Inference Datasets,conclusion,Insight-tree,"in the four years since (poliak et al., 2018) single sentence relation leakage bias has proven to remain a difficult issue. efforts to debias nli have led to datasets that merely exhibit different kinds of bias than those shown before, or less saturated benchmarks that continue to exhibit cheating features. future work must prioritize reducing observable bias directly using a model-driven approach.",{},https://arxiv.org/pdf/2112.09237v3.pdf
1105,236459873,Explanations for CommonsenseQA: New Dataset and Models,conclusion and future work,Insight-tree,"we have presented desiderata of what constitutes an explanation in the case of common-sense qa. based on it, we generated a human-annotated explanation dataset ecqa for commonsenseqa. we have also proposed models to retrieve and generate common-sense facts required to justify the answer choice. we have publicly released our crowdsourced ecqa dataset and code/models. in future work, we plan to explore directions to design rlbased schemes for joint training of property ranker and property selector components in the xr system and joint training of xgp and xgf-ii to generate free-flow explanation. another direction is to improve the accuracy and interpretability of the existing models for commonsenseqa using the ecqa dataset.","{218487313: 'Yadav et al., 2020', 52822214: 'Yang et al., 2018;'}",https://aclanthology.org/2021.acl-long.238.pdf
1106,202565945,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,conclusion,Insight-tree,"in this work, we proposed a self-assembling neural modular network for multi-hop qa. we designed three modules that reason between the question and text-based context. the resulting model outperforms both the single-hop baseline and the original nmn on hotpotqa . because of the interpretable nature of our model, we presented analyses to show that our model does in fact learn to perform compositional reasoning and can dynamically assemble the modular network based on the question.","{139103297: 'Chen and Durrett, 2019;', 189927896: 'Jiang and Bansal 2019', 174801764: 'Min et al., 2019a', 174801080: 'Min et al. 2019b'}",https://www.aclweb.org/anthology/D19-1455.pdf
1107,248476403,Inferring Implicit Relations with Language Models,conclusion,Insight-tree,"in this work, we propose the task of implicit relation inference, which decouples the inference of reasoning steps from their execution. we introduce implicitrelations, a benchmark that includes over 600 questions implicit reasoning questions along with more than 2,000 annotated implicit relations. we show that large lms can infer implicit relations well in the in-context setup across multiple types of questions and reasoning skills, but that this success does not transfer to an improvement in the downstream task of answering implicit reasoning questions. our work sheds light on the types of capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for further improving the ability of models to infer implicit relations.","{230799347: 'Geva et al., 2021', 225075843: 'Lin et al., 2021;', 211258645: 'Perez et al., 2020;', 219573621: 'Talmor et al. 2020', 237263476: 'Talmor et al., 2021', 211003735: 'Wolfson et al. 2020', 52822214: 'Yang et al., 2018;', 233219869: 'Yasunaga et al., 2021;'}",https://arxiv.org/pdf/2204.13778v1.pdf