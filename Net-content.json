{"252819457": {"num": 0, "type": 1, "title": "Type-dependent prompt CycleQAG : Cycle consistency for Multi-hop Question Generation", "pdfurl": "https://www.aclanthology.org/2022.coling-1.549.pdf", "conclusion": "in this work, we propose type-dependent prompt cycleqag with cycle consistency. since multihop qg needs to know more diverse information because it needs to gather more scattered pieces of information for generating a question, we introduce the nce for the first time in the qg task. also, we demonstrate that the intermediate task is effective in the qg task. furthermore, we show a significant performance improvement by using prompt-style fine-tuning to make the most of the information obtained from the intermediate task. the experiments show that the proposed model outperforms in all automatic evaluations comparing with the existing text-based multi-hop model and several qg models.", "Solved": NaN, "Unsolved": NaN}, "252819460": {"num": 1, "type": 1, "title": "CausalQA: A Benchmark for Causal Question Answering", "pdfurl": NaN, "conclusion": "we constructed webis-causalqa-22, the first large benchmark dataset of 1.1 million causal questionanswer pairs, which serves to advance research in causal question answering. to ensure diversity of questions, we extracted them using seven hand-crafted high-precision lexical rules to capture as many subtypes of causal questions as possible. these rules were derived from a new typology of causal questions, which in turn is based on relevant related work on question typologies. a manual analysis of a sample of questions was used to characterize causal questions in terms of two dimensions: (1) their semantic properties, i.e., according to which element of the causal structure the question is asked (antecedent, consequent, or the causal chain) and (2) their pragmatic interpretation, i.e., the underlying intention or assumed information need of the questioner (e.g., prevention of medical problems). furthermore, a subsequent analysis of the causal questions contained in a search engine log showed that a significant proportion of 5% of question queries are causal. finally, we evaluated the state-of-the-art model unifiedqa on our corpus as an initial baseline for causal question answering.", "Solved": NaN, "Unsolved": NaN}, "257038341": {"num": 2, "type": 1, "title": "Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey", "pdfurl": "https://export.arxiv.org/pdf/2302.10035v2.pdf", "conclusion": "we give a comprehensive review of large-scale multi-modal pre-trained models (mm-ptms) in this paper.firstly, we introduce the background of mm-ptms, with a focus on conventional deep learning, and pre-training in nlp, cv, and speech.then, the task definition, key challenges, and benefits of mm-ptms are discussed.after that, we dive into the reviews of mm-ptms and discuss the pre-training data, objectives, networks, knowledge enhanced pre-training, etc.we review the downstream tasks including generative, classification, and regression tasks, and also give an overview of model parameters of mm-ptms and hardware for the pre-training.experimental results of several representative tasks are also discussed and visualized.finally, we point out some research directions that are worth to be focused on.we summarize this paper and hope our survey can provide some useful insights for the mm-ptms.", "Solved": NaN, "Unsolved": NaN}, "247447562": {"num": 3, "type": 1, "title": "Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering", "pdfurl": "https://www.aclanthology.org/2022.acl-long.493.pdf", "conclusion": "this paper proposes hyperlink-induced pretraining (hlp), a pre-training method for openqa passage retrieval by leveraging the online textual relevance induced by hyperlink-based topology. our experiments show that hlp gains significant improvements across multiple qa datasets under different scenarios, consistently outperforming other pre-training methods. our method provides insights into openqa passage retrieval by analyzing the underlying bi-text relevance. future work involves addressing tasks like ms marco where the granularity of the information-seeking target is at the passage level. ", "Solved": NaN, "Unsolved": NaN}, "258865162": {"num": 4, "type": 1, "title": "Why Does ChatGPT Fall Short in Providing Truthful Answers?", "pdfurl": "https://export.arxiv.org/pdf/2304.10513v2.pdf", "conclusion": "our paper investigates the common failures of chat-gpt in complex open-domain question answering. we identify four types of errors: comprehension, factualness, specificity, and inference. we also examine the key abilities knowledge memorization, knowledge recall, and knowledge reasoning, which are critical to these failures. additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on recall, and the effect of decomposition on reasoning. finally, we suggest several techniques to help users more effectively use chatgpt as a question-answering tool and enable system builders to develop better qa systems. our research contributes to the understanding of what influencing the truthfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more reliable llms.", "Solved": NaN, "Unsolved": NaN}, "257019916": {"num": 5, "type": 1, "title": "Complex QA & language models hybrid architectures, Survey", "pdfurl": "https://export.arxiv.org/pdf/2302.09051v4.pdf", "conclusion": "in this paper, we present a comprehensive survey of language model hybrid architectures for answering complex questions. we review the various skills required and typical approach, datasets and metrics that are used, the current limits of large language models for complex qa, the potential of hybrid architectures, better training and prompting strategies for this goal. we also identify the main challenges and research avenues for solving more complex questions including knowledge capitalization. we identify the need to address multi-sensitivity data in language models architectures and potential approaches. finally, we outline research topics and highlight the potential of exploration in this field. this paper aims to provide a comprehensive and useful resource for readers interested in the development of complex non-factoid question answering.", "Solved": NaN, "Unsolved": NaN}, "242075660": {"num": 6, "type": 1, "title": "Multi-Hop Question Generation Using Hierarchical Encoding-Decoding and Context Switch Mechanism", "pdfurl": NaN, "conclusion": "in this paper, we propose a novel question generation model incorporating the hierarchical encoding-decoding structure in order to inject the structural information of input documents, and a context switch mechanism for the purpose of stabilizing the decoding and making the generation process more consistent. the automatic metric results in table 1 show our model achieves the best performance against baseline models on rouge-l in automatic metrics evaluation, although our model does not outperform baseline models on the other baseline models. nonetheless, the results in table 1 prove that our proposed context switch mechanism improves the model's performance on automatic metrics. furthermore, the human evaluation results also show our model outperforms all baseline models on four criteria we used. the experimental results of both automatic evaluation and human evaluation support the effectiveness of our proposed approach on the multi-hop qg task. in addition, we also conduct extensive studies analyzing the model's performance on different question types according to both automatic evaluation metrics and human evaluation scores. future work will include incorporating our method into pre-trained language models.", "Solved": NaN, "Unsolved": NaN}, "218470415": {"num": 7, "type": 1, "title": "Self-supervised Knowledge Triplet Learning for Zero-shot Question Answering", "pdfurl": "https://arxiv.org/pdf/2005.00316v1.pdf", "conclusion": "in this work, we propose a new framework of knowledge triplet learning over knowledge graphs. we show learning all three possible functions, f r ,f h , and f t helps the model to perform zero-shot multiple-choice question answering. we learn from the atomic knowledge graph and evaluate our framework on the socialiqa dataset. our framework achieves state-of-the-art in the zero-shot question answering task and sets a strong baseline in the few-shot question answering task. ", "Solved": NaN, "Unsolved": NaN}, "202539031": {"num": 8, "type": 1, "title": "Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases", "pdfurl": "https://www.aclweb.org/anthology/D19-1418.pdf", "conclusion": "our key contribution is a method of using human knowledge about what methods will not generalize well to improve model robustness to domain-shift. our approach is to train a robust model in an ensemble with a pre-trained naive model, and then use the robust model alone at test time. extensive experiments show that our method works well on two adversarial datasets, and two changing-prior datasets, including a 12 point gain on vqa-cp. future work includes learning to automatically detect dataset bias, which would allow our method to be applicable with less specific prior knowledge.  ", "Solved": NaN, "Unsolved": NaN}, "248811545": {"num": 9, "type": 1, "title": "Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering", "pdfurl": "https://arxiv.org/pdf/2205.07257v1.pdf", "conclusion": "this paper puts forward the view with empirical evidence for qa that contrary to popular belief, multi-source domain generalization (dg) is better modeled as a problem of addressing model underfitting than overfitting. our experimental results show that by simply learning the training domains well, even when the number of such domains is relatively small, strong out-of-domain generalization can be achieved without the need for cross-domain regularization. we rely on knowledge distillation in our experiments for improved source domain learning over erm. in light of these findings, we believe that focusing our efforts on adequately fitting the source domain patterns might be a more reasonable path forward for dg. that said, further research is needed on the topic before a definitive conclusion can be reached; we hope that our work will inspire future explorations of this problem.", "Solved": NaN, "Unsolved": NaN}, "264828956": {"num": 10, "type": 1, "title": "Poisoning Retrieval Corpora by Injecting Adversarial Passages", "pdfurl": "https://export.arxiv.org/pdf/2310.19156v1.pdf", "conclusion": "we proposed a new attack for dense retrievers, in which adversarial passages are inserted into the corpus to mislead their retrieval outputs.we show that even a small number of adversarial passages can successfully attack state-of-the-art dense retrievers and generalize to queries from unseen domains.these findings have important implications for the future deployment of robust retrieval systems in real-world applications.", "Solved": NaN, "Unsolved": NaN}, "235790370": {"num": 11, "type": 1, "title": "A Systematic Survey of Text Worlds as Embodied Natural Language Environments", "pdfurl": "https://www.aclanthology.org/2022.wordplay-1.1.pdf", "conclusion": "environment complexity is limited, and it's currently difficult to author complex worlds. two competing needs are currently at odds: the desire for complex environments to learn complex skills, and the desire for environment variation to encourage robustness in models. current tooling emphasizes creating varied procedural environments, but those environments have limited complexity, and require agents to complete straightforward tasks. economically creating complex, interactive environments that simulate a significant fraction of real world interactions is still well beyond current simulators or libraries -but required for higher-fidelity interactive worlds that have multiple meaningful paths toward achieving task goals. generating these environments semi-automatically (e.g. ammanabrolu et al., 2020a) may offer a partial solution. independent of tooling, libraries and other middleware offer near-term solutions to more complex environment modeling, much in the same way 3d game engines are regularly coupled with physics engine middleware to dramatically reduce the time required to implement forces, collisions, lighting, and other physics-based modeling. currently, few analogs exist for text worlds. the addition of a chemistry engine that knows ice warmed above the freezing point will change to liquid water, or a generator engine that knows the sun is a source of sunlight during sunny days, or an observation engine that knows tools (like microscopes or thermometers) can change the observation model of a pomdp -may offer tractability in the form of modularization. efforts using large-scale crowdsourcing to construct knowledge bases of commonsense knowledge (e.g., atomic, sap et al., 2019) may be required to support these efforts.current planning languages offer a partial solution for environment modelling. while simulators partially implement facilities for world modeling, some (e.g. c\u00f4t\u00e9 et al., 2018;shridhar et al., 2020b) suggest using mature planning languages like strips (fikes and nilsson, 1971) or pddl (mcdermott et al., 1998) for more full-featured modeling. this would not be without significant development effort -existing implementations of planning languages typically assume full-world observability (in conflict with pomdp modelling), and primarily agent-directed state-space changes, making complex world modeling with partial ob-servability, and complex environment processes (such as plants that require water and light to survive, or a sun that rises and sets causing different items to be observable in day versus night) outside the space of being easily implemented with off-the-shelf solutions. in the near-term, it is likely that a domain-specific language specific to complex text world modeling would be required to address these needs while simultaneously reducing the time investment and barrier-to-entry for end users.analyses of environment complexity can inform agent design and evaluation. text world articles frequently emphasize agent modeling contributions over environment, methodological, or analysis contributions -but these contributions are critical, especially in the early stages of this subfield. agent performance in easy environments has increased incrementally, while medium-to-hard environments have seen comparatively modest improvements. agent performance is typically reported as a distribution over a large number of environments, and the methodological groundwork required to understand when different models exceed others in time or performance over these environment distributions is critical to making forward progress. transfer learning in the form of training on one set of environments and testing on others has become a standard feature of benchmarks (e.g. hausknecht et al., 2020), but focused contributions that work to precisely characterize the limits of what can be learned from (for example) omniquest and transferred to zork, and what capacities must be learned elsewhere, will help inform research programs in agent modeling and environment design.transfer learning between text world and 3d environments. tasks learned at a high-level in text worlds help speed learning when those same models are transferred to more complex 3d environments (shridhar et al., 2020b). this framing of transfer learning may resemble how humans can converse about plans for future actions in locations remote from those eventual actions (as when we apply knowledge learned in classrooms to the real world). as such, text-plus-3d environment rendering shows promise as a manner of controlling for different sources of complexity in multi-modal task learning (from high-level task-specific knowledge to low-level perceptual knowledge), and appears a promising research methodology for imparting complex task knowledge on agents that are able to navigate high-fidelity virtual environments. ", "Solved": NaN, "Unsolved": NaN}, "237385768": {"num": 12, "type": 1, "title": "WebQA: Multihop and Multimodal QA", "pdfurl": "https://arxiv.org/pdf/2109.00590v4.pdf", "conclusion": "webqa is a new multi-hop, multi-modal question answering challenge for our community. designed to simulate the heterogeneous information landscape one might expect during a web search, webqa covers a series of opendomain general visual queries while also forcing models to still reason about text. our task requires a system to determine relevant sources, perform aggregation and reasoning. we also propose a novel general recipe for evaluation on webqa which measures both fluency and accuracy.", "Solved": NaN, "Unsolved": NaN}, "239009834": {"num": 13, "type": 1, "title": "MixQG: Neural Question Generation with Mixed Answer Types", "pdfurl": "https://arxiv.org/pdf/2110.08175v2.pdf", "conclusion": "in this paper, we present mixqg, a question generation model pre-trained on a collection of qa datasets with a mix of answer types. we show through experiments that the resulting model is a strong starting point for further fine-tuning which achieves state-of-the-art results on target datasets in commonly-used similarity metrics as well as our designed human evaluation. we release our code and the model checkpoints to facilitate qg research and downstream applications.", "Solved": NaN, "Unsolved": NaN}, "248987702": {"num": 14, "type": 1, "title": "A Fine-grained Interpretability Evaluation Benchmark for Neural NLP", "pdfurl": "https://www.aclanthology.org/2022.conll-1.6.pdf", "conclusion": "we provide a new interpretability evaluation benchmark which contains three tasks with both english and chinese annotated data. there are three limitations in our work.\u2022 how to evaluate the quality of human-annotated rationales is still open. we have several annotators to perform quality control based on human intuitions and experiences. meanwhile, we compare model behaviors on full inputs and humanannotated rationales to evaluate the sufficiency and comprehensiveness of rationales, as shown in table 4 and table 7. however, this manner has damaged the original input distribution and brings uncontrollable factors on model behaviors. therefore, how to automatically and effectively evaluate the quality of human-annotated rationales should be studied in the future.\u2022 we find that the interpretability of model architectures and saliency methods vary with tasks, especially with the input form of the task. thus our benchmark should contain more datasets of each task type ( e.g., single-sentence task, sentencepair similarity task and sentence-pair inference task) to further verify these findings. and we will build evaluation datasets for more tasks in the future.\u2022 due to space limitation, there is no analysis of the relationships between metrics, e.g., the relationship between plausibility and accuracy, and the relationship between faithfulness and robustness. we will take these analyses in our future work.finally, we hope more evaluation metrics and analyses are proposed based on our benchmark. and we hope our benchmark can facilitate the research progress of interpertability.", "Solved": NaN, "Unsolved": NaN}, "211572791": {"num": 15, "type": 1, "title": "DC-BERT: DECOUPLING QUESTION AND DOCUMENT FOR EFFICIENT CONTEXTUAL ENCODING", "pdfurl": "https://arxiv.org/pdf/2002.12591v1.pdf", "conclusion": "this paper introduces dc-bert to decouple question and document for efficient contextual encoding. dc-bert has been successfully applied to document retrieval, a key component in opendomain qa, achieving 10x speedup while retaining most of the qa performance. with the capability of processing high-throughput of questions each with a large collection of retrieved documents, dc-bert brings open-domain qa one step closer to serving real-world applications.", "Solved": NaN, "Unsolved": NaN}, "258078778": {"num": 16, "type": 1, "title": "Datamator: An Intelligent Authoring Tool for Creating Datamations via Data Query Decomposition", "pdfurl": "https://export.arxiv.org/pdf/2304.03126v3.pdf", "conclusion": "in this paper, we presented the authoring tool datamator, developed for creating datamations. to the best of our knowledge, it is the first tool that supports datamation design and generation. given a dataset and a question, datamator can automatically decompose the question into a sequence of data analysis operators and generate a datamation based on unit visualization. datamator also allows the user to modify and edit the generated results. our user studies showed that datamator is highly rated for generating datamations to explain data analysis processes. its editing function also showed to be effective in correcting the automatically generated results.", "Solved": NaN, "Unsolved": NaN}, "254564418": {"num": 17, "type": 1, "title": "Momentum Contrastive Pre-training for Question Answering", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.291.pdf", "conclusion": "this paper presents a novel pre-training method mcross for extractive qa which contains two tasks: 1) contrastive learning and 2) answer term prediction. specifically, mcross adapts moco frameworks to maintain consistency in answering cloze-like and natural questions, enabling pretrained models to have a more comprehensive understanding of supporting passages. the empirical experiments on three public datasets demonstrate that our approach can obtain noticeable improvements in extractive qa tasks in supervised and zero-shot scenarios.", "Solved": NaN, "Unsolved": NaN}, "254564419": {"num": 18, "type": 1, "title": "In Defense of Cross-Encoders for Zero-Shot Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2212.06121v1.pdf", "conclusion": "in this work we study how parameter count influences the zero-shot effectiveness of neural retrievers. we begin by showing that in-domain effectiveness, i.e., when retrievers are fine-tuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al.", "Solved": NaN, "Unsolved": NaN}, "247476296": {"num": 19, "type": 1, "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach", "pdfurl": "https://export.arxiv.org/pdf/2203.08383v2.pdf", "conclusion": "we explore an iterative prompting framework towards driving a \"train of thought\" from plms for multi-step reasoning tasks. we show the superiority of this iterative scheme, and also effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme. in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors. in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities. the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely a human can track along the plm's train of thought and make edits and corrections at different steps, which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process. we leave these investigations as future work. ", "Solved": NaN, "Unsolved": NaN}, "258558037": {"num": 20, "type": 1, "title": "Empowering Language Model with Guided Knowledge Fusion for Biomedical Document Re-ranking", "pdfurl": "https://export.arxiv.org/pdf/2305.04344v1.pdf", "conclusion": "in this work, we proposed an effective approach to re-rank the documents by utilizing the knowledge graph and integrating the external knowledge into the plms. to effectively fuse the language and graph information in the knowledge-enriched framework, we introduced a mutual informationbased objective function, which ensures the fused representations are non-redundant and informative in nature. extensive experiments on biomedical and open-domain datasets show the effectiveness of the proposed approach. ", "Solved": NaN, "Unsolved": NaN}, "215238741": {"num": 21, "type": 1, "title": "Knowledge Fusion and Semantic Knowledge Ranking for Open Domain Question Answering", "pdfurl": "https://arxiv.org/pdf/2004.03101v2.pdf", "conclusion": "in this work, we have pushed the current state-ofthe-art by 2.2% on openbookqa and 7.28% on qasc, two tasks that need external knowledge and knowledge composition for question answering. our semantic knowledge ranking and knowledge fusion question answering model over the bertbased language model demonstrably improves the performance on openbookqa and qasc. we also provide a dataset to learn semantic knowledge ranking using the annotations present in qasc, openbookqa, and scitail. we have analyzed the performance of the components in our qa system. our analysis shows the need to further improve knowledge ranking and knowledge composition.", "Solved": NaN, "Unsolved": NaN}, "234741852": {"num": 22, "type": 1, "title": "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance", "pdfurl": "https://www.aclanthology.org/2021.acl-long.254.pdf", "conclusion": "we propose a new challenging qa dataset tat-qa, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. to answer questions in tat-qa, the close relation between table and paragraphs and numerical reasoning are required. we also propose a baseline model tagop based on tat-qa, aggregating information from hybrid context and performing numerical reasoning over it with pre-defined operators to compute the final answer. experiments show tat-qa dataset is very challenging and more effort is demanded for tackling qa tasks over hybrid data. we expect our tat-qa dataset and tagop model would serve as a benchmark and baseline respectively to help build more advanced qa models, facilitating the development of qa technologies to address more complex and realistic hybrid data, especially those requiring numerical reasoning.", "Solved": NaN, "Unsolved": NaN}, "264818780": {"num": 23, "type": 1, "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text", "pdfurl": "https://export.arxiv.org/pdf/2310.20170v1.pdf", "conclusion": "we introduce the divknowqa, designed to evaluate the proficiency of question-answering systems, especially those enhanced by retrieval tools, in addressing knowledge-intensive questions with a strong emphasis on multi-hop multi-source retrieval.this dataset is constructed through automated data generation and subsequent human verification, minimizing manual effort.our evaluation encompasses both standard llms and llms augmented with retrieval tools.notably, we identify that this task presents a new challenge for state-of-the-art models due to the demand for structured knowledge retrieval and the inherent lack of prior knowledge in this context.to tackle this challenge, we propose the detllm, which incorporates diverse retrieval tools including innovative symbolic query generation for retrieving information from the structured knowledge source.in the future, we are keen on enhancing llms' capabilities in understanding and generating symbolic language, as well as exploring methods to improve performance on knowledge-intensive and complex question-answering tasks.", "Solved": NaN, "Unsolved": NaN}, "264147039": {"num": 24, "type": 1, "title": "Unsupervised Domain Adaption for Neural Information Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2310.09350v1.pdf", "conclusion": "in this paper, we have explored the use of large language models (llms) for synthetic query generation and compared them to the rule-based independent cropping method for unsupervised domain adaptation.our results show that llm-based methods outperform independent cropping in all scenarios by a significant margin.although llms require more time, the benefits they offer in terms of performance make them a viable alternative for query generation.", "Solved": NaN, "Unsolved": NaN}, "253098080": {"num": 25, "type": 1, "title": "Exploring The Landscape of Distributional Robustness for Question Answering Models", "pdfurl": "https://export.arxiv.org/pdf/2210.12517v1.pdf", "conclusion": "we conduct an extensive evaluation of the robustness of different model and adaptation methods on 15 distribution shifts in question answering. our in-depth analysis suggests several concrete directions for future work: improving the in-distribution performance of icl methods and understanding why different few-shot fine-tuning methods yield the squadshifts wiki dataset is derived from the same data source (wikipedia) as squad. as a result, models lie closer to the y = x diagonal than on other distribution shifts. (middle) progress on squad is a weaker indicator for progress on searchqa for fully fine-tuned models and few-shot fine-tuned models. we find that zero-shot and icl models are less robust than fine-tuned and few-shot models with the exception of larger language models. (right) on the squad\u2192drop distribution shift, we observe that progress beyond 70 f1 on squad yields quick progress on drop for fine-tuned models.", "Solved": NaN, "Unsolved": NaN}, "254564450": {"num": 26, "type": 1, "title": "Prompting Is Programming: A Query Language for Large Language Models Prompting Is Programming: A Query Language for Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2212.06094v3.pdf", "conclusion": "in this work, we introduce the concept of language model programming, a novel way to interact with (large) language models. we presented lmql, a high-level query language, offering a concise and intuitive syntax. lmql implements purpose-designed evaluation semantics, which enable efficient query execution. we have substantiated this claim in a series of case studies, where we demonstrate that complex, state-of-the-art prompting techniques can be implemented as intuitive, concise and efficient lmql programs that reduce (compute) costs by up to 80%.", "Solved": NaN, "Unsolved": NaN}, "237101158": {"num": 27, "type": 1, "title": "A Robustly Optimized BERT Pre-training Approach with Post-training", "pdfurl": NaN, "conclusion": "in the paper, we present a 'pre-training'+'post-training'+'fine-tuning' three-stage paradigm and a language model named ppbert based on the three-stage paradigm, which is a supplementary framework for the standard 'pre-training'+'fine-tuning' two-stage architecture. our proposed three-stage paradigm helps to incorporate task-awareness knowledge and domain knowledge within pre-trained model, also reduce the bias in the training corpus. ppbert can benefits from the regularization effect since it leverages cross-domain or cross-task data, which helps model generalize better with limited data and adapt to new domains or tasks better. with the latest plms as baseline and encoder backbone, ppbert is evaluated on 24 well-known benchmarks, which outperforms strong baseline models and obtains new sota results. we hope this work can encourage further research into the language models training, and the future works involve the choice of other transfer learning sources such as cv etc.", "Solved": NaN, "Unsolved": NaN}, "220045416": {"num": 28, "type": 1, "title": "Low-Resource Generation of Multi-hop Reasoning Questions", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.601.pdf", "conclusion": "we have proposed an approach to generate the questions required multi-hop reasoning in low-resource conditions. we first built a multi-hop qg model and guided it to satisfy the logical rationality by the reasoning chain extracted from a given text. in order to tackle the labeled data shortage problem, we learned the structural patterns from the unlabeled data by the hidden semi-markov model. with the patterns as a prior, we transferred this fundamental knowledge into the generation model to produce the optimal results. experimental results on the hotpotqa data set demonstrated the effectiveness of our approach. moreover, we explored the generated results to facilitate the real-world application of machine reading comprehension. we will investigate the robustness and scalability of the model.", "Solved": NaN, "Unsolved": NaN}, "235694196": {"num": 29, "type": 1, "title": "Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems", "pdfurl": "https://arxiv.org/pdf/2107.00368v2.pdf", "conclusion": "the common paradigm in the natural language processing community to develop models for a new benchmark is to either train a new model or fine-tune a pre-trained one. in addition to their high computational costs and environmental effects [8], their accuracies drop significantly for a new data distribution [5]. in this paper, we investigated    the effect of light-weight ensemble-based approach on the generalization of machine reading comprehension models to out-of-distribution data. the experiments were conducted using eight different datasets, six mrc models, and three settings including heterogeneous (different base models with the same training dataset), homogeneous (the same base model trained on different datasets), and hybrid.", "Solved": NaN, "Unsolved": NaN}, "258762869": {"num": 30, "type": 1, "title": "BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval", "pdfurl": "https://www.aclanthology.org/2023.acl-long.365.pdf", "conclusion": "in this paper, we propose an effective method called berm to improve the generalization ability of dense retrieval without target domain data and additional modules. the basic idea of berm is learning the domain-invariant feature, that is, matching signal. to achieve it, we introduce a novel concept of dense retrieval to represent the matching information between two texts, the matching representation. further, we propose two requirements for matching and text representations as the constraint in the training of dense retrieval to enhance the ability to extract essential matching information from the passage according to different queries under the premise of balanced expression of the text. the two requirements unlock the ability of dense retrieval to capture matching signal without additional interaction. experimental results show that berm is a flexible method that can be combined with different dense retrieval training methods without inference overhead to improve the out-of-domain generalization ability. in domain adaptation setting, our method is also effective and performs better than baselines.", "Solved": NaN, "Unsolved": NaN}, "233033850": {"num": 31, "type": 1, "title": "Discrete Reasoning Templates for Natural Language Understanding", "pdfurl": "https://www.aclweb.org/anthology/2021.eacl-srw.12.pdf", "conclusion": "we propose using reasoning templates for tackling reading comprehension tasks that involve reasoning over multiple paragraphs. we show that this approach is competitive with state of the art models on a subset of drop's subtraction questions, while requiring much less training data and providing better visibility of the model's decision making. in future work, we plan on extending to further templates and investigate how to learn templates instead of working from a predefined set.", "Solved": NaN, "Unsolved": NaN}, "237940861": {"num": 32, "type": 1, "title": "Paradigm Shift in Natural Language Processing", "pdfurl": "https://arxiv.org/pdf/2109.12575v2.pdf", "conclusion": "recently, prompt-based tuning, which is to formulate some nlp task into a (m)lm task, has exploded in popularity. they can achieve considerable performance with much less training data. in contrast, other potential unified paradigms, i.e. matching, mrc, and seq2seq, are underexplored in the context of pre-training. one of the main reasons is that these paradigms require large-scale annotated data to conduct pre-training, especially seq2seq is notorious for data hungry.", "Solved": NaN, "Unsolved": NaN}, "218581117": {"num": 33, "type": 1, "title": "A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering", "pdfurl": "https://arxiv.org/pdf/2005.05257v2.pdf", "conclusion": "we introduce a resource of law statutes, a dataset of hand-curated rules and cases in natural language, and a symbolic solver able to represent these rules and solve the challenge task. our handbuilt solver contrasts with our baselines based on current nlp approaches, even when we adapt them to the legal domain.", "Solved": NaN, "Unsolved": NaN}, "258865288": {"num": 34, "type": 1, "title": "Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation", "pdfurl": "https://export.arxiv.org/pdf/2305.14750v1.pdf", "conclusion": "we introduce answer-based claim decomposition, which aims to decompose a question into a series of true/false claims. through experiments on three datasets with gpt-3.5, including a new challenge dataset obscureqa, we show how our technique can be used to perform fine-grained self-evaluation. we find that there is a significant difference in the proportion of claims satisfied for incorrect and correct responses, but there is no indication that gpt-3.5 believes that the gold answer satisfies more abcd claims than its incorrect answers. finally, to investigate the reliability of our approach, we conduct an error analysis and based on our findings, suggest remedies to overcome these errors.", "Solved": NaN, "Unsolved": NaN}, "258887816": {"num": 35, "type": 1, "title": "Linguistic Properties of Truthful Response", "pdfurl": "https://www.aclanthology.org/2023.trustnlp-1.12.pdf", "conclusion": "so far, we have discussed two main contributions of our paper: 1. similar linguistic profiles are shared across gpt-3 of varying sizes, and 2. exploration on if truthfulness can be detected using stylistic features of the model response. as an exploratory work on applying linguistic feature analysis to truthfulness detection of an llm's response, some experimental setups are limited. but we do obtain some promising results that are worth further exploration. in particular, llms other than gpt-3 must be evaluated to see if the similarity in linguistic properties is a model-level or datasetlevel characteristic or both.", "Solved": NaN, "Unsolved": NaN}, "237365386": {"num": 36, "type": 1, "title": "Topic Knowledge Acquisition and Utilization for Machine Reading Comprehension in Social Media Domain", "pdfurl": "https://www.aclanthology.org/2021.ccl-1.88.pdf", "conclusion": "in this paper, we focus on machine reading comprehension in social media domain. we propose a novel method to address the problem of lacking in background knowledge in this task. utilizing the nature of clustering of social media, we retrieve and refine topic knowledge from the relevant messages, and then integrate the knowledge into an mrc model, tkr. experimental results show that our proposed method outperforms the recently proposed models and the bert-based baselines, which proves the method effective overall. by introducing different amount of topic knowledge, we demonstrate the effectiveness of our refined knowledge. moreover, the ablation study further validates the contribution of the key modules of tkr for utilizing the knowledge.", "Solved": NaN, "Unsolved": NaN}, "250390665": {"num": 37, "type": 1, "title": "CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.148.pdf", "conclusion": "in this paper, we present cs1qa, a dataset for codebased question answering in introductory programming course. cs1qa's crowdsourced data from a programming course provide rich information that code understanding models need to consider to correctly answer the given questions. we introduce three tasks for cs1qa, whose output can help students debug and reduce workloads for the teaching staff. results from the baseline models indicate that tasks for cs1qa are challenging for current language understanding models. cs1qa promotes further research to better represent and understand source code for code-based question answering.", "Solved": NaN, "Unsolved": NaN}, "219687051": {"num": 38, "type": 1, "title": "Self-supervised Learning: Generative or Contrastive", "pdfurl": "https://arxiv.org/pdf/2006.08218v5.pdf", "conclusion": "this survey comprehensively reviews the existing selfsupervised representation learning approaches in natural language processing (nlp), computer vision (cv), graph learning, and beyond. self-supervised learning is the present and future of deep learning due to its supreme ability to utilize web-scale unlabeled data to train feature extractors and context generators efficiently. despite the diversity of algorithms, we categorize all self-supervised methods into three classes: generative, contrastive, and generative contrastive according to their essential training objectives. we introduce typical and representative methods in each category and sub-categories. moreover, we discuss the pros and cons of each category and their unique application scenarios. finally, fundamental problems and future directions of self-supervised learning are listed.", "Solved": NaN, "Unsolved": NaN}, "245131402": {"num": 39, "type": 1, "title": "GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.168.pdf", "conclusion": "in this work we propose gpl, a novel unsupervised domain adaptation method for dense retrieval models. it generates queries for a target corpus and pseudo labels these with a cross-encoders. pseudolabeling overcomes two important short-comings of previous methods: not all generated queries are of high quality and pseudo-labels efficiently detects those. further, training with mined hard negatives is possible as the pseudo labels performs efficient denoising.", "Solved": NaN, "Unsolved": NaN}, "239016079": {"num": 40, "type": 1, "title": "COVIDRead: A Large-scale Question Answering Dataset on COVID-19", "pdfurl": "https://arxiv.org/pdf/2110.09321v1.pdf", "conclusion": "in this paper, we have presented covidread, a new dataset for benchmark evaluation of methods for qa/mrc on covid-19. the dataset is created in a semi-automatic way. the dataset consists of context-question-answer triples over 100k, out of which 40k are human-annotated. researchers and editors/associate editors would be benefited out of this kind of models and the dataset. we have proposed three approaches that are based on viz. (i.) vanilla bert (ii.) bio-medical bert (i.e., biobert) and (iii.) clinical bert. our proposed models are competitive compared to the existing state-of-the-art models. our future works would include:", "Solved": NaN, "Unsolved": NaN}, "233296016": {"num": 41, "type": 1, "title": "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "pdfurl": "https://arxiv.org/pdf/2104.08663v4.pdf", "conclusion": "in this work, we presented beir: a heterogeneous benchmark for information retrieval. we provided a broader selection of target tasks ranging from narrow expert domains to open domain datasets. we included nine different retrieval tasks spanning 18 diverse datasets.", "Solved": NaN, "Unsolved": NaN}, "212633743": {"num": 42, "type": 1, "title": "Natural Language QA Approaches using Reasoning with External Knowledge", "pdfurl": "https://arxiv.org/pdf/2003.03446v1.pdf", "conclusion": "in this paper we have surveyed 2 recent research on nlqa when external knowledge -beyond what is given in the test part -is needed in correctly answering the questions. we gave several motivating examples, mentioned several datasets, discussed available knowledge repositories and methods used in selecting needed knowledge from larger repositories, and analyzed and grouped several models and architectures of nlqa systems based on how the knowledge is expressed in them and the type of reasoning module used in them. although there have been some related recent surveys, such as [67], none of them focus on how exactly knowledge and reasoning is done in the nlqa systems dealing with datasets that require external knowledge. our survey touched upon knowledge types that include structured knowledge, textual knowledge, knowledge embedded in a neural network, knowledge provided via specially constructed examples and combinations of them. we explored how symbolic, neural and mixed models process and reason with such knowledge. based on our observations for various models following are some questions and future directions. following up on the lifecycleqa dataset where some concepts, such as \"indicates\", were manually defined, several questions -with partial answers -may come to mind. (i) how big is the list of such concepts? if a list of them is made and they are defined then these definitions can be directly used or compiled into neural models. can cyc and the book [27] be starting points in this direction? (ii) can these definitions be learned from data? how? unless one only focuses on specific datasets, the challenge in learning these definitions would be that for each of them specialized examples would have to be created. (iii) when is it easier to just write the definitions in a logical language? when is it easier to learn from data sets? some concepts are easy to define logically but may require lot of examples to teach a system. there are concepts whose definitions took decades for researchers to formalize. an example is the solutions of frame problem. so it is easier to use that formalization rather than learning from scratch. on the other hand the notion of \"cause\" is still being fine tuned. another direction of future work centers around the question of how well neural network can do reasoning; what kind of reasoning they can do well and what are the challenges ?", "Solved": NaN, "Unsolved": NaN}, "250562707": {"num": 43, "type": 1, "title": "Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge", "pdfurl": "https://aclanthology.org/2022.findings-naacl.12.pdf", "conclusion": "we have proposed a new approach to multi-hop question answering over tabular and textual data. the approach, referred to as dehg, takes question answering as a problem of reasoning answers on the basis of a heterogeneous information graph. dehg employs bert in encoding of questions and passages respectively and generates pointers in decoding of answer generation. experimental results show that dehg significantly outperforms the state-of-the-art methods.", "Solved": NaN, "Unsolved": NaN}, "258762903": {"num": 44, "type": 1, "title": "From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader *", "pdfurl": "https://export.arxiv.org/pdf/2212.04755v3.pdf", "conclusion": "this work presents a novel mrc-style pre-training model called pmr.pmr can fully resolve the learning objective and model architecture gaps that frequently appear in fine-tuning existing mlms.experimental results from multiple dimensions, including effectiveness in solving few-shot tasks and ood generalization, show the benefits of bridging the gap between pre-training and fine-tuning for span extraction tasks.pmr also shows promising potential in explaining the sequence classification process and unifying nlu tasks.", "Solved": NaN, "Unsolved": NaN}, "232290456": {"num": 45, "type": 1, "title": "HOPPER: MULTI-HOP TRANSFORMER FOR SPATIOTEMPORAL REASONING", "pdfurl": "https://arxiv.org/pdf/2103.10574v2.pdf", "conclusion": "this work presents hopper with a novel multi-hop transformer to address object permanence in videos. hopper achieves 73.2% top-1 accuracy at just 1 fps on cater, and demonstrates the benefits of multi-hop reasoning. in addition, the proposed multi-hop transformer uses an iterative attention mechanism and produces a step-by-step reasoning chain that improves interpretability. multi-hop models are often difficult to train without supervision for the middle hops. we propose several training methods that can be applied to other tasks to address the problem of lacking a ground truth reasoning chain. in the future, we plan to experiment on real-world video datasets and extend our methods to deal with other complex tasks (such as video qa).", "Solved": NaN, "Unsolved": NaN}, "247187611": {"num": 46, "type": 1, "title": "Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.158.pdf", "conclusion": "selective prediction ability is crucial for nlp systems to be reliably deployed in real-world applications and we presented the most systematic study of existing selective prediction approaches. our study involved experiments in iid, ood, and adv settings with 17 datasets across several nlp tasks. we showed that despite leveraging additional resources (held-out data/computation), existing approaches fail to consistently and considerably outperform the simplest baseline (maxprob ", "Solved": NaN, "Unsolved": NaN}, "250390687": {"num": 47, "type": 1, "title": "MultiSpanQA: A Dataset for Multi-Span Question Answering", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.90.pdf", "conclusion": "we present multispanqa, a reading comprehension dataset where answers consist of multiple discrete spans. as part of this, we proposed a method for classifying the semantic structure of answers, based on the semantic relation between answer spans. we also provide an expanded version of the dataset which includes unanswerable questions and single-answer questions, to make it both more challenging and more realistic. we additionally presented a number of models for multi-span qa extraction, and found that the best-performing model was sequence tagging-based, augmented by a span number prediction module and span adjustment module.", "Solved": NaN, "Unsolved": NaN}, "254408864": {"num": 48, "type": 1, "title": "A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches", "pdfurl": "https://export.arxiv.org/pdf/2212.04072v1.pdf", "conclusion": "in this study, we focused on the multi-hop mrc approaches. in this regard, after presenting the multi-hop mrc problem definition, the multi-hop mrc techniques had been explained based on 31 studies from 2018 to 2022. in addition to categorize the approaches based on the main technique, they also were reviewed in detail including the architecture, superiority, and motivations. in the following, a fine-grain comprehension of the approaches and techniques was prepared, and finally, some open issues in this field were discussed.", "Solved": NaN, "Unsolved": NaN}, "248299683": {"num": 49, "type": 1, "title": "STANDING ON THE SHOULDERS OF GIANT FROZEN LANGUAGE MODELS", "pdfurl": "https://arxiv.org/pdf/2204.10019v1.pdf", "conclusion": "while fine-tuning huge lms can often yield excellent performance, this approach is expensive at training time, requires serving a plethora of models at runtime, and provides poor adaptability in the face of variations in the targeted task. this paper has shown that a better alternative exists: freezing a single, huge pretrained lm and learning much smaller neural modules that specialize the lm to different tasks. while prompt tuning, prefix tuning, and other existing frozen model methods cited above can be seen as a simple instantiations of this idea, this paper shows that much more complex architectures can achieve much stronger performance.", "Solved": NaN, "Unsolved": NaN}, "232269988": {"num": 50, "type": 1, "title": "Evaluating Document Coherence Modelling", "pdfurl": "https://arxiv.org/pdf/2103.10133v1.pdf", "conclusion": "we propose the new task of detecting whether there is an intruder sentence in a document, generated by replacing an original sentence with a similar sentence from a second document. to benchmark model performance over this task, we construct a large-scale dataset consisting of documents from english wikipedia and cnn news articles. experimental results show that pretrained lms which incorporate larger document contexts in pretraining perform remarkably well in-domain, but experience a substantial drop cross-domain. in follow-up analysis based on human annotations, substantial divergences from human intuitions were observed, pointing to limitations in their ability to model document coherence. further results over a linguistic probe dataset show that pretrained models fail to identify some linguistic characteristics that affect document coherence, suggesting room to improve for them to truly capture document coherence, and motivating the construction of a dataset with intruder text at the intra-sentential level.", "Solved": NaN, "Unsolved": NaN}, "220045477": {"num": 51, "type": 1, "title": "Benefits of Intermediate Annotations in Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.497.pdf", "conclusion": "we show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. however, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. we proposed a simple semi-supervision technique to expose the model to these annotations. we believe that in future they can be used more directly to yield better performance gains. we have also released these annotations for the research community at https: //github.com/ddua/intermediate_annotations. motteux was also without heirs and bequeathed sandringham, together with another norfolk estate and a property in surrey, to the third son of his close friend, emily lamb, the wife of lord palmerston. at the time of his inheritance in 1843, charles spencer cowper was a bachelor diplomat, resident in paris. on succeeding to motteux's estates, he sold the other properties and based himself at sandringham. he undertook extensions to the hall, employing samuel sanders teulon to add an elaborate porch and conservatory. cowper's style of living was extravagant he and his wife spent much of their time on the continent and within 10 years the estate was mortgaged for \u00a389,000. the death of their only child, mary harriette, from cholera in 1854 led the couple to spend even more time abroad, mainly in paris, and by the early 1860s cowper was keen to sell the estate. figure 9: predicted relevant spans for question answered correctly with annotation (prediction:\"charles spencer cowper\") and incorrectly without annotations (prediction:\"lord palmerston\") by xlnet on quoref", "Solved": NaN, "Unsolved": NaN}, "202541222": {"num": 52, "type": 1, "title": "Large Scale Question Answering using Tourism Data", "pdfurl": "https://arxiv.org/pdf/1909.03527v2.pdf", "conclusion": "in the spirit of defining a question answering challenge that is closer to a real-world qa setting, we introduce the novel task of identifying the correct entity answer to a given user question based on a collection of unstructured reviews describing entities. we harvest a dataset of over 48,000 qa pairs, which enables end to end training of models.", "Solved": NaN, "Unsolved": NaN}, "218570919": {"num": 53, "type": 1, "title": "CAiRE-COVID: A Question Answering and Multi-Document Summarization System for COVID-19 Research", "pdfurl": "https://arxiv.org/pdf/2005.03975v1.pdf", "conclusion": "we have described our system, caire-covid, comprising of three major modules, information retrieval, question answering, and summarization, which uses the cord-19 dataset consisting of published scientific articles concerning covid-19. our system can answer user queries related to covid-19 by retrieving relevant paragraphs from articles available in the dataset, using our qa models to answer the question, and also generate two versions of a concise summary of the top paragraphs via the two summarization models. we believe that getting factual information regarding covid-19 and showing them in a comprehensible way, we can prioritise scientific facts about the virus, and help the community in the fight against the ongoing global pandemic.", "Solved": NaN, "Unsolved": NaN}, "252780709": {"num": 54, "type": 1, "title": "Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering", "pdfurl": "https://www.aclanthology.org/2022.coling-1.152.pdf", "conclusion": "in this paper, we examined the multi-hop reasoning capabilities of generative qa models, finding that overall models take shortcuts when answering multi-hop questions, not demonstrating convincing multi-hop reasoning capability. when trained only on single-hop questions, models generalize poorly to multi-hop questions, while approximation using the concatenation of single-hop questions and sparql queries improves the multi-hop performance significantly. further directions include better approximations of multi-hop questions and advanced modeling techniques that encourage compositional ability.", "Solved": NaN, "Unsolved": NaN}, "246016165": {"num": 55, "type": 1, "title": "Double Retrieval and Ranking for Accurate Question Answering", "pdfurl": "https://www.aclanthology.org/2023.findings-eacl.130.pdf", "conclusion": "in this paper, we propose, dar, a transformer architecture based on two reranking heads: (i) the answer reranker (as2 model) and the answer support q: how many viewers did \"family guy\" premier to? c1: family guy officially premiered after fox's broadcast of super bowl xxxiii on january 31, 1999, with \"death has a shadow. c2: the show debuted to 22 million viewers, and immediately generated controversy regarding its adult content. c3: at the end of its first season, the show was #33 in the nielsen ratings, with 12.8 million households tuning i.", "Solved": NaN, "Unsolved": NaN}, "252907685": {"num": 56, "type": 1, "title": "MTEB: Massive Text Embedding Benchmark", "pdfurl": "https://www.aclanthology.org/2023.eacl-main.148.pdf", "conclusion": "in this work, we presented the massive text embedding benchmark (mteb). consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, mteb aims to provide reliable embedding performance estimates. by opensourcing mteb alongside a leaderboard, we provide a foundation for further pushing the state-ofthe-art of available text embeddings.", "Solved": NaN, "Unsolved": NaN}, "264935600": {"num": 57, "type": 1, "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL", "pdfurl": "https://export.arxiv.org/pdf/2311.01173v1.pdf", "conclusion": "while llms incorporate vast world knowledge and corpus statistics, they may be unfamiliar with (possibly private) client db schemas, which can be very large, rendering impractical or expensive any attempt to upload the full schema in-context along with questions for text-to-sql applications.", "Solved": NaN, "Unsolved": NaN}, "237420723": {"num": 58, "type": 1, "title": "Towards Retrieval-based Conversational Recommendation", "pdfurl": "https://arxiv.org/pdf/2109.02311v1.pdf", "conclusion": "one potential threat to the validity in user studies like ours lies in the reliability of the study participants. given that we applied more than one quality-assurance measure-participant selection, an attention check, manual inspection-we are confident that our results are reliable. given also that most participants are fluent in english and regular movie watchers, we believe that the participants are representative at least for a subset of potential users of an online crs.another potential limitation is that we so far only analyzed two language generation systems. the question therefore remains to what extent the findings of our study would generalize to other approaches. since both analyzed systems (kbrd and kgsf) were published in the last two years, and since they were published at top-ranked scientific conferences, we believe that they are good representatives of the state-of-the-art in neural generation-based systems. moreover, an earlier analysis of the deepcrs system in [16] indicates that similar phenomena might be found also for other approaches. note also that our study can be easily extended to include alternative or even newer approaches, as long as the respective authors share the needed artifacts for reproducibility. as of now, we did not find any work that demonstrate superior quality than the kgsf system, and that the source code of such a system is available. to ensure replicability of our own work, we also share all the code and data used for our analyses online.so far, we analyzed our approach only with the help of the redial dataset. this choice was necessary to ensure a fair comparison with two recent works (kbrd and kgsf), which also relied on this dataset. recently, a number of alternative dataset was proposed, e.g., [5,14,65]. an evaluation of all compared methods on other datasets is however beyond the scope of our present work, which aimed to assess the relative performance of generation-based and retrievalbased system based on the dataset for which they were originally designed and tuned.regarding the general nature of the proposed crb-crs system, it is a retrieval-based system. however, as discussed in section 3, we also rely on a small set of heuristics in a few processing phases, which are, for example, implemented using keyword lists and string matching. these heuristics, which are not yet learned automatically, are documented in the provided source code of our system. automating the construction of these rules and keyword lists, e.g., based on movielens metadata, is a part of our future work to avoid any knowledge-engineering bottlenecks.", "Solved": NaN, "Unsolved": NaN}, "247292088": {"num": 59, "type": 1, "title": "Question-Answer Sentence Graph for Joint Modeling Answer Selection", "pdfurl": "https://www.aclanthology.org/2023.eacl-main.68.pdf", "conclusion": "to our knowledge, our model is the first graphbased approach for jointly modeling sentence-level semantics of question-answer pairs for as2 as an offline processing application, such as those required by community qa, forums, etc. this is different from previous methods using graphs, e.g., multihop or graph-based qa, which mainly model semantics via entities. our approach builds query-specific small-scale training graphs for offline learning, through (q, a) pairs as nodes, and edges encoding relations between members of pairs to capture both supporting question-question, and answer-answer dependencies. further, we demonstrate that our approach achieves significant performance gains over existing sota models on as2 for metrics of p@1, map, and mrr.", "Solved": NaN, "Unsolved": NaN}, "174801080": {"num": 60, "type": 1, "title": "Multi-hop Reading Comprehension through Question Decomposition and Rescoring", "pdfurl": "https://www.aclweb.org/anthology/P19-1613.pdf", "conclusion": "we proposed decomprc, a system for multihop rc that decomposes a multi-hop question into simpler, single-hop sub-questions. we recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions. moreover, decomprc achieved further gains from the decomposition scoring step. decomprc achieved the state-of-the-art on hotpotqa distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines. in this section, we describe span annotation collection procedure for bridging and intersection questions.", "Solved": NaN, "Unsolved": NaN}, "248228026": {"num": 61, "type": 1, "title": "StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts", "pdfurl": "https://arxiv.org/pdf/2204.08292v1.pdf", "conclusion": "in this paper, we proposed a new dataset named stepgame that requires a robust multi-hop spatial reasoning ability to be solved and mitigates the issues observed in the babi dataset. then, we introduced tp-mann, a tensor productbased memory-augmented neural network architecture that achieves state-of-the-art performance on both datasets. further analysis also demonstrated the importance of a recurrent memory module for multi-hop reasoning.", "Solved": NaN, "Unsolved": NaN}, "234334398": {"num": 62, "type": 1, "title": "READTWICE: Reading Very Large Documents with Memories", "pdfurl": "https://arxiv.org/pdf/2105.04241v2.pdf", "conclusion": "readtwice performs well on several qa tasks, particularly narrativeqa where long-range dependencies among entities appear to be very important. the proposed method is conceptually simple, easy to implement and is capable of reading entire books. for future work, we plan to explore new memory types, hierarchies and aggregation functions. we also aim to apply the model to other tasks, particularly long text summarization, likely to benefit from a memory-forming mechanism.", "Solved": NaN, "Unsolved": NaN}, "215238846": {"num": 63, "type": 1, "title": "Multi-Step Inference for Reasoning Over Paragraphs", "pdfurl": "https://arxiv.org/pdf/2004.02995v2.pdf", "conclusion": "we propose a multi-step reading comprehension model that performs chained inference over natural language text. we have demonstrated that our model substantially outperforms prior work on ropes, a challenging new reading comprehension dataset. we have additionally presented some analysis of ropes that should inform future work on this dataset. while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets.", "Solved": NaN, "Unsolved": NaN}, "253098174": {"num": 64, "type": 1, "title": "Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge", "pdfurl": "https://export.arxiv.org/pdf/2210.12338v1.pdf", "conclusion": "in this paper, we present a new framework, core, for odqa over heterogeneous knowledge sources.", "Solved": NaN, "Unsolved": NaN}, "233189566": {"num": 65, "type": 1, "title": "Globalizing BERT-based Transformer Architectures for Long Document Summarization", "pdfurl": "https://www.aclweb.org/anthology/2021.eacl-main.154.pdf", "conclusion": "in this paper, we have introduced a novel transformer-based model for long document summarization based on propagation layers that spread information between multiple transformer windows. this model preserves the architecture of commonly used pre-trained language models, thus allowing the transfer of parameters. an evaluation, conducted on top of the bert model in the context of an extractive summarization task, further revealed its effectiveness in dealing with long documents compared to other adaptations of bert and previously proposed models. in the future, we plan to adapt our model to other tasks that require understanding long documents, as question-answering and document-scale machine translation. a baselines: implementation details bertsumext:", "Solved": NaN, "Unsolved": NaN}, "258865346": {"num": 66, "type": 1, "title": "On Degrees of Freedom in Defining and Testing Natural Language Understanding", "pdfurl": "https://export.arxiv.org/pdf/2305.15130v1.pdf", "conclusion": "the prevalence of exaggerated claims about the achievements of foundation models motivates us to reconsider how we define and evaluate nlu. our formulation of nlu using the response-dependent interpretation mitigates the issues of the turing and octopus tests; it stipulates that observers and target conditions, including tasks and abilities, must be specified. however, current practices for creating nlu datasets are yet to be aligned, which provides researchers with the freedom to choose convenient strategies. to organize essential practices using a standard guideline, we introduce the validity ar-gument, which guides stakeholders to collect and interpret evidence for validating that the test subject executes its intended behavior. our proposed checklist helps researchers find relevant practices for benchmarking nlu, but we continually revise it by investigating potential refutation to promote more credible nlu studies.", "Solved": NaN, "Unsolved": NaN}, "249888962": {"num": 67, "type": 1, "title": "A Dense Representation Framework for Lexical and Semantic Matching", "pdfurl": "https://export.arxiv.org/pdf/2206.09912v2.pdf", "conclusion": "we present a simple yet effective approach to densifying lexical representations for passage retrieval. this work introduces a dense representation framework and proposes a new scoring function to compute relevance scores between dense lexical representations (dlrs) derived from queries and passages. using our framework, we can combine lexical and semantic representations into dense hybrid representations (dhrs) for hybrid retrieval. our experiments show that dlrs can accurately approximate any \"off-the-shelf\" lexical model. furthermore, when combined with other semantic representations (as dhrs), the resulting models can achieve comparable effectiveness to existing state-of-the-art hybrid retrieval methods.", "Solved": NaN, "Unsolved": NaN}, "211296452": {"num": 68, "type": 1, "title": "DIFFERENTIABLE REASONING OVER A VIRTUAL KNOWLEDGE BASE", "pdfurl": "https://arxiv.org/pdf/2002.10640v1.pdf", "conclusion": "we present drkit, a differentiable module that is capable of answering multi-hop questions directly using a large entity-linked text corpus. drkit is designed to imitate traversal in kb over the text corpus, providing ability to follow relations in the \"virtual\" kb over text. we achieve state-of-the-art results on the metaqa dataset for answering natural language questions, with a 9 point increase in the 3-hop case. we also developed an efficient implementation using sparse operations and inner product search, which led to a 10-100x increase in queries/sec over baseline approaches. a metaqa: implementation details we use p = 400 dimensional embeddings for the mentions and queries, and 200-dimensional embeddings each for the start and end positions. this results in an index of size 750mb. when computing a e\u2192m , the entity to mention co-occurrence matrix, we only retain mentions in the top 50 paragraphs matched with an entity, to ensure sparsity. further we initialize the first 4 layers of the question encoder with the transformer network from pre-training. for the first hop, we assign z 0 as a 1-hot vector for the least frequent entity detected in the question using an exact match. the number of nearest neighbors k and the softmax temperature \u03bb were tuned on the dev set of each task, and we found k = 10000 and \u03bb = 4 to work best. we pretrain the index on a combination of the metaqa corpus, using the kb provided with metaqa for distance data, and the wikidata corpus.   indexing only entity-mentions in single-hop questions over all spans. note that drkit-entities has a high hits@1 performance on the rare relations subset, showing that there is generalization to less frequent data due to the natural language representations of entities and relations.", "Solved": NaN, "Unsolved": NaN}, "256630981": {"num": 69, "type": 1, "title": "LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models", "pdfurl": "https://aclanthology.org/2022.findings-emnlp.265.pdf", "conclusion": "in this paper, we construct a synthetic nonmonotonic reasoning benchmark, logicnmr, with explicit facts and rules, to capture the iterative update on the knowledge base.we probe whether the pre-trained language models have truly mastered the non-monotonic reasoning ability.the experimental results show that even though the fine-tuned language models all achieve a high accuracy, they perform worse on generalization, proof-based traceability and robustness to irrelevant information.consequently, we cannot give a positive answer to the research problem whether the language models master the non-monotonic reasoning ability.it suggests us to explore a better approach to take advantage of the language models to conduct non-monotonic reasoning tasks.", "Solved": NaN, "Unsolved": NaN}, "52822214": {"num": 70, "type": 1, "title": "HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D18-1259.pdf", "conclusion": "we present hotpotqa, a large-scale question answering dataset aimed at facilitating the development of qa systems capable of performing explainable, multi-hop reasoning over diverse natural language. we also offer a new type of factoid comparison questions to test systems' ability to extract and compare various entity properties in text.", "Solved": NaN, "Unsolved": NaN}, "247476426": {"num": 71, "type": 1, "title": "Relation Leakage in Elicited Natural Language Inference Datasets", "pdfurl": "https://arxiv.org/pdf/2112.09237v2.pdf", "conclusion": "we have introduced useful tools and techniques for analyzing elicited sentence relation leakage bias in nli datasets, and applied them to a large, representative set of popular current datasets.", "Solved": NaN, "Unsolved": NaN}, "225067214": {"num": 72, "type": 1, "title": "READONCE Transformers: Reusable Representations of Text for Transformers", "pdfurl": "https://www.aclanthology.org/2021.acl-long.554.pdf", "conclusion": "this work introduced readonce transformers, a novel approach for using large scale transformerbased language models to both build and consume reusable document representations. akin to humans' ability to read a document and extract useful information without knowing the enduse, readonce representations are compact, information-capturing document representations that can be pre-computed once, in a task-and example-independent fashion.", "Solved": NaN, "Unsolved": NaN}, "257427664": {"num": 73, "type": 1, "title": "Challenges in Explanation Quality Evaluation", "pdfurl": "https://export.arxiv.org/pdf/2210.07126v2.pdf", "conclusion": "this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face. we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate the discussed characteristics. we support our arguments with empirical evidence of a crowdsourced case study that we conducted for the example of explainable question answering systems from the hotpotqa leaderboard.", "Solved": NaN, "Unsolved": NaN}, "244119766": {"num": 74, "type": 1, "title": "Less Is More: Domain Adaptation with Lottery Ticket for Reading Comprehension", "pdfurl": "https://aclanthology.org/2021.findings-emnlp.95.pdf", "conclusion": "in this work, we propose alter, a simple and effective domain adaptation paradigm for few-shot reading comprehension. we exploit a small fraction of parameters of the over-parameterized source domain model to adapt to the target domain by first identifying and then fine-tuning the lottery subnetwork. we introduce self-attention attribution, an interpreting method for transformer, to identify better subnetworks and improve the target domain performance. further exploration on using several heuristic methods to reveal subnetwork structures find that subnetwork structures are critical to the effectiveness besides using fewer parameters. ", "Solved": NaN, "Unsolved": NaN}, "221507798": {"num": 75, "type": 1, "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.200.pdf", "conclusion": "we introduce kilt, a benchmark for assessing models that need to condition on specific knowledge in a defined snapshot of wikipedia to solve tasks spanning five domains. the goal is to catalyze and facilitate research towards general and explainable models equipped with task-agnostic representations of knowledge. our experiments show promising results for a general solution combining dense retrieval and seq2seq generations, although there is large room for improvements. in particular, we find that provenance of current models is generally low.", "Solved": NaN, "Unsolved": NaN}, "207847640": {"num": 76, "type": 1, "title": "Blockwise Self-Attention for Long Document Understanding", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.232.pdf", "conclusion": "in this work, we study the lightweight bert model with the goal of achieving both efficiency and effectiveness. we profile and analyze the memory bottlenecks of bert and focus on optimize dotproduct self-attention, which consumes quadratic memory with respect to the sequence length. to reduce both time and memory consumption, we present blockbert, which sparsifies the attention matrices to be sparse block matrices. the proposed model achieves time and memory saving without significant loss of performance.", "Solved": NaN, "Unsolved": NaN}, "255570137": {"num": 77, "type": 1, "title": "Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding", "pdfurl": "https://export.arxiv.org/pdf/2301.03765v1.pdf", "conclusion": "in this paper, we propose cross-model comparative loss, a simple task-agnostic loss function, to improve the utility of neurons in nlu models. comparative loss is essentially a ranking loss based on the comparison principle between the full model and its ablated models, with the expectation that the less ablation there is, the smaller the task-specific loss. to ensure comparability among multiple ablated models, we progressively ablate the models and provide two controlled ablation methods based on dropout and context cropping, applicable to a wide range of tasks and models.", "Solved": NaN, "Unsolved": NaN}, "248512731": {"num": 78, "type": 1, "title": "Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI", "pdfurl": "https://arxiv.org/pdf/2205.01809v2.pdf", "conclusion": "in order to provide an epistemologically grounded characterisation of natural language explanations, this paper attempted to bridge the gap in the notion of scientific explanation (salmon, 2006;salmon, 1984), studying it as both a formal object and as a linguistic expression. the combination of a systematic survey with a corpus analysis on natural language explanations (jansen et al., 2014;jansen et al., 2018),", "Solved": NaN, "Unsolved": NaN}, "252917981": {"num": 79, "type": 1, "title": "PROMPTING GPT-3 TO BE RELIABLE", "pdfurl": "https://export.arxiv.org/pdf/2210.09150v2.pdf", "conclusion": "our work systematically studies the reliability of gpt-3 from four key facets: generalizability, fairness, calibration, and factuality. we develop effective prompting strategies to make gpt-3 outperform supervised models by large margins on these facets. our work reveals new insights of llms and provides practical recommendations for users of gpt-3. we hope our work can inspire more future work to: (1) examine more facets of reliability, such as avoiding harmful generations; (2) apply the prompting methods in this paper to more real-world applications, such as incorporating human feedback for collaborative multi-step planning; (3) further explore more effective prompting strategies to improve reliability, such as post-hoc calibration on language model probabilities.", "Solved": NaN, "Unsolved": NaN}, "252872925": {"num": 80, "type": 1, "title": "Improving Question Answering with Generation of NQ-like Questions", "pdfurl": "https://export.arxiv.org/pdf/2210.06599v1.pdf", "conclusion": "we clearly observe from the results that adding filtered nq-like questions from the qb data has given a boost over using only nq questions. in finer detail, we observe that questions from the last sentence of the qb are of higher quality than from intermediate sentences and therefore provide a higher boost to performance even with less sam-ples. even by simply adding questions generated from last sentence, we increase the exact match accuracy by nearly 2 points. we also observe that the bleu score of answers generated from quality controlled nq like system is 16 points more than the bleu score of the baseline qb system for the rag system and by 13 points for the drqa system. this shows that our algorithm to generate nq-like questions has been effective in improving the quality of the training dataset.", "Solved": NaN, "Unsolved": NaN}, "247085284": {"num": 81, "type": 1, "title": "Deep Understanding based Multi-Document Machine Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2204.03494v1.pdf", "conclusion": "in this paper, we propose a simple but effective deep understanding based multi-document mrc model. it uses neither any sophisticated technologies nor any pretrained language models. we evaluate our model on dureader and triviaqa web, two widely used benchmark multi-document mrc datasets. experiments show that our model achieves very competitive results on both datasets.", "Solved": NaN, "Unsolved": NaN}, "248512744": {"num": 82, "type": 1, "title": "On Continual Model Refinement in Out-of-Distribution Data Streams", "pdfurl": "https://www.aclanthology.org/2022.acl-long.223.pdf", "conclusion": "in this paper, we propose a novel continual learning formulation named continual model refinement (cmr). the cmr problem aims to efficiently fix prediction errors when learning in outof-distribution data streams without catastrophically forgetting the acquired knowledge. for studying such a realistic and complex problem, we presented a dedicated evaluation protocol with a general method to create non-stationary, diverse ood data streams for analysis. also, we design multiple evaluation metrics to deliver a comprehensive yet concise measurement of cmr methods. the proposed cmr problem with our comprehensive analysis opens up a range of new opportunities for studying continual learning problems that are closer to real-world applications for the nlp community and beyond. for example, based on our results and analysis about (q3) and (q6), we find that it is promising to study how we can integrate both regularization methods and replay methods for mitigating the forgetting issue while improving the generalization ability. the analysis about (q5) suggests that developing more stable ranking criteria is also important to conditional replay methods (e.g., our simple extension maxloss can outperform mir under specific settings). developing cmr methods of which the configurations can generalize to diverse types of streams is also an important challenge. we release our codebase and processed datasets for supporting the reproducibility of our experiments and future research.", "Solved": NaN, "Unsolved": NaN}, "256631017": {"num": 83, "type": 1, "title": "Answer Quality Aware Aggregation for Extractive QA Crowdsourcing", "pdfurl": "https://aclanthology.org/2022.findings-emnlp.457.pdf", "conclusion": "in this paper, we propose a novel answer annotation aggregation method for eqa crowdsourcing.we show that without any fine-tuning, our methods can achieve comparable performance with the trained qa and nli model using  4: an example from newsqa dataset.there are 7 different answer annotations for the question.some of the answers are overlapped.for each answer we report its ranking scores with ac af sms acaf-sms .", "Solved": NaN, "Unsolved": NaN}, "226283753": {"num": 84, "type": 1, "title": "COSATA: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences", "pdfurl": "https://www.aclweb.org/anthology/2020.emnlp-demos.10.pdf", "conclusion": "cosata is an open-source constraint satisfaction solver for easily expressing and evaluating multifact compositional patterns in semi-structured tables of text, paired with an interpreted language that allows expressing micro-models. the tool, source, examples, and documentation are available at http://www.github.com/clulab/cosata/ .", "Solved": NaN, "Unsolved": NaN}, "245616876": {"num": 85, "type": 1, "title": "Using Bloom's Taxonomy to Classify Question Complexity", "pdfurl": "https://www.aclanthology.org/2021.icnlsp-1.34.pdf", "conclusion": "we have shown that bloom's revised taxonomy can be transferred from pedagogy to qa systems. the diagonal of the matrix is a determinant for defining complex questions, ranging from simple questions in the upper left to complex questions on the bottom right. for the proof of concept, we added pos tags to the questions as syntactic information to train a domain-independent classifier for question complexity. we argued that question words also contribute to complexity, so they were not transformed. although the unequal distribution of the training data only allowed a binary classification for two representative classes a1 and d3, the classifier already provides good results for computing question complexity.", "Solved": NaN, "Unsolved": NaN}, "202565869": {"num": 86, "type": 1, "title": "Let's Ask Again: Refine Network for Automatic Question Generation", "pdfurl": "https://www.aclweb.org/anthology/D19-1326.pdf", "conclusion": "in this work, we proposed refine networks (refnet) for question generation to focus on refining and improving the initial version of the generated question. our proposed refnet model consisting of a preliminary decoder and a refinement decoder with dual attention network outperforms the existing state-of-the-art models on the squad, hotpot-qa and drop datasets. along with automated evaluations, we also conducted human evaluations to validate our findings. we further showed that using reward-refnet improves the initial draft on specific aspects like fluency, answerability and originality. as a future work, we would like to extend refnet to have the ability to decide whether a refinement is needed on the generated initial draft.", "Solved": NaN, "Unsolved": NaN}, "207765742": {"num": 87, "type": 1, "title": "Multi-Paragraph Reasoning with Knowledge-enhanced Graph Neural Network", "pdfurl": "https://arxiv.org/pdf/1911.02170v1.pdf", "conclusion": "multi-paragraph reasoning is crucial for answering open-domain questions in practice, while it is still not considered in most existing openqa systems. in this work, we propose a novel openqa model kgnn, which performs reasoning over paragraphs via a knowledge enhanced graph neural network. experimental results show that kgnn outperforms strong baselines with a large margin on the hotpotqa dataset, and also has the ability to tackle more informative texts. we hope our work can shed some lights to the combination of knowledge graph and text for openqa.", "Solved": NaN, "Unsolved": NaN}, "253581551": {"num": 88, "type": 1, "title": "Data-Efficient Autoregressive Document Retrieval for Fact Verification", "pdfurl": "https://export.arxiv.org/pdf/2211.09388v1.pdf", "conclusion": "we show that distant supervision and pre-training enables high precision autoregressive document retrieval with fewer annotated training data. while previous work has studied the utility of pre-training   (2020)).", "Solved": NaN, "Unsolved": NaN}, "208201969": {"num": 89, "type": 1, "title": "Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets", "pdfurl": "https://arxiv.org/pdf/1911.09241v1.pdf", "conclusion": "existing analysis work in mrc is largely concerned with evaluating the capabilities of systems.by contrast, in this work, we proposed an analysis methodology for the benchmarking capacity of datasets.our methodology consists of input-ablation tests, in which each ablation method is associated with a skill requisite for mrc.we exemplified 12 skills and analyzed 10 datasets.the experimental results suggest that for benchmarking sophisticated nlu, datasets should be more carefully designed to ensure that questions correctly evaluate the intended skills.in future work, we will develop a skill-oriented method for crowdsourcing questions.", "Solved": NaN, "Unsolved": NaN}, "233240821": {"num": 90, "type": 1, "title": "Are Multilingual BERT models robust? A Case Study on Adversarial Attacks for Multilingual Question Answering", "pdfurl": "https://arxiv.org/pdf/2104.07646v1.pdf", "conclusion": "we have shown several novel adversaries that successfully attack mbert for mlqa. specifically, we show that the language of the adversarial statement impacts the attack with priority given to english and the language of the question regardless of the other languages in the qa pair. we also show that including such attack strategies while training our defense brings back performance without the need for complex neural network engineering. not only do the strategies improve results for their corresponding attack, they help for all our attacks indicating model robustness. in the future, we plan to expose vulnerabilities on other multilingual lms and datasets and explore more sophisticated defense strategies.", "Solved": NaN, "Unsolved": NaN}, "218487030": {"num": 91, "type": 1, "title": "Teaching Machine Comprehension with Compositional Explanations", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.145.pdf", "conclusion": "in this paper, we propose to teach extractive mrc with explanations, with a focus on annotation efficiency. we believe explanations stating \"why\" and justifying \"deduction process\" opens up a new way to communicate human's generalization abilities to mrc model training. we begin with a small set of semi-structured explanations and compose nmteachers to augment training data. nmteachers are modularized functions where each module has a strict and softened form, enabling broader coverage from each explanation. extensive experiments on different datasets and mrc models demonstrate the efficiency of our system. having achieved encouraging results for mrc, we look forward to extending this framework to tasks such as non-fact-based qa and multi-hop reasoning. ", "Solved": NaN, "Unsolved": NaN}, "233240823": {"num": 92, "type": 1, "title": "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph", "pdfurl": "https://arxiv.org/pdf/2104.07302v2.pdf", "conclusion": "we proposed transfernet, an effective and transparent framework for multi-hop qa over knowledge graph or text-formed relation graph.it achieved 100% accuracy on 2-hop and 3-hop questions of label-formed metaqa, nearly solving the dataset.on the more challenging webqsp, compwebq and text-formed metaqa, it also outperforms other state-of-the-art models significantly.qualitative analysis shows the good interpretability of trans-fernet.", "Solved": NaN, "Unsolved": NaN}, "237605111": {"num": 93, "type": 1, "title": "BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles", "pdfurl": "https://arxiv.org/pdf/2109.11087v2.pdf", "conclusion": "in this paper, we introduce birdqa, a large-scale, bilingual multiple-choice question answering dataset to facilitate the development of qa systems capable of solving tricky riddles. the huge gap between the human and machine leaves much room for improvement. in future work, we plan to extend birdqa with riddles in other languages and incorporate figurative language understanding into riddle solving. we hope that birdqa will stir more research for question answering on riddles.", "Solved": NaN, "Unsolved": NaN}, "237940982": {"num": 94, "type": 1, "title": "FQuAD2.0: French Question Answering and knowing that you know nothing", "pdfurl": "https://arxiv.org/pdf/2109.13209v1.pdf", "conclusion": "in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions. to the best of our knowledge, this is the first french (and, perhaps most importantly, non-english) adversarial question answering dataset. we trained various baseline models using camembert architectures. our best model, a finetuned camembert large , reaches 83% f1 score and 82.3% f1 no ans , the latter measuring its ability to distinguish answerable questions from unanswerable ones. the study of learning curves with respect to the number of samples used for training such models show that our baseline models would benefit from additional unanswerable questions. in the future, we plan to collect additional samples to expand fquad2.0. for comparison, its english cousin squad2.0 (rajpurkar et al., 2018) contains 53,775 unanswerable questions. such a large-scale dataset would of course enable the acquisition of even better models as the ones presented in sec-tions 5 and 6. as far as data collection is concerned, we could also collect additional answers for each unanswerable question. by following the same procedure as in d' hoffschmidt et al. (2020), this would allow for the computation of human performance, measuring the inherent difficulty of the challenge provided by fquad2.0.", "Solved": NaN, "Unsolved": NaN}, "203610361": {"num": 95, "type": 1, "title": "Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks", "pdfurl": "https://www.aclweb.org/anthology/D19-5306.pdf", "conclusion": "in this paper, we investigated the role played by interlinked sentence representation for complex, multi-hop question answering under the focus of supporting facts identification, i.e. retrieving the minimum set of facts required to answer a given question. we emphasise that this problem is worth pursuing, showing that the performance of stateof-the-art models substantially deteriorates as the size of the accompanying context increases.", "Solved": NaN, "Unsolved": NaN}, "261908732": {"num": 96, "type": 1, "title": "Conversations with Search Engines: SERP-based Conversational Response Generation", "pdfurl": "https://arxiv.org/pdf/2004.14162v2.pdf", "conclusion": "in this paper, we propose conversations with search engines as task for the community to consider and we contribute two types of result: first, we release a new test set, saac, which is more suitable and challenging for this research than existing resources.second, we propose an end-to-end neural model, case, to advance the state-of-the-art.we implement state-of-the-art methods from related tasks and conduct extensive experiments to show that: (1) the proposeed case model can achieve state-of-the-art performance; (2) the proposed sti and ppg modules can bring large improvements; and (3) saac is a more challenging dataset than previously introduced ones, leaving significant room for further improvements.", "Solved": NaN, "Unsolved": NaN}, "247595263": {"num": 97, "type": 1, "title": "SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS", "pdfurl": "https://export.arxiv.org/pdf/2203.11171v4.pdf", "conclusion": "we introduced a simple yet effective method called self-consistency, and observed that it significantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. beyond accuracy gains, self-consistency is also useful for collecting rationales when performing reasoning tasks with language models, and for providing uncertainty estimates and improved calibration of language model outputs.", "Solved": NaN, "Unsolved": NaN}, "258865410": {"num": 98, "type": 1, "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2305.15053v1.pdf", "conclusion": "we study a real-world information-seeking settingtip of the tongue retrieval-in which users issue long, complex queries for re-finding items despite being unable to articulate identifying details about those items. we introduce a simple but effective approach to handling these complex queries that decomposes them into sub-queries or clues that are routed to expert retrievers for specialized scoring. our simple framework allows for modular composition of different retrievers and leveraging of pretrained models for specific modalities such as clip for document images. we observe improvements of up to 7% relative gain for recall@5 when incorporating query decomposition into existing retrievers on our newly-introduced whatsthatbook, a large challenging dataset of real-world, tip-of-thetongue queries for books.", "Solved": NaN, "Unsolved": NaN}, "248862979": {"num": 99, "type": 1, "title": "LogiGAN: Learning Logical Reasoning via Adversarial Pre-training", "pdfurl": "https://export.arxiv.org/pdf/2205.08794v2.pdf", "conclusion": "in this work, we hypothesize that (i) logic ability plays a key role in a wide scope of tasks requiring general reasoning; and (ii) plms' logic ability can be further improved beyond their original linguistic ability. we correspondingly propose logigan, an unsupervised adversarial pre-training framework for logical reasoning enhancement. logigan circumvents the non-differentiable challenge of sequential gan via a novel generator-verifier scoring consensus mechanism, and enables largescale pre-training with longer target length. extensive experiments and ablation studies reveal the effectiveness and functional components of logigan, providing evidence to our major hypothesis.", "Solved": NaN, "Unsolved": NaN}, "251953412": {"num": 100, "type": 1, "title": "LEXMAE: LEXICON-BOTTLENECKED PRETRAINING FOR LARGE-SCALE RETRIEVAL", "pdfurl": "https://export.arxiv.org/pdf/2208.14754v2.pdf", "conclusion": "in this work, we propose to improve the lexicon-weighing retrieval by pre-training a lexiconbottlenecked masked autoencoder (lexmae) which alleviates the objective mismatch between the masked language modeling encoders and relevance-oriented lexicon importance. after pretraining lexmae on large-scale collections, we first observe great zero-shot performance. then after fine-tuning the lexmae on the large-scale retrieval benchmark, we obtain state-of-the-art retrieval quality with very high efficiency and also deliver state-of-the-art zero-shot transfer performance on beir benchmark. further detailed analyses on the efficacy-efficiency trade-off in terms of retrieval latency and storage memory also verify the superiority of our fine-tuned lexmae.", "Solved": NaN, "Unsolved": NaN}, "233189637": {"num": 101, "type": 1, "title": "NLQuAD: A Non-Factoid Long Question Answering Data Set", "pdfurl": "https://www.aclweb.org/anthology/2021.eacl-main.106.pdf", "conclusion": "we introduce nlquad, a non-factoid long question answering data set from bbc news articles. nlquad's question types and the long lengths of its context documents as well as answers, make it a challenging real-world task. we propose to use intersection over union (iou) as an evaluation metric for long question answering. to establish a baseline performance, we experimented with the bert, roberta, and longformer question answering models. longformer outperforms the other methods with an iou of 73.57%, but the results show that the performance of state-of-the-art question answering systems is far from perfect. we hope nlquad will inspire more research in the area of document-level language understanding and question answering.", "Solved": NaN, "Unsolved": NaN}, "258179336": {"num": 102, "type": 1, "title": "Tool Learning with Foundation Models", "pdfurl": "https://export.arxiv.org/pdf/2304.08354v2.pdf", "conclusion": "this paper studies the paradigm of tool learning with foundation models. we first recapitulate the cognitive origins of tool use in human history and categorize tools from the perspective of the user interface. then we review the ai paradigm shift brought about by foundation models and discuss the complementary roles of tools and foundation models. we perform a comprehensive literature review for existing exploration in tool learning and start with formulating a general tool learning framework. then we highlight core research problems such as bridging user intents with appropriate tools, better planning by leveraging the reasoning abilities of foundation models, training strategies for tool learning, and how to facilitate generalization for tool learning. finally, we discuss important research topics, including safe and trustworthy tool learning, tool learning for large complex systems, ai tool creation, personalized tool learning, embodied tool learning, knowledge conflict issue in tool augmentation, etc. in general, this paper serves as a systematic investigation of tool learning. we hope this paper could facilitate research in integrating tools with foundation models in the future.", "Solved": NaN, "Unsolved": NaN}, "174799117": {"num": 103, "type": 1, "title": "Revisiting Joint Modeling of Cross-document Entity and Event Coreference Resolution", "pdfurl": "https://www.aclweb.org/anthology/P19-1409.pdf", "conclusion": "we presented a neural approach for resolving cross-document event and entity coreference. we represent a mention using its text, context, andinspired by the joint model of lee et al. (2012)we make an event mention representation aware of coreference clusters of entity mentions to which it is related via predicate-argument structures, and vice versa. our model achieves state-of-the-art results, outperforming previous models by 10.5 conll f 1 points on events, and providing the first cross-document entity coreference results on ecb+. future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions. ", "Solved": NaN, "Unsolved": NaN}, "259361037": {"num": 104, "type": 1, "title": "MultiVENT: Multilingual Videos of Events with Aligned Natural Text", "pdfurl": "https://export.arxiv.org/pdf/2307.03153v1.pdf", "conclusion": "we introduce multivent, a multimodal, multilingual dataset grounded in natural language documents for event-centric video retrieval and information acquisition. this dataset consists of 2,396 videos covering 260 current events reported in five target languages (arabic, chinese, english, korean, and russian) paired with multilingual natural language video descriptions and long-form event-centric text documents. we use this dataset to characterize online news coverage and how models can use this online content for information acquisition. we propose a multilingual video retrieval benchmark using multivent and present multiclip, multilingual video retrieval model to serve as a baseline for the task. we evaluate this model and related retrieval approaches on msr-vtt and multivent to illustrate the importance of pretraining on multilingual data for evaluation on multivent. in future work, we aim to explore the effect that joint vision-ocr embeddings can have on video retrieval in text-heavy contexts. also in future work, a repaq-adjacent system [25] for automatically extracting question-answer pairs from video content and video-document pairs could be developed and applied to multivent. through this, a framework for teaching models to perform open-domain question-answering tasks with multimodal background corpora could be established, expanding the domain of questions a model can answer.", "Solved": NaN, "Unsolved": NaN}, "235421967": {"num": 105, "type": 1, "title": "A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering", "pdfurl": "https://www.aclanthology.org/2021.acl-long.318.pdf", "conclusion": "to alleviate the spurious solution problem in weakly supervised qa, we propose to explicitly exploit the semantic correlations between a question and its solution via mutual information maximization. during training, we pair a task-specific model with a question reconstructor which guides the task-specific model to predict solutions that are consistent with the questions. experiments on four qa datasets demonstrate the effectiveness of our learning method. as shown by automatic and manual analyses, models trained with our method are more resistant to spurious solutions during training, and are more precise in locating information that is relevant to the questions during inference, leading to higher accuracy of both answers and solutions.", "Solved": NaN, "Unsolved": NaN}, "254408974": {"num": 106, "type": 1, "title": "Successive Prompting for Decomposing Complex Questions", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.81.pdf", "conclusion": "we present a way to successively decompose complex questions into simple qa pairs, which allows for modular qd and qa systems that can be trained and queried independently. when performing in-context learning, we showed that successive prompting yields an improvement of 4.6 f1 over chain-of-thought prompting. when replacing just the in-context qa module with a fine-tuned one, which is adept at handling list type questions, we further improve the overall performance by 9.5 f1.", "Solved": NaN, "Unsolved": NaN}, "237592852": {"num": 107, "type": 1, "title": "Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering", "pdfurl": "https://www.aclanthology.org/2021.sustainlp-1.7.pdf", "conclusion": "in this work, we provided insights on the performance of state-of-the-art dense retrieval for multihop questions. we showed that rerank+dpr 2 (our hybrid model) outperforms mdr (the state-of-theart multi-hop dense retrieval model) in the low re-source setting, and it is competitive with mdr in the setting where mdr uses considerably more computational resources. finally, we highlighted that fully dense retrieval models get harmed when using limited computational resources. for future work, we plan to build on our insights to improve the performance of multi-hop models by combining the strengths of lexical and dense retrieval. also, we aim to develop less computationally expensive multi-hop retrieval models.", "Solved": NaN, "Unsolved": NaN}, "252918040": {"num": 108, "type": 1, "title": "Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding", "pdfurl": "https://export.arxiv.org/pdf/2210.08536v1.pdf", "conclusion": "in this paper, we presented a seminal knowledge prompting paradigm, based on which a novel knowledge-prompting-based plm framework kp-plm was proposed. we constructed contextual knowledge sub-graphs for contexts and employed continuous prompting mapping to generate knowledge prompts. after that, we designed two selfsupervised pre-training tasks to learn semantic knowledge from prompts. finally, we conducted extensive experiments to evaluate the model performance. experimental results validate the effectiveness of knowledge prompting in boosting the performance of plms.", "Solved": NaN, "Unsolved": NaN}, "246016282": {"num": 109, "type": 1, "title": "Reasoning over Hybrid Chain for Table-and-Text Open Domain QA", "pdfurl": "https://arxiv.org/pdf/2201.05880v1.pdf", "conclusion": "in this paper, we present a chain-centric reasoning and pre-training (carp) framework for table-andtext question answering. when answering the questions given retrieved table and passages, carp first extracts explicit hybrid chain to reveal the intermediate reasoning process leading to the answer across table and text. the hybrid chain provides a guidance for qa, and explanation of the intermediate reasoning process. to enhance the extraction model with better reasoning ability and alleviate data sparsity problem, we design a novel chaincentric pre-training method. this method synthesizes the reasoning corpus in a larger scale and of higher reasoning complexity, which is achieved by automatically synthesizing heterogeneous reasoning paths from tables and passages in wikipedia and reversely generating multi-hop questions. we find that the pre-training task boosts performance on the hybrid chain extraction model, especially for questions requiring more complex reasoning, which leads to significant improvement on the performance of the qa model. the hybrid chain also provides better interpretability of the reasoning process. our system achieves the state-of-the-art result on a ", "Solved": NaN, "Unsolved": NaN}, "260460827": {"num": 110, "type": 1, "title": "Neural Conversational QA: Learning to Reason v.s. Exploiting Patterns", "pdfurl": "https://arxiv.org/pdf/1909.03759v1.pdf", "conclusion": "in this paper we show how the existing neural models exploit spurious patterns that exist in the data for the sharc task -a conversation qa that requires reasoning over rules expressed in natural language. we demonstrate how existing models can exploit spurious patterns in such conversational qa datasets and introduce an augmented version of the sharc dataset that discourages a model from exploiting such spurious clues. we also present a simple yet effective model, ur-canet, that learns embedding representation from the dialog history, dialog turns, and the history of past follow-up question and answer pairs. the network generate intermediate representations which is input to a copy decoder to generate a follow-up question. urcanet outperforms existing systems on both the original sharc corpus and the augmented sharc corpus.", "Solved": NaN, "Unsolved": NaN}, "237048095": {"num": 111, "type": 1, "title": "A Dataset for Answering Time-Sensitive Questions", "pdfurl": "https://arxiv.org/pdf/2108.06314v5.pdf", "conclusion": "though time-sensitive facts are pervasive in our daily text corpus, there has been little prior work exploring this direction. in this paper, we build the first dataset to investigate whether existing models can understand time-sensitive facts. our experiments show that the sota models are still lagged behind humans in temporal reasoning. in order to empower the future nlp models to understand temporal information, different temporal-aware models need to be proposed. finally, this paper opens up new research directions for better modeling temporal information in text representations.", "Solved": NaN, "Unsolved": NaN}, "263829795": {"num": 112, "type": 1, "title": "Human Mobility Question Answering (Vision Paper)", "pdfurl": "https://export.arxiv.org/pdf/2310.04443v2.pdf", "conclusion": "in this paper, we present a novel human mobility question answering task which aims at better understanding mobility behaviours.associated with this mobqa task, we discuss a blueprint on how to build a suitable mobqa dataset.additionally, a plausible model design tailored for the mobqa task is also suggested.broader impact.we hope this study will offer fresh concepts and insights for human mobility research.besides the human mobility domain, the mobqa task would also boost the corresponding research of language processing and question answering systems.for example, a new research topic could be how to develop language models suitable for sequential behaviour data.further, the introduced mobqa task could emerge new applications such as mobility chatbots for social good.future work.as the first attempt to explore the question answering of human mobility data, this paper constitutes an initial step in establishing the dataset for the mobqa task.we intend to expand the question templates to include diverse questions in the dataset.after finalising the mobqa dataset, we will focus on developing deep learning models for the mobqa task based on the design introduced in this paper and evaluating our solution.", "Solved": NaN, "Unsolved": NaN}, "253098276": {"num": 113, "type": 1, "title": "Re-Examining Calibration: The Case of Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2205.12507v2.pdf", "conclusion": "this paper investigates calibration in the realistic application of odqa where users need to decide whether to trust the model prediction based on the confidence scores. although confidence scores produced by existing calibration methods improves the popular ece metric, these confidence scores do not help distinguish correct and wrong predictions. we propose to use the macroce metric to remedy the flaws, and existing calibration methods fail on our macroce metric. we further propose a simple and effective calibration method conscal that leverages training consistency. our human study confirms both the effectiveness of conscal as well as the alignment between macroce and human preference. our work advocates and paves the path for user-centric calibration, and our con-scal method is a promising direction for better calibration. future work can adapt our calibration metric and method to more diverse tasks (such as generative tasks) and explore other ways to further improve user-centric calibration.", "Solved": NaN, "Unsolved": NaN}, "246638887": {"num": 114, "type": 1, "title": "E M E R G I N G T R E N D S Emerging Trends: SOTA-Chasing", "pdfurl": NaN, "conclusion": "many papers are sota-chasing, and more will do so in the future. sota-chasing comes with many costs. we discussed three costs:", "Solved": NaN, "Unsolved": NaN}, "249395505": {"num": 115, "type": 1, "title": "On the Advance of Making Language Models Better Reasoners", "pdfurl": "https://arxiv.org/pdf/2206.02336v2.pdf", "conclusion": "we introduce diverse, an effective and general method to make large language models better reasoners. as a continuation of the line of research that prompting language models using multi-step reasoning paths, the key insights of diverse are three-fold: diverse prompts, voting verifier, and step-level correctness. experimental results clearly show that diverse can bring significant and consistent improvements. for example, with codedavinci-002, diverse achieves new state-of-theart results in most of the reasoning tasks, outperforming the 540b palm model combined with previous prompting approaches.", "Solved": NaN, "Unsolved": NaN}, "237439283": {"num": 116, "type": 1, "title": "On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.596.pdf", "conclusion": "relevance performance is still undercounted: while the expert-generated relevance ratings produced in this work provide more accurate estimates of performance compared to single gold explanations when used in fully-automatic evaluations, these automatic estimates still undercount overall model performance. in our experiments we show the expert ratings primarily provide a vehicle for training better models, but that automatically evaluating relevance performance still remains a challenge, even with a large targeted increase in relevance annotation. further, annotators reported that determining relevance of single facts in isolation is challenging because it lacks the broader compositional context of the rest of the candidate explanation, suggesting ultimate limits to the utility of exhaustive annotation.", "Solved": NaN, "Unsolved": NaN}, "237502773": {"num": 117, "type": 1, "title": "Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.490.pdf", "conclusion": "we have proposed suqa, an rc system augmented with an abstractive explainer component. our experiments have demonstrated that the abstractive explainer can generate more concise explanations than an extractive explainer with limited supervison, while keeping explanations sufficient for qa.", "Solved": NaN, "Unsolved": NaN}, "219965751": {"num": 118, "type": 0, "title": "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets", "pdfurl": "https://arxiv.org/pdf/2006.11880v1.pdf", "conclusion": "we conducted a comprehensive survey of recent efforts on the tasks, evaluation metrics and benchmark datasets of machine reading comprehension (mrc). we discussed the definition and taxonomy of mrc tasks, and proposed a new classification method for mrc tasks. the computing methods of different mrc evaluation metrics have been introduced with their usage in each type of mrc tasks also analyzed. we also introduced attributes and characteristics of mrc datasets, with 47 mrc datasets described in detail. finally, we discussed the open issues for future research of mrc and we argued that high-quality multi-modal mrc datasets and the research findings of cognitive neuroscience may help us find better ways to construct more challenging datasets and develop related mrc algorithms to achieve the ultimate goal of human-level machine reading comprehension.", "Solved": NaN, "Unsolved": NaN}, "202565945": {"num": 119, "type": 1, "title": "Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning", "pdfurl": "https://www.aclweb.org/anthology/D19-1455.pdf", "conclusion": "in this work, we proposed a self-assembling neural modular network for multi-hop qa. we designed three modules that reason between the question and text-based context. the resulting model outperforms both the single-hop baseline and the original nmn on hotpotqa . because of the interpretable nature of our model, we presented analyses to show that our model does in fact learn to perform compositional reasoning and can dynamically assemble the modular network based on the question.", "Solved": NaN, "Unsolved": NaN}, "231639356": {"num": 120, "type": 1, "title": "Situation and Behavior Understanding by Trope Detection on Films", "pdfurl": "https://arxiv.org/pdf/2101.07632v1.pdf", "conclusion": "in this work, we presented a brand new task with a new dataset timos for situation and behavior understanding in films. unlike previous tasks and datasets, trope detection requires deep cognitive skills, including consciousness, systematic generalization, and casual and motivational comprehension. modern learning systems including contextual embedding bert, movie tag prediction system, and recurrent relational network reach at most 23.97 f1 score, which is far behind human performance (64.87) and suggests that trope detection is a challenging task. we proposed a multi-level comprehension framework with multi-stream attention and multistep reasoning to tackle this problem and boost performance to 25.00 f1. we carefully analyze the task along with the network's behavior. we adopt human evaluation to verify the answerability of our dataset and discuss potential directions. we believe that our research could pave a new path for future research on deep cognition.", "Solved": NaN, "Unsolved": NaN}, "226236740": {"num": 121, "type": 1, "title": "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps", "pdfurl": "https://arxiv.org/pdf/2011.01060v2.pdf", "conclusion": "in this study, we presented 2wikimultihopqa-a large and high quality multi-hop dataset that provides comprehensive explanations for predictions. we utilized logical rules in the kb to create more natural questions that still require multi-hop reasoning. through experiments, we demonstrated that our dataset ensures multi-hop reasoning while being challenging for the multi-hop models. we also demonstrated that bootstrapping the multi-hop mrc dataset is beneficial by utilizing large-scale available data on wikipedia and wikidata.", "Solved": NaN, "Unsolved": NaN}, "263608648": {"num": 122, "type": 1, "title": "ADAPTING LLM AGENTS THROUGH COMMUNICATION", "pdfurl": "https://export.arxiv.org/pdf/2310.01444v2.pdf", "conclusion": "we introduced learning-through-communication (ltc), a paradigm that adapts llm agents to new tasks and environments via communication-based iterative learning.within this ltc framework, we have designed three communication modes for common tasks including decision-making, knowledgeintensive reasoning, and numeric reasoning.these communication modes facilitate interactions between llm agents and their environments, as well as other agents such as gpt-4 and humans.the history of these interactions can be autonomously organized into training data for ppo training so that the agent can adapt to the new task.our approach represents a closed loop where the agent self-interacts with the environment or other agents, and learning to improve itself with minimal human intervention.empirically, we have demonstrated that ltc performs strongly in success rate and efficiency across three different tasks: alfworld, hotpotqa, and gsm8k.it consistently outperforms existing llm agent and instruction tuning baselines, showing the promise of the ltc paradigm in adapting llm agents to new tasks and environments with minimal human effort.as for future work, we plan to explore more diverse communication patterns for different tasks, and involve the communication with human during the iterative learning process.we will open source our code to facilitate further research in this line.", "Solved": NaN, "Unsolved": NaN}, "201058633": {"num": 123, "type": 1, "title": "Reasoning Over Paragraph Effects in Situations", "pdfurl": "https://www.aclweb.org/anthology/D19-5808.pdf", "conclusion": "we present ropes, a new reading comprehension benchmark containing 14,322 questions, which aims to test the ability of systems to apply knowledge from reading text in a new setting. we hope that ropes will aide efforts in tying language and reasoning together for more comprehensive understanding of text.", "Solved": NaN, "Unsolved": NaN}, "229923145": {"num": 124, "type": 1, "title": "FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.301.pdf", "conclusion": "in this paper, we develop general methods to improve the performance of large pre-trained seq2seq models for jointly producing nl rationales and answer predictions. specifically, we introduce sentence markers into seq2seq models to tackle explanation fabrication, we enable larger input passage sizes using the fusion-in-decoder architecture, and we infuse knowledge by fine-tuning on restructured qa datasets. we show that a universal model can perform favourably compared to the best task-specific fine-tuned models. our methods improve the state of the art on rationale extraction metrics and task accuracy on multiple eraser benchmarks while reducing the extent to which seq2seq models fabricate explanations to justify incorrect predictions, thereby improving the reliability and verifiability of the generated rationales. ", "Solved": NaN, "Unsolved": NaN}, "264487241": {"num": 125, "type": 1, "title": "Length is a Curse and a Blessing for Document-level Semantics", "pdfurl": "https://export.arxiv.org/pdf/2310.16193v1.pdf", "conclusion": "in this work, we questioned the length generalizability of contrastive learning-based text encoders.we observed that, despite their seemingly strong representational power, this ability is strongly vulnerable to length-induced semantic shifts.we formalized length attack, demystified it, and defended against it with la(ser) 3 .we found that, teaching the models \"my longer-self = myself\" provides a standalone semantic signal for more robust and powerful unsupervised representation learning.", "Solved": NaN, "Unsolved": NaN}, "233296201": {"num": 126, "type": 1, "title": "Generative Context Pair Selection for Multi-hop Question Answering", "pdfurl": "https://arxiv.org/pdf/2104.08744v1.pdf", "conclusion": "we have presented a generative formulation of context pair selection in multi-hop question answering models. by encouraging the context selection model to explain the entire question, it is less susceptible to bias, performing substantially better on adversarial data than existing methods that use discriminative selection. our proposed model is simple to implement and can be used with any existing (or future) answering model; we will release code to support this integration. since context pair selection scales quadratically with the number of contexts, it is not ideal for scenarios that involve a large number of possible contexts. however, it allows for deeper inter-document interaction as compared to other approaches that use summarized document representations. with more reasoning steps, selecting relevant documents given only the question becomes challenging, increasing the need for inter-document interaction. this paper focuses on biases found in question answering models that make its reasoning capabilities brittle. it uses an existing method of testing model performance on adversarial held-out set as an evaluation metric. this work does not deal with any social impacts of biases in natural language processing systems.", "Solved": NaN, "Unsolved": NaN}, "261245264": {"num": 127, "type": 1, "title": "LONGBENCH: A BILINGUAL, MULTITASK BENCH- MARK FOR LONG CONTEXT UNDERSTANDING", "pdfurl": "https://export.arxiv.org/pdf/2308.14508v1.pdf", "conclusion": "in this paper, we introduce longbench, a multi-task bilingual benchmark tailored for gauging long context understanding abilities of llms. longbench covers six key categories and a total of 21 tasks, with data lengths extending from thousands of tokens up to tens of thousands of tokens. we also develop longbench-e which features a more evenly data length distribution. we conduct extensive experiments on longbench and longbench-e, yielding insightful conclusions about the capabilities of current llms on long context understanding. moreover, our analysis suggests that longbench and longbench-e serve as ideal testbeds for future research in long context modeling. table 6 lists the instantiation of (i, c, a) for each dataset in longbench. table 7 reports the number of data on each task that falls in the length range of 0-4k, 4k-8k, and 8k+ in longbench-e.", "Solved": NaN, "Unsolved": NaN}, "211003735": {"num": 128, "type": 1, "title": "BREAK It Down: A Question Understanding Benchmark", "pdfurl": "https://www.aclweb.org/anthology/2020.tacl-1.13.pdf", "conclusion": "in this paper, we presented a formalism for question understanding. we have shown it is possible to train crowd-workers to produce such representations with high quality at scale, and created break, a benchmark for question decomposition with over 83k decompositions of questions from 10 datasets and 3 modalities (db, images, text). we presented the utility of qdmr for both open-domain question answering and semantic parsing, and constructed a qdmr parser with reasonable performance. qdmr proposes a promising direction for modeling question understanding, which we believe will be useful for multiple tasks in which reasoning is probed through questions.", "Solved": NaN, "Unsolved": NaN}, "258865495": {"num": 129, "type": 1, "title": "Few-shot Unified Question Answering: Tuning Models or Prompts?", "pdfurl": "https://export.arxiv.org/pdf/2305.14569v1.pdf", "conclusion": "in this work, we explore the viability of prompttuning as a solution to unified qa and conduct a thorough analysis of its promise, effectiveness, and trade-offs compared with the model-tuning paradigm on a set of 16 qa datasets, focusing particularly on several few-shot scenarios. as a result, we obtain several key findings and insights that hopefully will inform which paradigm to prefer under which scenarios. prompt tuning is quite competitive with model-tuning in the lower extreme of the few-shot scenarios, given a good initialization. while parameter-sharing leads to superior performance in the few-shot setting, the trends flip in the full-shot setting, a simple knowledge transfer approach (i.e., an average of relevant prompts) is as effective as complex methods without introducing additional parameters. pre-training the backbone model on the source tasks significantly benefits prompt tuning. while initializing from a strong prior is very helpful for prompt tuning, its benefit is not as substantial when using a larger backbone model, especially when the number of training examples exceeds a certain threshold.", "Solved": NaN, "Unsolved": NaN}, "251224409": {"num": 130, "type": 1, "title": "Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval", "pdfurl": "https://www.aclanthology.org/2023.tacl-1.26.pdf", "conclusion": "in this paper, we present aggretriever, a singlevector dense retrieval model that exploits all contextualized token embeddings from the input to bert. we introduce a simple approach to aggregate the contextualized token embeddings into a dense vector, agg . experiments show that agg combined with the standard [cls] vector achieves better retrieval effectiveness than using the [cls] vector alone for both in-domain and zero-shot evaluations. our work demonstrates that mlm pre-trained transformers can be fine-tuned into effective dense retrievers without further pre-training or expensive fine-tuning strategies.", "Solved": NaN, "Unsolved": NaN}, "221292890": {"num": 131, "type": 1, "title": "Continual Domain Adaptation for Machine Reading Com-prehension", "pdfurl": "https://arxiv.org/pdf/2008.10874v1.pdf", "conclusion": "we introduce the continual domain adaptation task for mrc. so far as we know, this is the first study on the continual learning perspective of mrc. we build two datasets cda-q and cda-c for the cda task, by re-organizing existing mrc collections into different domains with respect to the question type and passage type. we conduct preliminary experiments showing the existence of catastrophic forgetting (cf) phenomenon of existing mrc models under the cda setting. further, we propose regularization-based regbertqa and dynamic-architecture progbertqa to tackle the cda for mrc. we conduct extensive experiments to analysis the effectiveness of both methods and validate that the proposed dynamic-architecture based model achieves the best performance.", "Solved": NaN, "Unsolved": NaN}, "256389465": {"num": 132, "type": 1, "title": "EmbedDistill: A Geometric Knowledge Distillation for Information Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2301.12005v2.pdf", "conclusion": "we propose embeddistill -a novel distillation method for ir that goes beyond simple score matching. en route, we provide a theoretical understanding of the teacher-student generalization gap in an ir setting which not only motivated embeddistill but also inspired new design choices for the student de models: (a) reusing the teacher's document encoder in the student and (b) aligning query embeddings of the teacher and student. this simple approach delivers consistent quality and computational gains in practical deployments and we demonstrate them on msmarco, nq, and beir benchmarks. finally, we found embeddistill retains 95-97% of the teacher performance to with 1/10th size students.", "Solved": NaN, "Unsolved": NaN}, "216641884": {"num": 133, "type": 0, "title": "A Survey of Document Grounded Dialogue Systems (DGDS)", "pdfurl": "https://arxiv.org/pdf/2004.13818v1.pdf", "conclusion": "the document grounded dialogue system (dgds) can mine document(s) information and discuss specific document(s) in a real human conversation. we believe that extracting unstructured document(s) information in dialogue is the future trend of the ds because a large amount of human knowledge is contained in these document(s). the research of the dgds not only possesses a broad application prospect but also facilitates the ds to better understand human knowledge and natural language. this article introduces the dgds, defines the related concepts, analyzes the current datasets and models, and provides views on future research trends in this field, hoping to be helpful for the community.", "Solved": NaN, "Unsolved": NaN}, "246823260": {"num": 134, "type": 1, "title": "QA4QG: USING QUESTION ANSWERING TO CONSTRAIN MULTI-HOP QUESTION GENERATION", "pdfurl": "https://arxiv.org/pdf/2202.06538v1.pdf", "conclusion": "in this paper, we propose a novel framework, qa4qg, a qaaugmented bart-based framework for mqg. it is the first work to explore large pre-trained language models for mqg and takes advantage of an additional multi-hop qa module to further constrain the question generation. our results on the hotpotqa dataset show that qa4qg outperforms all state-of-the-art models, with an increase of 8 bleu-4 and 8 rouge points compared to the best results previously reported. our work suggests the advantage of introducing pre-trained language models and qa modules for the mqg task.", "Solved": NaN, "Unsolved": NaN}, "250562913": {"num": 135, "type": 1, "title": "Continual Machine Reading Comprehension via Uncertainty-aware Fixed Memory and Adversarial Domain Adaptation", "pdfurl": "https://export.arxiv.org/pdf/2208.05217v1.pdf", "conclusion": "in this paper, an incremental learning mrc model with uncertainty-aware fixed memory and adversarial domain adaptation, ma-mrc, is proposed for continual mrc and alleviating catastrophically forgetting. inspired by the human learning process, there are two main ideas of ma-mrc: a memory that stores a small number of samples in previous seen domain data and always focuses on what the model most needs to replay; and adversarial learning the domain adaptation in a two-player game to learn better transfer representations between previous and current domain data. experimental results show that the proposed ma-mrc can achieve a good continuous learning performance without catastrophically forgetting under cda-c and cda-q settings.", "Solved": NaN, "Unsolved": NaN}, "235313508": {"num": 136, "type": 1, "title": "Question Answering Over Temporal Knowledge Graphs", "pdfurl": "https://www.aclanthology.org/2021.acl-long.520.pdf", "conclusion": "in this paper we introduce cronquestions, a new dataset for temporal knowledge graph question answering. while there exist some temporal kgqa datasets, they are all based on non-temporal kgs (e.g., freebase) and have relatively few questions. our dataset consists of both a temporal kg as well as a large set of temporal questions requiring various structures of reasoning. in order to develop such a large dataset, we used a synthetic   generation procedure, leading to a question distribution that is artificial from a semantic perspective. however, having a large dataset provides an opportunity to train models, rather than just evaluate them. we experimentally show that increasing the training dataset size steadily improves the performance of certain methods on the tkgqa task. we first apply large pre-trained lm based qa methods on our new dataset. then we inject kg embeddings, both temporal and non-temporal, into these lms and observe significant improvement in performance. we also propose a new method, cronkgqa, that is able to leverage temporal kg embeddings to perform tkgqa. in our experiments, cronkgqa outperforms all baselines. these results suggest that kg embeddings can be effectively used to perform temporal kgqa, although there remains significant scope for improvement when it comes to complex reasoning questions.", "Solved": NaN, "Unsolved": NaN}, "222310757": {"num": 137, "type": 1, "title": "F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering", "pdfurl": "https://arxiv.org/pdf/2010.06283v1.pdf", "conclusion": "we manually analyze outputs of the models by qi et al. (2019) and  and identify the following two problems. silent facts. the models make use of facts without including them into their explanations (cf., figure 1). as a result, the predicted answer does not occur in the explanation, leaving the user uninformed about where it came from.unused facts. the models predict facts to be relevant without any relation to the predicted answer. the second fact of the explanation in figure  1 is an example for this. we also found examples where the facts predicted to be relevant do not even contain the entities from the question.", "Solved": NaN, "Unsolved": NaN}, "216553833": {"num": 138, "type": 1, "title": "Contextualized Representations Using Textual Encyclopedic Knowledge", "pdfurl": "https://arxiv.org/pdf/2004.12006v2.pdf", "conclusion": "we presented a method to build text representations by jointly contextualizing the input with dynamically retrieved textual encyclopedic knowledge. we showed consistent improvements, in-and out-of-domain, across multiple reading comprehension benchmarks that require factual reasoning and knowledge well represented in the background collection.", "Solved": NaN, "Unsolved": NaN}, "229923177": {"num": 139, "type": 1, "title": "ERNIE-DOC: A Retrospective Long-Document Modeling Transformer", "pdfurl": "https://www.aclanthology.org/2021.acl-long.227.pdf", "conclusion": "in this paper, we proposed ernie-doc, a document-level language pretraining model based on the recurrence transformers paradigm. two well-designed mechanisms, namely the retrospective feed mechanism and the enhanced recurrent mechanism, enable ernie-doc, which theoretically has the longest possible dependency, to model bidirectional contextual information of a complete document. additionally, ernie-doc is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context. experiments on various downstream tasks demonstrate that ernie-doc outperforms existing strong pretraining models such as roberta, longformer, and bigbird and achieves sota results on several language modeling and language understanding benchmarks.", "Solved": NaN, "Unsolved": NaN}, "222140777": {"num": 140, "type": 1, "title": "DaNetQA: a yes/no Question Answering Dataset for the Russian Language", "pdfurl": "https://export.arxiv.org/pdf/2010.02605v2.pdf", "conclusion": "in this paper, a new question answering dataset, danetqa, is presented.it comprises binary yes/no questions, paired with paragraphs, which should be used to answer the questions.the overall collection procedure follows the design of the boolq dataset, which is a magnitude larger in size than danetqa, partially due to the use of proprietary sources.we establish a straightforward baseline, exploiting fasttext and rubert models and experiment with multiple transfer learning settings.our results show, that on the one hand, the english dataset can be leveraged to improve the results for the russian one.however, we can not confirm, that we can re-use boolq for training the model while keeping danetqa for evaluation only.this brings us to the following conclusion: although the re-creation of english datasets in other languages may seem like a redundant and secondary activity, the current state of the cross-lingual models does not allow for perfect language transfer.it is not enough to train the model on the english data.it seems impossible to gain high-quality results if the model is not trained in the target language.this highlights the need for future development: the development of more advanced cross-lingual contextualized encoders as well as more sophisticated datasets to evaluate cross-lingual tasks.as for danetqa development, we plan to enlarge the dataset with more question-paragraph pairs and to extend the dataset with an unanswerable question, affecting though the task setting.", "Solved": NaN, "Unsolved": NaN}, "244896105": {"num": 141, "type": 1, "title": "MetaQA: Combining Expert Agents for Multi-Skill Question Answering", "pdfurl": "https://www.aclanthology.org/2023.eacl-main.259.pdf", "conclusion": "in this work, we propose an alternative to multidataset models for multi-skill qa. we propose to combine expert agents to create a collaborative system for question answering (qa) called metaqa. it considers questions, answer predictions, and confidence scores from the agents to select the best answer to a question. through quantitative experiments, we show that our model avoids the limitations of multi-dataset models and outperforms the baselines thanks to the agent collaboration established. additionally, since metaqa learns to match questions with answers instead of end-to-end qa, it is highly data-efficient to train. we leave as future work: i) combining partially correct answer predictions to generate a better one, ii) adding new agents without retraining metaqa by fixing most of the weights and only training the weights of the new agent selection network, and iii) identifying a priori agents that are likely to give an incorrect answer to skip them at run-time.", "Solved": NaN, "Unsolved": NaN}, "254044526": {"num": 142, "type": 0, "title": "Dense Text Retrieval based on Pretrained Language Models: A Survey", "pdfurl": "https://export.arxiv.org/pdf/2211.14876v1.pdf", "conclusion": "in this survey, we thoroughly review the recent progress of dense retrieval based on pretrained language models (plm). as an important evolution of language intelligence techniques, plms empower dense retrieval models with excellent modeling capacities to capture and represent text semantics for relevance matching. our survey has extensively discussed the key issues and the mainstream solutions in four major aspects to develop dense retrieval systems, including architecture, training, indexing and integration. next, we briefly summarize the discussions of this survey and introduce some remaining issues for dense retrieval.", "Solved": NaN, "Unsolved": NaN}, "237420912": {"num": 143, "type": 1, "title": "FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.491.pdf", "conclusion": "we present an effective few-shot question answering (qa) system that combines the use of pre-trained text-to-text models and a fine-tuning framework aligned with their pre-training counterpart. through experimental studies on various qa benchmarks and few-shot configurations, we show that this system can produce significant gains including in scenarios where the training data is extremely scarce (an absolute gain of 34 f1 points on average in comparison to the current standard of the fine-tuning framework). we also present extensions to multilingual and larger model settings and show that the gains translate well to these settings (eg:-up to an absolute 40 f1 point gain in comparison to xlm-roberta + a span-selection objective). through ablation studies, we study the impact of model size, fine-tuning objectives, inputoutput design and illustrate the factors leading to such strong gains. for future, as our framework doesn't explicitly enforce the answer to be a span in the input text, it'd be interesting to consider its applications to generative qa tasks.", "Solved": NaN, "Unsolved": NaN}, "214233456": {"num": 144, "type": 1, "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/2020.tacl-1.10.pdf", "conclusion": "we present the first free-form multiple-choice chinese machine reading comprehension dataset (c 3 ), collected from real-world language exams, requiring linguistic, domain-specific, or general world knowledge to answer questions based on the given written or orally oriented texts. we study the prior knowledge needed in this challenging machine reading comprehension dataset and carefully investigate the impacts of distractor plausibility and data augmentation (based on similar resources for english) on the performance of state-of-the-art neural models. experimental results demonstrate the there is still a significant performance gap between the best-performing model (68.5%) and human readers (96.0%) and a need for better ways for exploiting rich resources in other languages.", "Solved": NaN, "Unsolved": NaN}, "219124082": {"num": 145, "type": 0, "title": "Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models", "pdfurl": NaN, "conclusion": "we present a structured survey of methods that reveal heuristics and spurious correlations in datasets, methods which show that neural models inherit those correlations or assess their capabilities otherwise, and methods that mitigate this by adversarial training, data augmentation and model architecture or training procedure improvements. various nli datasets are reported to contain spurious correlations between input and expected output, might be unsuitable to evaluate some task modality due to dataset design or suffer from quality issues. rte is a popular target task for these data-centred investigations with more than half of the surveyed papers focusing on it. nli models, in turn, are shown to exploit those correlations and to rely on superficial lexical cues. furthermore, they lack generalisation beyond the evaluation set resulting in poor performance on out-of-distribution evaluation sets, generated adversarially or targeted at a specific capability. efforts to achieve robustness include augmenting the training data with adversarial examples, making use of external resources and modifying the neural network architecture or training objective.", "Solved": NaN, "Unsolved": NaN}, "233240947": {"num": 146, "type": 1, "title": "Time-Stamped Language Model: Teaching Language Models to Understand the Flow of Events", "pdfurl": "https://arxiv.org/pdf/2104.07635v1.pdf", "conclusion": "we proposed the time-stamped language model (tslm model), a novel approach based on a simple and effective idea, which enables pre-trained qa models to process procedural texts and produce different outputs based on each step to track entities and their changes. tslm utilizes a timestamp function that causes the attention modules in the transformer-based lm architecture to incorporate past, current, and future information by computing a timestamp embedding for each input token. our experiments show a 3.1% improvement on the f1 score and a 10.4% improvement over the recall metric on propara dataset. our model further outperforms the state-of-the-art models with a 1.55% margin in the npn-cooking dataset accuracy for the location prediction task.", "Solved": NaN, "Unsolved": NaN}, "233296243": {"num": 147, "type": 1, "title": "Learning with Instance Bundles for Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.584.pdf", "conclusion": "we have presented a way to use contrastive estimation in a supervised manner to learn from distinguishing cues between multiple related qa pairs, or instance bundles. our experiments with multiple ce-based loss functions, defined over a joint neighborhood of questions and answers, have shown that these models outperform existing methods on two datasets: ropes and hotpotqa. apart from presenting several ways to create instance bundles, we also explore theoretical connections between unlikelihood training and contrastive estimation, and initial exploration into when instance bundles are likely to be effective with these methods. we believe our results give strong motivation for further work in techniques to both create and use instance bundles in nlp datasets. the code is available at https://github.com/ddua/ contrastive-estimation.", "Solved": NaN, "Unsolved": NaN}, "257771900": {"num": 148, "type": 1, "title": "Explicit Planning Helps Language Models in Logical Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2303.15714v3.pdf", "conclusion": "in this paper, we presented leap, an lm-based logical reasoning system that integrates explicit planning into the inference method.we also proposed a method that learns to prevent the explicit planning from being misguided.our proposed methods exhibit intriguing technical connections to other reasoning systems and can be likened to the deliberative system 2 in \"dual process\" theories of reasoning.in our experiments, our planning-based system outperforms strong baseline methods including the selection-inference method and chain-of-thought prompting.we will discuss several exciting avenues for further improvements in appendix a.", "Solved": NaN, "Unsolved": NaN}, "257921404": {"num": 149, "type": 1, "title": "Rethinking the Role of Token Retrieval in Multi-Vector Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2304.01982v2.pdf", "conclusion": "multi-vector retrieval leverages query and document token representations for effective information retrieval. in this paper, we propose xtr that simplifies the existing three-stage inference of multivector models by improving the initial token retrieval stage. specifically, xtr scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens. as a result, xtr achieves state-of-the-art performances on zero-shot information retrieval benchmarks while greatly reducing the flops of the scoring stage. we further show that indeed our objective encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query.", "Solved": NaN, "Unsolved": NaN}, "257636734": {"num": 150, "type": 0, "title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey", "pdfurl": "https://export.arxiv.org/pdf/2303.12023v1.pdf", "conclusion": "in this paper, we propose a new concept, logical reasoning over natural language as knowledge representation (lrnl), and provide a detailed and up-to-date review of lrnl. moreover, we have introduced the philosophical foundations, advantages of lrnl, benchmarks and methods, challenges, desirable tasks & methods, and the relation of lrnl to related nlp fields ( \u00a7a.1).", "Solved": NaN, "Unsolved": NaN}, "219171199": {"num": 151, "type": 1, "title": "A Cognitive Method for Automatically Retrieving Complex Information on a Large Scale", "pdfurl": NaN, "conclusion": "we present a new framework 2scr-ir sensor to tackle multi-hop retrieval problems on a large scale, which retrieves reasoning paths over the cognitive graph to provide users with useful explicit evidence chains. our retriever model learns to sequentially retrieve evidence paragraphs to construct reasoning paths, which is subsequently re-ranked by the sensor that determines the final information presented as the one extracted from the best reasoning path. our retriever obtains state-of-the-art results using the hotpotqa dataset, which shows the efficiency of our framework. the state-of-theart performance on squad is achieved, demonstrating the robustness of our method. besides, our analysis shows that 2scr-ir can produce reliable and explainable reasoning chains. in the future, we may incorporate new advances in building cognitive graphs from the web context to solve more difficult reasoning problems.", "Solved": NaN, "Unsolved": NaN}, "260440449": {"num": 152, "type": 1, "title": "LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS", "pdfurl": "https://arxiv.org/pdf/2011.04006v1.pdf", "conclusion": "we proposed long range arena (lra), a new benchmark for evaluating progress on efficient transformer research. our new benchmark is challenging and probes at model capabilities in dealing with diverse data types and structures such as text, mathematics, and visual data. our benchmark comprises of tasks ranging from 1k to 16k tokens. for the first time, we conduct an extensive side-by-side comparison of ten recently proposed efficient transformer models. the experimental results show that these tasks are very challenging even for long-range transformer models. the overall results show that there is no one-size-fits-all solution and trade-offs have to be made in terms of model quality and speed/memory. we plan to open source our code and benchmarks to facilitate future benchmarking, research and model development.", "Solved": NaN, "Unsolved": NaN}, "256461186": {"num": 153, "type": 1, "title": "Graph-Induced Transformers for Efficient Multi-Hop Question Answering", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.702.pdf", "conclusion": "this work presented git, the graph-induced transformer that drastically improved mhqa models' sample efficiency and replaces graphs in the models while retaining their performance. our empirical evidences demonstrated that models can enjoy the benefits of connective inductive bias of graphs without additional graph modules in place. the design of git also allowed us to reuse the parameters of plm while incorporating the graph information. future directions of our work may include using git in downstream nlp applications where the graph inductive bias is necessary and dataset is scarce.", "Solved": NaN, "Unsolved": NaN}, "211126663": {"num": 154, "type": 1, "title": "Transformers as Soft Reasoners over Language", "pdfurl": "https://arxiv.org/pdf/2002.05867v2.pdf", "conclusion": "just as mccarthy advocated 60 years ago for machines reasoning (\"taking advice\") in logic, we have shown (in a restricted setting) that machines can by trained to reason over language. while we have assumed a particular semantics of inference, the methodology we have used is general: characterize the desired behavior in a formal way, synthesize examples, generate linguistic equivalents, and train a model. the result, at least within our experiments, appears to be both nat-ural and robust, in a way distinct from working with the original formalization.", "Solved": NaN, "Unsolved": NaN}, "218595722": {"num": 155, "type": 1, "title": "Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.599.pdf", "conclusion": "in this work, we present a novel multi-grained mrc framework based on graph attention networks and bert. we model documents at different levels of granularity to learn the hierarchical nature of the document. on the natural questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers. the experiments show that our proposed methods are effective and outperform the previously existing methods by a large margin. improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals. besides, the currently existing methods actually cannot process a long document without truncating or slicing it into fragments. how to model long documents is still a problem that needs to be solved.  question: what 's the dog 's name on tom and jerry long answer: tom ( named \" jasper \" in his debut appearance ) is a grey and white domestic shorthair cat . \" tom \" is a generic name for a male cat . he is usually but not always , portrayed as living a comfortable , or even pampered life , while jerry ...", "Solved": NaN, "Unsolved": NaN}, "261951885": {"num": 156, "type": 1, "title": "QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation", "pdfurl": "https://export.arxiv.org/pdf/2309.10326v2.pdf", "conclusion": "in this paper, we propose qasnowball, a novel iterative bootstrapping framework that can continually generate high-quality and large-scale qa data.", "Solved": NaN, "Unsolved": NaN}, "253157773": {"num": 157, "type": 1, "title": "COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning", "pdfurl": "https://export.arxiv.org/pdf/2210.15212v2.pdf", "conclusion": "coco-dr improves zerodr accuracy by combating the distribution shifts using continuous contrastive learning and implicit distributionally robust optimization. coco helps models better capture the sequence representations of target corpora in pretraining. implicit dro improves model robustness by reweighting query clusters in fine-tuning.", "Solved": NaN, "Unsolved": NaN}, "252367252": {"num": 158, "type": 1, "title": "ScreenQA: Large-Scale Question-Answer Pairs Over Mobile App Screenshots", "pdfurl": "https://export.arxiv.org/pdf/2209.08199v1.pdf", "conclusion": "in this work, we proposed the screenqa task. we annotated a large-scale screenqa dataset, which contains more than 80,000 question-answer pairs. compared to other vision-language multimodal problems, such as document image understanding and visual question answering, screenqa poses its unique challenges: rich in text, diverse in apps, and blended with icons and symbols. we hope to use the screenqa task and the dataset to encourage the community to look into this screen content understanding problem, as it enables new technologies and new user experiences. figure 4: data annotation interfaces for question and answer collection. a) question annotation was performed in a sequential manner, the later and non-overlapping annotators can see all previous questions to diversify question framing and avoid duplication. we also used the sequential process to provide more feedback and training to the annotators for quality improvement. b) the answer annotators were tasked to determine if the question is valid and if the question is answerable from the screen context. if both are positive, the annotators need to answer the questions by 1) selecting or drawing the bounding boxes of ui elements, 2) fill the text for each selected/drawn bounding box on right right, and 3) ranking them appropriately. the annotators were also tasked to review and make necessary corrections if the question has grammatical errors or typos.  a) the two question annotation passes were capped at five and three questions, respectively, resulting in the maximum eight questions in total. b) the cases when a single bounding box forms a sufficient answer amount to 92% of the questions, hence removed from the chart for the clarity of the long tail. anything beyond 11 bounding boxes is less than 0.05%, accumulatively less than 0.1%.", "Solved": NaN, "Unsolved": NaN}, "252918165": {"num": 159, "type": 1, "title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.105.pdf", "conclusion": "in this work, we investigate performing it with unlabeled data for zero-shot cross-task generalization. we first empirically find that the it performance is largely restricted by the number of distinct tasks, instructions, and training samples in data-scarce tasks. then, we propose udit to take better advantage of the instructions by constructing pseudo-labeled data from the unlabeled plain texts. through udit, it is possible to perform it with unlabeled data when there are few or no humanannotated samples, which offers a better way to incorporate unlabeled data compared with other approaches. through comprehensive analysis, we find that the domain diversity and the matching between the pseudo-labeled data and corresponding instructions are essential for udit. in contrast, noises in individual task clusters and colossal data amount are less influential. there are three directions for future work: (1) designing automatic and generalizable methods to construct pseudo-labeled data for instruction tuning.", "Solved": NaN, "Unsolved": NaN}, "234742165": {"num": 160, "type": 1, "title": "FEW-NERD: A Few-shot Named Entity Recognition Dataset", "pdfurl": "https://www.aclanthology.org/2021.acl-long.248.pdf", "conclusion": "we propose few-nerd, a large-scale few-shot ner dataset with fine-grained entity types. this is the first few-shot ner dataset and also one of the largest human-annotated ner dataset. few-nerd provides three unified benchmarks to assess approaches of few-shot ner and could facilitate future research in this area. by implementing state-of-the-art methods, we carry out a series of experiments on few-nerd, demonstrating that few-shot ner remains a challenging problem and worth exploring. in the future, we will extend few-nerd by adding cross-domain annotations, distant annotations, and finer-grained entity types. few-nerd also has the potential to advance the construction of continual knowledge graphs.", "Solved": NaN, "Unsolved": NaN}, "237347226": {"num": 161, "type": 1, "title": "Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning", "pdfurl": "https://export.arxiv.org/pdf/2104.08808v4.pdf", "conclusion": "we present the continual learning of few-shot learners (clif) challenge to simulate the scenario where a learner continually accumulate (generalizable) knowledge over a sequence of nlp tasks, while retaining its performance on the seen tasks. we propose evaluation protocols to study the performance of existing continual learning algorithm, and present our method bihnet-reg. we demonstrate the potentials of building a nlp system that, through continual training, can perform more tasks and also become more efficient in mastering new tasks. future works include extending our work to task agnostic scenarios where the distribution of data may shift continuously and studying algorithms for continual refinement of large-scale pre-trained models with emerging unlabeled data.   ", "Solved": NaN, "Unsolved": NaN}, "235794845": {"num": 162, "type": 1, "title": "Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking", "pdfurl": "https://arxiv.org/pdf/2107.05002v2.pdf", "conclusion": "in this paper we propose a multilingual extractive rc modeling approach named xltt, by a crosslingual transposition rethinking approach for lowresource extractive reading comprehension using multilingual adaptive attention to model existing erc training datasets in a multilingual context.experimental results demonstrate the effectiveness of our multilingual erc modeling approach.", "Solved": NaN, "Unsolved": NaN}, "259203489": {"num": 163, "type": 1, "title": "GLIMMER: generalized late-interaction memory reranker", "pdfurl": "https://export.arxiv.org/pdf/2306.10231v1.pdf", "conclusion": "retrieval-augmented language models are powerful but slow in inference, while pre-computed memory-augmented models are fast at the cost of quality. hybrid late-interaction models such as lu-men present a good quality-compute trade-off. we introduce glimmer, an improved late-interaction model that also incorporates learned end-to-end reranking and multi-task training to achieve an even better trade-off. glimmer achieves strong gains in quality at faster speeds compared to lumen and fid on the kilt benchmark of knowledgeintensive tasks.", "Solved": NaN, "Unsolved": NaN}, "250390946": {"num": 164, "type": 1, "title": "Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering", "pdfurl": "https://www.aclanthology.org/2022.suki-1.7.pdf", "conclusion": "in this paper, we study cqa over structured and unstructured knowledge sources (i.e., kb and text particularly), and focus on studying the knowledge transfer between different knowledge sources. to facilitate the transfer, we first propose a unified cqa framework, simultqa to bridge kbqa and textqa systems. empirical results show that knowledge transfer enables substantial improvements on low-resource domains. more importantly, we conduct fine-grained analyses to shed more light on how knowledge is transferred to inspire future research on knowledge transfer between sources, and we conclude the paper with insights for future cqa datasets and systems.", "Solved": NaN, "Unsolved": NaN}, "250390947": {"num": 165, "type": 1, "title": "Explicit Graph Reasoning Fusing Knowledge and Contextual Information for Multi-hop Question Answering", "pdfurl": "https://www.aclanthology.org/2022.dlg4nlp-1.8.pdf", "conclusion": "in this paper, we apply explicit graph reasoning to extracted knowledge and contextual information for multi-hop reasoning. we extract clues at multiple levels of granularity relating entity nodes, and construct a semantic graph from these clues. we then combine a masked attention mechanism and two-stage graph reasoning to perform interpretable inference over the semantic graph. experimental results on hotpotqa dataset show the effectiveness of our model. in future work, we hope to extend the range and precision of the entity relations used, and we hope to extend our model to accommodate more complex multi-hop questions with unknown number of hops and non-linear reasoning.", "Solved": NaN, "Unsolved": NaN}, "247411106": {"num": 166, "type": 1, "title": "LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2203.06169v2.pdf", "conclusion": "in this paper, we introduce laprador, an unsupervised pretrained dense retriever that achieves state-of-the-art performance on the zero-shot text retrieval benchmark beir.we propose iterative contrastive learning (icol) for efficiently training laprador and lexicon-enhanced dense retrieval (ledr) to combine lexical matching with laprador.our experiments verify the effectiveness of both icol and ledr, shedding light on a new paradigm for unsupervised text retrieval.for future work, we plan to extend unsupervised laprador to multilingual and multi-modal retrieval.", "Solved": NaN, "Unsolved": NaN}, "253237669": {"num": 167, "type": 1, "title": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.142.pdf", "conclusion": "this work proposes a novel method that extracts distant and incidental signals from parallel news to facilitate general question representation. such parallel news signals intuitively bridge the reasoning gap in pre-trained language models due to reporting biases. to support this intuition, we train a model named decompt5 on such distant supervision and show that it improves 20%-30% on two semantic parsing benchmarks, namely overnight and torque, that directly evaluate query understanding. with decompt5 as the basis, we design a well-motivated question-answering pipeline decompentail that follows a decomposition, correction, and entailment scheme. we show that decompentail improves on strategyqa and hotpotqa by 3.7% and 8%, respectively.", "Solved": NaN, "Unsolved": NaN}, "253581733": {"num": 168, "type": 1, "title": "Task-aware Retrieval with Instructions", "pdfurl": "https://export.arxiv.org/pdf/2211.09260v2.pdf", "conclusion": "this paper lays the foundation for building a general-purpose task-aware retriever that can follow natural language instructions. we introduced a new problem, retrieval with instructions, to model users' intents explicitly. we presented berri, the first large-scale retrieval dataset with expert-written annotations. building upon berri, we trained the first instruction-following retrieval system by massive multi-task instruction-tuning, tart, adopting two widely used architectures. tart advances the state of the art on the popular zeroshot retrieval benchmarks beir and lotte as well as on our newly introduced challenging evaluation setup, x 2 -retrieval. our analysis shows that key factors to building a successful multi-task instruction-following retrieval system include informative instructions at training and test time, diversity in data and model scale, and carefully designed negative samples. we conclude with two interesting open questions, which future work can explore.", "Solved": NaN, "Unsolved": NaN}, "232335785": {"num": 169, "type": 1, "title": "Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2", "pdfurl": "https://arxiv.org/pdf/2103.13033v1.pdf", "conclusion": "in this paper, we introduce chainruler, a dataset for multi-hop deductive argumentation, and assess gpt-2's zero-shot ability both to solve the inference tasks and to generate effective problem elaborations, i.e., texts which -once added to the context -improve performance. our main findings are:", "Solved": NaN, "Unsolved": NaN}, "226236844": {"num": 170, "type": 1, "title": "Exploring Question-Specific Rewards for Generating Deep Questions", "pdfurl": "https://www.aclweb.org/anthology/2020.coling-main.228.pdf", "conclusion": "in this paper, we optimize three question-specific rewards via reinforcement learning on a seq2seq based question generator, aiming to improve the fluency, relevance and answerability of the generated questions. through comprehensive analytic experiments, including automatic and human evaluation, consistency validation, and meso analysis, we show that the effectiveness of a reward is poorly reflected by automatic evaluation metrics such as bleu. instead, we find a reward that correlates well with the human judgement generally has better effects on improving the question quality. in future works, we believe these observations can help to guide the design of other qg-specific rewards that target on unexplored aspects of question generation, such as the informativeness and the utility of questions.", "Solved": NaN, "Unsolved": NaN}, "262013357": {"num": 171, "type": 1, "title": "Investigating Answerability of LLMs for Long-Form Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2309.08210v1.pdf", "conclusion": "with the emergence of llms like chatgpt and open-source successful llms, it is extremely important to understand the capabilities and limitations of different llms.in order to test deeper reasoning abilities of llms by referring to longer contexts, we evaluate answers generated by llms on questions generated by chatgpt on summaries of long documents.results show that our proposed method of question generation poses a challenging setup for llms and shed light on performance gaps between massive llms and open-source llms.we hope our analysis motivates future research directions such as leveraging longer contexts in a constrained sequence length setting and developing better long-form text generation for smaller llms.", "Solved": NaN, "Unsolved": NaN}, "256358835": {"num": 172, "type": 1, "title": "Graph Attention with Hierarchies for Multi-hop Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2301.11792v1.pdf", "conclusion": "in this paper, we proposed two extensions to hierarchical graph network (hgn) for the multihop question answering task on hotpotqa. first, we completed the hierarchical graph structure by adding new edges between the query and context sentence nodes. second, we introduced gath as the mechanism for neural node updates, a novel extension to gat that can update node representations sequentially, based on hierarchical levels. to the best of our knowledge, this is the first time the hierarchical graph structure is directly exploited in the update mechanism for information propagation.", "Solved": NaN, "Unsolved": NaN}, "215785913": {"num": 173, "type": 1, "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data", "pdfurl": "https://arxiv.org/pdf/2004.07347v3.pdf", "conclusion": "we present hybridqa, which is collected as the first hybrid question answering dataset over both tabular and textual data. we release the data to facilitate the current research on using heterogeneous information to answer real-world questions. we design hybrider as a strong baseline and offer interesting insights about the model. we believe hybridqa is an interesting yet challenging nextproblem for the community to solve.", "Solved": NaN, "Unsolved": NaN}, "226281918": {"num": 174, "type": 1, "title": "Synonym Knowledge Enhanced Reader for Chinese Idiom Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/2020.coling-main.329.pdf", "conclusion": "in this paper, we give a quantitative analysis to prove that the literal meanings of many chinese idioms are far from their semantics, and also verify that the synonymic relationship can mitigate this inconsistency, which is beneficial for chinese idiom reading comprehension. we propose the synonym knowledge enhanced reader to fully utilize the relationship. experimental results show that our model achieves state-of-the-art performance among different settings of chid, a large-scale chinese idiom reading comprehension dataset. similar to chinese idioms, the inconsistency exists in a number of other language elements, e.g., slangs in english, where the potential use of the synonymic relationship among them requires a further exploration.", "Solved": NaN, "Unsolved": NaN}, "258378176": {"num": 175, "type": 1, "title": "Combining Parameter-efficient Modules for Task-level Generalisation", "pdfurl": "https://www.aclanthology.org/2023.eacl-main.49.pdf", "conclusion": "we argued that a modular design is crucial to ensure that neural networks can learn from a few examples and generalise robustly across tasks by recombining autonomous facets of knowledge. to this end, we proposed a model where a subset of latent, discrete skills from a fixed inventory is allocated to each task in an end-to-end fashion. the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters. we evaluate the sample efficiency of our model on multitask instruction following through reinforcement learning and its few-shot adaptability on multitask text-to-text generation through supervised learning. in both experiments, we surpass competitive baselines such as conditional parameter generation (hyperformer) and mixture of experts (task-moe). finally, we show that modularity helps interpret multi-task models by inferring explicit relationships between tasks according to the skills they share.", "Solved": NaN, "Unsolved": NaN}, "263829952": {"num": 176, "type": 1, "title": "Policy-Gradient Training of Language Models for Ranking", "pdfurl": "https://export.arxiv.org/pdf/2310.04407v1.pdf", "conclusion": "in this work, we introduce neural pg-rank, a novel training algorithm designed to address challenges associated with training llm-based retrieval models.as a rigorous approach that reduces the dependence on intricate heuristics and directly optimizes relevant ranking metrics, neural pg-rank has demonstrated its effectiveness when training objective aligns with evaluation setup -specifically, in the context of second-stage reranking -by exhibiting remarkable in-domain performance improvement and presenting subtantial out-of-domain generalization to some critical datasets employed in downstream question answering.our work establishes a principled bridge between training objectives and practical utility of the collective set of retrieved results, thereby paving the way for future research endeavors aimed at constructing highly effective retrieval-based llm pipelines that are tailored for practical applications.", "Solved": NaN, "Unsolved": NaN}, "258841029": {"num": 177, "type": 1, "title": "Improving Language Models via Plug-and-Play Retrieval Feedback", "pdfurl": "https://export.arxiv.org/pdf/2305.14002v1.pdf", "conclusion": "in conclusion, this paper presents a novel pipeline, refeed, designed to improve large language models' performance in a plug-and-play framework, effectively addressing the challenges arising from knowledge-intensive tasks. by employing a retrieval method to provide automatic feedback on generated outputs and integrating this feedback to refine the outputs without the need for expensive fine-tuning, refeed offers a practical and efficient solution. we introduce two innovative modules within the refeed pipeline: diverse answer generation and an ensemble approach. these two modules further enhance refeed to produce more reliable and accurate answers by considering a wider array of retrieved documents and mitigating the risk of misleading retrieval feedback. our extensive experiments on four challenging knowledgeintensive benchmarks demonstrate the effectiveness of refeed in achieving state-of-the-art performance under the few-shot setting. we believe by continuing to refine and optimize the refeed pipeline, we can unlock its full potential and ex-pand its applicability across a diverse range of scenarios and applications.", "Solved": NaN, "Unsolved": NaN}, "261048776": {"num": 178, "type": 1, "title": "ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2308.10025v1.pdf", "conclusion": "in this paper, we present controlretriever, a generic and efficient approach capable of controlling retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions. with a parameter-isolated architecture, con-trolretriever effectively preserves the original capability of the retrieval model, meanwhile efficiently empowering it with a new facet of controllable retrieval conditioned on task-specific instructions. furthermore, we also propose a novel llm-guided instruction synthesizing and iterative training (list) strategy, which iteratively trains controlretriever based on extensive automatically-generated retrieval data with diverse instructions, capitalizing the advancement of large language models. extensive experiments demonstrate the superior zero-shot transferability of controlretriever on various retrieval tasks.", "Solved": NaN, "Unsolved": NaN}, "246485449": {"num": 179, "type": 1, "title": "JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2202.01764v1.pdf", "conclusion": "in this paper, we proposed the japanese question answering dataset, jaquad. we collected the contexts from japanese wikipedia articles and 39k+ questions were manually annotated by fluent japanese speakers. jaquad has the same format as squad, and the characteristics of the data are generally similar to korquad 1.0. in the experiments, we fine-tuned a japanese pre-trained language model with jaquad as a baseline and achieved 78.92% for f1 score and 63.38% for em on test set. the baseline reaches promising results, but there is plenty of room for improvement. extension of the dataset, such as covering longer answers, is left for future work. the dataset and our experiments are available at https://github.com/skelterlabsinc/jaquad.", "Solved": NaN, "Unsolved": NaN}, "252873161": {"num": 180, "type": 1, "title": "How (Not) To Evaluate Explanation Quality", "pdfurl": "https://export.arxiv.org/pdf/2210.07126v1.pdf", "conclusion": "this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face. we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate those characteristics. finally, we propose guidelines for a more effective evaluation, which we hope to inspire future work and ultimately drive the field towards reliable and meaningful explanation quality evaluation. our discussion is backed up with examples, well-known theories and empirical findings from a crowdsourced case study that we conducted for the example of explainable question answering systems.", "Solved": NaN, "Unsolved": NaN}, "258832847": {"num": 181, "type": 1, "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2305.11747v3.pdf", "conclusion": "we introduce halueval, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing hallucinations.to automatically generate large-scale samples, we propose a two-step approach, i.e., sampling-then-filtering.we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one.besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.we find that, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated content.finally, we suggest several strategies to help llms recognize hallucinations.our benchmark can facilitate research in understanding what types of content and to which extent llms tend to hallucinate, ultimately paving the way for building more effective and reliable llms in the future.", "Solved": NaN, "Unsolved": NaN}, "233296336": {"num": 182, "type": 1, "title": "Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models", "pdfurl": "https://arxiv.org/pdf/2104.08410v1.pdf", "conclusion": "in this paper, we propose an evaluation pipeline for pretrained models by testing their transferability without word identity information. specifically, we take an english pretrained bert off-the-shelf and fine-tune it with a scrambled english dataset. we conduct analyses across six tasks covering both classification and sequence labeling. by evaluating performance against multiple baselines, we aim to assess where bert can transfer knowledge even without word identities. we find considerable transfer for bert as compared to even powerful baselines, by only for classification tasks.", "Solved": NaN, "Unsolved": NaN}, "249848272": {"num": 183, "type": 1, "title": "UNIFIED-IO: A UNIFIED MODEL FOR VISION, LANGUAGE, AND MULTI-MODAL TASKS", "pdfurl": "https://export.arxiv.org/pdf/2206.08916v2.pdf", "conclusion": "for object detection, while unified-io generally produces accurate outputs (see appendix a.4), we find the recall is often poor in cluttered images. prior work (chen et al., 2022b) has shown this can be overcome with extensive data augmentation techniques, but these methods are not currently integrated into unified-io. our use of a pre-trained vq-gan greatly simplifies our training and is surprisingly effective for dense prediction tasks. however, it does mean unified-io has limited image generation capabilities (recent works (yu et al., 2022b) have shown this method can be greatly improved but was not available at the time of development). we also found in a small-scale study that our model does not always understand prompts not in the training data (see appendix 4.5).", "Solved": NaN, "Unsolved": NaN}, "247292369": {"num": 184, "type": 1, "title": "Feeding What You Need by Understanding What You Learned", "pdfurl": "https://www.aclanthology.org/2022.acl-long.403.pdf", "conclusion": "we design a competency assessment framework for mrc capabilities, which describes model skills in an explainable and multi-dimensional manner. by leveraging the framework, we further uncover and disentangle the connections between various data properties and model performance on a specific task, as well as propose a capability boundary breakthrough curriculum (cbbc) strategy to maxi-mize the data value and improve training efficiency. the experiments performed on four benchmark datasets verified that our approach can significantly improve the performance of existing mrc models. our work shows a deep understanding of model capabilities and data properties helps monitor the model skills during training and improves learning efficiency. our framework and learning strategy are also generally applicable to other nlp tasks. ", "Solved": NaN, "Unsolved": NaN}, "246294995": {"num": 185, "type": 1, "title": "Reasoning Like Program Executors", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.48.pdf", "conclusion": "we introduce poet, a new pre-training paradigm for boosting reasoning capability of language models via imitating program executors. experimental results on six datasets demonstrate that poet can significantly boost existing language models on several reasoning skills, including numerical, logical and multi-hop reasoning. our best language model under poet can reach highly competitive performance with previous specialized models. in the future, we hope our work could inspire more transference of reasoning knowledge from program executors to models. and we will also investigate the causes of the reasoning transfer with more insightful experiments, since we still do not know how the reasoning transfer occurs.", "Solved": NaN, "Unsolved": NaN}, "248426967": {"num": 186, "type": 1, "title": "HYBRIDIALOGUE: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.41.pdf", "conclusion": "in this paper, we presented a novel dataset, hy-bridialogue, for information-seeking dialogue where knowledge is grounded in both tables and text. while previous work has combined table and text modality in the question-answering space, this has not been utilized in the dialogue setting. our results in the various tasks demonstrate that there is still significant room for improvement and illustrate the need to build models that can adapt well to this hybrid format. in addition to the baseline tasks, future research can utilize hybridialogue to explore automatic multihop question decomposition.", "Solved": NaN, "Unsolved": NaN}, "232307160": {"num": 187, "type": 1, "title": "Self-Supervised Test-Time Learning for Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.95.pdf", "conclusion": "in this work, we propose test-time learning (ttl) as a new framework for unsupervised extractive question answering (eqa). we present four variants of ttl with a simple but effective context expansion method. we utilize four question-answer pair generation methods for eqa and propose using qa-srl as an additional source of qa pairs, to supplement prior methods. we show ttl enables \"understanding\" of contexts at test-time, without human-authored annotations, and significantly improves eqa, including low parameter models. we envision ttl as a framework that can direct work in reading comprehension to be viewed as a problem of ever-evolving datasets instead of a static corpus. natural language itself undergoes continuous evolution (gentner and france, 1988;traugott and dasher, 2001;hamilton et al., 2016) via changes in preference for syntactical structures; creation of new words and phrases; and changing usage frequencies and semantics for existing words. ttl can potentially be applied to such scenarios with semantic drift or domain shift. further improvements w.r.t. selection of similar contexts for k-neighbor ttl could be explored by leveraging hard sample selection, hard negative mining, bootstrapping, and contrastive learning, along with improved currculum strategies.", "Solved": NaN, "Unsolved": NaN}, "189927896": {"num": 188, "type": 1, "title": "Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA", "pdfurl": "https://arxiv.org/pdf/1906.07132v1.pdf", "conclusion": "in this work, we identified reasoning shortcuts in the hotpotqa dataset where the model can locate the answer without multi-hop reasoning. we constructed adversarial documents that can fool the models exploiting the shortcut, and found that the performance of a state-of-the-art model dropped significantly under our adversarial examples. we showed that this baseline can improve on the adversarial evaluation after being trained on the adversarial data. we next proposed to use a control unit that dynamically attends to the question to guide the bi-attention in multi-hop reasoning. trained on the regular data, this 2-hop model is more robust against the adversary than the baseline; and after being trained with adversarial data, this model achieved further improvements on the adversarial evaluation and also outperforms the baseline. overall, we hope that these insights and initial improvements will motivate the development of new models that combine explicit compositional reasoning with adversarial training.", "Solved": NaN, "Unsolved": NaN}, "260887004": {"num": 189, "type": 1, "title": "Performance Prediction for Multi-hop Questions", "pdfurl": "https://export.arxiv.org/pdf/2308.06431v1.pdf", "conclusion": "in this paper, we introduce the task of query performance prediction for multi-hop questions. we present an approach to estimate a difficulty score of a multi-hop question based on the clues in the question. we propose retrieval paths based on overlapping terms between the question and its supporting documents. our experimental evaluation shows significant correlations between the performance of the retrievers used in our evaluation and our estimated difficulty scores, and those correlations are much higher than those obtained by our qpp baselines from the literature. the same trend is observed for the end-to-end models with the performance 7 https://github.com/mhmdsmdi/performance-prediction-for-multihop-qa figure 3: performance, in terms of f1-score, of mdr [37] with the adaptive retriever compared to a constant retriever while k varied, showing that the adaptive retriever achieves a higher performance under the same budget considerably dropped for the questions that are deemed difficult by our model. determining the difficulty of a multi-hop question using a pre-retrieval method can assist the retrievers to have a better chance of retrieving all required documents to answer the question. as a possible future direction, our models may be improved by considering more refined retrieval path types and maybe parameter settings. also analyzing the performance of down-stream tasks using our difficulty score estimation is another possible direction.", "Solved": NaN, "Unsolved": NaN}, "252715485": {"num": 190, "type": 1, "title": "Decomposed Prompting : A MODULAR APPROACH FOR SOLVING COMPLEX TASKS", "pdfurl": "https://export.arxiv.org/pdf/2210.02406v2.pdf", "conclusion": "we proposed a new approach, decomposed prompting, to solve complex tasks using few-shot prompts, by decomposing them into a prompting program built out of simpler sub-tasks. drawing inspiration from software libraries, our decomposer and shared sub-tasks are designed in a modular fashion: they use their own few-shot prompts, allowing one to independently optimize each prompt, decompose a sub-task further if necessary, or even seamlessly replace it with a symbolic system. we show that decomposed prompting outperforms prior work on four different tasks and generalization settings, establishing it as an effective few-shot paradigm for solving complex tasks. we treat the number of paragraphs to retrieve (k) in nodecomp-ctxt and decomp-ctxt models as a hyperparameter. we select it based on a grid search on a set of values to maximize performance on a held out set of 100 questions for each dataset. for nodecomp-ctxt, we search k \u2208 {6, 8, 10} for gpt3 models and k \u2208 2, 4, 6, 8 for flan-t5-* models. for decomp-ctxt, we search k \u2208 {2, 4, 6} for gpt3 and flan-t5-* models. note that the ranges are different between gpt3 and flan-t5-* as gpt3 can fit in more number of tokens. the ranges are different for nodecomp-ctxt and decomp-ctxt as k refers to number of paragraphs retrieved in each round of retrieval, and nodecomp-ctxt has only one step of retrieval whereas decomp-ctxt usually has multiple retrieval steps.", "Solved": NaN, "Unsolved": NaN}, "259924447": {"num": 191, "type": 1, "title": "CodeQueries: A Dataset of Semantic Queries over Code", "pdfurl": "https://export.arxiv.org/pdf/2209.08372v2.pdf", "conclusion": "we presented the codequeries dataset to test the ability of neural models to understand code semantics on the proposed problem of answering semantic queries over code. it requires a model to perform single-or multi-hop reasoning, understand structure and semantics of code, distinguish between positive and negative examples, and accurately identify answer and supporting-fact spans. our evaluation shows that codequeries is challenging for the best-in-class generative and embedding approaches under different prompting or fine-tuning settings. we are considering extensions to our dataset to include more semantic queries and more programming languages.", "Solved": NaN, "Unsolved": NaN}, "238744031": {"num": 192, "type": 1, "title": "ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers", "pdfurl": "https://www.aclanthology.org/2022.acl-long.253.pdf", "conclusion": "we propose a challenging dataset conditionalqa that contains questions with conditional answers.  table 4: em/f1 w/ conditions on the subset of questions with conditional answers. \"best overall\" uses the best checkpoints/hyper-parameters on the full dataset, while \"best conditional\" uses the best ones on the subset of questions.", "Solved": NaN, "Unsolved": NaN}, "256461282": {"num": 193, "type": 1, "title": "Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.356.pdf", "conclusion": "we take a step forward in constructing the explainable method for multi-hop question answering by proposing two effective improvements. the global differentiable learning strategy learns optimal reasoning paths by exploring latent probability space to alleviate the problem of semantic space mismatch and error propagation. the dynamic adaptive reasoner improves generalization to unseen sub-questions.  figure 4: case study. the green font represents the correct predicted answer, and the red font represents the incorrect. our method successfully learns the intermediate reasoning process and shows better interpretability.", "Solved": NaN, "Unsolved": NaN}, "57721315": {"num": 194, "type": 1, "title": "Multi-Style Generative Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/1901.02262v2.pdf", "conclusion": "this study sheds light on multi-style generative rc. our proposed model, masque, is based on multi-source abstractive summarization and learns multi-style answers together. it achieved stateof-the-art performance on the q&a task and the q&a + nlg task of ms marco 2.1 and the summary task of narrativeqa. the key to its success is transferring the style-independent nlg capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder. in particular, the capability of copying words from the question and passages can be shared among the styles, while the capability of controlling the mixture weights for the generative and copy distributions can be acquired for each style. our future work will involve exploring the potential of our multi-style learning towards natural language understanding.", "Solved": NaN, "Unsolved": NaN}, "258865636": {"num": 195, "type": 1, "title": "TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering", "pdfurl": "https://export.arxiv.org/pdf/2305.14682v1.pdf", "conclusion": "this paper presents tacr, a ", "Solved": NaN, "Unsolved": NaN}, "260334056": {"num": 196, "type": 1, "title": "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2307.16877v1.pdf", "conclusion": "below, we highlight several key findings of this paper and discuss some of its limitations.which evaluation metrics are best? our analysis on correctness ( \u00a74) and faithfulness ( \u00a75) demonstrates that widely-used metrics are not suitable for evaluating the correctness (due to errors such as elaborate answers, open-ended questions, and list of named-entities) and faithfulness (due to partially grounded responses). correlating the metrics with human judgements (table 2 and table 5) reveals that recall and gpt4-eval are the best lexical and model-based metrics for correctness and k-precision and llmcritic (gpt-4) are the best lexical and model-based metrics for faithfulness, respectively. however, these model-based metrics, especially the ones based on llms, are usually slow to run, expensive, difficult to reproduce, and may exhibit systematic biases. while we propose that recall and k-precision are the most widely-accessible and human-aligned metrics for correctness and faithfulness, respectively, we emphasize that these simple lexical-based metrics are easy to hack. one model can copy all the retrieved knowledge as the output, leading to high recall and k-precision metrics. however, such a model will be penalized heavily when evaluated for faithfulness w.r.t. irrelevant knowledge.instruction-following models according to the most human aligned and easy to use metrics (i.e., recall and k-precision), we conclude that gpt-3.5 outperforms other models on majority of the datasets in correctness w.r.t information need. however, when analyzing the faithfulness w.r.t relevant knowledge, flan-t5 is shown to be the best model in all three datasets. moreover, our further analysis on the models' faithfulness w.r.t irrelevant knowledge demonstrates that models struggle to correctly identify whether the provided knowledge is relevant or not.limitations it is worth mentioning that the experiments for evaluating the faithfulness of the models are conducted in a modified setting, where a relevant or irrelevant passage is provided in the prompt on purpose. this is different from the real-world scenario, where the retrieved passages can contain a mix of relevant and irrelevant knowledge.finally, it should also be noted that beyond qualitative investigation, we did not explore a wide range of prompts for the tasks studied in this work. recent work has shown that the performance of instruction-following models can vary greatly depending upon the provided prompt (zhao et al., 2021;liu et al., 2023b). we leave it to future works to investigate better prompts for instructionfollowing models in a retrieval-augmented setting.", "Solved": NaN, "Unsolved": NaN}, "252692968": {"num": 197, "type": 1, "title": "RECITATION-AUGMENTED LANGUAGE MODELS", "pdfurl": "https://export.arxiv.org/pdf/2210.01296v2.pdf", "conclusion": "in this paper, we propose a novel recitation-augmented generation framework to improve language models' performance in the closed-book question-answering setting. we hypothesize that for knowledge-intensive nlp tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. in addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. we show promising results over three large language models and across three different closed-book qa datasets, demonstrating the effectiveness of our proposed recite-and-answer approach.", "Solved": NaN, "Unsolved": NaN}, "218487272": {"num": 198, "type": 1, "title": "What-if I ask you to explain: Explaining the effects of perturbations in procedural text", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.300.pdf", "conclusion": "explaining the effects of a perturbation is critical, and we have presented the first system that can do this reliably. quartet not only predicts meaningful explanations, but also achieves a new state-of-the-art on the end-task itself, leading to an interesting finding that models can make better predictions when forced to explain. our work opens up new directions for future research: 1) can additional background context from the web improve explainable reasoning? 2) can such structured explanations be applied to other nlp tasks? we look forward to future progress in this area.", "Solved": NaN, "Unsolved": NaN}, "234334701": {"num": 199, "type": 1, "title": "REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training", "pdfurl": "https://arxiv.org/pdf/2105.04201v2.pdf", "conclusion": "in this paper, we present a novel pre-training approach, rept, to bridge the gap between pretrained language models and machine reading comprehension through retrieval-based pre-training. specifically, we design two retrieval-based pretraining tasks equipped with self-supervised learning, namely surrounding sentences prediction (ssp) and retreval based masked language modeling (rmlm), to enhance plms with the capability of evidence extraction for mrc. the experiments over five different datasets validate the effectiveness of our proposed method. in the future, we plan to extend the proposed pre-training approach to the more challenging open-domain settings.", "Solved": NaN, "Unsolved": NaN}, "252531182": {"num": 200, "type": 1, "title": "Evaluation of Question Answering Systems: Complexity of judging a natural language", "pdfurl": "https://export.arxiv.org/pdf/2209.12617v1.pdf", "conclusion": "the question-answering task is among the oldest challenges in artificial intelligence and it is still one of the most important tasks in natural language processing to this day as it enables humans to interact with a machine in a natural way. for dealing with domain-specific information and different formats of data which could be either structured or unstructured data there are three main paradigms for constructing qa systems: (1) information retrieval-based question answering, (2) knowledge base question answering, and (3) generative question answering.", "Solved": NaN, "Unsolved": NaN}, "226226674": {"num": 201, "type": 1, "title": "CHIME: Cross-passage Hierarchical Memory Network for Generative Review Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2020.coling-main.229.pdf", "conclusion": "in this paper, we have proposed chime, a cross-passage hierarchical memory network for multi-passage generative review qa. it is built on the xlnet generator  by adding a memory module consisting of a context and a answer memory which guarantees a more accurate refining process for crosspassage evidence collection and answer generation. the sequential process adopted in chime makes it possible to elaborate longer text passages and some straightforward interpretability. we have assessed experimentally a significant quality improvement using different state-of-the-art metrics to measure the lexical and semantic coherence of the generated text. we plan to further extend chime to model with multiple ground truth simultaneously and leverage the available product attributes.", "Solved": NaN, "Unsolved": NaN}, "258378227": {"num": 202, "type": 1, "title": "MAFiD: Moving Average Equipped Fusion-in-Decoder for Question Answering over Tabular and Textual Data", "pdfurl": "https://www.aclanthology.org/2023.findings-eacl.177.pdf", "conclusion": "in this paper, we address long range-reasoning for the multi-hop table-and-text qa and propose mafid, which extends fid by equipping ema and the gated cross-attention layer for the encoder and decoder parts, respectively, to design an effective way of combining various types of encoded representations. the experimental results on hy-bridqa showed that the proposed mafid achieved state-of-the-art performances in both the development and blind test sets. in future work, we will extend mafid to open-domain table-and-text qa and explore a unified approach that integrates single-row and multi-row reasoning.", "Solved": NaN, "Unsolved": NaN}, "247187956": {"num": 203, "type": 1, "title": "Semantic Sentence Composition Reasoning for Multi-Hop Question Answering", "pdfurl": "https://arxiv.org/pdf/2203.00160v1.pdf", "conclusion": "to improve the retrieval results and multi-hop reasoning in question answering, we present a multistage semantic matching module and a factual sentences composition module, respectively. experimental results fully demonstrate our modules outperform the standard ir system and two-step ir method. we first combine the semantic sentence composition module with multi-stage semantic matching module, and it is suitable for any open-domain qa architecture. further exploration will be done on the other question answering datasets with our proposed modules.", "Solved": NaN, "Unsolved": NaN}, "221819379": {"num": 204, "type": 1, "title": "Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA", "pdfurl": "https://arxiv.org/pdf/2009.09139v3.pdf", "conclusion": "we believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. in a large-scale 24-task nlp experiment, ca-mtl outperforms fully tuned single task models by 2.3% for bert large and by 1.2% for roberta large using 1.12 times the number of parameters, while single task fine-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. when a bert vanilla mtl model sees its performance drop as the number of tasks increases, ca-mtl scores continue to climb. performance gains are not driven by a single task as it is often the case in mtl. each ca-mtl module that adapts a transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. this evidence shows that ca-mtl is able to mitigate task interference and promote more efficient parameter sharing. we showed that mt-uncertainty is able to avoid degrading performances of low resource tasks. tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. overall, ca-mtl offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. extending such ideas will be an objective for future work.", "Solved": NaN, "Unsolved": NaN}, "236986870": {"num": 205, "type": 1, "title": "How Optimal is Greedy Decoding for Extractive Question Answering?", "pdfurl": "https://export.arxiv.org/pdf/2108.05857v2.pdf", "conclusion": "we investigate the optimality of greedy decoding for extractive question answering by comparing it to exact-extract, an optimal decoding algorithm that guarantees both extractiveness and exactness. while the greedy algorithm lags behind exact-extract in the zero-shot setting, training the model on as few as 16 labeled examples shrinks the performance gap substantially. this gap continues to narrow as more examples are available, typically converging to less than 1 point (f1) when training on 1024 examples. overall, our results showcase the impressive ability of pretrained language models to adapt to extractive question answering while relying only on a naive decoding algorithm. ", "Solved": NaN, "Unsolved": NaN}, "222067190": {"num": 206, "type": 1, "title": "MaP: A Matrix-based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/2020.aacl-main.69.pdf", "conclusion": "in this paper, we first investigate different approaches of span extraction in mrc. to improve the current vector-based conditional approach, we propose a matrix-based conditional approach. more careful consideration of the dependencies between the start and end positions of the answer span can predict their values better. we also propose a sampling-based training strategy to address the training process of the matrix-based conditional approach. the final experimental results on a wide of datasets demonstrate the effectiveness of our approach and training strategy.", "Solved": NaN, "Unsolved": NaN}, "256358906": {"num": 207, "type": 1, "title": "KN O WDA: ALL-IN-ONE KNOWLEDGE MIXTURE MODEL FOR DATA AUGMENTATION IN LOW- RESOURCE NLP TASKS", "pdfurl": "https://export.arxiv.org/pdf/2206.10265v2.pdf", "conclusion": "this paper explores multi-task learning paradigms at a massive scale for data augmentation in low-resource nlp tasks for the first time. we demonstrate that the proposed knowledge mixture training enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences). experiments verified the effectiveness of our knowda, and knowda outperforms state-of-the-art data augmentation approaches on the popular benchmarks fewglue, conll'03, and wikiann. we also perform ablation studies indicating the importance of including demonstrations and the impact of different keys. moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration. ", "Solved": NaN, "Unsolved": NaN}, "256274941": {"num": 208, "type": 1, "title": "Causal Reasoning About Entities and Events in Procedural Texts", "pdfurl": "https://www.aclanthology.org/2023.findings-eacl.31.pdf", "conclusion": "we present crepe, a benchmark for causal reasoning about events and entities in procedural texts. we show that mainstream llms such as gpt-3 perform close to chance on crepe, while using code-like event representation as a prompt to code language model codex greatly improves the performance. further, we experiment with various ways to encode entity information into this representation and find that eliciting chain-of-thought reasoning from codex further improves performance while existing cot approaches with gpt-3 are ineffective. we clearly show that llms benefit from lower-level entity information when making predictions about higher-level events. future work should explore related tasks such as next-event prediction, event temporal ordering, etc., by injecting relevant information about entities into our representation. our code-representation of events allows more powerful expressions than simply entailment and negation considered in this work. future work may explore other forms of code chain-of-thought such as first-order logic. these expressions generated by llms can be computed objectively, thus ameliorating llms' hallucinations and improving the interpretability and faithfulness of predictions.", "Solved": NaN, "Unsolved": NaN}, "216867332": {"num": 209, "type": 1, "title": "Robust Question Answering Through Sub-part Alignment", "pdfurl": "https://arxiv.org/pdf/2004.14648v2.pdf", "conclusion": "in this work, we presented a model for doing question answering through sub-part alignment.by having our model structured around an explicit alignment scoring process, we show that our approach can to generalize better to other domains.having alignments also makes it possible to filtering out bad model predictions (by treating the scores as confidence values) and interpreting the model's behavior (by examining the alignments and scores directly).", "Solved": NaN, "Unsolved": NaN}, "254097925": {"num": 210, "type": 1, "title": "Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2212.03296v1.pdf", "conclusion": "open-domain and multi-hop qa is an important problem for both humans and computers. to compare how humans and computers search and answer complex questions, our interface collects human question answering data as agents search with traditional and neural search engines alongside question answering models that suggest queries and answers. humans often use shorter queries, apply dynamic search chains, and use world knowledge. future qa models should have the ability to generate novel queries, \"discard\" irrelevant results, and explicitly check answers. moreover, computer agents for qa should also be able to use diverse retrievers to find evidence to answer questions, learning from the insights found in human data. with an agent trained on our data, we could have the \"best of both worlds\" to combine the ingenuity and tacit knowledge of humans with an indefatigable agent with access to all the world's information.", "Solved": NaN, "Unsolved": NaN}, "256461326": {"num": 211, "type": 1, "title": "Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.4.pdf", "conclusion": "automatic question generation is an important task in the improvement of artificial intelligent systems. in this work, we propose a novel primal-dual approach for question generation. it integrates question generation with its dual problem question answering into a unified framework. a knowledge distillation module is introduced into the framework to improve model generalization on uncommon word generation. experimental results on two benchmarks demonstrate the effectiveness of the primal-dual modeling.", "Solved": NaN, "Unsolved": NaN}, "237502990": {"num": 212, "type": 1, "title": "Adaptive Information Seeking for Open-Domain Question Answering", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.293.pdf", "conclusion": "this work presents an adaptive informationseeking approach for open-domain question answering, called aiso. it models the open-domain qa task as a pomdp, where the environment contains a large corpus and the agent is asked to sequentially select retrieval function and reformulate query to collect the evidence. aiso achieves stateof-the-art results on two public datasets, which demonstrates the necessity of different retrieval functions for different questions. in the future, we will explore other adaptive retrieval strategies, like directly optimizing various informationseeking metrics by using reinforcement learning techniques.", "Solved": NaN, "Unsolved": NaN}, "218487313": {"num": 213, "type": 1, "title": "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.414.pdf", "conclusion": "we introduced a simple, unsupervised approach for evidence retrieval for question answering. our approach combines three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using glove embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, and (c) a simple stopping condition that concludes the iterative process when all terms in the given question and candidate answers are covered by the retrieved justifications. overall, despite its simplicity, unsupervised nature, and its sole reliance on glove embeddings, our approach outperforms all previous methods (including supervised ones) on the evidence selection task on two datasets: multirc and qasc. when these evidence sentences are fed into a roberta answer classification component, we achieve the best qa performance on these two datasets. further, we show that considerable improvements can be obtained by aggregating knowledge from parallel evidence chains retrieved by our method. in addition of improving qa, we hypothesize that these simple unsupervised components of air will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria.", "Solved": NaN, "Unsolved": NaN}, "260682258": {"num": 214, "type": 1, "title": "Towards General Text Embeddings with Multi-stage Contrastive Learning", "pdfurl": "https://export.arxiv.org/pdf/2308.03281v1.pdf", "conclusion": "this paper presents a multi-stage contrastive learning approach to develop text embedding model that can be applied to various tasks. our model benefits from a diverse training data mixture, enabling it to achieve good generalization performance for single vector embedding. through extensive evaluation on multiple benchmarks, we demonstrate the effectiveness and versatility of our text embedding model. our future work will focus on scaling the model to support longer context, extending it to support multilingual and multi-modal applications, as well as exploring the benefits of prompts and instructions. academic paper the scientific articles usually have a higher quality due to its formal nature. for each paper, we use the title as query and its abstract as document for constructing text pairs. the articles are mined from different websites (such as arxiv, biorxiv, medrxiv, pubmed and semantic scholar) to cover a wide range of topics.", "Solved": NaN, "Unsolved": NaN}, "237593105": {"num": 215, "type": 1, "title": "NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset", "pdfurl": "https://arxiv.org/pdf/2109.10604v2.pdf", "conclusion": "in this work, we present a new qa datasets with complex numerical questions and interpretable reasoning graph. we also introduce an automatic evaluation metric for the generated reasoning process. we finally present an initial model producing the reasoning process while answering questions. the experiments show that noahqa is challenging and will become an interesting direction in both numerical qa and explainable qa.", "Solved": NaN, "Unsolved": NaN}, "216035859": {"num": 216, "type": 1, "title": "Logic-Guided Data Augmentation and Regularization for Consistent Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.499.pdf", "conclusion": "we introduce a logic guided data augmentation and consistency-based regularization framework for accurate and globally consistent qa, especially under limited training data setting. our approach significantly improves the state-of-the-art models across three substantially different qa datasets. notably, our approach advances the state-of-the-art on quarel and wiqa, two standard benchmarks requiring rich logical and language understanding. we further show that our approach can effectively learn from extremely limited training data.", "Solved": NaN, "Unsolved": NaN}, "202539540": {"num": 217, "type": 1, "title": "QUARTZ: An Open-Domain Dataset of Qualitative Relationship Questions", "pdfurl": "https://www.aclweb.org/anthology/D19-1608.pdf", "conclusion": "understanding and applying textual qualitative knowledge is an important skill for questionanswering, but has received limited attention, in part due the lack of a broad-coverage dataset to study the task. quartz aims to fill this gap by providing the first open-domain dataset of qualitative relationship questions, along with the requisite qualitative knowledge and a rich set of annotations. specifically, quartz removes the requirement, present in all previous qualitative reasoning work, that a fixed set of qualitative relationships be formally pre-specified. instead, quartz tests the ability of a system to find and apply an arbitrary relationship on the fly to answer a question, including when simple reasoning (arguments, polarities) is required.", "Solved": NaN, "Unsolved": NaN}, "261884440": {"num": 218, "type": 1, "title": "Bridging Dense and Sparse Maximum Inner Product Search", "pdfurl": "https://export.arxiv.org/pdf/2309.09013v1.pdf", "conclusion": "we began this research with a simple question: can we apply dense mips algorithms to sparse vectors? that led us to investigate different dimensionality reduction techniques for sparse vectors as a way to contain the curse of dimensionality. we showed, for example, that the jl transform and sinnamon behave differently on sparse vectors and can preserve inner product to different degrees. we also thoroughly evaluated the effect of clustering on sparse mips in the context of an ivf-based retrieval system. coupling dimensionality reduction with clustering realized an effective ivf system for sparse vectors, summarized in algorithms 1 and 2.", "Solved": NaN, "Unsolved": NaN}, "255545881": {"num": 219, "type": 1, "title": "Mind Reasoning Manners: Enhancing Type Perception for Generalized Zero-shot Logical Reasoning over Text", "pdfurl": "https://export.arxiv.org/pdf/2301.02983v1.pdf", "conclusion": "to study the zero-shot capability of the logical reasoning models, we propose the first benchmark for the generalized zero-shot logical reasoning, named zslr. it includes six splits sampled with three strategies and two metrics to comprehensively evaluate the performances. also, we propose a model taco to enhance the reasoning type perception through the heuristic input reconstruction and the type-aware contrastive learning. also, we conduct extensive experiments on the zero-shot splits, full-data setting as well as other dataset. superior results illustrate the effectiveness and generalization capability of the proposed modules.", "Solved": NaN, "Unsolved": NaN}, "186206745": {"num": 220, "type": 1, "title": "Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/1906.05210v1.pdf", "conclusion": "we presented an interpretable 3-module, multihop, reading-comprehension system 'epar' which constructs a 'reasoning tree', proposes an answer candidate for every root-to-leaf chain, and merges key information from all reasoning chains to make the final prediction. on wikihop, our system outperforms all published models on the dev set, and achieves results competitive with the current stateof-the-art on the test set. on medhop, our system outperforms all previously published models on the leaderboard test set. we also presented multiple reasoning-chain recovery tests for the explainability of our system's reasoning capabilities.", "Solved": NaN, "Unsolved": NaN}, "258865693": {"num": 221, "type": 1, "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration", "pdfurl": "https://export.arxiv.org/pdf/2305.15262v1.pdf", "conclusion": "we raise concerns about the use of parallelintegrated methods to address context length restriction: (1) pcw is functionally equal with a simple weighted sum ensemble on label distribution among context windows; (2) pcw degrades the multi-step reasoning capabilities of llms in complex tasks requiring knowledge understanding. de-spite the fact that parallel-integrated methods sometimes show better classification performance when the label space is large, they merely brute-force ensemble each window's context, consequently weakening logical reasoning and knowledge comprehension.", "Solved": NaN, "Unsolved": NaN}, "222140958": {"num": 222, "type": 1, "title": "Context Modeling with Evidence Filter for Multiple Choice Question Answering", "pdfurl": "https://arxiv.org/pdf/2010.02649v1.pdf", "conclusion": "we propose evidence filter to alleviate the effect of unrelated sentences and enhance the saliency of evidences potentially without human efforts. results on openbookqa indicate the effectiveness of our method. our future work is to enhance the evidence filter by more complex components.", "Solved": NaN, "Unsolved": NaN}, "208267807": {"num": 223, "type": 1, "title": "Published as a conference paper at ICLR 2020 LEARNING TO RETRIEVE REASONING PATHS OVER WIKIPEDIA GRAPH FOR QUESTION ANSWERING", "pdfurl": "https://export.arxiv.org/pdf/1911.10470v2.pdf", "conclusion": "this paper introduces a new graph-based recurrent retrieval approach, which retrieves reasoning paths over the wikipedia graph to answer multi-hop open-domain questions. our retriever model learns to sequentially retrieve evidence paragraphs to form the reasoning path. subsequently, our reader model re-ranks the reasoning paths, and it determines the final answer as the one extracted from the best reasoning path. our experimental results significantly advance the state of the art on hotpotqa by more than 14 points absolute gain on the full wiki setting. our approach also achieves the state-of-the-art performance on squad open and natural questions open without any architectural changes, demonstrating the robustness of our method. our method provides insights into the underlying entity relationships, and the discrete reasoning paths are helpful in interpreting our framework's reasoning process. future work involves end-to-end training of our graph-based recurrent retriever and reader for improving upon our current two-stage training.", "Solved": NaN, "Unsolved": NaN}, "239616544": {"num": 224, "type": 1, "title": "ITERATIVE HIERARCHICAL ATTENTION FOR ANSWER- ING COMPLEX QUESTIONS OVER LONG DOCUMENTS", "pdfurl": "https://arxiv.org/pdf/2106.00200v2.pdf", "conclusion": "we consider on the problem of answering complex questions over long structured documents. like multi-hop open qa tasks, this problem requires not only conventional \"machine reading\" abilities, but the ability to retrieve relevant information and refine queries based on retrieved information. additionally, it requires the ability to navigate through a document, by understanding the relationship between sections of the document and parts of the question. in our framework, navigation is modeling similarly to retrieval in multi-hop models: the model attends to a document section, and uses a compact neural encoding of the section to update the query. unlike most prior multihop qa models, however, queries are updated in embedding space, rather than by appending to a discrete representation of question text. this approach is end-to-end differentiable and very fast. experiments also demonstrate that this use of hierarchical attention can significantly improve the performance on qa tasks: in fact, the dochopper model achieves the start-of-the-art results on four challenging qa datasets, outperforming the baseline models by 3-5%, while also being 3-10 times faster.", "Solved": NaN, "Unsolved": NaN}, "248572452": {"num": 225, "type": 1, "title": "ProQA: Structural Prompt-based Pre-training for Unified Question Answering", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.313.pdf", "conclusion": "we introduce proqa, a unified qa paradigm that adopts a single model for solving various qa tasks with the bridge of a structural prompt. structural prompt simultaneously models the common ability required for various tasks and keeps the speciality of each task, through a structurally designed learnable input schema. we further conduct structural prompt-based pre-training, seeking to empower the model with general qa-centric ability and injects the semantic knowledge of the structural prompt into the pre-training model. experimental results on 11 qa benchmarks demonstrate that proqa can significantly boost performance on all settings. further analyses show that our method can better mitigate the catastrophic forgetting issue during continual learning, and our method can be adapted to a newly involved task more quickly, by taking the advantages of the structural prompt. in the future, we hope our analysis could inspire more explorations on the unified qa methods, or the unification of distinct tasks with complex inputs modeling by the structural prompt. we also hope structural prompt can be further utilized into the unification of more tasks with complex inputs.", "Solved": NaN, "Unsolved": NaN}, "253157925": {"num": 226, "type": 1, "title": "DyREx: Dynamic Query Representation for Extractive Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2210.15048v1.pdf", "conclusion": "in this paper, we propose dyrex, a method to dynamically compute query representations to calculate the start and end positions of answer spans in extractive question answering. our approach consistently outperforms the dominant approach on a wide range of qa datasets, and the gain is even more significant in a few-shot scenario. in future work, it would be interesting to adapt dyrex for multi-span extraction tasks such as named entity recognition and keyphrase extraction.", "Solved": NaN, "Unsolved": NaN}, "204800552": {"num": 227, "type": 1, "title": "Domain-agnostic Question-Answering with Adversarial Training", "pdfurl": "https://www.aclweb.org/anthology/D19-5826.pdf", "conclusion": "we leverage adversarial learning to learn domaininvariant features. in our experiments, the proposed method consistently improves the performance of baseline and it is applicable to any qa model. in future work, we will try adversarial learning for pre-training model with diverse set of domains.", "Solved": NaN, "Unsolved": NaN}, "258832937": {"num": 228, "type": 1, "title": "\"According to . . . \" Prompting Language Models Improves Quoting from Pre-Training Data", "pdfurl": "https://export.arxiv.org/pdf/2305.13252v1.pdf", "conclusion": "large language models struggle with hallucination, or generating incorrect information, despite the large amount of factual pre-training data they were trained on. to help alleviate this problem, we proposed according-to prompts, asking language models to ground their output to their pretraining corpus. to quantify the extent to which models achieve this goal, we introduced a new metric, quip-score, that efficiently and quickly measures the percent of the model's generation that exists as exact quotes in the pre-training corpus. we showed that prompting models with grounding prompts greatly improves the quip-score while anti-grounding prompts reduces the quip-score. our analysis also shows that quip-score increases with instruction-tuning, popularity of the entity in the question, and model size. we hope that this work brings more attention to the positive aspects of llm memorization and encourages more work into understanding how and when language model output is grounded to its pre-training data.", "Solved": NaN, "Unsolved": NaN}, "256868909": {"num": 229, "type": 1, "title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2302.07452v1.pdf", "conclusion": "we present dragon, dense retriever trained with diverse augmentation. we propose a unified framework of data augmentation (da) to understand the recent progress of training dense retrievers. based on the framework, we extensively study how to improve dense retrieval training through query and relevance label augmentation. our experiments uncover some insights into training a dense retriever, which contradicts common wisdom that cross encoder is the most effective teacher and human-like queries are the most suitable training data for dense retrieval. instead, we propose a diverse data augmentation recipe, query augmentation with the mixture of sentence cropping and generative queries, and progressive relevance label augmentation with multiple teachers. with our recipe of da, we are the first to demonstrate that a single bert-base-sized dense retriever can achieve state-of-the-art effectiveness in both supervised and zero-shot retrieval tasks. we believe that dragon can serve as a strong foundation retrieval model for domain adaptation retrieval tasks (wang et al., 2022;dai et al., 2022) or the existing retrieval augmented language models (izacard et al., 2022;shi et al., 2023;mallen et al., 2022).", "Solved": NaN, "Unsolved": NaN}, "256459309": {"num": 230, "type": 1, "title": "Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data", "pdfurl": "https://export.arxiv.org/pdf/2302.00674v4.pdf", "conclusion": "recall the desiderata for our algorithm, expressed in the introduction: our algorithm should (1) make no assumptions on the available auxiliary data a-priori, (2) scale well with the number of auxiliary datasets, and (3) add minimal memory and computational overhead.(1) when designing our algorithm, we purposefully formulate the problem as a multi-armed bandit.mab algorithms, in general, make no assumptions on the quality of rewards and, in particular, exp3 even assumes that the auxiliary datasets will play an adversarial role when returning rewards.(2) as previously mentioned, our algorithms have a single-turn computational complexity that is independent of the number of auxiliary datasets.(3) finally, our method adds minimal computational overhead beyond usual training computations.every gradient that we utilize for our reward functions are also used to update the model, adding no additional computations.the only computational overhead is to compute gradient alignment (three vector dot products, two scalar square roots, and two scalar multiplications) or magnitude similarity (four vector dot products, two scalar square roots, three scalar multiplications, and one scalar addition).additionally, our method adds a small amount of memory overhead, used to store gradients between model updates.our rewards consider only the gradient w.r.t the language modelling head and, in practice, require 0.25gb per auxiliary gradient to store, slightly increasing the space complexity above standard fine-tuning.", "Solved": NaN, "Unsolved": NaN}, "216562224": {"num": 231, "type": 1, "title": "Semantics-Aware Inferential Network for Natural Language Understanding", "pdfurl": "https://arxiv.org/pdf/2004.13338v1.pdf", "conclusion": "this work focuses on two typical nlu tasks, machine reading comprehension and natural language inference by refining the use of semantic clues and inferential model. the proposed semantics-aware inferential network (sain) is capable of taking multiple semantic structures as input of an inferential network by closely integrating semantics and reasoning steps in a creative way. experiment results on 11 benchmarks, including 4 nli tasks and 7 mrc tasks, show that our model outperforms all previous strong baselines, which consistently indicate the general effectiveness of our model 6 .", "Solved": NaN, "Unsolved": NaN}, "222272049": {"num": 232, "type": 1, "title": "Case Study: Deontological Ethics in NLP", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.297.pdf", "conclusion": "two principles of deontological ethics-namely the generalization principle and respect for autonomy via informed consent-can be used to decide if an action is ethical. despite the limitations of these principles, they can provide useful insights into making nlp systems more ethical. through the four case studies discussed in this paper, we demonstrate how these principles can be used to evaluate the decisions made by nlp systems and to identify the missing aspects. for each of the case studies, we also present potential directions for nlp research to move forward and make the system more ethical.", "Solved": NaN, "Unsolved": NaN}, "256459315": {"num": 233, "type": 1, "title": "The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models", "pdfurl": "https://www.aclanthology.org/2023.eacl-main.113.pdf", "conclusion": "in this work, we investigate the effects of training mrc models with unanswerable questions on their robustness against adversarial attacks. we construct adversarial samples from answerable and unanswerable questions in squad 2.0 and evaluate three mrc models fine-tuned on either squad 1.1 (v1 models) or squad 2.0 (v2 models) independently. adversarial attacks on answerable questions reveal that v2 models initially show little improved robustness over v1 models yet possess a latent ability to deal with these attacks that v1 models do not; the correct responses are often hidden as second-best answers, an indicator of the \"hidden robustness\" of v2 models resulting from additional training on unanswerable questions. by eliminating the \"unanswerable\" option and forcing v2 models to output an answer to any answer-  able questions, we leverage this hidden robustness to improve the performance of mrc models to attacks on answerable questions. furthermore, we also show that this robustness translates well to out-of-domain test sets.", "Solved": NaN, "Unsolved": NaN}, "250264121": {"num": 234, "type": 1, "title": "QA Is the New KR: Question-Answer Pairs as Knowledge Bases", "pdfurl": "https://arxiv.org/pdf/2207.00630v1.pdf", "conclusion": "symbolic kbs organize information into small modular components (e.g., entities, kg triples, wikidata statements) that can be combined compositionally to answer complex queries. while many recent papers have focused on tasks like open qa, where questions are answered from text without using a kb, broad-coverage symbolic kbs continue to be widely used in practice, and, despite recent progress in methods for \"multi-hop\" qa, are still the only computationally efficient way of answering questions that combine information for multiple documents. however, the broad-coverage kbs that are currently in wide use are largely collections of information that easily collected and integrated, and need not reflect the actual infor-mation needs of users. in this position paper, we advocate for a new approach to constructing kbs, and in particular, an approach to collecting modular, compositionally-combinable knowledge components from text, driven by a sample of user's questions and answers.", "Solved": NaN, "Unsolved": NaN}, "260379194": {"num": 235, "type": 1, "title": "Teaching Smaller Language Models To Generalise To Unseen Compositional Questions", "pdfurl": "https://export.arxiv.org/pdf/2308.00946v2.pdf", "conclusion": "we have argued that an ability to reason over imperfect and incomplete information is a critical skill with which question-answering models must be endowed. to facilitate such ability we create ratd datasets that are designed to impart heuristic reasoning strategies with context of a form similar to that which retrieved contexts for downstream tasks will have. we show that training on ratd datasets improves performance on all unseen evaluation datasets with retrieved contexts. this sometimes comes at a small cost in situations where questions come with gold contexts that are in a form that our model is already good at utilizing (sqa gf , drop, and iirc g ) although we suggest that in practice such gold contexts are the less common case. (r1)", "Solved": NaN, "Unsolved": NaN}, "259859005": {"num": 236, "type": 1, "title": "Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension", "pdfurl": "https://aclanthology.org/2023.findings-acl.812.pdf", "conclusion": "in this paper, we have presented ci4mrc, a novel causal interventional paradigm to address name bias in mrc: the pre-trained knowledge concerning names is a confounder limiting the robust performance.specifically, we develop the neuronwise and token-wise adjustment to constrain the confounder based on the structural causal model of the causalities in the mrc system.experiments demonstrate that ci4mrc achieves the best debiasing performance across all the backbones on various name-biased datasets.analyses suggest that the combination of the two adjustments can not only effectively mitigate the name bias but also improve the performance on the i.i.d evaluation.", "Solved": NaN, "Unsolved": NaN}, "221971009": {"num": 237, "type": 1, "title": "SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.47.pdf", "conclusion": "in short, we propose sparta, a novel ranking method, that learns sparse representation for better open-domain qa. experiments show that the proposed framework achieves the state-of-the-art performance for 4 different open-domain qa tasks in 2 languages and 11 retrieval qa tasks. this confirm our hypothesis that token-level interaction is superior to sequence-level interaction for better evidence ranking. analyses also show the advantages of sparse representation, including interpretability, generalization and efficiency. our findings also suggest promising future research directions. the proposed method does not support multi-hop reasoning, an important attribute that enables qa systems to answer more complex questions that require collecting multiple evidence passages. also, current method only uses a bag-ofword features for the query. we expect further gain by incorporating word-order information.", "Solved": NaN, "Unsolved": NaN}, "253397574": {"num": 238, "type": 1, "title": "Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps", "pdfurl": "https://www.aclanthology.org/2022.aacl-main.57.pdf", "conclusion": "this paper presented an effective unsupervised domain adaptation method, cai. we showed that the combination of splade with cai and the lexical approach gave a state-of-the-art performance on datasets with a large vocabulary and wordfrequency gap. in addition, cai outperformed gpl and was robust enough to show high accuracy even when bow representations were used for query expression. finally, our analysis showed that splade with cai addressed the problem of the exact matching of low-frequency words in training data. we believe that cai works on smaller mlms by distilling adalm because yao et al. (2021) showed that a distilled adalm achieved higher performance than bert on nlp tasks and formal et al. (2021) showed that the results of splade initialized with distilbert-base 14 was competitive on ms marco with other ir models initialized with bert.", "Solved": NaN, "Unsolved": NaN}, "252815949": {"num": 239, "type": 1, "title": "Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.336.pdf", "conclusion": "to solve the problem of lacking global structure in ldqa methods, we propose compress graph selector network to capture the global structure over the long document when selecting evidence pieces. extensive experiments demonstrate the strong per-formance of the model.", "Solved": NaN, "Unsolved": NaN}, "258841172": {"num": 240, "type": 1, "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions", "pdfurl": "https://export.arxiv.org/pdf/2305.14010v1.pdf", "conclusion": "the main limitation of ifqa dataset is that it only covers event-based questions, due to the nature of creating counterfactual presuppositions. therefore, our dataset is not intended for training general opendomain qa models or evaluate their capabilities.for data collection, we relied heavily on human annotators, both for question annotation and verification. despite our efforts to mitigate annotator bias by providing explicit instructions and examples and by sampling annotators from diverse populations, it is not possible to completely remove this bias. in addition, we use heuristic rules to select only a small portion of wikipedia passages and then present them to human annotators (as mentioned in section 3.1.1), which might lead to pattern-oriented bias in the annotated data.for evaluated models, large language models performance on our dataset may preserve biases learned from the web text during pre-training or and make biased judgments as a result.", "Solved": NaN, "Unsolved": NaN}, "211532373": {"num": 241, "type": 1, "title": "Generating Followup Questions for Interpretable Multi-hop Question Answering", "pdfurl": "https://arxiv.org/pdf/2002.12344v1.pdf", "conclusion": "followup queries are essential to solving the difficult cases of multi-hop qa, and real followup questions are an advance in making this process interpretable. we have shown that pointer generator networks can effectively learn to read partial information and produce a fluent, relevant question about what is not known, which is a complement to their typical role in summarizing what is known. our task poses a novel challenge that tests semantic properties of the generated output.", "Solved": NaN, "Unsolved": NaN}, "254685782": {"num": 242, "type": 1, "title": "MASTER: MULTI-TASK PRE-TRAINED BOTTLE- NECKED MASKED AUTOENCODERS ARE BETTER DENSE RETRIEVERS", "pdfurl": "https://export.arxiv.org/pdf/2212.07841v1.pdf", "conclusion": "in this paper, we proposed master, a multi-task pre-trained bottlenecked masked autoencoder for dense retrieval task. in our approach, we adopted a bottlenecked multi-decoder architecture to integrate a variety of pre-training tasks, and devised three types of pre-training tasks about corrupted passages recovering, related passage recovering and plms outputs recovering. the three types of tasks focused on compressing the semantic information within the passages, modeling relations among passages, and learning the knowledge from external public generative plms, respectively, leading to more informative and effective dense vectors. experimental results have shown that our approach outperforms several competitive baselines.  we compare our approach with a variety of methods: of bert on these nlu tasks. it indicates that our multi-task pre-training can also enrich the useful knowledge about nlu tasks for the plm.", "Solved": NaN, "Unsolved": NaN}, "165163607": {"num": 243, "type": 1, "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions", "pdfurl": "https://www.aclweb.org/anthology/N19-1300.pdf", "conclusion": "we have introduced boolq, a new reading comprehension dataset of naturally occurring yes/no questions. we have shown these questions are challenging and require a wide range of inference abilities to solve. we have also studied how transfer learning performs on this task, and found crowd-sourced entailment datasets can be leveraged to boost performance even on top of language model pre-training. future work could include building a document-level version of this task, which would increase its difficulty and its correspondence to an end-user application. embeddings h 1 , h 2 , ... . then pool these embeddings by computing attention scores a i = w \u00b7 h i , p = sof tmax(a), and then the sum v * = i p i h i . likewise we compute p * from the premise.", "Solved": NaN, "Unsolved": NaN}, "253157979": {"num": 244, "type": 1, "title": "TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.821.pdf", "conclusion": "we present tasa, an automatic adversarial attack method for qa models. it generates twin answer sentences, perturbed answer sentence (pas), and distracting answer sentence (das), to construct a new adversarial context in a qa sample. it can deceive models and misguide them to an incorrect answer based on their pitfalls that overly rely on matching sensitive keywords during predicting answers. in experiments, tasa achieves remarkable attack performance on five datasets and three victim models with satisfactory sample quality. our additional analysis also proves that it is possible to get more robust qa models via tasa in the future. during fine-tuning bert on different qa datasets, we set the maximum input sequence length as 384, using an adam optimizer whose initial learning rate is 6.25e\u22125 with the batch size 32. the epoch number is 3 and the final model after all epochs will be saved as the victim model. spanbert we also use the huggingfancetransformers to implement the model, along with spanbert-large-cased version 5 to initialize the weights. it contains 24 layers with a hidden size of 1024. a linear layer is added to predict the start and end positions of the answer span.", "Solved": NaN, "Unsolved": NaN}, "227334750": {"num": 245, "type": 1, "title": "KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning", "pdfurl": "https://arxiv.org/pdf/2012.03551v1.pdf", "conclusion": "we have proposed a pre-training method by cooperatively modeling the generative and discriminative knowledge injecting approaches. our model can be easily extended to larger pre-training corpus and does not introduce any modifications for downstream tasks during finetuning. experiments show our model consistently outperforms all base models on a variety of question answering datasets, demonstrating that our kgplm is a preferred choice for the knowledge intensive nlp tasks.  ", "Solved": NaN, "Unsolved": NaN}, "222141025": {"num": 246, "type": 1, "title": "PROVER: Proof Generation for Interpretable Reasoning over Rules", "pdfurl": "https://arxiv.org/pdf/2010.02830v1.pdf", "conclusion": "we introduce prover, an interpretable joint model that answers binary questions over natural language rule-bases and generates corresponding proofs. the proofs are generated through the node and edge modules of the model in the presence of multiple global constraints during training and ilp inference. our model improves state-of-theart qa accuracy in the zero-shot scenario by 6% and generates proofs accurately. prover also generalizes much better to higher depth questions with up to 15% absolute improvement in qa performance over ruletakers. prover's modeling is relatively generic, and similar proof generation methods can be explored in traditional multi-hop qa tasks. prover can also be a helpful aid to formal reasoners in scenarios where rules are fuzzy and creating rule-bases in a formal language is tedious or infeasible. ", "Solved": NaN, "Unsolved": NaN}, "158046817": {"num": 247, "type": 1, "title": "Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs", "pdfurl": "https://arxiv.org/pdf/1905.07374v1.pdf", "conclusion": "we propose a new gnn-based method for multihop rc across multiple documents. we introduce the hde graph, a heterogeneous graph for multiple-hop reasoning over nodes representing different granularity levels of information. we use co-attention and self-attention to encode candidates, documents, entities of mentions of candidates and query subjects into query-aware representations, which are then employed to initialize graph node representations. evaluated on wiki-hop, our end-to-end trained single neural model achieves the state-of-the-art performance on the blind test set. in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa.", "Solved": NaN, "Unsolved": NaN}, "257901155": {"num": 248, "type": 1, "title": "UKP-SQuARE v3: A Platform for Multi-Agent QA Research", "pdfurl": "https://www.aclanthology.org/2023.acl-demo.55.pdf", "conclusion": "in this work, we have extended ukp-square to support multi-agent models. in particular, we deployed a routing system, tweac (geigle et al., 2021), a method to combine adapter weights, made (friedman et al., 2021), and a model that combines the prediction of multiple skills, metaqa (puerto et al., 2023). we have conducted experiments on these three models and unifiedqa (khashabi et al., 2020), a multi-dataset system, to analyze the trade-off between the performance, efficiency, and flexibility of these systems. we showed that in scenarios where new domains or expertise are often needed, metaqa provides the best tradeoff since its performance is close to the best model, it is compatible with any qa format, cheap to train, and its inference runtime is close to tweac and made using the parallel engine provided by ukp-square. however, when simple deployment is needed or the model is not expected to be updated, made and unifiedqa might be more appropriate.", "Solved": NaN, "Unsolved": NaN}, "233241188": {"num": 249, "type": 1, "title": "TWEAC: Transformer with Extendable QA Agent Classifiers", "pdfurl": "https://arxiv.org/pdf/2104.07081v2.pdf", "conclusion": "we analyzed how to automatically select suitable qa agents specializing in different questions, as an alternative to a single qa system that tries to cover all possible questions. we presented a scalable meta-qa system that allows for a flexible extension with different qa agents. for newly posed questions, we rank agents by their ability to answer them and select the most suitable ones.", "Solved": NaN, "Unsolved": NaN}, "232135266": {"num": 250, "type": 1, "title": "Rissanen Data Analysis: Examining Dataset Characteristics via Description Length", "pdfurl": "https://arxiv.org/pdf/2103.03872v1.pdf", "conclusion": "in this work, we proposed rissanen data analysis (rda), a method for examining the characteristics of a dataset. we began by viewing the labels of a dataset as being generated by a program over the inputs, then positing that a capability is helpful if it reduces the length of the shortest label-generating program. instead of evaluating minimum program length directly, we use blockwise prequential coding to upper bound minimum description length (mdl). while the choice of learning algorithm a influences absolute mdl values, we only interpret mdl relative to other mdl values estimated with the same a. in particular, we conduct rda by comparing mdl with or without access to a subroutine with a certain capability, and we say that a capability is useful when invoking the subroutine reduces mdl.", "Solved": NaN, "Unsolved": NaN}, "237498988": {"num": 251, "type": 1, "title": "Connecting Attributions and QA Model Behavior on Realistic Counterfactuals", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.447.pdf", "conclusion": "we show that feature attributions can reveal known dataset biases and reasoning shortcuts in hotpotqa without having to perform a detailed manual analysis. this confirms the suitability of our attribution methods for at least this use case: model designers can look at them in a semi-automated way and determine how robust the model is going to be when faced with counterfactuals.our analysis also highlights the limitations of current explanation techniques. we experimented with other counterfactuals by permuting the order of the paragraphs in the context, which often gave rise to different predictions. we believe the model prediction was in these cases impacted by biases in positional embeddings (e.g., the answer tends to occur in the first retrieved paragraph), which cannot be indicated by current attribution methods. we believe this is a useful avenue for future investigation. by first thinking about what kind of counterfactuals and what kind of behaviours we want to explain, we can motivate the development of new explanation techniques to serve these needs.", "Solved": NaN, "Unsolved": NaN}, "207917676": {"num": 252, "type": 1, "title": "Bend but Don't Break? Multi-Challenge Stress Test for QA Models", "pdfurl": "https://www.aclweb.org/anthology/D19-5818.pdf", "conclusion": "we conclude our discussion by presenting suggestions for good future practices when building and presenting new models and datasets. we constructively offer these points and have no intent to criticize authors whose prior work we reference.", "Solved": NaN, "Unsolved": NaN}, "259164782": {"num": 253, "type": 1, "title": "When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants", "pdfurl": "https://www.aclanthology.org/2023.acl-short.141.pdf", "conclusion": "we present an empirical efficiency analysis of vanilla transformers and their self-attention-based efficient variants across modalities, metrics and input context sizes. we find substantial differences across modalities and metrics when analyzing the tipping point for efficient variants. finally, the layerwise analysis finds that self-attention is not the only bottleneck. we recommend that all efficient model papers should report such cross-modal, layerwise profiling results on multiple efficiency metrics covering a variety of use-cases to provide a full picture of the benefits of the model.", "Solved": NaN, "Unsolved": NaN}, "258378353": {"num": 254, "type": 1, "title": "Socratic Question Generation: A Novel Dataset, Models, and Evaluation", "pdfurl": "https://www.aclanthology.org/2023.eacl-main.12.pdf", "conclusion": "we created a novel dataset socratiq to support research on automatic socratic question generation. we applied latest research in prompt-based conditional text generation to fine-tune existing large language models from gpt, t5, and prophetnet to learn soqg. through our study and the release of this novel dataset, we take a first step towards enabling future research on models for soqg as well as impactful applications in areas such as counseling and education (inkster et al., 2018;fitzpatrick et al., 2017).", "Solved": NaN, "Unsolved": NaN}, "237263476": {"num": 255, "type": 1, "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification", "pdfurl": "https://arxiv.org/pdf/2201.05320v1.pdf", "conclusion": "in this work, we propose gamification as a general framework for creating diverse and challenging nlu benchmarks. we use this framework to collect csqa2, a new benchmark that contains 14,343 yes/no questions. we perform a detailed analysis of csqa2, which elucidates the unique properties of our dataset, and thoroughly evaluate on a strong suite of baselines. we find that the best model, unicorn-11b, achieves an accuracy of 70.2%, dozens of points lower than human accuracy. we argue that gamification is a promising approach for creating challenge sets that expose the weaknesses of current state-of-the-art models.", "Solved": NaN, "Unsolved": NaN}, "247188085": {"num": 256, "type": 1, "title": "Read before Generate! Faithful Long Form Question Answering with Machine Reading", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.61.pdf", "conclusion": "we propose a new end-to-end framework rbg that jointly models answer generation and machine reading to tackle the faithfulness issue in lfqa. experiments on two lfqa datasets, eli5 and ms marco, demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics. the detailed analysis further proves the competency of our method in generating fluent, relevant, and more faithful answers. we also propose to evaluate the factual correctness of lfqa model by answering questions of extractive qa tasks (e.g., natural questions), which may be helpful to evaluate the faithfulness of lfqa model efficiently.", "Solved": NaN, "Unsolved": NaN}, "214071925": {"num": 257, "type": 1, "title": "Translucent Answer Predictions in Multi-Hop Reading Comprehension", "pdfurl": "https://ojs.aaai.org/index.php/AAAI/article/download/6272/6128", "conclusion": "tap is a novel architecture for the multi-hop reasoning based rcqa task. core to this system is logix, a new approach that effectively addresses the challenges of local context and global interactions present in multi-passage, multihop qa. we have shown that tap advances the state-of-theart on hotpotqa dataset, reaching rank-1 and rank-2 in its ensemble and single model variants at the time of submission. finally, by restricting the input of the ap to logix's selected supporting facts, tap admits interpretability that can be used to debug the model for performance enhancement purposes prior to its deployment.", "Solved": NaN, "Unsolved": NaN}, "258685562": {"num": 258, "type": 1, "title": "Answering Complex Questions over Text by Hybrid Question Parsing and Execution", "pdfurl": "https://export.arxiv.org/pdf/2305.07789v1.pdf", "conclusion": "we propose hpe for answering complex questions over text, which combines the strengths of neural network approaches and symbolic approaches. we parse the question into h-expressions followed by the hybrid execution to get the final answer. our extensive empirical results demonstrate that hpe has a strong performance on various datasets under supervised, few-shot, and zero-shot settings. moreover, our model has a strong interpretability exposing its underlying reasoning process, which facilitates understanding and possibly fixing its errors. by replacing our text reader with kb or ", "Solved": NaN, "Unsolved": NaN}, "211126910": {"num": 259, "type": 1, "title": "FQuAD: French Question Answering Dataset", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.107.pdf", "conclusion": "in the present work, we introduce the french question answering dataset. the contexts are collected from the set of high quality wikipedia articles. with the help of french college students, 60,000+ questions have been manually annotated. the fquad dataset is the result of two different annotation processes. first, fquad1.0 is collected to build a 25,000+ questions dataset. second, the dataset is enriched to reach 60,000+ questions resulting in fquad1.1. the development and test sets have both been enriched with additional answers for the evaluation process.", "Solved": NaN, "Unsolved": NaN}, "261341823": {"num": 260, "type": 1, "title": "Learning on Structured Documents for Conditional Question Answering", "pdfurl": "https://www.aclanthology.org/2023.ccl-1.51.pdf", "conclusion": "in this paper, we present learning on structured documents (lsd), a self-supervised learning method for conditional question answering. lsd uses a conditional question generation method to leverage massive structured documents while improving conciseness, and applies contrastive learning to learn effective semantic representations from complex documents. we further propose a pipeline that could generate multiple answers and conditions to better handle the cqa task. we verify the effectiveness of the proposed method on the conditionalqa dataset. for future work, we plan to investigate how to better generate conditional questions and improve our model's performance in providing correct answers. despite the effectiveness of lsd in utilizing the structure of massive unsupervised data, there are still some potential points for improvement. one issue is that the state generator is only trained on answerable questions, leading to a distribution bias that there might be unanswerable questions. in addition, our pipeline can still not handle the position where a sentence has more than one answer, which limits our model's performance for broader scenarios. we will resolve these issues in future work. table 7: statistics of our scraped dataset. we present document count, average document length measured by word (avg. w) and sentences (avg. s), average sentence length (avg w/s) and tag distribution (h:p:li/tr).", "Solved": NaN, "Unsolved": NaN}, "258841216": {"num": 261, "type": 1, "title": "LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2305.13718v2.pdf", "conclusion": "in this paper, we first analysed the performance of chatgpt on logical reasoning benchmarks, as well as that using logically enhanced chain-of-thought prompting. from the results we can conclude that though language modeling can help compress all data both with and without supervision into neural models and accept human instructions, it is still really weak in perform logical reasoning. specifically, llms often fail at reaching correct deductions based on the given facts and rules, and cannot distil the logical reasoning structure from observed prompts to generalize to new problems. we hope these observations could bring some insights to future research on introducing logic prior into large language models. one step further, we combine previous data-driven approach, i.e., merit+, a selfsupervised pre-training method for logical reasoning, with flan-t5-3b, and the results have also demonstrated its effectiveness.  ", "Solved": NaN, "Unsolved": NaN}, "263334529": {"num": 262, "type": 1, "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method", "pdfurl": "https://export.arxiv.org/pdf/2310.00305v1.pdf", "conclusion": "in this paper, we study different prompting methods for using llms in news claim verification.we introduce a hierarchical step-by-step (hiss) method that prompts llm to perform the verification in fine-grained steps, aiming to mitigate the omission of thoughts and fact hallucination.validated on two public datasets, hiss prompting improves the performance of llms on the task over fullysupervised sota models and its strong few-shot icl-based counterparts.hiss prompted explanations show superior explainability in their coverage readability.in the future, we will build a conversational factchecking model based on llms which can be userfriendly and incorporate human fact-checkers in the loop.", "Solved": NaN, "Unsolved": NaN}, "225075843": {"num": 263, "type": 1, "title": "Differentiable Open-Ended Commonsense Reasoning", "pdfurl": "https://arxiv.org/pdf/2010.14439v2.pdf", "conclusion": "we introduce and study a new task -open-ended commonsense reasoning (opencsr) -which is both realistic and challenging.we construct three opencsr versions of widely used datasets targeting commonsense reasoning with a novel crowdsourced collection of multiple answers, and evaluate a number of baseline methods for this task.", "Solved": NaN, "Unsolved": NaN}, "236771976": {"num": 264, "type": 1, "title": "MuSiQue: Multihop Questions via Single-hop Question Composition", "pdfurl": "https://www.aclanthology.org/2022.tacl-1.31.pdf", "conclusion": "constructing multihop datasets is a tricky process. it can introduce shortcuts and artifacts that models can exploit to circumvent the need for multihop reasoning. a bottom-up process of constructing multihop from single-hop questions allows systematic exploration of a large space of multihop candidates and greater control over which questions we compose. we showed how to use such a carefully controlled process to create a challenging dataset that, by design, requires connected reasoning by reducing potential reasoning shortcuts, minimizing train-test leakage, and including harder distractor contexts. empirical results show that -ans has a substantially higher human-model gap and is significantly less cheatable via disconnected reasoning than previous datasets. the dataset also comes with unanswerable questions, and question decompositions which we hope spurs further work in developing models that get right answers for the right reasons.", "Solved": NaN, "Unsolved": NaN}, "238744204": {"num": 265, "type": 1, "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?", "pdfurl": "https://export.arxiv.org/pdf/2110.06918v3.pdf", "conclusion": "in this paper, we propose spar, a salient-phrase aware dense retriever, which can augment any dense retriever with the lexical matching capacity and out-of-domain generalization from a sparse retriever. this is achieved by training a dense lexical model \u03bb to imitate the behavior of the teacher sparse retriever, the feasibility of which remained unknown until this work. we show that spar outperforms previous state-of-the-art dense and sparse retrievers, matching or even exceeding more complex hybrid systems, on various in-domain and outof-domain evaluation datasets. for future work we plan to explore if a dense retriever can be trained to learn lexical matching directly without relying on a teacher model. this way, we can avoid imitating the errors of the sparse retriever, and devise new ways of training dense retrievers that can potentially surpass hybrid models. moreover, there are several intriguing findings in this work that may warrant further study, such as why spar's acc@k improves relatively to the hybrid model as k increases, and why joint training is less effective than post-hoc vector concatenation.", "Solved": NaN, "Unsolved": NaN}, "258822799": {"num": 266, "type": 1, "title": "HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2305.11747v1.pdf", "conclusion": "we introduce helma, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing and improving hallucinations. to automatically generate large-scale samples, we propose a chatgpt-based two-step approach, i.e., samplingthen-filtering. we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.", "Solved": NaN, "Unsolved": NaN}, "258564753": {"num": 267, "type": 1, "title": "DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation", "pdfurl": "https://export.arxiv.org/pdf/2305.05589v1.pdf", "conclusion": "in this paper we proposed a novel qa domain adaptation framework called domaininv, an unsupervised algorithm which does not require the use of labeled target domain, neither it depends on the synthetic data or pseudo labeled target domain. domaininv uses 1) domain invariant fine tuning which fine tunes the qa model using the target style on the source domain and 2) adversarial label correction which identifies the target distributions which are still far apart from source domain and optimize the feature generator to bring them closer near to source support class wisely. evaluation of domaininv showed that it outperforms all the baselines and achieves the superior performance establishing the new benchmark on qa domain adaptation.", "Solved": NaN, "Unsolved": NaN}, "261823125": {"num": 268, "type": 1, "title": "CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration", "pdfurl": "https://export.arxiv.org/pdf/2309.07822v2.pdf", "conclusion": "in our paper, we present a novel approach for automatic data augmentation by llm generated counterfactual instances diverse in surface form and semantic content. our results show that augmenting training data of smaller models with llm generated cfs consistently improves generalization capabilities of the underlying models across six ood extractive qa datasets. we further show that models trained on cf augmented data are easier to calibrate, both when considering the standard confidence-based setup as well as the explanationaugmented calibration setup. finally, show that rationale-augmented calibrator models prefer concise explanations, rather than comprehensive ones. by highlighting the fact that more diverse cf instances improve the quality of the models' internal representations by covering a broader part of the input space we pave the way for future works exploring the relation between surface form and semantic diversity of data used for augmentation and the models' generalization performance.", "Solved": NaN, "Unsolved": NaN}, "253098647": {"num": 269, "type": 1, "title": "Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models", "pdfurl": "https://export.arxiv.org/pdf/2210.12607v1.pdf", "conclusion": "in this work, we proposed cft as an improvement upon end-to-end learning. to enable research on this topic, we developed a new schema for generating recommendation datasets, which we instantiated in two domains. we showed that cft indeed consistently outperforms end-to-end learning, as much as 32% for local dining. furthermore, we found evidence suggesting that more component tasks can be beneficial for cft. finally, instantiating chain of thought prompting in our dataset and cft in sports understanding, we found cft to be as good or better with lms only 7.4% of the size.", "Solved": NaN, "Unsolved": NaN}, "203836061": {"num": 270, "type": 1, "title": "Multi-hop Question Answering via Reasoning Chains", "pdfurl": "https://arxiv.org/pdf/1910.02610v2.pdf", "conclusion": "in this work, we learn to extract reasoning chains to answer multi-hop reasoning questions. experimental results show that the chains are as effective as human annotations, and achieve strong performance on two large datasets. however, as remarked in past work (chen and durrett, 2019;min et al., 2019a), there are several aspects of hot-potqa and wikihop which make them require multi-hop reasoning less strongly than they otherwise might. as more challenging qa datasets are built based on lessons learned from these, we feel that reasoning in a more explicit way and properties of chain-like representations will be critical. this work represents a first step towards this goal of improving qa systems in such settings.", "Solved": NaN, "Unsolved": NaN}, "234470046": {"num": 271, "type": 1, "title": "Encoding Explanatory Knowledge for Zero-shot Science Question Answering", "pdfurl": "https://www.aclanthology.org/2021.iwcs-1.5.pdf", "conclusion": "in this paper, we proposed a neural encoding mechanism for explanatory knowledge acquisition and transfer, n-xkt. we evaluated the impact of the encoding mechanism on downstream science qa. the proposed model delivers better generalisation and accuracy for qa tasks that require multi-hop and explanatory inference. the proposed encoding mechanism can be used to deliver zero-shot inference capabilities, providing comparable performance when compared to supervised models on qa. these results supports the hypothesis that pretraining tasks targeting abstract and explanatory knowledge acquisition can constitute and impor- ", "Solved": NaN, "Unsolved": NaN}, "258833055": {"num": 272, "type": 1, "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "pdfurl": "https://export.arxiv.org/pdf/2303.11366v4.pdf", "conclusion": "at its core, reflexion is an optimization technique that uses natural language to do policy optimization.policy optimization is a powerful approach to improve action choice through experience, but it may still succumb to non-optimal local minima solutions.in this study, we limit long-term memory to a sliding window with maximum capacity, but we encourage future work to extend the memory component of reflexion with more advanced structures such as vector embedding databases or traditional sql databases.specific to code generation, there are many practical limitations to testdriven development in specifying accurate input-output mappings such as non-deterministic generator functions, impure functions that interact with apis, functions that vary output according to hardware specifications, or functions that invoke parallel or concurrent behavior that may be difficult to predict.", "Solved": NaN, "Unsolved": NaN}, "258822815": {"num": 273, "type": 1, "title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations", "pdfurl": "https://www.aclanthology.org/2023.acl-long.784.pdf", "conclusion": "we present quest, a new benchmark of queries which contain implicit set operations with corresponding sets of relevant entity documents. our experiments indicate that such queries present a challenge for modern retrieval systems. future work could consider approaches that have better inductive biases for handling set operations in natural language expressions (for example, vilnis et al. (2018)). the attributions in quest can be leveraged for building systems that can provide finegrained attributions at inference time. the potential of pretrained generative lms and multi-evidence aggregation methods to answer set-seeking selective queries, while providing attribution to sources, can also be investigated.", "Solved": NaN, "Unsolved": NaN}, "240353952": {"num": 274, "type": 1, "title": "Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.806.pdf", "conclusion": "we present dcqa that connects pieces in a document via open-ended questions and full-sentence answers. dcqa is collected via a new paradigm that regards the main purpose of a new sentence as an answer to a free-form question evoked earlier in the context. consequently, this paradigm yields both discourse and semantic links across all sentences in a document. dcqa is introduced with the goal of providing a more scalable data collection paradigm, also as initial resource, for answering open-ended questions for discourse comprehension. our experiments showed that dcqa provides valuable supervision for such tasks.", "Solved": NaN, "Unsolved": NaN}, "226955938": {"num": 275, "type": 1, "title": "Unsupervised Explanation Generation for Machine Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2011.06737v1.pdf", "conclusion": "in this paper, we aim to improve the explainability for the machine reading comprehension task, which is different from most of the previous works that were only striving for better objective evaluation scores. to achieve this goal, we propose a novel mechanism called recursive dynamic gating (rdg) to gradually refine the amount of the input information in each layer of the pre-trained language model. also, we propose an attention smoothing technique that will increase the accuracy of the rdg mechanism. experimental results on three multiple-choice machine reading comprehension datasets show that the proposed rdg mechanism could not only improve the objective evaluation scores, but also show an advantage over the traditional attention mechanism in explainability.", "Solved": NaN, "Unsolved": NaN}, "261030563": {"num": 276, "type": 1, "title": "Beam Retrieval: General End-to-End Retrieval for Multi-Hop Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2308.08973v1.pdf", "conclusion": "we present beam retrieval, a general end-to-end retrieval framework for multi-hop qa. this approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. experimental results on three benchmark datasets prove the effectiveness of beam retrieval and demonstrate it could substantially improve the qa performance of downstream reader. in general, beam retrieval establishes a strong baseline for complex multi-hop qa, where we hope that future work could explore more advanced solutions.", "Solved": NaN, "Unsolved": NaN}, "237490850": {"num": 277, "type": 1, "title": "Extract, Integrate, Compete: Towards Verification Style Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2109.05149v1.pdf", "conclusion": "in this paper, we present a novel verification style reading comprehension dataset named vgaokao from the chinese language tests of gaokao for chinese native speakers, which embed multiple advanced language understanding skills. to address the challenges in vgaokao, we propose a new extract-integrate-compare approach for complementary evidence retrieval/integration and option discrimination. experiments show that our approach outperforms several strong baselines, with additional merits of efficiency and explainability. we believe vgaokao is a challenging test-bed for natural language understanding in chinese and encourage further research in verification style reading comprehensionn.", "Solved": NaN, "Unsolved": NaN}, "261640869": {"num": 278, "type": 1, "title": "Retrieving Chinese Questions and Answers Based on Deep-Learning Algorithm", "pdfurl": NaN, "conclusion": "in light of the current state of open-domain reading comprehension question-answering methods, this study proposed the qarcg method based on the dual-process theory of cognitive science.the qarcg approach views open-domain question answering as a combination of retrieval and reasoning systems.system 1, responsible for retrieval, extracts triples from given supporting text and iteratively retrieves information from wikipedia, constructing a cognitive graph with reasoning paths.system 2, responsible for reasoning, learns the interaction information between paragraphs using rnn based on the built cognitive graph.it reorders and scores different reasoning paths, and predicts the answer's span based on the highest-scoring reasoning path's paragraphs.the integration of retrieval and reasoning reduces the loss in graph construction and maintains graph structure, thereby enhancing interpretability and overcoming the lack of reasoning interpretability in traditional end-to-end reading comprehension methods.additionally, it addresses the requirement for existing large-scale knowledge graphs in knowledge graph question answering.", "Solved": NaN, "Unsolved": NaN}, "259089062": {"num": 279, "type": 1, "title": "Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training", "pdfurl": "https://export.arxiv.org/pdf/2306.03166v1.pdf", "conclusion": "in this work, we propose recontriever to further explore the potential of contrastive pre-training to reduce the demand of human-annotated data for dense retrievers. benefiting from multiple positives from the same document as well as relevance-aware contrastive loss, our model achieves remarkable performance under zero-shot cases. additional results on low data resources further verify its value under various practical scenarios.", "Solved": NaN, "Unsolved": NaN}, "264555175": {"num": 280, "type": 1, "title": "Detrimental Contexts in Open-Domain Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2310.18077v1.pdf", "conclusion": "the reader models in retrieve-then-read pipelines are sensitive to the retrieved contexts when generating answers.damaging passages in this set can lead to incorrect responses.filtering damaging passages results in increases in em scores without the need for architectural modifications.despite  shortcomings in evaluating qa with exact match, we demonstrate that by filtering passages, models can achieve 10% higher em scores using subsets of context that are 20x times smaller.", "Solved": NaN, "Unsolved": NaN}, "257985191": {"num": 281, "type": 1, "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2304.03158v1.pdf", "conclusion": "this paper proposes a multi-view contextual masked auto-encoding pre-training architecture for better passage retrieval. experiment results show that multi-view representation and multi-view decoding paradigms significantly contribute to effec-tive retrieval performance. our method also shows good robustness and stability. in the future, we will further explore incorporating new pre-training paradigms to get more effective and robust retrievers.", "Solved": NaN, "Unsolved": NaN}, "228375214": {"num": 282, "type": 1, "title": "Multilingual Transfer Learning for QA Using Translation as Data Augmentation", "pdfurl": "https://arxiv.org/pdf/2012.05958v1.pdf", "conclusion": "in this work, we highlight open challenges in the existing multilingual approach by (lewis et al. 2020) and (clark et al. 2020). specifically, we show that large pretrained multi-lingual lms are not enough for this task. we produce several novel strategies for multilingual qa that go beyond zero-shot training and outshine the previous baseline built on top of mbert. we present a translation model that has 14 times more training data. further, our at and laf strategies utilize translation as data augmentation to bring the language-specific embeddings of the lm closer to each other. these approaches help us significantly improve the cross-lingual transfer. empirically, our models demonstrate strong results and all approaches improve over the previous zs strategy. we hope these techniques spur further research in the field such as exploring other multilingual lms and invoking additional networks on top of large lms for multilingual nlp.", "Solved": NaN, "Unsolved": NaN}, "221655732": {"num": 283, "type": 1, "title": "Multi-Hop Fact Checking of Political Claims", "pdfurl": "https://arxiv.org/pdf/2009.06401v3.pdf", "conclusion": "in this paper, we studied the novel task of multi-hop reasoning for fact checking of real-world political claims, which encompasses both evidence retrieval and claim veracity prediction. we presented politihop, the first political fact checking dataset with annotated evidence sentences. we compared several models on politihop and found that the multi-hop architecture transformer-xh slightly outperforms bert in most of the settings, especially in terms of evidence retrieval, where bert is easily fooled by named entity overlaps between the claim and evidence sentences. the performance of transformer-xh is further improved when retrieving more than two evidence sentences and the number of hops larger than one, which corroborates the assumption of the multi-hop nature of the task. in the first setting, the models are trained for 4 epochs on liar-plus. in the second setting, the models are trained for 8 epochs on politihop. in the third setting, models are trained for 4 epochs on liar-plus, followed by 4 epochs on politihop. in every setting, models are evaluated on the dev set and the model with the best label prediction macro-f1 score is saved, which enables early stopping. for the fourth setting, we pre-train the model for 2 epochs on the fever dataset, followed by 4 epochs on liar-plus, the fine-tune on politihop for 4 epochs.", "Solved": NaN, "Unsolved": NaN}, "257833654": {"num": 284, "type": 1, "title": "Quantifying the Academic Quality of Children's Videos using Machine Comprehension", "pdfurl": "https://export.arxiv.org/pdf/2303.17201v1.pdf", "conclusion": "in this research, we demonstrate the ability of an rc model to assess academic quality by introducing a new dataset consisting of questions and answers from children's videos. we then determine the academic value of the top channels by measuring the number of textbook questions answered correctly by the model. our analysis of over 80,000 videos posted on the top 100 channels provides a comprehensive evaluation of the academic quality of content on ytk and utilizes a large dataset of middle school textbook questions on various topics. our findings reveal the academic topics covered in these children's videos, and we compare the quality of the channels.", "Solved": NaN, "Unsolved": NaN}, "258865847": {"num": 285, "type": 1, "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering", "pdfurl": "https://www.aclanthology.org/2023.acl-long.110.pdf", "conclusion": "in this work, we present a novel pre-training scheme for multi-document tasks. first, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. second, we generate high-quality large-scale qa pre-training data using a controlled generation approach, in which each qa pair originates from a salient sentence in one of the documents in the set.", "Solved": NaN, "Unsolved": NaN}, "254853816": {"num": 286, "type": 1, "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings", "pdfurl": "https://export.arxiv.org/pdf/2212.09741v3.pdf", "conclusion": "we introduced instructor, a single model that creates broadly-applicable text embeddings using natural language instructions. we constructed medi, a collection of diverse datasets, to finetune instructor with instructions. our extensive experiments showed that instructor achieves state-of-the-art performance on text embedding benchmarks, as well as prompt retrieval for fewshot in-context learning. we hope that researchers and practitioners will benefit from our embeddings or our datasets for tasks of their interest.", "Solved": NaN, "Unsolved": NaN}, "258841283": {"num": 287, "type": 1, "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2305.14283v3.pdf", "conclusion": "this paper introduces the rewrite-retrieve-read pipeline, where a query rewriting step is added for the retrieval-augmented llm.this approach is applicable for adopting a frozen large language model as the reader and a real-time web search engine as the retriever.further, we propose to apply a tuneable small language model the rewriter, which can be trained to cater to the frozen retriever and reader.", "Solved": NaN, "Unsolved": NaN}, "259316419": {"num": 288, "type": 1, "title": "Meta-training with Demonstration Retrieval for Efficient Few-shot Learning", "pdfurl": "https://export.arxiv.org/pdf/2307.00119v1.pdf", "conclusion": "we have proposed a meta-training method ( \u00a73.2) that retrieves ( \u00a73.1) semantically similar demonstrations from a diverse demonstration bank ( \u00a73.3). our method achieves higher performance on average across many tasks than other strong parameterefficient few-shot baselines ( \u00a75). in future work, one could explore a mixture of demonstration retrieval and passage retrieval for improved performance on a wider variety of tasks-including knowledge-intensive tasks.", "Solved": NaN, "Unsolved": NaN}, "248085193": {"num": 289, "type": 1, "title": "Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2204.04581v3.pdf", "conclusion": "in this paper, we propose a more accurate and efficient architecture to utilize qa-pairs as representation units of knowledge. our proposed model qamat outperforms repaq significantly, while leveraging our less expensive training procedure. furthermore, we show how a qa-backed model can perform compositional reasoning and address more complex queries. in the future, we hope to further close the gap with state-of-the-art documentbased retrieve-and-read models and extend this approach to a broader set of tasks.", "Solved": NaN, "Unsolved": NaN}, "202773198": {"num": 290, "type": 1, "title": "Answering Complex Open-domain Questions Through Iterative Query Generation", "pdfurl": "https://www.aclweb.org/anthology/D19-1261.pdf", "conclusion": "in this paper, we presented golden (gold entity) retriever, an open-domain multi-hop question answering system for scalable multi-hop reasoning. through iterative reasoning and retrieval, golden retriever greatly improves the recall of gold supporting facts, thus providing the question answering model a much better set of context documents to produce an answer from, and demonstrates competitive performance to the state of the art. generating natural languages queries for each step of reasoning, golden retriever is also more interpretable to humans compared to previous neural retrieval approaches and affords better understanding and verification of model behavior. we start from the wikipedia dump file containing the introductory paragraphs used in hotpotqa that yang et al. (2018) provide, 10 and add the fields corresponding to wikipedia page titles and the introductory paragraphs (text) into the index.", "Solved": NaN, "Unsolved": NaN}, "253018834": {"num": 291, "type": 1, "title": "QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.147.pdf", "conclusion": "in this paper, we propose a novel self-supervised framework called qada for qa domain adaptation. qada introduces: (1) hidden space augmentation tailored for qa data to enrich target training corpora; and (2) an attention-based contrastive adaptation to learn domain-invariant features that generalize across source and target domain. our experiments demonstrate the effectiveness of qada: it achieves a superior performance over state-ofthe-art baselines in qa domain adaptation.", "Solved": NaN, "Unsolved": NaN}, "226278099": {"num": 292, "type": 1, "title": "HOVER: A Dataset for Many-Hop Fact Extraction And Claim Verification", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.309.pdf", "conclusion": "we present hover, a fact extraction and verification dataset requiring evidence retrieval from as many as four wikipedia articles that form reasoning graphs of diverse shapes. we show that the performance of existing state-of-the-art models degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of robust many-hop reasoning in achieving strong results. we hope that hover will encourage the development of models capable of performing complex many-hop reasoning in the tasks of information retrieval and verification.   document retrieval, sentence selection, and claim verification. the fine-tuning is done with a batch size of 16 and the default learning rate of 5e-5 without warmup. we set k r = 20, k p = 5, \u03ba p = 0.5, and \u03ba s = 0.3 based on the memory limit and the dev set performance. we select our system with the best dev-set verification accuracy and report its scores on the hidden test set. the entire pipeline is visualized in fig. 2. for document retrieval and sentence selection tasks, we fine-tune the bert on 4 nvidia v100 gpus for 3 epochs. the training of both tasks takes around 1 hour. for claim verification task, we fine-tune the bert on a single nvidia v100 for 3 epochs. the training finishes in 30 minutes.", "Solved": NaN, "Unsolved": NaN}, "256846551": {"num": 293, "type": 1, "title": "STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK", "pdfurl": "https://export.arxiv.org/pdf/2302.06729v1.pdf", "conclusion": "we aim to enable machines to perform multi-step reasoning while explaining their answers. we believe that teaching machines how to manipulate premises and reach conclusions can be an important step towards true language understanding. with that in mind, we introduce street, a new multi-task reasoning and explanation resource covering various forms of reasoning in the context of questionanswering. we hope this benchmark will allow for a more systematic evaluation of the reasoning capabilities of natural language systems. future avenues of research include exploring the reasoning capabilities and knowledge retrieval and using supervised models trained on multi-step reasoning data to bootstrap unsupervised learning for multi-step reasoning.", "Solved": NaN, "Unsolved": NaN}, "232075995": {"num": 294, "type": 1, "title": "Published as a conference paper at ICLR 2021 LEARNING REASONING PATHS OVER SEMANTIC GRAPHS FOR VIDEO-GROUNDED DIALOGUES", "pdfurl": "https://export.arxiv.org/pdf/2103.00820v2.pdf", "conclusion": "we proposed pdc, a novel approach to learning a reasoning path over dialogue turns for videogrounded dialogues. our approach exploits the compositional semantics in each dialogue turn to construct a semantic graph, which is then used to derive an optimal path for feature propagation. our experiments demonstrate that our model can learn to retrieve paths that are most relevant to the current question. we hope our approach can motivate further study to investigate reasoning over multiple turns, especially in complex settings with interconnected dialogue flows (sun et al., 2019).", "Solved": NaN, "Unsolved": NaN}, "253510370": {"num": 295, "type": 1, "title": "World Knowledge in Multiple Choice Reading Comprehension", "pdfurl": "https://export.arxiv.org/pdf/2211.07040v2.pdf", "conclusion": "we propose an approach that can automatically flag questions that can be answered without contextual information. however, the remaining questions are not necessarily high-quality questions, since many other aspects make up question quality. second, the experiments are conducted using only the electra model, though it is expected similar trends will be picked up by alternative transformer-based language models. further, exams might be aimed at a level where a lack of specific knowledge may be assumed. our work does not consider variable candidate knowledge levels, and our evaluation was only done by highly educated (we'd like to think) graduate students. finally, we acknowledge that our human evaluation was limited in size and questions, however it is clearly demonstrated that for low 'shortcut entropy' questions, comprehension is not necessarily required.", "Solved": NaN, "Unsolved": NaN}, "235313893": {"num": 296, "type": 1, "title": "Knowing More About Questions Can Help: Improving Calibration in Question Answering", "pdfurl": "https://arxiv.org/pdf/2106.01494v1.pdf", "conclusion": "we introduce a richer feature space for question answering calibrators with question and context embeddings and paraphrase-augmented inputs. our work suggests deciding the correctness of a qa system depends on both the semantics of the questioncontext and the confidence of the model. we thoroughly test our calibrator in domain shift, adversarial, and open domain qa settings. the experiments show noticeable gains in performance across all settings. we further demonstrate our calibrator's general applicability by using it as a reranker in extractive open domain qa. to summarize, our calibrator is simple, effective and general, with potential to be incorporated into existing models or extended for other nlp tasks.", "Solved": NaN, "Unsolved": NaN}, "258865893": {"num": 297, "type": 1, "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts", "pdfurl": "https://export.arxiv.org/pdf/2305.14628v2.pdf", "conclusion": "we proposed the more framework where we construct a pool of specialized qa models that excel at different reasoning types, and then train an answer selector to select the best answer among them.experiments on 12 datasets covering four reasoning types demonstrate that more achieve better generalizability than all baselines.more importantly, the inter-expert agreement features in more offer useful signals for training effective calibrators that improve selective qa and also improve human verification of the system's final predictions.", "Solved": NaN, "Unsolved": NaN}, "239016681": {"num": 298, "type": 1, "title": "Learning to Solve Complex Tasks by Talking to Agents", "pdfurl": "https://arxiv.org/pdf/2110.08542v1.pdf", "conclusion": "in this work we motivate a new challenge task of solving complex task by communicating with existing ai agents. developing approaches for this challenge, we argue, could allow for more generalizable, privacy-preserving and efficient models. towards this goal, we introduce a new benchmark dataset commaqa which involves multihop questions with three multi-hop reasoning challenges, all solvable by composing four qa agents. each agent has an internal knowledge base (similar to ai assistants or large lms) that can be queried via natural language queries. experiments with state-of-art language models indicated that they struggle to solve commaqa, even when provided with agents' internal knowledge. in contrast, a model that is able to learn to communicate with the agents, albeit using annotated decompositions, is able to solve this task. this indicates the need and potential of such approaches to solve complex tasks. we hope this dataset will enable future work on learning to communicate with agents without relying on this additional supervision. theory 1: what movies have people from the country $1 acted in? a1:select(textqa, _, \"who are from $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"which movies has {} been an actor in?\") theory 2: what movies have the directors from $1 directed? a1:select(textqa, _, \"who is from the country $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"which movies has {} directed?\") theory 3: what awards have movies produced by people born in $1 won? a1:select(textqa, _, \"who were born in the year $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"for which movies was {} the producer?\") a3:project_keys_flat_unique(tableqa, a2, \"which awards did the movie {} win?\") theory 4: what awards have movies written by people born in $1 won? a1:select(textqa, _, \"who were born in the year $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"what movies has {} written?\") a3:project_keys_flat_unique(tableqa, a2, \"which awards were given to {}?\")", "Solved": NaN, "Unsolved": NaN}, "234095341": {"num": 299, "type": 1, "title": "VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2021.acl-short.131.pdf", "conclusion": "in this work we introduce and examine a powerful yet simple model for reading comprehension on long texts which we call vault, based on the hypothesis that with a large sequence length long answers can be classified effectively without computationally heavy graph-based models. we validate our approach by showing it yields f1 scores competitive with heavier methods at a fraction of the decoding cost on two very different domain benchmark datasets that require reading long texts.  compare the correct answers produced by vault with the incorrect answers produced by the ablated model from the last row of table 3 (nq) and roberta baseline from the first row of table 2 (techqa).", "Solved": NaN, "Unsolved": NaN}, "258947821": {"num": 300, "type": 1, "title": "Fact-driven Logical Reasoning for Machine Reading Comprehension", "pdfurl": "https://export.arxiv.org/pdf/2105.10334v2.pdf", "conclusion": "in this work, we propose extracting a general form called \"fact unit\" to cover both commonsense and temporary knowledge units for logical reasoning. our proposed focal reasoner not only better uncovers the logical structures within the context but also better captures the logical interactions between context and options. experimental results verify the effectiveness of our method.", "Solved": NaN, "Unsolved": NaN}, "196170479": {"num": 301, "type": 1, "title": "ELI5: Long Form Question Answering", "pdfurl": "https://arxiv.org/pdf/1907.09190v1.pdf", "conclusion": "we introduce the first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers. we show that abstractive models generate coherent answers and are competitive with extractive models in human evaluation. proposed models are far from human performance, in part due to the inability to exploit the long full web text. we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers.", "Solved": NaN, "Unsolved": NaN}, "258841328": {"num": 302, "type": 1, "title": "QLORA: Efficient Finetuning of Quantized LLMs", "pdfurl": "https://export.arxiv.org/pdf/2305.14314v1.pdf", "conclusion": "we have shown evidence that our method, qlora, can replicate 16-bit full finetuning performance with a 4-bit base model and low-rank adapters (lora). despite this evidence, we did not establish that qlora can match full 16-bit finetuning performance at 33b and 65b scales. due to the immense resource costs, we leave this study to future work.another limitation is the evaluation of instruction finetuning models. while we provide evaluations on mmlu, the vicuna benchmark, and the oa benchmark, we did not evaluate on other benchmarks such as bigbench, raft, and helm, and it is not ensured that our evaluations generalize to these benchmarks. on the other hand, we perform a very broad study on mmlu and develop new methods for evaluating chatbots.from the evidence presented, it appears that the performance of these benchmarks likely depends how similar the finetuning data is to the benchmark dataset. for example, flan v2 is similar to mmlu, but dissimilar to chatbot benchmarks and vice versa for the chip2 dataset and both models score accordingly on the mmlu and vicuna benchmarks. this highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place. do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? maybe something else? because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction. we should ensure as a community that the benchmarks measure what we care about.while we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible ai evaluation of guanaco. we evaluate the likelihood of guanaco-65b to generate a socially biased sequence of tokens compared to other models in table 8. we see that the average score in guanaco-65b is much lower than other raw pretrained models. as such, it seems that finetuning on the oasst1 dataset reduces the bias of the llama base model. while these results are encouraging, it is unclear if guanaco does also well when assessed on other types of biases. we leave further evaluation of analyzing biases in guanaco and similar chatbots to future work.an additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base models, or different adapter methods. besides lora, there is also a wide variety parameter efficient finetuning (peft) methods that have been shown to work well. however, it is unclear if these methods scale to large models. we used lora as many results established its robustness but other adapters might yield better performance. since finetuning after quantization seems to recover most of the information that is lost during quantization this might enable much more aggressive quantization. for example, 3-bit gptq quantization of the basemodel with lora might also yield 16-bit full finetuning performance after finetuning.", "Solved": NaN, "Unsolved": NaN}, "218487535": {"num": 303, "type": 1, "title": "Obtaining Faithful Interpretations from Compositional Neural Networks", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.495.pdf", "conclusion": "we introduce the concept of module-wise faithfulness, a systematic evaluation of faithfulness in neural module networks (nmns) for visual and textual reasoning. we show that na\u00efve training of nmns does not produce faithful modules and propose several techniques to improve module-wise faithfulness in nmns. we show how our approach leads to much higher module-wise faithfulness at a low cost to performance. we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models. ", "Solved": NaN, "Unsolved": NaN}, "248476403": {"num": 304, "type": 1, "title": "Inferring Implicit Relations with Language Models", "pdfurl": "https://arxiv.org/pdf/2204.13778v1.pdf", "conclusion": "in this work, we propose the task of implicit relation inference, which decouples the inference of reasoning steps from their execution. we introduce implicitrelations, a benchmark that includes over 600 questions implicit reasoning questions along with more than 2,000 annotated implicit relations. we show that large lms can infer implicit relations well in the in-context setup across multiple types of questions and reasoning skills, but that this success does not transfer to an improvement in the downstream task of answering implicit reasoning questions. our work sheds light on the types of capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for further improving the ability of models to infer implicit relations.", "Solved": NaN, "Unsolved": NaN}, "235186933": {"num": 305, "type": 1, "title": "Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting", "pdfurl": "https://www.aclanthology.org/2021.acl-long.465.pdf", "conclusion": "we explored the task of difficulty-controllable question generation, with question difficulty redefined as the inference steps required to answer it. a step-by-step generation framework was proposed to accomplish this objective, with an input sampler to extract the reasoning chain, a question generator to produce a simple question, and a question rewriter to further adapt it into a more complex one. a dataset was automatically constructed based on hotpotqa to facilitate the research. extensive evaluations demonstrated that our method can effectively control difficulty of the generated questions, and keep high question quality at the same time.", "Solved": NaN, "Unsolved": NaN}, "253018873": {"num": 306, "type": 1, "title": "Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers", "pdfurl": "https://export.arxiv.org/pdf/2210.11265v2.pdf", "conclusion": "this paper stimulates the compositional reasoning process of humans in decision-making, and makes the following hypotheses: (1) the intuitive perception system (system 1) and cognitive reasoning system (system 2) can be decoupled and (2) the complex decision-making can be disentangled into multi-step execution of fundamental reasoning skills. correspondingly, we propose reason-former, a compositional general-purpose reasoning framework. reasonformer decouples the representation module and reasoning modules, which are pre-trained to expert in fundamental reasoning skills. the reasoning modules are dynamically composed in parallel and cascaded manner to form a whole reasoning process. reasonformer is endto-end and unified in solving multiple tasks with one model. extensive experiments on 11 tasks reveal the compositional reasoning ability of reason-former and disentangling of representation and reasoning modules.", "Solved": NaN, "Unsolved": NaN}, "238856959": {"num": 307, "type": 1, "title": "Can Explanations Be Useful for Calibrating Black Box Models?", "pdfurl": "https://www.aclanthology.org/2022.acl-long.429.pdf", "conclusion": "limitations despite showing promising results in improving model generalization performance, our attribution-based approach does suffer from intensive computation cost. using either lime or shap to generate attributions requires running inference a fair number of perturbations when the input size is large (see appendix for details), which limits our method's applicability. but this doesn't undermine the main contribution of this paper, answering the question in the title, and our approach is still applicable as-is in the scenarios where we pay for access to the model but not per query.", "Solved": NaN, "Unsolved": NaN}, "251280129": {"num": 308, "type": 1, "title": "SPANDROP: Simple and Effective Counterfactual Learning for Long Sequences", "pdfurl": "https://export.arxiv.org/pdf/2208.02169v1.pdf", "conclusion": "in this paper, we presented spandrop, a simple and effective method for learning from long sequences, which ablates parts of the sequence at random to generate counterfactual data to distill the sparse supervision signal that is predictive of the desired output. we show via theoretical analysis and carefully designed synthetic datasets that spandrop and its variant based on the beta-bernoulli distribution, beta-spandrop, help models achieve competitive performance with a fraction of the data by introducing diverse augmented training examples, and generalize better to previously unseen data. our experiments on four real-world nlp datasets confirm these theoretical findings, and demonstrate span-drop's efficacy on strong neural models even when data is abundant.", "Solved": NaN, "Unsolved": NaN}, "252519173": {"num": 309, "type": 1, "title": "PROMPTAGATOR : FEW-SHOT DENSE RETRIEVAL FROM 8 EXAMPLES", "pdfurl": "https://export.arxiv.org/pdf/2209.11755v1.pdf", "conclusion": "in this paper, we have presented promptagator, a novel approach to few-shot retrieval. we showed that it is possible to create task-specific, end-to-end retrievers with only a few annotated examples. the few-shot examples, amplified by prompt-based llm query generation, simplifies the complexity of training neural retrievers for a new tasks and leads to promising retrieval performance gains. it hopefully inspires future research to further push the limit of few-shot retrieval, towards generalizable retrieval systems that can seamlessly and efficiently adapt to many tasks.", "Solved": NaN, "Unsolved": NaN}, "208089867": {"num": 310, "type": 1, "title": "TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration *", "pdfurl": "https://www.aclweb.org/anthology/D19-5309.pdf", "conclusion": "the textgraphs 2019 shared task on multi-hop inference for explanation regeneration received four team submissions that exceeded the performance of the baseline system. the systems used a variety of methods from additional knowledge resources (such as conceptnet or framenet) to directly training language models to perform multihop inference by predicting chains of facts. the top-performing system increased baseline performance by nearly a factor of two on this task, achieving a new state-of-the-art.", "Solved": NaN, "Unsolved": NaN}, "224803601": {"num": 311, "type": 1, "title": "OPEN QUESTION ANSWERING OVER TABLES AND TEXT", "pdfurl": "https://arxiv.org/pdf/2010.10439v1.pdf", "conclusion": "we focus on the problem of performing open question answering over tables and text in this paper. one interesting question we would like to ask in the future is: can we extend open question answering system to more modalities? some questions can be better answered by images and other resources, but the task can be drastically more challenging by including more modalities, as we have learned from this paper. finally, we believe the techniques we proposed might be useful for other open-qa setting, especially the comparisons between iterative retriever and fusion retriever. ", "Solved": NaN, "Unsolved": NaN}, "238856979": {"num": 312, "type": 1, "title": "Representation Decoupling for Open-Domain Passage Retrieval", "pdfurl": "https://arxiv.org/pdf/2110.07524v1.pdf", "conclusion": "in this paper, we analyze the contrastive conflicts problem in current contrastive learning framework adopted in the open-domain passage retrieval area. to solve such problem, we propose dense contextual sentence representation (dscr). concretely, we first decouple the original passage representations into contextual sentence-level ones, and then refine the original contrastive learning framework by creating sentence-aware positive and negative samples. our dcsr achieves significant performance gain compared to original dpr baseline, especially on datasets with severe conflicting problem. extensive experiments shows that our dcsr also enjoys better transferability, indicating that dcsr well captures the universality in different datasets.", "Solved": NaN, "Unsolved": NaN}, "264555286": {"num": 313, "type": 1, "title": "Knowledge Corpus Error in Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2310.18076v1.pdf", "conclusion": "in this work, we demonstrate that generated contexts may be more helpful than retrieved contexts in open-domain question answering.by revisiting the formulation of question answering, we identify a gap where retriever inevitably ignores potentially helpful contexts outside of the corpus.we call this knowledge corpus error, and design an experiment in order to observe knowledge corpus error empirically.paraphrasing the human-annotated gold contexts with llms led to increased reader performance in 3 out of 4 qa benchmarks, implying the existence of knowledge corpus error.", "Solved": NaN, "Unsolved": NaN}, "237513496": {"num": 314, "type": 1, "title": "Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.583.pdf", "conclusion": "in this paper, we have presented a novel paradigm of training a question filter to capture the semantics of a qa system's answering capability by distilling the knowledge of the answer scores from it. our experiments on three academic and one industrial qa benchmark show that the trained question models can estimate the pr/re curves of the qa system well, and can be used to effectively filter questions while only incurring a small drop in recall. an interesting future work direction is to analyze the impact/behavior of the question filters in a cross-domain setting, where the training and testing corpora are from different domains. this would allow examining the transferability of the semantics learned by the question filters. a complementary future work direction could be knowledge distillation from a sophisticated answer verification module like (rodriguez et al., 2019;kamath et al., 2020;zhang et al., 2021).", "Solved": NaN, "Unsolved": NaN}, "258841368": {"num": 315, "type": 1, "title": "What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems Question: Which non-Swedish actress also starred in The Light Between Oceans?", "pdfurl": "https://export.arxiv.org/pdf/2305.14331v1.pdf", "conclusion": "large general-purpose language models, such as gpt family of models [brown et al. 2020;openai 2023] , lamda [thoppilan et al. 2022], palm [anil et al. 2023;chowdhery et al. 2022], and others, have propagated into informationseeking workflows of a general audience. a vast host of existing and ongoing work in nlp examine the deficiencies of these language models, ranging from hallucinated generations  with background with background and highlights fig. 4. users' subjective rating of the system for the usefulness of highlights, background, their confidence in the ai system, selfconfidence, and satisfaction with ai. users rate self-confidence marginally higher in the condition with background than the condition without (left). however, users rate their satisfaction with ai in the condition with background slightly lower than without. users' satisfaction rating is slightly lower even after introducing highlights with the background (right), with a slightly higher rating of background utility in the condition with highlights than without. however, there are no other discernible differences in ratings in the background condition with or without highlight.", "Solved": NaN, "Unsolved": NaN}, "225311514": {"num": 316, "type": 1, "title": "The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification", "pdfurl": "https://web.archive.org/web/20200919100551/https:/res.mdpi.com/d_attachment/electronics/electronics-09-01472/article_deploy/electronics-09-01472.pdf", "conclusion": "limited by the size of knowledge bases, some knowledge gaps cannot be filled effectively in practice. we are studying a creative reasoning mechanism to solve this problem. in addition, fact verification is a very open and challenging task. it needs not only the support of linguistic features and background knowledge, but also the support of more complex multi-dimension information, such as social content and spatiotemporal information. for example, claims evolve over time, and what was fake yesterday is true today. ", "Solved": NaN, "Unsolved": NaN}, "239016730": {"num": 317, "type": 1, "title": "On the Robustness of Reading Comprehension Models to Entity Renaming", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.37.pdf", "conclusion": "in this paper, we systematically study the robustness of mrc models to entity name substitution. specifically, we propose a substitution framework along with candidate names of different implications. we experiment with three pretrained language models on five mrc datasets. we find that models trained on distantly-supervised datasets are susceptible to entity name substitution, while models trained on human-annotated datasets are relatively robust, with gpe renaming harder than per and org renaming. the lack of robustness can be further attributed to model's overreliance on entity knowledge and name clues. we also find that spanbert, which is pretrained using span-level objectives, shows better robustness than bert and roberta. leveraging these insights, we study defense approaches based on continual pretraining and demonstrate that entity-based masking policies are beneficial to model's robustness. future works include systematically studying the effect of background knowledge in mrc, and developing more effective methods to improve the robustness of mrc models.", "Solved": NaN, "Unsolved": NaN}, "235399966": {"num": 318, "type": 1, "title": "FINQA: A Dataset of Numerical Reasoning over Financial Data", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.300.pdf", "conclusion": "this paper introduces finqa, a new expertannotated qa dataset that aims to tackle numerical reasoning over real-world financial data. the questions in finqa pose great challenge for existing models to resolve domain-specific knowledge, as well as to acquire complex numerical reasoning abilities. we propose baseline frameworks and con-duct comprehensive experiments and analysis. the results show that current large pre-trained models still fall far behind the human expert performance. this encourages potential future work on developing pre-training tasks for such realistic, complex application domains. we believe finqa should serve as a valuable resource for the research community.", "Solved": NaN, "Unsolved": NaN}, "128344862": {"num": 319, "type": 1, "title": "Repurposing Entailment for Multi-Hop Question Answering Tasks", "pdfurl": "https://arxiv.org/pdf/1904.09380v1.pdf", "conclusion": "using entailment for question answering has seen limited success. neural entailment models are designed and trained on tasks defined over sentence pairs, whereas qa often requires reasoning over longer texts spanning multiple sentences. we propose multee, a novel qa model that addresses this mismatch. it uses an existing entailment model to both focus on relevant sentences and aggregate information from these sentences. results on two challenging qa datasets, as well as our ablation study, indicate that entailment based qa can achieve state-of-the-art performance and is a promising direction for further research.", "Solved": NaN, "Unsolved": NaN}, "212644640": {"num": 320, "type": 1, "title": "A Framework for Evaluation of Machine Reading Comprehension Gold Standards", "pdfurl": "https://www.aclweb.org/anthology/2020.lrec-1.660.pdf", "conclusion": "in this paper, we introduce a novel framework to characterise machine reading comprehension gold standards. this framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach. furthermore we applied the framework to analyse popular state-of-the-art gold standards for machine reading comprehension: we reveal issues with their factual correctness, show the presence of lexical cues and we observe that semantics-altering grammatical modifiers are missing in all of the investigated gold standards. studying how to introduce those modifiers into gold standards and observing whether state-of-the-art mrc models are capable of performing reading comprehension on text containing them, is a future research goal. a future line of research is to extend the framework to be able to identify the different types of exploitable cues such as question or entity typing and concrete overlap patterns. this will allow the framework to serve as an interpretable estimate of reading comprehension complexity of gold standards. finally, investigating gold standards under this framework where mrc models outperform the human baseline (e.g. squad) will contribute to a deeper understanding of the seemingly superb performance of deep learning approaches on them.", "Solved": NaN, "Unsolved": NaN}, "220445474": {"num": 321, "type": 1, "title": "CopyBERT: A Unified Approach to Question Generation with Self-Attention", "pdfurl": "https://www.aclweb.org/anthology/2020.nlp4convai-1.3.pdf", "conclusion": "we showed that having a unified encoder-decoder transformer model initialized with contextualized word embeddings and further extended with copy mechanism can already give state-of-the-art, without additional pre-training on generation tasks (dong et al., 2019). we also sped up the training of qg models that use bert by choosing predictions on output embeddings that are offset by one position ( \u00a73.3). this work shows the significance of explicitly using self-attentions of bert like models. these models can further be used in other tasks such as abstractive summarization and machine translation to see qualitative improvements.", "Solved": NaN, "Unsolved": NaN}, "238856994": {"num": 322, "type": 1, "title": "Towards Efficient NLP: A Standard Evaluation and A Strong Baseline", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.240.pdf", "conclusion": "in this work, we present elue, which is a public benchmark and platform for efficient models, and elasticbert, which is a strong baseline (backbone) for efficient static (dynamic) models. both of the two main contributions are aimed to build the pareto frontier for nlu tasks, such that the position of existing work can be clearly recognized, and future work can be easily and fairly measured. our future work is mainly in four aspects: (1) including more baselines in elue, (2) supporting the evaluation for more frameworks such as tensor-flow (abadi et al., 2016) ", "Solved": NaN, "Unsolved": NaN}, "263609132": {"num": 323, "type": 1, "title": "LARGE LANGUAGE MODELS CANNOT SELF-CORRECT REASONING YET", "pdfurl": "https://export.arxiv.org/pdf/2310.01798v1.pdf", "conclusion": "our research shows that llms are not yet capable of self-correcting their reasoning.this implies that expecting these models to inherently recognize and rectify their inaccuracies might be overly optimistic, at least with the current state of technology.more broadly, this underscores the need for ongoing improvement and a multi-faceted approach when deploying llms in real-world applications.in light of these findings, it is imperative for researchers and practitioners to approach the concept of self-correction with a discerning perspective, acknowledging its potential and recognizing its boundaries.by doing so, we can better equip this technique to address the limitations of llms, steering their evolution towards enhanced accuracy and reliability.", "Solved": NaN, "Unsolved": NaN}, "248722227": {"num": 324, "type": 1, "title": "Exploring Universal Intrinsic Task Subspace via Prompt Tuning", "pdfurl": "https://export.arxiv.org/pdf/2110.07867v3.pdf", "conclusion": "we study the hypothesis that plm adaptations to various tasks can be reparameterized as optimizations within a unified low-dimensional intrinsic task subspace. we develop an analysis tool ipt. it first finds a subspace by jointly decomposing the adaptive parameters of multiple tasks and then tunes parameters within the subspace for unseen data and tasks. experiments show the found subspaces contain good solutions for plm adaptations, which is strong evidence for our hypothesis.", "Solved": NaN, "Unsolved": NaN}, "222208820": {"num": 325, "type": 1, "title": "Cross-Thought for Sentence Encoder Pre-training", "pdfurl": "https://arxiv.org/pdf/2010.03652v1.pdf", "conclusion": "we propose a novel approach, cross-thought, to pre-train sentence encoder. experiments demonstrate that using cross-thought trained with short sequences can effectively improve sentence embedding. our pre-trained sentence encoder with further finetuning can beat several strong baselines on many nlp tasks.", "Solved": NaN, "Unsolved": NaN}, "234341179": {"num": 326, "type": 1, "title": "Improving Cross-Lingual Reading Comprehension with Self-Training", "pdfurl": "https://arxiv.org/pdf/2105.03627v1.pdf", "conclusion": "this paper presents the first self-training approach to improve cross-lingual machine reading comprehension. the experiments were conducted on largescale datasets in four different languages. the results showed that our approach improved the perfor-mance significantly compared to the baseline with 1 -16 em and 1 -8 f1 scores. we also analyzed how self-training improves cross-lingual reading comprehension in several aspects and found that improvements are correlated to zero-shot performance but not the number of pseudo-labels. ", "Solved": NaN, "Unsolved": NaN}, "261049149": {"num": 327, "type": 1, "title": "Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2308.10585v1.pdf", "conclusion": "to theoretically and empirically advance the numerical reasoning research in the llms era, in this paper, we employ equations as imrs to solve the numerical reasoning task by addressing two problems: (1) theoretically, how to prove that the equation is an imr with higher generation accuracy than programs; (2) empirically, how to improve the generation accuracy of equations with llms. for the first problem, we present and prove a proposition to compare the generation accuracy of different   ", "Solved": NaN, "Unsolved": NaN}, "258841405": {"num": 328, "type": 1, "title": "HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision", "pdfurl": "https://export.arxiv.org/pdf/2305.14237v1.pdf", "conclusion": "we present hug, a probabilistic, principled approach for explainable multi-hop reasoning without rationale supervision. hug explicitly models multi-hop reasoning by considering the dependency between documents and between sentences within a document. experimental results demonstrate that hug outperforms other state-of-the-art methods that do not rely on rationale labels.", "Solved": NaN, "Unsolved": NaN}, "245334850": {"num": 329, "type": 1, "title": "MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding", "pdfurl": "https://arxiv.org/pdf/2112.10728v2.pdf", "conclusion": "we present a new qa task, mumuqa, along with an evaluation benchmark for multimedia news understanding. the task is challenging in the requirement of cross-media grounding over images, captions, and news body text. we demonstrate the benefit of using multimedia knowledge extraction, both for generating silver-standard training data and for a pipeline-based multimedia qa system. the multimedia baselines are still considerably behind human performance, suggesting ample room for improvement. future work will incorporate other forms of media in news, such as video and audio, to facilitate information seeking from more comprehensive data sources. another direction is to infuse the endto-end multimedia qa system with additional input from the grounding and visual attribute extraction systems.  ", "Solved": NaN, "Unsolved": NaN}, "237415234": {"num": 330, "type": 1, "title": "CDLM: Cross-Document Language Modeling", "pdfurl": "https://arxiv.org/pdf/2101.00406v2.pdf", "conclusion": "we presented a novel pretraining strategy and technique for cross-document language modeling, providing better encoding for cross-document (cd) downstream tasks. our contributions include the idea of leveraging clusters of related documents for pretraining, via cross-document masking, along with a new long-range attention pattern, together driving the model to learn to encode cd relationships. this was achieved by extending the global attention mechanism of the longformer model to apply already in pretraining, creating encodings that attend to long-range information across and within documents. our experiments assess that our crossdocument language model yields new state-of-theart results over several cd benchmarks, while, in fact, employing substantially smaller models. our analysis showed that cdlm implicitly learns to recover long-distance cd relations via the attention mechanism. we propose future research to extend this framework to train larger models, and to develop cross-document sequence-to-sequence models, which would support cd tasks that involve a generation phase.", "Solved": NaN, "Unsolved": NaN}, "260735813": {"num": 331, "type": 1, "title": "Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2308.04711v3.pdf", "conclusion": "we have implemented methods for combining explanatory context from two knowledge sources: llmgenerated rationales and retrieved paragraphs from wikipedia.the first method involves training our smaller reasoning model on ratd datasets such that it becomes proficient at reasoning over long, noisy contexts which contain information from both knowledge sources.the second method is to use rationale ranking model scores for each knowledge source as guidance in constructing contexts that may contain information from both, or either knowledge source.we have shown that both methods are individually effective in significantly improving unseen question-answering performance both versus the baselines established by hartill et al. (2023) and versus a baseline that ablates both rr and ratd methods (section 3.4.3).", "Solved": NaN, "Unsolved": NaN}, "233296709": {"num": 332, "type": 1, "title": "CROSSFIT : A Few-shot Learning Challenge for Cross-task Generalization in NLP", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.572.pdf", "conclusion": "in this paper, we study the problem of building better few-shot learners via acquiring cross-task generalization ability from diverse nlp tasks. towards our goal, we introduce the crossfit challenge, an task setup that standardizes the training pipeline, data access and evaluation protocol. we also present the nlp few-shot gym, a repository of 160 diverse few-shot nlp tasks, to support crossfit learning in different scenarios. we empirically demonstrated that cross-task generalization can be acquired via multi-task learning and meta-learning; confirmed that the selection of seen tasks would influence the few-shot performance on unseen tasks.", "Solved": NaN, "Unsolved": NaN}, "235352906": {"num": 333, "type": 1, "title": "Prediction or Comparison: Toward Interpretable Qualitative Reasoning", "pdfurl": "https://arxiv.org/pdf/2106.02399v1.pdf", "conclusion": "in this paper, we aimed to solve the qualitative reasoning task in an interpretable manner. inspired by human cognition, we first summarized the questions into two categories, prediction and comparison. then an end-to-end trained reasoning component that contains two reasoning chains was designed. both reasoning chains contained multiple neural modules that provide transparent intermediate predictions for the understanding and reasoning process. the experimental results showed the effectiveness of our approach, and the analysis of each module and case study demonstrated the superior interpretability compared with the \"blackbox\" model. moreover, we found that some questions could be solved by both reasoning chains, thus increasing the default tolerance and generalization capability. furthermore, a human evaluation was conducted to validate the function of the synthetic text and provide an additional explanation for the superior performance achieved by our method. however, the error analysis showed the inadequacy under complicated scenarios. therefore, our future work will focus on applying interpretable reasoning on complex reasoning tasks. the annotated data and models are shared publicly 3 . ", "Solved": NaN, "Unsolved": NaN}, "202572622": {"num": 334, "type": 1, "title": "PubMedQA: A Dataset for Biomedical Research Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-1259.pdf", "conclusion": "we present pubmedqa, a novel dataset aimed at biomedical research question answering using yes/no/maybe, where complex quantitative reasoning is required to solve the task. pubmedqa has substantial automatically collected instances as well as the largest size of expert annotated yes/no/maybe questions in biomedical domain. we provide a strong baseline using multi-phase fine-tuning of biobert with long answer as additional supervision, but it's still much worse than just single human performance. there are several interesting future directions to explore on pubmedqa, e.g.: (1) about 21% of pubmedqa contexts contain no natural language descriptions of numbers, so how to properly handle these numbers is worth studying; (2) we use binary bow statistics prediction as a simple demonstration for additional supervision of long answers. learning a harder but more informative auxiliary task of long answer generation might lead to further improvements.", "Solved": NaN, "Unsolved": NaN}, "259316561": {"num": 335, "type": 1, "title": "Improving Multitask Retrieval by Promoting Task Specialization", "pdfurl": "https://export.arxiv.org/pdf/2307.00342v1.pdf", "conclusion": "multitask retrieval has compelling practical advantages such as model simplicity and memory efficiency, but it lags behind task-specific retrieval in the existing literature. we have shown that it is possible to significantly improve the performance of multitask retrieval by promoting task specialization. the key steps are the use of a base model optimized for multitasking with appropriate prompting and a per-parameter adaptive learning technique that upweights the task gradients by the parameters' sensitivity to the task losses. we have achieved strong results on the kilt retrieval benchmark. ", "Solved": NaN, "Unsolved": NaN}, "215737171": {"num": 336, "type": 1, "title": "Longformer: The Long-Document Transformer", "pdfurl": "https://arxiv.org/pdf/2004.05150v1.pdf", "conclusion": "we present longformer, a transformer-based model that is scalable for processing long documents and that makes it easy to perform a wide range of document-level nlp tasks without chunking/shortening the long input and without complex architecture to combine information across these chunks. longformer employs an attention pattern that combines local and global information while also scaling linearly with the sequence length. longformer achieves state-of-the-art results on the character-level language modeling tasks of text8 and enwik8. when pretrained, longformer consistently outperforms roberta on long document tasks and sets new state-of-the-art results on wiki-hop and triviaqa. for future work, we would like to explore other attention patterns that are more efficient by dynamically adapting to the input. we also would like to apply our model to other relevant long document tasks such as summarization.", "Solved": NaN, "Unsolved": NaN}, "258866004": {"num": 337, "type": 1, "title": "Machine Reading Comprehension using Case-based Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2305.14815v2.pdf", "conclusion": "we present cbr-mrc, a semi-parametric model for machine reading comprehension that is simple, accurate, and interpretable.our model stores a collection of cases, retrieves the most relevant cases for a given test question, and then explicitly reuses the reasoning patterns encoded in the embeddings of these cases to predict an answer.we show that our model performs well for both extracting answers and identifying supporting evidence on several mrc tasks compared to fully-parametric baselines.we also demonstrate the ability of our model to transfer to new domains with limited labeled data.finally, we analyze our model under varying conditions of lexical diversity and find that it is robust to high lexical diversity, whereas fullyparametric models show a drop in performance.", "Solved": NaN, "Unsolved": NaN}, "264426324": {"num": 338, "type": 1, "title": "PaRaDe: Passage Ranking using Demonstrations with Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2310.14408v1.pdf", "conclusion": "in this work we present passage ranking with demonstrations (parade), an extensive study on the topic of using demonstrations to improve reranking performance of llms.we show the challenges of applying demonstrations effectively, and that performance heavily relies on selecting \"good\" demonstrations.we propose a simple yet effective selection method, named difficulty-based selection (dbs), and confirm its effectiveness in both reranking using query likelihood scoring and query generation tasks.for future work, we plan to combine difficulty-based selection with similaritybased selection as an effort to further improve the robustness and effectiveness of the selected demonstrations, and extend dbs to other ranking paradigms (qin et al., 2023;sun et al., 2023).", "Solved": NaN, "Unsolved": NaN}, "238744411": {"num": 339, "type": 1, "title": "Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model", "pdfurl": "https://arxiv.org/pdf/2110.06560v1.pdf", "conclusion": "we propose a novel encoder-decoder model incorporating soft templates and moe to address the problem of complexity-controllable question generation. as most domains do not have training data for ccqg models, we propose a simple and effective cross-domain estimator to predict the missing complexity levels of questions. in the extensive experiments of both ccqg and complexity assessment tasks, our models achieve superior performance over the competitive baselines across all experimental settings. in the future, we will consider anaphora resolution and numerical reasoning in complexity estimator, and explore the performance of our model in different applications, such as examination and assisting qa systems.", "Solved": NaN, "Unsolved": NaN}, "253098851": {"num": 340, "type": 1, "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.174.pdf", "conclusion": "we explore an iterative prompting framework towards driving a \"chain of thought\" from plms for multi-step reasoning tasks. we show the superiority of this iterative scheme, and also the effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme. in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors. in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities. the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely, a human can track along the plm's chain of thought and make edits and corrections at different steps, similarly as in (mo et al., 2022a), which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process. we leave these investigations as future work.", "Solved": NaN, "Unsolved": NaN}, "254853987": {"num": 341, "type": 1, "title": "Source-Free Domain Adaptation for Question Answering with Masked Self-training", "pdfurl": "https://export.arxiv.org/pdf/2212.09563v1.pdf", "conclusion": "in this paper, we explore the possibility of transferring knowledge for unsupervised domain adaptation on question answering, without access to initial domain data. we proposed a novel self-trainingbased approach, mdaqa. we specially design an attention mask module to automatically keep key knowledge from the source domain and learn to mitigate domain shift between source and target domains. the module can be easily integrated into existing language models. our comprehensive experiments on well-known benchmark datasets demonstrate that mdaqa outperforms previous methods by a clear margin. it can also achieve decent performance even when the available target domain data is highly limited. this makes mdaqa have a very wide range of application scenarios.", "Solved": NaN, "Unsolved": NaN}, "174801764": {"num": 342, "type": 1, "title": "Compositional Questions Do Not Necessitate Multi-hop Reasoning", "pdfurl": "https://arxiv.org/pdf/1906.02900v1.pdf", "conclusion": "in summary, we demonstrate that question compositionality is not a sufficient condition for multi-hop reasoning. instead, future datasets must carefully consider what evidence they provide in order to ensure multi-hop reasoning is required. there are at least two different ways to achieve this.", "Solved": NaN, "Unsolved": NaN}, "222225265": {"num": 343, "type": 1, "title": "Visuo-Linguistic Question Answering (VLQA) Challenge", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.413.pdf", "conclusion": "in this work, we introduced the visuo-linguistic question answering (vlqa) challenge that we believe has the potential to open new research avenues in areas of joint vision & language. our experiments show that a system equipped with state-of-the-art vision-language pre-training does not perform well on the task that requires joint image-text inference. there is a room for significant improvement in capability of these models to tackle multi-modal contexts. our future work would include further expansion of this dataset and building generic ai models that can learn novel visual concepts from a small set of examples.", "Solved": NaN, "Unsolved": NaN}, "202660724": {"num": 344, "type": 1, "title": "Revealing the Importance of Semantic Retrieval for Machine Reading at Scale", "pdfurl": "https://www.aclweb.org/anthology/D19-1258.pdf", "conclusion": "we proposed a simple yet effective hierarchical pipeline system that achieves state-of-the-art results on two mrs tasks. ablation studies demonstrate the importance of semantic retrieval at both paragraph and sentence levels in the mrs system. the work can give general guidelines on mrs modeling and inspire future research on the relationship between semantic retrieval and downstream comprehension in a joint setting. table 6: hyper-parameter selection for the full pipeline system. h and k are the retrieval filtering hyperparameters mentioned in the main paper. p-level and s-level indicate paragraph-level and sentence-level respectively. \"{}\" means values enumerated from a set. \"[]\" means values enumerated from a range with inter-val=0.1 \"bs.\"=batch size \"# e.\"=number of epochs", "Solved": NaN, "Unsolved": NaN}, "258866037": {"num": 345, "type": 1, "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy", "pdfurl": "https://export.arxiv.org/pdf/2305.15294v2.pdf", "conclusion": "we demonstrate the effectiveness of iter-retgen in answering questions with complex information needs.despite simple, iter-retgen outperforms retrieval-augmented methods that have a more complex workflow, which we believe could serve as a strong baseline for future research on retrieval-augmented generation.we also show that generation-augmented retrieval adaptation can further improve the performance of iter-retgen while also reducing overheads.", "Solved": NaN, "Unsolved": NaN}, "253018998": {"num": 346, "type": 1, "title": "Inferring Implicit Relations in Complex Questions with Language Models", "pdfurl": "https://export.arxiv.org/pdf/2204.13778v2.pdf", "conclusion": "we propose the task of implicit relation inference, which decouples inference of reasoning steps from their execution. we introduce implicitrela-tions, a benchmark that includes more than 2,000 annotated implicit relations. we show large lms can infer implicit relations across multiple types of questions and reasoning skills, but this success does not translate to an improvement in answering implicit reasoning questions. our work sheds light on capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for improving the ability of models to infer implicit relations.", "Solved": NaN, "Unsolved": NaN}, "201698166": {"num": 347, "type": 1, "title": "Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations", "pdfurl": "https://arxiv.org/pdf/1908.11513v1.pdf", "conclusion": "in this paper, we propose a meta-learning based model named meta-kgr for multi-hop reasoning over few-shot relations of knowledge graphs.meta-kgr uses training triples with highfrequency relations to find well-initialized parameters and fast adapt to few-shot relations.the meta information learned from high-frequency relations is helpful for few-shot relations.in experiments, our models achieve good performance on few-shot relations and outperform previous work in most cases.some empirical analysis also demonstrates that our models are robust and generalized to different types of knowledge graphs.", "Solved": NaN, "Unsolved": NaN}, "248218489": {"num": 348, "type": 1, "title": "Improving Passage Retrieval with Zero-Shot Question Generation", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.249.pdf", "conclusion": "in this work, we propose upr, an approach to perform unsupervised passage re-ranking for opendomain retrieval. to re-rank, upr computes a relevance score for question generation conditioned on each retrieved passage using pre-trained language models. extensive experiments across a wide range of qa datasets show that an unsupervised pipeline consisting of retriever and upr greatly outperforms strong supervised retriever models. in addition, upr further improves the performance of supervised retrievers. on the open-domain qa task, by just performing inference using re-ranked passages and a pre-trained reader model, we achieve new state-of-the-art results.", "Solved": NaN, "Unsolved": NaN}, "224705407": {"num": 349, "type": 1, "title": "Multi-hop Question Generation with Graph Convolutional Network", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.416.pdf", "conclusion": "multi-hop qg task is more challenging and worthy of exploration compared to conventional singlehop qg. to address the additional challenges in multi-hop qg, we propose mulqg, which does multi-hop context encoding with graph convolutional network and encoding fusion via a gated reasoning module. to the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentencelevel information. the model performance on hot-potqa dataset demonstrates its effectiveness on aggregating scattered pieces of evidence across the paragraphs and fusing information effectively to generate multi-hop questions. the strong reasoning ability of the multi-hop encoder in the mulqa model can potentially be leveraged in complex generation tasks for the future work.  table a1: performance comparison between our multqg model and fine-tuning state-of-the-art large pre-trained models on hotpotqa test set.", "Solved": NaN, "Unsolved": NaN}, "233444226": {"num": 350, "type": 1, "title": "Dynabench: Rethinking Benchmarking in NLP", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.324.pdf", "conclusion": "we introduced dynabench, a research platform for dynamic benchmarking. dynabench opens up exciting new research directions, such as investigating the effects of ensembles in the loop, distributional shift characterisation, exploring annotator efficiency, investigating the effects of annotator expertise, and improving model robustness to targeted adversarial attacks in an interactive setting. it also facilitates further study in dynamic data collection, and more general cross-task analyses of humanand-machine interaction. the current iteration of the platform is only just the beginning of a longer journey. in the immediate future, we aim to achieve the following goals: anyone can run a task. having created a tool that allows for human-in-the-loop model evaluation and data collection, we aim to make it possible for anyone to run their own task. to get started, only three things are needed: a target model, a (set of) context(s), and a pool of annotators.", "Solved": NaN, "Unsolved": NaN}, "238857091": {"num": 351, "type": 1, "title": "Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.316.pdf", "conclusion": "in this paper, we present modir, a new representation learning method that improves the zero-shot generalization ability of dense retrieval models. we first show that dense retrieval models differ from classification models in that they emphasize locality properties in the representation space. then we present a momentum-based adversarial training method that robustly pushes text encoders to provide a more domain invariant representation space for dense retrieval. our experiments demonstrate that, compared with ance, a recent sota dr model, modir's improvements are robust overall and significant on datasets where zerodr's evaluation is more accurate.", "Solved": NaN, "Unsolved": NaN}, "214795146": {"num": 352, "type": 1, "title": "R 3 : A Reading Comprehension Benchmark Requiring Reasoning Processes*", "pdfurl": NaN, "conclusion": "in this work, we present r 3 , a large-scale reading comprehension dataset in which a qa system is required to give answers to questions over diverse natural language, but also needed to present the reasoning processes. we hope this dataset can facilitating the development of explainable qa systems. ", "Solved": NaN, "Unsolved": NaN}, "258866060": {"num": 353, "type": 1, "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2305.15064v3.pdf", "conclusion": "we propose autoplan, a prompt-based method, to enable llm to solve interactive decision-making tasks without gradient computation or in-context demonstrations.autoplan conditions llm on an additional task plan described in natural language, which is obtained through an iterative three-stage process.experiments show that autoplan achieves better results than baselines and is also efficient during inference.the ablation study further confirms the effectiveness of batching and explicit reflection", "Solved": NaN, "Unsolved": NaN}, "258841488": {"num": 354, "type": 1, "title": "Question Answering as Programming for Solving Time-Sensitive Questions", "pdfurl": "https://export.arxiv.org/pdf/2305.14221v3.pdf", "conclusion": "in this work we propose a novel approach, qaap (question answering as programming), to tackle the challenges posed by time-sensitive factual questions.by leveraging llms' exceptional abilities in natural language understanding and programming, qaap can transform diversely expressed text into well-structured codes, enabling llms to capture both the desired knowledge and the underlying constraints, particularly the temporal aspects.experiments demonstrate that existing llms face significant difficulty in effectively comprehending the temporal constraint stated in the question.while our approach consistently demonstrates superior performance over strong baselines with llms.we hope this work can shed light on the future research direction on enhancing llms' reasoning ability to tackle real-world questions with various constraints and developing more efficient methods to reduce the hallucinations llms frequently encounter.", "Solved": NaN, "Unsolved": NaN}, "237364113": {"num": 355, "type": 1, "title": "Contrastive Domain Adaptation for Question Answering using Limited Text Corpora", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.754.pdf", "conclusion": "this work contributes a novel framework for domain adaptation of qa systems in settings with limited text corpora. we develop caqa in which we combine techniques from from question generation and domain-invariant learning to answer out-of-domain questions. different from existing works in question answering, we achieve this by proposing a contrastive adaptation loss. extensive experiments show that caqa is superior to other state-of-the-art approaches by achieving a substantially better performance on out-of-domain data. qagen-t5: we apply lm-filtering as in (shakeri et al., 2020) and select qa pairs with highest scores for each context paragraph. qagen-t5 models are trained similarly to aqgen and qagen, we separately keep the best qg and qa models according to validation performance on the squad dev set. hyperparameter search: in our experiments, we empirically search for hyperparameters \u03b2 and \u03c3 in the contrastive adaptation loss through additional experiments. we experiment with different values of \u03b2 in the range [10 \u22121 , 10 \u22122 , 10 \u22123 ] and gaussian noise n (0, \u03c3) applied on all token embeddings with standard deviation \u03c3 ranging from 0 to 10 \u22122 . the best combination of \u03b2 and \u03c3 as per the training set is then selected, these numbers can be found in table 4.  all parameters that have not been mentioned explicitly above were used as reported in their original paper b additional results", "Solved": NaN, "Unsolved": NaN}, "232147859": {"num": 356, "type": 1, "title": "Semantic Models for the First-stage Retrieval: A Comprehensive Review", "pdfurl": "https://arxiv.org/pdf/2103.04831v4.pdf", "conclusion": "the purpose of this survey is to summarize the current research status on semantic retrieval models, analyze existing methodologies, and gain some insights for future development. it includes a brief review of early semantic retrieval methods, a detailed description of recent neural semantic retrieval methods and the connection between them. specially, we pay attention to neural semantic retrieval methods, and review them from three major paradigms, including sparse retrieval methods, dense retrieval methods and hybrid retrieval methods. we also refer to key topics about neural semantic retrieval models learning, such as loss functions and negative sampling strategies. in addition, we discuss several challenges and promising directions that are important for future researches. we look forward to working with the community on these issues.", "Solved": NaN, "Unsolved": NaN}, "202767252": {"num": 357, "type": 1, "title": "BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels", "pdfurl": "https://www.aclweb.org/anthology/D19-1249.pdf", "conclusion": "in this paper, we have presented the bipar, a bilingual parallel machine reading comprehension dataset on novels. from bilingual parallel passages of chinese and english novels, we manually created diversified parallel questions and answers of different types via crowdsourced workers with a multi-layer quality control system. although bipar is an extractive mrc dataset, in-  table 6: fine-grained results in terms of different answer types and question categories on the monolingual task. the left side of the slash is the f1 score on the english data, while the right is on chinese. all f1 scores are calculated on the 100 questions as described in section 4.3.", "Solved": NaN, "Unsolved": NaN}, "236477336": {"num": 358, "type": 1, "title": "Latent Reasoning for Low-Resource Question Generation", "pdfurl": "https://www.aclanthology.org/2021.findings-acl.265.pdf", "conclusion": "we proposed a jointly optimized two-phase model named plar for low-resource question generation. plar effectively utilizes non-parallel singlehop and multi-hop question answering data to perform optimization. we further designed a planning mechanism to guide the generation process of subquestions so that the generation results are valid to compose a multi-hop question. experimental results confirm that plar achieves better performance compared with the state-of-the-art under various metrics, especially in a question answering based evaluation. for future work, we will explore the heterogeneous multi-hop qg task that requires reasoning beyond plain texts, e.g., tables.", "Solved": NaN, "Unsolved": NaN}, "221662105": {"num": 359, "type": 1, "title": "CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management", "pdfurl": NaN, "conclusion": "in this paper, we propose a general system, caire-covid, with open-domain qa and query focused multi-document summarization techniques for efficiently mining scientific literature given a query. the system has shown its efficiency on the kaggle cord-19 challenge, which was evaluated by medical researchers, and a series of experimental results also proved the effectiveness of our proposed methods and the competency of each module. the system is also easy to be generalized to general domain-agnostic literature information mining, especially for possible future pandemics. we have launched our website 2 for real-time interactions and released our code 3 for broader use.  ", "Solved": NaN, "Unsolved": NaN}, "254366618": {"num": 360, "type": 1, "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training", "pdfurl": "https://export.arxiv.org/pdf/2212.03533v1.pdf", "conclusion": "in this work, we train a general-purpose text embedding model e5 from weak supervision signals. we adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. e5 offers strong off-the-shelf performance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. when further customized for downstream tasks, e5 achieves superior fine-tuned performance compared to existing embedding models with 40\u00d7 more parameters on the large, 56-task mteb benchmark datasets. table 9: details for each data source after filtering. the \"others\" category includes \"sim-plewiki\", \"gooaq\", \"wikihow\", \"yahoo answers\" from https://huggingface.co/datasets/ sentence-transformers/embedding-training-data. ", "Solved": NaN, "Unsolved": NaN}, "247593883": {"num": 361, "type": 1, "title": "REASONING OVER PUBLIC AND PRIVATE DATA IN RETRIEVAL-BASED SYSTEMS", "pdfurl": "https://arxiv.org/pdf/2203.11027v1.pdf", "conclusion": "we first adapt the widely used benchmark, hotpotqa [yang et al., 2018], to study our problem. hotpotqa contains multi-hop questions, which are each answerable by multiple wikipedia passages. we create hotpotqa-pair by splitting the wikipedia corpus into d g and d p by randomly assigning wikipedia articles to one or the other. this results in questions entirely reliant on p \u2208 d p , entirely reliant on d \u2208 d g , or reliant on a mix of one private and one public document, allowing us to evaluate performance under the pair constraints.ultimately however, d p and d g come from a single wikipedia distribution in hopotqa-pair. while it is possible that public and private data come from the same distribution (e.g., organizations routinely develop internal wikis in the style of public wikipedia), private and public data will intuitively often reflect different linguistic styles, structures, and topics, that further evolve over time [hawking, 2004]. we observe all existing textual multi-hop benchmarks focus on retrieving from a single distribution (table 1). additionally, we cannot combine existing benchmarks over two different corpora because this will not yield questions requiring one passage from each domain. methodologically, in the pair setting we likely will not have access to training data from all downstream (private) domains. to evaluate with a realistically private set of information and pair set up, we create a new benchmark concurrentqa. 6 following from the simple security property and *-property in the blp model. 7 single-hop can also avoid performance degradations arising from using two enclaves. recall that a non-private system retrieves the top k overall passages, so if for example kp = k 2 and kg = k 2 , such that kp + kg = k, the system may not retrieve the optimal k passages that the non-private system would have retrieved (e.g., consider when the overall top k passages for a question are in dg). however letting kp \u2208 [0..k], kg \u2208 [0..k] circumvents this challenge, at the cost of retrieving a few more passages per hop.", "Solved": NaN, "Unsolved": NaN}, "240354203": {"num": 362, "type": 1, "title": "Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge", "pdfurl": "https://www.aclanthology.org/2021.mrqa-1.12.pdf", "conclusion": "in this paper, we proposed an unsupervised mcqa method, which exploits the pseudo labels generated by some basic rules or external non-mcqa datasets. the proposed method significantly outperforms the baseline approaches on race and is even comparable with the supervised learning performance on mc500. we hope this paper sheds light on unsupervised learning in nlp tasks.  ", "Solved": NaN, "Unsolved": NaN}, "252873630": {"num": 363, "type": 1, "title": "Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning", "pdfurl": "https://www.aclanthology.org/2023.acl-long.231.pdf", "conclusion": "in this work, we proposed a novel counterfactual reasoning approach to reduce disconnected reasoning in multi-hop qa. we used the causal graph to explain the existing multi-hop qa approaches' behaviors, which consist of the shortcut impacts and reasoning impacts. the shortcut impacts capture the disconnected reasoning and they are formulated as the natural direct causal effects. then we constructed the counterfactual examples during the training phase to estimate the both natural direct effects of question and context on answer prediction as well as supporting facts identification. the reasoning impact represents the multi-hop reasoning and is estimated by introducing learnable parameters.", "Solved": NaN, "Unsolved": NaN}, "226278305": {"num": 364, "type": 1, "title": "Answer Span Correction in Machine Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2011.03435v1.pdf", "conclusion": "we describe a novel method for answer span correction in machine reading comprehension. the proposed method operates by marking an original, possibly incorrect, answer prediction in context and then making a new prediction using a corrector model. we show that this method corrects the predictions of a state-of-the-art english-language reader in different error categories. in our experiments, the approach also generalizes well to multilingual and cross-lingual mrc in seven languages. future work will explore joint answer span cor-", "Solved": NaN, "Unsolved": NaN}, "249191335": {"num": 365, "type": 1, "title": "Learning Open Domain Multi-hop Search Using Reinforcement Learning", "pdfurl": "https://www.aclanthology.org/2022.suki-1.4.pdf", "conclusion": "we proposed a focused reading methodology to automatically learn how to direct search in large corpora while iteratively building a knowledge base. the knowledge base is modeled as a graph, which in turn is used to focus the search toward documents that appear relevant. our methodology complements existing information retrieval and machine tools. we evaluated focused reading on a set of search problems extracted from english wikipedia and demonstrated that reinforcement learning with a state representation based on features about dynamics of the search process and the properties of the corpus is more effective and efficient than heuristic baselines. in this methodology, inference in a knowledge graph acquired during the search process is agnostic of the semantics of the concepts and their relations. their quality depends on the machine reading components used to extract them.", "Solved": NaN, "Unsolved": NaN}, "102353837": {"num": 366, "type": 1, "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis", "pdfurl": "https://www.aclweb.org/anthology/N19-1242.pdf", "conclusion": "we proposed a new task called review reading comprehension (rrc) and investigated the possibility of turning reviews as a valuable resource for answering user questions. we adopted bert as our base model and proposed a joint post-training approach to enhancing both the domain and task knowledge. we further explored the use of this approach in two other review-based tasks: aspect extraction and aspect sentiment classification. experimental results show that the post-training approach before fine-tuning is effective.", "Solved": NaN, "Unsolved": NaN}, "218487733": {"num": 367, "type": 1, "title": "Exploring and Predicting Transferability across NLP Tasks", "pdfurl": "https://arxiv.org/pdf/2005.00770v1.pdf", "conclusion": "in this work, we conduct a large-scale empirical study of the transferability between 33 nlp tasks across three broad classes of problems, encompassing classification, question answering, and sequence labeling. we show that the benefits of transfer learning are more pronounced than previously thought, especially when target training data is limited, and we develop methods that learn vector representations of tasks that can be used to reason about the relationships between them. these task embeddings allow us to predict source tasks that will positively transfer to a given target task. our analysis suggests that data size, the similarity between the source and target tasks and domains, and task complexity are crucial for effective transfer, particularly in data-constrained regimes. ", "Solved": NaN, "Unsolved": NaN}, "254069686": {"num": 368, "type": 1, "title": "Penalizing Confident Predictions on Largely Perturbed Inputs Does Not Improve Out-of-Distribution Generalization in Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2211.16093v1.pdf", "conclusion": "we first showed that entropy maximization often fails to transfer to unseen perturbation types. maximizing the entropy terms for various types of perturbations is effective in mitigating this problem. the failure of entropy maximization to improve out-of-distribution generalization may be caused by the unnaturalness of the perturbed inputs. modifying the perturbation functions to effectively improve outof-distribution generalization is future work.", "Solved": NaN, "Unsolved": NaN}, "234679223": {"num": 369, "type": 1, "title": "QAConv: Question Answering on Informative Conversations", "pdfurl": "https://arxiv.org/pdf/2105.06912v2.pdf", "conclusion": "qaconv is a new dataset that conducts qa on informative conversations such as emails, panels, and channels. we show the unique challenges of our tasks in both chunk mode with oracle partial conversations and full mode with a retrieval stage. we find that state-of-the-art qa models have limited dialogue understanding and tend to predict our answerable qa pairs as unanswerable. we provide a new testbed for qa on conversation tasks to facilitate future research.", "Solved": NaN, "Unsolved": NaN}, "257913786": {"num": 370, "type": 1, "title": "QUADRo: Dataset and Models for QUestion-Answer Database Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2304.01003v1.pdf", "conclusion": "in this paper, we have described our study to scale qa-based on q/a db to open domain applications. this required to build a large db, which we built only using publicly available q/a pairs, reaching a significant size of \u2248 6.3m items. to enable retrieval from these large dbs, inspired by the latest neural ir technology, we modeled neural retrieval for q/a pairs. we proposed two different methods based on only questions, and on q/a pairs, where questions and answers can be seen as context. we analyzed the significant impact of using architectures with separate encoder versus dual encoders, in accuracy and efficiency.", "Solved": NaN, "Unsolved": NaN}, "260334522": {"num": 371, "type": 1, "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution", "pdfurl": "https://export.arxiv.org/pdf/2307.16883v1.pdf", "conclusion": "generative search with the ability to cite supporting sources has gained a lot of traction lately. however, the absence of accessible high-quality data inhibits progress in building open-source information-seeking models. in this paper, we seek to bridge this gap in the community by introducing hagrid, a new dataset for building endto-end generative retrieval models. our dataset is collected via a human-machine collaboration that starts with generating explanatory answers to  information-seeking queries from gpt-3.5, followed by a human assessment of correctness and attributability of the generated answers. hagrid facilitates the development of open-source models for information-seeking scenarios. our human study has shed light on the room for improvement, i.e. around 40% of gpt-3.5 generated answers are not informative and over 20% fail to demonstrate attribution to the quotes. moving forward, future research endeavors may focus on building more accurate models, aimed at mitigating the errors commonly encountered in current llms.", "Solved": NaN, "Unsolved": NaN}, "215745470": {"num": 372, "type": 1, "title": "From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap", "pdfurl": "https://www.aclweb.org/anthology/2020.nlp4convai-1.10.pdf", "conclusion": "task-oriented dialogue systems aim to help users to achieve a variety of tasks. it is not unusual to have hundreds of different domains in modern taskoriented virtual assistants. how can we ensure the dialogue system is robust enough to scale to different tasks given limited amount of data? some approaches focus on domain expansion by training on several source domains and then adapting to the target domain. while such methods can be successful in certain cases, it is hard for them to generalize to other completely different out-of-domain tasks.", "Solved": NaN, "Unsolved": NaN}, "252693442": {"num": 373, "type": 1, "title": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering", "pdfurl": "https://www.aclanthology.org/2022.coling-1.138.pdf", "conclusion": "in this paper, we introduce mintaka, an end-toend question answering dataset linked to wikidata. mintaka addresses an important gap in qa datasets by being large-scale, complex, naturally-elicited, and multilingual. our baselines show that there is room for improvement in existing methods to handle complex questions, especially in all languages. with the release of mintaka, we hope to encourage researchers to continue pushing the boundaries of question answering to handle more complex questions in more languages.", "Solved": NaN, "Unsolved": NaN}, "260682695": {"num": 374, "type": 1, "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies", "pdfurl": "https://export.arxiv.org/pdf/2308.03188v2.pdf", "conclusion": "in this paper, we present a comprehensive survey of self-correcting large language models with automated feedback. we broadly categorize and analyze various self-correction strategies, including training-time, generation-time, and post-hoc corrections. we also discuss the major application areas of self-correction, including correcting factual errors, enhancing reasoning abilities, and improving code generation, among others. finally, we outline a number of potential future directions and associated challenges in this field. our goal with this paper is to provide a com-prehensive and useful resource for readers interested in the development of this rapidly evolving domain. to aid in this effort, we create a continually-updated reading list in a github repository: https://github.com/teacherpeterpan/ self-correction-llm-papers. ", "Solved": NaN, "Unsolved": NaN}, "263830473": {"num": 375, "type": 1, "title": "MINPROMPT: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2310.05007v1.pdf", "conclusion": "in this paper, we present minprompt, a robust data augmentation framework that leverages a graph-based algorithm and unsupervised question generation to extract minimally meaningful qa training samples from raw text.our contributions reside in the application of minimal data augmentation, enhancing computational efficiency and model performance while mitigating overfitting.", "Solved": NaN, "Unsolved": NaN}, "252873674": {"num": 376, "type": 1, "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2205.03401v2.pdf", "conclusion": "caveats and risks of explanations from large language models our analysis suggests that llms' internal \"reasoning\" does not always align with explanations that it generates, as shown by our consistency results. more concerning, the explanations might not be factually grounded in the provided prompt. this shortcoming should caution against any deployment of this technology in practice: because the explanations are grammatical english and look very convincing, they may deceive users into believing the system's responses even when those responses are incorrect. section 6 of bender et al. (2021) discusses these risks in additional detail. the fact that language models can hallucinate explanations is also found in other work (zhou and tan, 2021). this result is unsurprising in some sense: without sufficient supervision or grounding, language models do not learn meaning as distinct from form (bender and koller, 2020), so we should not expect their explanations to be strongly grounded.", "Solved": NaN, "Unsolved": NaN}, "259316688": {"num": 377, "type": 1, "title": "Analyzing Multiple-Choice Reading and Listening Comprehension Tests", "pdfurl": "https://export.arxiv.org/pdf/2307.01076v1.pdf", "conclusion": "this work highlights the trade-off between contextual comprehension and world knowledge in multiple-choice reading and listening comprehension tests. we found that automated reading comprehension systems perform significantly better than random, even with limited access to the context passage. these findings provide content creators with an approach to capture the balance between comprehension and world knowledge in their questions. we further investigated to what extent a context needs to be read before the correct answer can be deduced, finding that it is possible to answer some questions across several reading/listening comprehension datasets with only access to a fraction of the context. overall, our findings guide content creators in constructing more valid and reliable assessments, ensuring accurate evaluation of language proficiency.", "Solved": NaN, "Unsolved": NaN}, "252070866": {"num": 378, "type": 1, "title": "FOLIO: Natural Language Reasoning with First-Order Logic", "pdfurl": "https://export.arxiv.org/pdf/2209.00840v1.pdf", "conclusion": "we show the accuracy of the examples with different numbers of premises needed to reach the conclusions in figure 3. under the few-shot prompting setting, gpt-3 and   ", "Solved": NaN, "Unsolved": NaN}, "258823123": {"num": 379, "type": 1, "title": "CRITIC: LARGE LANGUAGE MODELS CAN SELF- CORRECT WITH TOOL-INTERACTIVE CRITIQUING", "pdfurl": "https://export.arxiv.org/pdf/2305.11738v2.pdf", "conclusion": "we propose critic, a novel plug-and-play framework that empowers frozen llms to self-verify and self-correct by interacting with the external environment. leveraging the intuition of critical thinking with external feedback, critic enables llms to validate their knowledge and improve their answers through introspection without requiring further training. experiments on diverse tasks and datasets have consistently shown the effectiveness, generality, and interoperability of critic. moreover, we shed light on the unreliability of llms in self-verification, highlighting the potential of external tool interaction to solve this problem. we hope our findings will inspire further exploration into the truthfulness of language models, ultimately leading to more trustworthy ai systems. ", "Solved": NaN, "Unsolved": NaN}, "227230675": {"num": 380, "type": 1, "title": "Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved Transformer for Multi-Hop Explanation Ranking", "pdfurl": "https://arxiv.org/pdf/2012.14164v1.pdf", "conclusion": "the lit architecture is a simple yet powerful adaptation of the transformer architecture to learn better cross-document interactions for multi-hop ranking. the structure can be easily integrated with any transformer language model to enable cross-referencing of knowledge statements and improved ranking performance. for example, lit can be a drop-in encoder for other multi-hop question answering datasets such as hotpotqa (yang et al., 2018). when applied to the challenging worldtree v2 dataset, lit achieves competitive performance with current state-of-the-art models despite a smaller footprint. we envision that this architecture can be beneficial to many nlp tasks which require multi-hop reasoning over documents.", "Solved": NaN, "Unsolved": NaN}, "248562643": {"num": 381, "type": 1, "title": "The Unreliability of Explanations in Few-Shot In-Context Learning PREPRINT", "pdfurl": "https://arxiv.org/pdf/2205.03401v1.pdf", "conclusion": "we have explored the capabilities of gpt-3 in using explanation in in-context learning for textual reasoning. through our experiments on two qa datasets and an nli dataset, we find that simply including explanations in the prompt does not always improve the performance of in-context learning.", "Solved": NaN, "Unsolved": NaN}, "155100120": {"num": 382, "type": 1, "title": "Dynamically Fused Graph Network for Multi-hop Reasoning", "pdfurl": "https://arxiv.org/pdf/1905.06933v2.pdf", "conclusion": "we introduce dynamically fused graph network (dfgn) to address multi-hop reasoning. specifically, we propose a dynamic fusion reasoning block based on graph neural networks. different from previous approaches in qa, dfgn is capable of predicting the sub-graphs dynamically at each reasoning step, and the entity-level reasoning is fused with token-level context. we evaluate dfgn on hotpotqa and achieve leading results. besides, our analysis shows dfgn can produce reliable and explainable reasoning chains. in the future, we may incorporate new advances in building entity graphs from texts, and solve harder reasoning problems, e.g. \"comparison\" in hot-potqa.", "Solved": NaN, "Unsolved": NaN}, "256627673": {"num": 383, "type": 1, "title": "Exploring the Benefits of Training Expert Language Models over Instruction Tuning", "pdfurl": "https://export.arxiv.org/pdf/2302.03202v2.pdf", "conclusion": "while we highlight some of the major drawbacks of instruction tuning and propose an alternative approach of instead training and retrieving experts in this paper, we do not perform experimental results over mt lms that have more than >11b parameters. for example, mt lms with >11b parameters may be less susceptible to negative task transfer because of increased model capacity. also, during the inference of unseen tasks, our retrieval mechanism assumes batch inference (i.e. having access to 32 samples of the target tasks without labels). finally, when showing the compositional instruction experiments, we assume the two optimal experts could be retrieved from the compositional instruction (concatenation of the two seen instructions) given as the input along with the evaluation instance. this might not necessarily be the case with more complex, compositional instructions, which might require a separate decomposition stage. we instead focus on showing the possibility merging experts can bring and leave developing novel methods of retrieving the optimal experts during inference for future work.", "Solved": NaN, "Unsolved": NaN}, "173188058": {"num": 384, "type": 1, "title": "MULTIQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/1905.13453v1.pdf", "conclusion": "in this work we performed a thorough empirical investigation of generalization and transfer over 10 rc datasets. we characterized the factors affecting generalization and obtained several state-ofthe-art results by training on 375k examples from 5 rc datasets. we open source our infrastructure for easily performing experiments on multiple rc datasets, for the benefit of the community. we highlight several practical take-aways: \u2022 pre-training on multiple source rc datasets consistently improves performance on a target rc dataset , even in the presence of bert representations. it also leads to substantial reduction in the number of necessary training examples for a fixed performance. \u2022 training the high-capacity bert-large representations over multiple rc datasets leads to good performance on all of the trained datasets without having to fine-tune on each dataset separately. \u2022 bert representations improve generalization, but their effect is moderate when the source of the context is web snippets compared to wikipedia and newswire. \u2022 performance over an rc dataset can be improved by retrieving web snippets for all questions and adding them as examples (context augmentation).", "Solved": NaN, "Unsolved": NaN}, "232307674": {"num": 385, "type": 1, "title": "Complementary Evidence Identification in Open-Domain Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2021.eacl-main.234.pdf", "conclusion": "in the paper, we propose a new problem of complementary evidence identification and define the criterion of complementary evidence in vector space. we further design an algorithm and a loss function to support efficient training and inference for complementary evidence selection. compared to the baseline, our approach improves more than 20% and remains to scale well to the computationally complex cases.", "Solved": NaN, "Unsolved": NaN}, "235731930": {"num": 386, "type": 1, "title": "FAVIQ: FAct Verification from Information-seeking Questions", "pdfurl": "https://www.aclanthology.org/2022.acl-long.354.pdf", "conclusion": "we introduced faviq, a new fact verification dataset derived from ambiguous information-seeking questions. we incorporate facts that real users were unaware of when posing the question, leading to false claims that are more realistic and challenging to identify without fully understanding the context. our extensive analysis shows that our data contains significantly less lexical bias than previous fact checking datasets, and include refute claims that are challenging and realistic. our experiments showed that the state-of-the-art models are far from solving faviq, and models trained on faviq lead to improvements in professional fact checking. altogether, we believe faviq will serve as a challenging benchmark as well as support future progress in professional fact-checking.", "Solved": NaN, "Unsolved": NaN}, "211010520": {"num": 387, "type": 1, "title": "Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2002.00293v1.pdf", "conclusion": "we have in this work investigated an rc annotation paradigm which includes a model in the loop that has to be \"beaten\" by the annotator. applying this approach with a series of progressively stronger rc models in the annotation loop, we arrived at three separate rc datasets, graduated by the difficulty of the model adversary. based on this dataset series we investigated several questions surrounding the annotation paradigm, in particular whether such datasets grow outdated as stronger models emerge, and about their generalisation to standard (non-adversarially collected) questions. we found that stronger rc models can still learn from data collected with a weak adversary in the loop, and their generalisation improves even on datasets collected with a very strong adversary. models trained on data collected with a model in the loop furthermore generalise well towards nonadversarially collected data, both on squad and on naturalquestions, yet we observe a slow deterioration with progressively stronger adversaries.", "Solved": NaN, "Unsolved": NaN}, "252199900": {"num": 388, "type": 1, "title": "Domain Adaptation for Question Answering via Question Classification", "pdfurl": "https://www.aclanthology.org/2022.coling-1.153.pdf", "conclusion": "in this paper, we propose a novel framework for qa domain adaptation. the proposed qc4qa combines question classification with self-supervised adaptation techniques. qc4qa leverages question classes to reduce domain discrepancies and resemble target data distribution in training. different from existing works, qc4qa achieves superior performance by introducing a simple question classifier and incorporating the question class information in the training objective. we demonstrate the efficiency and effectiveness of qc4qa compared to state-of-the-art approaches by achieving a substantially better performance on multiple datasets.", "Solved": NaN, "Unsolved": NaN}, "222208994": {"num": 389, "type": 1, "title": "SRLGRN: Semantic Role Labeling Graph Reasoning Network", "pdfurl": "https://arxiv.org/pdf/2010.03604v1.pdf", "conclusion": "we proposed a novel semantic role labeling graph reasoning network (srlgrn) to deal with multihop qa. the backbone graph of our proposed graph convolutional network (gcn) is created based on the semantic structure of the sentences. in creating the edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts. the cross paragraph argument-predicate structure of the sentences expressed in the graph provides an explicit representation of the reasoning path and helps in both finding and explaining the multiple hops of reasoning that lead to the final answer. srlgrn exceeds most of the sota results on the hotpotqa benchmark. moreover, we evaluate the model (excluding the paragraph selection module) on other reading comprehension benchmarks. our approach achieves competitive performance on squad v1.1 and v2.0.", "Solved": NaN, "Unsolved": NaN}, "252222436": {"num": 390, "type": 1, "title": "Pre-training for Information Retrieval: Are Hyperlinks Fully Explored?", "pdfurl": "https://export.arxiv.org/pdf/2209.06583v1.pdf", "conclusion": "in this work, we proposed a progressive pre-training strategy for adhoc retrieval, which consists of three hyperlink-based pre-training tasks, namely hyperlink prediction, symmetric hyperlink prediction, and most relevant document selection. these pre-training tasks can simulate different stages of a retrieval process, and gradually enhance the model's performance on relevance modeling. compared with existing methods, our strategy can make better use of utilize the hyperlink, which reveal the great potential of hyperlink in learning relevance modeling. the experimental results demonstrated the effectiveness of our model. in the future, we plan to integrate our method with other pre-training objectives (such as rop), which may bring further improvement.", "Solved": NaN, "Unsolved": NaN}, "229923812": {"num": 391, "type": 1, "title": "HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions", "pdfurl": "https://arxiv.org/pdf/2012.15534v1.pdf", "conclusion": "in this paper, we propose the hopretriever to collect reasoning evidence over wikipedia for multi-hop question answering. both the structured knowledge indicated by hyperlinks and the unstructured knowledge presented as introduc-tory documents in wikipedia, are involved and leveraged together in hopretriever to help the evidence collection. the experiment on the hotpotqa dataset shows that the performance of hopretriever improved observably as a result of combining the structured knowledge with unstructured knowledge, and outperforms all the published models on the leaderboard. moreover, by inspecting the proportion of the two kinds of knowledge in hops, which kind of knowledge leads the retrieving of each evidence piece can be observed directly, which also provides extra intuitive interpretations for the selection of each evidence.", "Solved": NaN, "Unsolved": NaN}, "258556908": {"num": 392, "type": 1, "title": "VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens", "pdfurl": "https://export.arxiv.org/pdf/2305.04241v2.pdf", "conclusion": "we propose a vip-token centric sequence compression method to compress/decompress the input/output sequences of transformer layers thereby reducing the complexity dependency on the sequence length n without sacrificing the model accuracy. specifically, we design the compression our empirical evaluation shows that our method can be directly incorporated into existing pretrained models with some additional training. also, it often has much higher efficiency compared to baselines with the same sequence length while offering better or competitive model accuracy. for future work, we believe that extending our method to the decoder of the encoder-decoder models will further boost the efficiency of transformers while maintaining similar model performance.    we provide a table 4 of notations that are used for more than once so that the readers can refer to their definition easily.", "Solved": NaN, "Unsolved": NaN}, "245144556": {"num": 393, "type": 1, "title": "Large Dual Encoders Are Generalizable Retrievers", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.669.pdf", "conclusion": "this paper presents the generalizable t5 retriever (gtr), a scaled-up dual encoder model with a fixed-size dot-product bottleneck layer. we show that scaling up the model size brings significant improvement on retrieval performance across the board on the beir zero-shot retrieval benchmark, especially for out-of-domain generalization. the gtr-xxl model performs at the level of state-ofthe-art performance on beir, outperforming many models that use earlier interactions between queries and documents. this sheds light on the research direction to continue enhancing the single vector representation model through better backbone encoders. moreover, our in-depth analysis reveals the impact of scaling up under the scenarios of different training stages, pre-training strategies, fine-tuning datasets, and bottleneck sizes, as well as how scaling up influences the retrieved document lengths. our findings can inform future work and is an integral part of the joint effort to improve dual encoder models.", "Solved": NaN, "Unsolved": NaN}, "254854129": {"num": 394, "type": 1, "title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking", "pdfurl": "https://export.arxiv.org/pdf/2212.09656v1.pdf", "conclusion": "this paper describes a system for multi-document question answering that uses a passage reranker to retrieve documents and large language models to reason over them and compose an answer. our system rivals state-of-the-art supervised models in three datasets: iirc, qasper, and strategyqa. our results suggest that using gpt-3 as a reader is close to human-level performance as long as relevant passages are provided, while current retrievers are the main bottleneck. we also show that inducing the model to give explanations before answering a question improves effectiveness.", "Solved": NaN, "Unsolved": NaN}, "211010545": {"num": 395, "type": 1, "title": "Conversations with Documents An Exploration of Document-Centered Assistance", "pdfurl": "https://arxiv.org/pdf/2002.00747v1.pdf", "conclusion": "in this paper, we explored the novel domain of document-centered digital assistance. we focused on a consumption scenario, in which individuals are a (co-)owner of a document. through a survey, we identified a set of primary capabilities people expect from a digital assistant in a document-centered scenario, as well as a large set of questions that gave us insight into the types of queries that people might pose about a document when they have an approximate or good idea what the document is about. our explorations shed light on the hierarchy of questions that might be posed, and demonstrate that the types of questions people ask in a document-centered scenario are different from the factoid questions in conventional qa datasets. we show that state-of-the-art qa models can be finetuned to perform with reasonable accuracy on the new dqa data. yet, it has proven to be an unsolved task, which makes this a fertile area for future work. this research opens a new direction for digital assistance. avenues for future work include deeper explorations of query rewriting to better tailor document-centered questions to conventional qa systems, and also exploring ways to scale up the data to a much larger and broader range of documents.  ", "Solved": NaN, "Unsolved": NaN}, "258823156": {"num": 396, "type": 1, "title": "Inference-time Re-ranker Relevance Feedback for Neural Information Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2305.11744v1.pdf", "conclusion": "we demonstrate that query representations can be improved using feedback from a cross-encoder reranker at inference time for better performance of dual-encoder retrieval. this work proposes for distillation using relevance feedback from the re-ranker as a better and faster alternative to the traditional strategy of re-ranking a larger pool of candidates for improving recall. our proposed distillation process is lightweight and im-proves retrieval accuracy across different domains, languages and modalities over a state-of-the-art retrieve-and-rerank pipeline with comparable latency. future work will explore relevance feedback for token-level query representations as well as disentangling term importance scores from query representations for better interpretability.", "Solved": NaN, "Unsolved": NaN}, "250334200": {"num": 397, "type": 1, "title": "Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling", "pdfurl": "https://arxiv.org/pdf/2207.03030v1.pdf", "conclusion": "we proposed a simple yet effective approach for multi-task training of the fid retrieval-augmented generation model on the kilt benchmark. we cleaned (and downsampled were necessary) the training set by removing query-answer pairs with low relevance confidence. we demonstrated that this approach substantially improves two imbalanced tasks, and has a smaller benefit on two of the remaining five tasks. by scaling the model capacity we achieve state-of-the-art results on five kilt tasks evaluated by the leaderboard.", "Solved": NaN, "Unsolved": NaN}, "219978758": {"num": 398, "type": 1, "title": "Improving QA Generalization by Concurrent Modeling of Multiple Biases", "pdfurl": "https://www.aclweb.org/anthology/2020.findings-emnlp.74.pdf", "conclusion": "in this paper we (1) investigate the impact of debiasing methods on qa model generalization for both single and multi-domain training scenarios, and (2) propose a new framework for improving the in-domain and out-of-domain performances by concurrent modeling of multiple biases. our framework weights each training example according to multiple biases and based on the strength of each bias in the training data. it uses the resulting bias weights in the training objective to prevent the model from mainly focusing on learning biases. we evaluate our framework using two different training objectives, i.e., multi-bias confidence regularization and multi-bias loss re-weighting, and show its effectiveness in both single and multidomain training scenarios. we further compare our framework with two state-of-the-art debiasing methods of utama et al. (2020) and mahabadi et al. (2020). we show that knowledge distillation, modeling multiple biases at once, and weighting the impact of each bias based on its strength in the training data are all important factors in improving the in-domain and out-of-domain performances. while recent literature on debiasing in nlp focuses on improving the performance on adversarial evaluation sets, this work opens new research directions on wider uses of debiasing methods. the main advantage of using our debiasing methods is that they improve the performance and generalization without requiring additional training data or larger models. future work could build upon our framework by applying it to a wide range of tasks beyond qa using task-specific bias models. ", "Solved": NaN, "Unsolved": NaN}, "257557511": {"num": 399, "type": 1, "title": "Secret-Keeping in Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2303.09067v1.pdf", "conclusion": "we have introduced the task of secret-keeping as an important, and under-explored problem in question answering. we identify a lack of suitable secretkeeping metrics and define secrecy, paranoia and information leakage to address the gap. we design and implement a secret-keeping approach that is model-agnostic, only requiring access to predefined secrets, and the output of a qa system to, detect the disclosure of secrets. we have identified a rich field for future work in secret-keeping including: \u2022 reducing paranoia and information leakage.", "Solved": NaN, "Unsolved": NaN}, "202572810": {"num": 400, "type": 1, "title": "Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-1253.pdf", "conclusion": "we proposed two semantics-enhanced rewards to regularize a qg model to generate semantically valid questions, and introduced a qa-based evaluation method that directly evaluates a qg model's ability to mimic human annotators in generating qa training data. experiments showed that our qg model achieves new state-of-the-art performances. further, we investigated how to use our qg system to augment qa datasets and conduct semi-supervised qa via two synthetic data generation methods along with a data filter and mixing mini-batch training. experiments showed that our approach improves both bidaf and bert qa baselines even without introducing new articles. qa for qa, we use squadv1.1 (rajpurkar et al., 2016). previous semi-supervised qa works sampled 10% from training set as the testing set dhingra et al., 2018). since we want to use the full training set in semi-supervised qa setup without any data size reduction, we instead split the original development set in half for validation and testing respectively. for semi-supervised qa, first, without introducing new articles, we generate new questions for squad training set by keeping all beam search outputs. second, with introducing new articles, we obtain new paragraphs with pre-extracted answer spans from harvestingqa . without using their provided questions, we use our best qg model to label questions. meanwhile, we investigate the influence of synthetic data size, so we sample 10% to 100% examples from harvest-ingqa, which are denoted as h1-h10 in our experiments.", "Solved": NaN, "Unsolved": NaN}, "258546701": {"num": 401, "type": 1, "title": "Query Expansion by Prompting Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2305.03653v1.pdf", "conclusion": "there are a number of limitations in our work: first, we only study sparse retrieval (bm25) which is where query expansion is important. dense retrieval systems (e.g. dual encoders) are less prone to the vocabulary gap and, as a result, are less likely to benefit from a query expansion. wang et al. [31] has already studied this setting in more detail and we leave the analysis of our prompts for a dense retrieval setting as future work. second, our work focuses on flan [32] instruction-finetuned language models. we chose these models due to their ability to follow instructions and the fact that these models are open-source. our work can naturally be extended to other language models [3,5,9,28] and we leave the study of such models as a topic for future research. third, we study specific prompt templates (see appendix a) and there may be other ways to formulate the different prompts. finally, the computational cost of llms may be prohibitive to deploy llm-based query expansions in practice. it may be possible to distill the output of the large model into a smaller servable model. how to productionize llm-based query expansions is left as an open problem.", "Solved": NaN, "Unsolved": NaN}, "199668753": {"num": 402, "type": 1, "title": "Towards Knowledge-Based Recommender Dialog System", "pdfurl": "https://www.aclweb.org/anthology/D19-1189.pdf", "conclusion": "in this paper, we propose a novel end-to-end framework, kbrd, which bridges the gap between the recommender system and the dialog system via knowledge propagation. through a series of experiments, we show that kbrd can reach better performances in both recommendation and dialog generation in comparison with the baselines. we also discuss how the two systems benefit each other. dialog information is effective for the recommender system especially in the setting of cold start, and the introduction of knowledge can strengthen the recommendation performance significantly. information from the recommender system that contains the user preference and the relevant knowledge can enhance the consistency and diversity of the generated dialogs.", "Solved": NaN, "Unsolved": NaN}, "259224595": {"num": 403, "type": 1, "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective", "pdfurl": "https://export.arxiv.org/pdf/2306.12756v1.pdf", "conclusion": "in this paper, we analyzed the out-of-distribution robustness of several representative generative and dense retrieval models on the kilt benchmark. specifically, we proposed three perspectives to define the out-of-distribution robustness. the results showed that generative retrieval models expose significant vulnerabilities in ood robustness. more research efforts are needed to develop robust generative retrieval models.", "Solved": NaN, "Unsolved": NaN}, "253522964": {"num": 404, "type": 1, "title": "Empowering Language Models with Knowledge Graph Reasoning for Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2211.08380v1.pdf", "conclusion": "we presented oreolm, a novel model that incorporates symbolic kg reasoning with existing lms. we showed that oreolm can bring significant performance gain to open-domain qa benchmarks, both for closed-book and open-book settings, as well as encoder-only and encoder-decoder models. additionally, oreolm produces reasoning paths that helps interpret the model prediction. in future, we'd like to improve oreolm by training to conduct more reasoning steps, supporting locial reasoning, and apply oreolm to a broader range of knowledge-intensive nlp tasks.", "Solved": NaN, "Unsolved": NaN}, "222178328": {"num": 405, "type": 1, "title": "Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering", "pdfurl": "https://arxiv.org/pdf/2010.03274v1.pdf", "conclusion": "our focus in this paper has been on recognizing valid chains of reasoning, assuming a retrieval step that retrieves a reasonable pool of candidates to start with (section 3.2). however, the retrieval step itself is not perfect: for qasc, designed so that at least one valid chain always exists, the retrieved pool of 10 contains no valid chains for 24% of the questions (upper bound in table 2), capping the overall system's performance. to gauge the performance of our model when coupled with an improved retrieval system, we ran an experiment where, at test time, we explicitly add the gold chain to the candidate pool if it does not get retrieved (and even if there is some other valid chain already in the pool). we find the p@1 score rises from 0.54 (table 2) to 0.82 (upper bound is now 1.0). this indicates the model scoring algorithm is performing well, and that improving the retrieval system, e.g., by considering may more chains per question or modifying the search algorithm itself, is likely to have the biggest impact on improving the overall system. note also that the corpus itself is an important component: finding valid chains requires the corpus to contain a broad diversity of general facts to build chains from, hence expanding/filtering the corpus itself is another avenue for improvement.", "Solved": NaN, "Unsolved": NaN}, "209202200": {"num": 406, "type": 1, "title": "Published as a conference paper at ICLR 2020 NEURAL MODULE NETWORKS FOR REASONING OVER TEXT", "pdfurl": "https://export.arxiv.org/pdf/1912.04971v2.pdf", "conclusion": "we show how to use neural module networks to answer compositional questions requiring symbolic reasoning against natural language text. we define probabilistic modules that propagate uncertainty about symbolic reasoning operations in a way that is end-to-end differentiable. additionally, we show that injecting inductive bias using unsupervised auxiliary losses significantly helps learning.", "Solved": NaN, "Unsolved": NaN}, "252283929": {"num": 407, "type": 1, "title": "Machine Reading, Fast and Slow: When Do Models \"Understand\" Language?", "pdfurl": "https://www.aclanthology.org/2022.coling-1.8.pdf", "conclusion": "making progress towards trustworthy nlp models requires specific definitions for the behavior expected of these models in different situations. we propose a framework for rc model analysis that involves: (a) the definition of the expected 'reasoning' steps; (b) analysis of model behavior. we contribute such definitions for two linguistic 'skills' (comparison and coreference resolution), and use parallel explainability techniques to investigate whether rc models based on bert family encoders answer such questions correctly for the right reasons. we find that to be the case for comparison, but not for coreference. moreover, we find that, even for comparison, the models 'break' when encountering out-of-distribution counterfactual perturbations, suggesting that they memorize specific lexical patterns rather than learn more general reasoning 'skills'. as such, more research is needed on developing definitions and tests for specific 'skills' expected of nlu models, as well as on more faithful interpretability techniques.", "Solved": NaN, "Unsolved": NaN}, "230433817": {"num": 408, "type": 1, "title": "Retrieving and Reading : A Comprehensive Survey on Open-domain Question Answering", "pdfurl": "https://arxiv.org/pdf/2101.00774v3.pdf", "conclusion": "in this work we presented a comprehensive survey on the latest progress of open-domain qa (openqa) systems. in particular, we first reviewed the development of openqa and illustrated a \"retriever-reader\" architecture. moreover, we reviewed a variety of existing openqa systems as well as their different approaches. finally, we discussed some salient challenges towards openqa followed by a summary of various qa benchmarks, hoping to reveal the research gaps so as to push further progress in this field. based on  our review of prior research, we claim that openqa would continue to be a research hot-spot. in particular, single-step and multi-step neural retrievers will attract increasing attention due to the demand for more accurate retrieval of related documents. also, more end-to-end openqa systems will be developed with the advancement of deep learning techniques. knowledge enhanced openqa is very promising not only because it is helpful to generating the answer but also because it serves as the source for interpreting the obtained answer. however, how to represent and make full use of the knowledge for openqa still needs more research efforts. furthermore, to equip openqa with a dialogue-like interface that enables interaction between human users and the system for information exchange is expected to attract increasing attention, which well aligns with real world application scenarios.  ", "Solved": NaN, "Unsolved": NaN}, "237485084": {"num": 409, "type": 1, "title": "ReasonBERT: Pre-trained to Reason with Distant Supervision", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.494.pdf", "conclusion": "we propose reasonbert, a novel pre-training method to enhance the reasoning ability of language models. the resulting model obtains substantial improvements on multi-hop and hybrid qa tasks that require complex reasoning, and demonstrates superior few-shot performance. in the future, we plan to use our query-evidence pairs collected by distant supervision to improve the retrieval performance for open-domain qa, as well as empower reasonbert to handle more types of reasoning, like comparison and numeric reasoning, in natural language understanding. ", "Solved": NaN, "Unsolved": NaN}, "259501085": {"num": 410, "type": 1, "title": "InPars Toolkit: A Unified and Reproducible Synthetic Data Generation Pipeline for Neural Information Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2307.04601v1.pdf", "conclusion": "we have introduced the inpars toolkit, a codebase designed to generate synthetic data using llms in a reproducible manner for neural ir tasks. the toolkit comprises an end-to-end pipeline that encompasses data generation, training, reranking, and evaluating the trained models. additionally, the codebase is integrated with two major libraries for commonly used datasets from the beir benchmark, and it supports both gpu and tpu training and inference. our goal is to make research on these methods more accessible and to pave the way for this emerging research trend in the ir community. our experiments have demonstrated that training reranker models using synthetic data and evaluating them on gpu infrastructure yielded results comparable to those obtained when training on the tpu setup. additionally, we have also made available all synthetic data generated for all beir datasets and the models finetuned on this data.", "Solved": NaN, "Unsolved": NaN}, "237433880": {"num": 411, "type": 1, "title": "Exploiting Reasoning Chains for Multi-hop Science Question Answering *", "pdfurl": "https://arxiv.org/pdf/2109.02905v1.pdf", "conclusion": "we propose a novel chain guided retriever-reader framework for multi-hop qa. our modeling for the reasoning chains is effective to find both direct and indirect facts and is less likely to introduce noise. moreover, our framework is corpus-independent and is capable of handling the setting without any ground-truth annotations. further analysis and discussions also elucidate some of the inner workings of our framework while maintaining the explainability at the same time.", "Solved": NaN, "Unsolved": NaN}, "233296924": {"num": 412, "type": 1, "title": "Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.696.pdf", "conclusion": "in this work, we develop a synthetic adversarial data generation pipeline for qa, identify the best components, and evaluate on a variety of robustness measures. we propose novel approaches for answer candidate selection, adversarial question generation, and synthetic example filtering and relabelling, demonstrating improvements over existing methods. furthermore, we evaluate the final models on three existing robustness measures and achieve state-of-the-art results on adversarialqa, improved learnability of various comprehension skills for checklist, and improved domain generalisation for the suite of mrqa tasks.", "Solved": NaN, "Unsolved": NaN}, "218610721": {"num": 413, "type": 1, "title": "Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.209.pdf", "conclusion": "we proposed the olp task and a method to create an olp benchmark. we created the large olp benchmark olpbench, which will be made publicly available 4 . we investigated the effect of leakage of evaluation facts, non-relational information, and entity-knowledge during model selection using a prototypical open link prediction model. our results indicate that most predicted true facts are genuinely new. jointly in the context of lp.", "Solved": NaN, "Unsolved": NaN}, "231883811": {"num": 414, "type": 1, "title": "Unification-based Reconstruction of Multi-hop Explanations for Science Questions", "pdfurl": "https://www.aclweb.org/anthology/2021.eacl-main.15.pdf", "conclusion": "this paper proposed a novel framework for multihop explanation reconstruction based on explanatory unification. an extensive evaluation on the worldtree corpus led to the following conclusions:", "Solved": NaN, "Unsolved": NaN}, "259224614": {"num": 415, "type": 1, "title": "Resources and Evaluations for Multi-Distribution Dense Information Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2306.12601v1.pdf", "conclusion": "in this work, we formalize an underexplored information retrieval task where the retriever applied to several different distributions at inference time, some of which are unseen during training. we created benchmarks for this task and evaluated several simple retrieval methods and models on these benchmarks. we show that these simple methods work well obtaining up to 8 points improvement in recall@10 over baselines and an average of 3.8 points improvement.", "Solved": NaN, "Unsolved": NaN}, "251407400": {"num": 416, "type": 1, "title": "FQuAD2.0: French Question Answering and Learning When You Don't Know", "pdfurl": NaN, "conclusion": "in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions.", "Solved": NaN, "Unsolved": NaN}, "258833449": {"num": 417, "type": 1, "title": "Fact-Checking Complex Claims with Program-Guided Reasoning", "pdfurl": "https://www.aclanthology.org/2023.acl-long.386.pdf", "conclusion": "we proposed programfc, a few-shot neurosymbolic model for fact-checking that learns to map input claims to a reasoning program consisting of a sequence of sub-task function calls for answering a question, for fact-checking a simple claim, and for computing a logical expression. then factchecking is performed by executing that program. programfc combines the advantages of symbolic programs, such as explainability, with the flexibility of end-to-end neural models. using codex as the program generator, programfc demonstrates promising performance on hover and feverous with only a small number of incontext demonstrations and no additional training. we also investigated the impact of model size and the benefits of programs for retrieval, and we analyzed the errors. the results indicated that pro-gramfc effectively balances model capability, learning efficiency, and interpretability.", "Solved": NaN, "Unsolved": NaN}, "237485098": {"num": 418, "type": 1, "title": "Zero-Shot Dialogue State Tracking via Cross-Task Transfer", "pdfurl": "https://arxiv.org/pdf/2109.04655v1.pdf", "conclusion": "in this paper, we present transferqa, a unified generative model that performs dst without using any dst training data. transferqa uses the textto-text transfer learning framework that seamlessly combines extractive qa and multi-choice qa for tracking both categorical slots and non-categorical slots. to enable our model to zero-shot \"none\" value slots, we introduce two effective ways to construct unanswerable questions, i.e., negative question sampling and context truncation. the experimental results on the multiwoz and sgd datasets demonstrate the effectiveness of our approach in both zero-shot and few-shot settings. we also show that improving the \"none\" value slot accuracy has", "Solved": NaN, "Unsolved": NaN}, "253080620": {"num": 419, "type": 1, "title": "LittleBird: Efficient Faster & Longer Transformer for Question Answering", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.352.pdf", "conclusion": "we propose littlebird, which is more efficient in terms of memory and computational time than existing transformer models for long sequences, and its effective way to train. it combines a novel position encoding method, bialibi, and pack & unpack with sliding window attention to achieve high speed and accuracy, particularly in question answering tasks for long documents. the distillation and training method with padding insertion allows the model to be trained by reusing the existing pre-trained language model for short inputs and work well for long inputs even if trained on short inputs. we demonstrated through experiments that the accuracy of question answering improves as the model is fed a longer input, and we achieved state-of-the-art performance in korquad2.0 using littlebird.", "Solved": NaN, "Unsolved": NaN}, "264426555": {"num": 420, "type": 1, "title": "Implications of Annotation Artifacts in Edge Probing Test Datasets", "pdfurl": "https://www.aclanthology.org/2023.conll-1.39.pdf", "conclusion": "ep tests are classification tasks to measure an llm's ability to encode syntactic and semantic knowledge.however, in many ep datasets, there is not a significant difference between the random vs pre-trained encoders, which raises questions about the validity of the tests (the \"classifier knowledge\" problem).we analyze 17 datasets across 10 datasets to find various biases and show that the ep classifiers are more prone to use heuristic mechanisms when random encoders are used instead of the pre-trained ones.when the dataset biases are removed, the pre-trained encoders do show a significant difference from the random ones as expected.information-theoretic probes have been proposed before to solve the \"classifier knowledge\" problem, we show why they might not be necessary.future work would extend the findings of this study to fine-tuned models.", "Solved": NaN, "Unsolved": NaN}, "253447227": {"num": 421, "type": 1, "title": "Using contradictions to improve QA systems", "pdfurl": "https://export.arxiv.org/pdf/2211.05598v1.pdf", "conclusion": "despite the results above, multiple choice qa and extractive qa with a provided context is a limited setting that doesn't indicate the results would extend to other popular settings where nli. given that laban et al. (2022) shows similar results that contradiction is an important signal in factual consistency we are hopeful that it would.", "Solved": NaN, "Unsolved": NaN}, "6855746": {"num": 422, "type": 1, "title": "RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response", "pdfurl": "https://arxiv.org/pdf/1407.6981v2.pdf", "conclusion": "we consider three types of attackers with different capabilities for collecting rappor reports.the least powerful attacker has access to a single report from each user and is limited by one-time differential privacy level 1 on how much knowledge gain is possible. this attacker corresponds to an eavesdropper that has temporary ability to snoop on the users' reports.a windowed attacker is presumed to have access to one client's data over a well-defined period of time. this attacker, depending on the sophistication of her learning model, could learn more information about a user than the attacker of the first type. nevertheless, the improvement in her ability to violate privacy is strictly bounded by the longitudinal differential privacy guarantee of \u221e. this more powerful attacker may correspond to an adversary such as a malicious cloud service employee, who may have temporary access to reports, or access to a time-bounded log of reports. the third type of attacker is assumed to have unlimited collection capabilities and can learn the permanent randomized response b with absolute certainty. because of the randomization performed to obtain b from b, she is also bounded by the privacy guarantee of \u221e and cannot improve upon this bound with more data collection. this corresponds to a worst-case adversary, but still one that doesn't have direct access to the true data values on the client.despite envisioning a completely local privacy model, one where users themselves release data in a privacy-preserving fashion, operators of rappor collections, however, can easily manipulate the process to learn more information than warranted by the nominal \u221e. soliciting users to participate more than once in a particular collection results in multiple permanent randomized responses for each user and partially defeats the benefits of memoization. in the webcentric world, users use multiple accounts and multiple devices and can unknowingly participate multiple times, releasing more information than what they expected. this problem could be mitigated to some extent by running collections per account and sharing a common permanent randomized response. notice the role of the operator to ensure that such processes are in place and the required or assumed trust on the part of the user.it is likely that some attackers will aim to target specific users by isolating and analyzing reports from that user, or a small group of users that includes them. even so, some randomly-chosen users need not fear such attacks at all: with probability 1 2 f h , clients will generate a permanent randomized response b with all 0s at the positions of set bloom filter bits. since these clients are not contributing any useful information to the collection process, targeting them individually by an attacker is counter-productive. an attacker has nothing to learn about this particular user. also, for all users, at all times, there is plausible deniability proportional to the fraction of clients providing no information.in one particular attack scenario, imagine an attacker that is interested in learning whether a given client has a particular value v, whose population frequency is known to be fv. the strongest evidence in support of v comes in the form of both bloom filter bits for v being set in the client's report (if two hash functions are used). the attacker can formulate its target set by selecting all reports with these two bits set. however, this set will miss some clients with v and include other clients who did not report v. false discovery rate (fdr) is the proportion of clients in the target set who reported a value different from v. figure 7 shows fdr as a function of fv, the frequency of the string v. notably, for relatively rare values, most clients in the target set will, in fact, have a value that is different from v, which will hopefully deter any would-be attackers.the main reason for the high fdr rate at low frequencies fv stems from the limited evidence provided by the observed bits in support of v. this is clearly illustrated by figure 8 where the probability that v was reported (1) or not reported (0) by the client is plotted as a function of fv. for relatively rare strings (those with less than 10% frequency), even when both bits corresponding to v are set in the report, the probability of v being reported is much smaller than of value v given the two bits observed in a rappor report s corresponding to the two bits set by string v. for rare strings, even when both bits are set to 1 (green lines), it is still much more likely that the client did not report v, but some other value.it not being reported. because the prior probability fv is so small, a single client's reports cannot provide sufficient evidence in favor of v.", "Solved": NaN, "Unsolved": NaN}, "240288835": {"num": 423, "type": 1, "title": "MetaICL: Learning to Learn In Context", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.201.pdf", "conclusion": "in this paper, we introduced metaicl, a new fewshot learning method where an lm is meta-trained to learn to in-context learn, i.e. condition on training examples to recover the task and make predictions. we experiment with a large, diverse collection of tasks, consisting of 142 unique tasks in total and 52 unique target tasks, using seven different settings. metaicl outperforms a range of strong baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer, and outperforms or matches 8x bigger models. we identify ingredients for success of metaicl such as the number and diversity of meta-training tasks. we also demonstrate that, while metaicl is better than recent work using natural instructions, they are complementary and the best performance is achieved by integrating metaicl with instructions.", "Solved": NaN, "Unsolved": NaN}, "256846917": {"num": 424, "type": 1, "title": "Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions", "pdfurl": "https://export.arxiv.org/pdf/2302.06801v1.pdf", "conclusion": "this work extensively covers research efforts on backdoor learning for nlp. to this end, we systematically and comprehensively survey state-of-the-art research studies on backdoor attacks and defenses. additionally, we thoroughly review and analyze various aspects of backdoor learning, including techniques, model architectures, evaluation metrics, and benchmark datasets. we argue that for backdoor learning to contribute to actual robustness, research studies should take into account an expansive view and strive to answer questions related to why such attacks and defenses are successful. it is crucial to determine whether any given technique is booming due to limitations and weaknesses associated with the target model (inherent incapability arising from intrinsic properties of a target model) or whether it is due to weaknesses or limitations in the dataset itself. finally, we offer insights into open challenges and future research directions worth pursuing.  [98] 2022 gradient-based adversarial attacks amazon, trojanai sentiment analysis generic bert, gpt, lstm, gru azizi et al. [6] 2021 sequence-to-sequence (seq-2-seq) generative model mr, yelp, ag news, hs sentiment analysis, topic classification black-box bert, cnn, lstm chan et al. [14] 2020 conditional adversarially regularized autoencoder snli, yelp, mnli sentiment analysis, nli generic bert, roberta, xlnet chan et al. [16] 2021 backdoor sentence insertion imdb, dbpedia sa, sc, pr grey-box lstm chan et al. [21] 2021 trigger construction imdb, sst-5 sa white-box bert,lstm eger et al. [35] 2019 visual text perturbations imdb, mr sa black-box bert,lstm nguyen et al. [119] 2020 input-aware trigger generator via diversity loss mnist, object recognition sa, sc resnet,lstm acc, asr qi et al. [131] 2021 syntactic trigger-based attack sst-2, ag news, olid sa, sc white-box bert,lstm qi et al. [132] 2021 invisible triggers via learnable combination of word substitution sst-2, ag news, olid sa, sc white-box bert,lstm wallace et al. [159] 2019 gradient guided search over token snli, squad, olid nli black-box bert, gpt-2 yang et al. [180] 2021 poisoned word embeddings sst-2, imdb, snli sa, sc, nli black-box bert yang et al. [182] 2021 negative data augmentation and modifying word embeddings yelp, imdb, twitter sa, toxic detection black-box bert zhang et al. [192] 2021 re-weighted training of language models webtext, twitter toxic detection, qa white-box bert, gpt-2, xlnet zhang et al. [195] 2021 neuron-level backdoor attack olid, gtsrb, sst-2, enron toxic and spam detection, sa black-box bert, roberta, vggnet li et al. [83] 2021 knowledge distillation gtsrb, cfair-10 image recognition black-box resnet, vggnet garg et al. [44] 2020 backdoor injection by adversarial weight perturbation mr, cfair-10 generic black-box wordcnn, lstm chen et al. [19] 2021 task-agnostics label replacement foundation model sst-2, qnli, rte generic white-box bert, gpt-2 gan et al. [39] 2021 triggerless genetic clean-labels sentence generation sst-2, ag news, olid sc, sa black-box bert", "Solved": NaN, "Unsolved": NaN}, "257766470": {"num": 425, "type": 0, "title": "Natural Language Reasoning, A Survey", "pdfurl": "https://export.arxiv.org/pdf/2303.14725v2.pdf", "conclusion": "we introduce both limitations of the current research and intrinsic in plms. firstly, there are gaps in defeasible reasoning and reasoning path evaluation.\u2022 research gap on defeasible reasoning. while defeasible reasoning is widely used in our daily life, this topic is still under-explored in nlp. [4] found that it is more challenging for chatgpt to perform abductive reasoning and inductive reasoning than deduction, among which induction is the much more difficult one. \u2022 lack of effective ways to evaluate reasoning paths. it is still challenging to automatically evaluate generated reasoning paths without ground truth. evaluating reasoning paths might become increasingly important to build explainable and reliable ai systems, especially when more people contact and use chatgpt-like products nowadays.secondly, there are also limitations intrinsic to plms.\u2022 soft deduction can produce invalid conclusions. transformers can only predict conclusions with probability, irrespective of whether the conclusion of deductive reasoning is necessarily true in nature, which might prevent it from precise reasoning. this characteristic can result in a sub-optimal solution to deductive problems (including arithmetic reasoning and symbolic reasoning). for example, while chatgpt is impressive on reasoning tasks, it still fails to achieve perfect performance on the simplest one-step deductive inference task [4].\u2022 biases on content. plms make their prediction based on context. while llms have made huge progress in reasoning, [32] found that llms are biased by content like humans when performing deduction. for example, they perform worse in abstract or counterfactual situations than the realistic ones. such biases will hinder them from actual reasoning and lead to wrong answers, degrading downstream performance. more severely, it might cause harmful societal influences due to some social biases such as gender, which also exist in gpt4 [16].", "Solved": NaN, "Unsolved": NaN}, "233231436": {"num": 426, "type": 1, "title": "AR-LSAT: Investigating Analytical Reasoning of Text", "pdfurl": "https://arxiv.org/pdf/2104.06598v2.pdf", "conclusion": "in this paper, we study the challenging task of analytical reasoning and introduce a dataset ar-lsat to facilitate research on analytical reasoning. we analyze the knowledge understanding and reasoning ability required for this task and present two basic approaches: a transformer-based approach and a logical-level reasoning framework, named analytical reasoning machine (arm). arm extracts symbolic knowledge, including participants, facts and rules mentioned in the context and extract logical functions from the rules. afterwards, it performs deep reasoning to find all the legitimate solutions to the problem posed and finally makes a prediction. arm sheds a light on the reasoning procedure for analytical reasoning, and each component can be further developed. experiments show that this task is very challenging for current transformer-based pre-trained language models and arm outperforms them with better performance and interpretability. further discussions are made to shed light on important future directions. ", "Solved": NaN, "Unsolved": NaN}, "241583187": {"num": 427, "type": 1, "title": "FaBULOUS: Fact-checking Based on Understanding of Language Over Unstructured and Structured information", "pdfurl": "https://www.aclanthology.org/2021.fever-1.4.pdf", "conclusion": "overall, this multi-modal task creates a plethora of new challenges to overcome and opens exciting avenues of research for the future of automated fact verification.", "Solved": NaN, "Unsolved": NaN}, "229923926": {"num": 428, "type": 1, "title": "Coreference Reasoning in Machine Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2021.acl-long.448.pdf", "conclusion": "we show that the high performance of recent models on the quoref dataset does not necessarily indicate that they are adept at performing coreference reasoning, and that qa based on coreference reasoning is a greater challenge than current scores suggest. we then propose a methodology for creating a dataset that better presents the coreference reasoning challenge for mrc. we provide our methodology to an annotator and create a sample dataset. our analysis shows that our dataset contains fewer biases compared to quoref, and the performance of state-of-the-art quoref models drops considerably on this evaluation set.", "Solved": NaN, "Unsolved": NaN}, "202785879": {"num": 429, "type": 1, "title": "Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-1260.pdf", "conclusion": "we introduced rocc, a simple unsupervised approach for selecting justification sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer. we coupled this method with a state-of-the-art bert-based supervised question answering system, and achieved a new state-ofthe-art on the multirc and arc datasets among approaches that do not use external resources during training. we showed that rocc-based qa approaches are more robust across domains, and generalize better to other related tasks like entailment. in the future, we envision that rocc scores can be used as distant supervision signal to train supervised justification selection methods.", "Solved": NaN, "Unsolved": NaN}, "216562779": {"num": 430, "type": 1, "title": "MAVEN: A Massive General Domain Event Detection Dataset", "pdfurl": "https://www.aclweb.org/anthology/2020.emnlp-main.129.pdf", "conclusion": "in this paper, we present a massive general domain event detection dataset (maven), which significantly alleviates the data scarcity and low coverage problems of existing datasets. we conduct a thorough evaluation of the state-of-the-art ed models on maven. the results indicate that general domain ed is still challenging and maven may facilitate further research. we also explore some promising directions with analytic experiments, including modeling multiple event correlations (sec-tion 5.3), utilizing the hierarchical event schema to distinguish close types (section 5.6), and improving other ed tasks with transfer learning (section 5.5). in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc. ", "Solved": NaN, "Unsolved": NaN}, "252212320": {"num": 431, "type": 1, "title": "SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2209.05917v3.pdf", "conclusion": "in this paper, we proposed a novel uni-encoder model, sparse retriever using a dual document encoder (spade), to alleviate the trade-off between effectiveness and efficiency of the ir system.we adopted a dual document encoder for lexical and semantic matching and developed a co-training strategy to mitigate the training intervention between encoders.we also utilized document-and corpus-level pruning during model training, enabling efficient retrieval using the inverted index.experimental results showed that spade achieves state-of-the-art performance among uni-encoder models with acceptable query latency, notably preferable for commercial ir systems.", "Solved": NaN, "Unsolved": NaN}, "253244513": {"num": 432, "type": 1, "title": "Natural Language Deduction with Incomplete Information", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.564.pdf", "conclusion": "in this work, we tackle the generation of missing premise statements in textual reasoning through the use of abduction. we introduce a new system capable of abductive and deductive step generation, which yields inferred missing premises while building a proof showing its reasoning. furthermore, we propose a novel validation method that reduces hallucination and other common failure modes in end-to-end and stepwise searches. future work can improve our system by scaling up the models used, plus using additional notions of validation as discussed in the error analysis. we believe our overall framework can be a promising foundation for future reasoning systems.", "Solved": NaN, "Unsolved": NaN}, "261065828": {"num": 433, "type": 1, "title": "Building Interpretable and Reliable Open Information Retriever for New Domains Overnight", "pdfurl": "https://export.arxiv.org/pdf/2308.04756v1.pdf", "conclusion": "in this work, we propose an information retrieval (ir) pipeline built on the advances of query decomposition and event linking. specifically, we associate the input query with generated events relevant to the query through the decomposition step and then link these events to real-world facts in a knowledge base with an event-linking model. we show that with simple passage selection through bm-25 and cross-domain qa supervision, our pipeline outperforms existing state-of-the-art unsupervised or cross-domain ir models on five datasets by an average of 6%. compared with existing methods, our pipeline does not involve heavy pre-training, parameter tuning, or domain-specific supervision.", "Solved": NaN, "Unsolved": NaN}, "232307813": {"num": 434, "type": 0, "title": "Local Interpretations for Explainable Natural Language Processing: A Survey", "pdfurl": "https://export.arxiv.org/pdf/2103.11072v2.pdf", "conclusion": "the continued growth of probing-based papers has also led to recent work examining best practices for probes, and how to interpret their results. hewitt and liang [66] considered how to ensure that a probe is truly reflective of the underlying information present in a model, and proposed the use of a control task, a randomised version of a probe task in which high performance is only possible by memorisation of inputs. hence, a faithful probe should perform well on a probe task and poorly on a corresponding control task if the underlying model does indeed contain the information being probed for. the authors found that most probes (including linear classifiers)are over-parameterised, and discuss methods for constraining complex probes (e.g. multilayer perceptrons) to improve faithfulness while still allowing them to achieve similar results.while most papers we have discussed above follow the intuition that probes should avoid complex probes to prevent memorisation, pimentel et al. [127] suggest that instead the probe with the best score on a given task should be chosen as the tightest estimate, since simpler models may simply be unable to extract the linguistic information present in a model, and such linguistic information cannot be 'added' by more complex probes (since their only input are hidden representations). in addition, the authors argue that memorisation is an important part of linguistic competence, and as such probes should not be artificially punished (via control tasks) for doing this. recent work has also presented methods that avoid making assumptions about probe complexity, such as mdl probing [101,170], which directly measures 'amount of effort' needed to achieve some extraction task, or directprobe [198], which directly examines intermediate representations of models to avoid having to deal with additional classifiers.finally, hall maudslay et al. [59] compared the structural probe [67] with a lightweight dependency parser (both given the same inputs), and demonstrate that the parser is generally able to extract more syntactic information from bert embedding. in contrast, the probe performs better with a different metric, showing that the choice of metric is important for probes: when testing for evidence of linguistic information, one should not only consider the nature of the probe, but also the metric used to evaluate it. furthermore, the significance of well-performing probes is not clear: models may encode linguistic information not actually used by the end-task [138], showing that the presence of linguistic information does not imply it is being used for prediction. more causal approaches such as amnesiac probing [50], which directly intervene in the underlying model's representations, may better distinguish between these cases.", "Solved": NaN, "Unsolved": NaN}, "247450729": {"num": 435, "type": 1, "title": "Efficient Long Sequence Encoding via Synchronization", "pdfurl": "https://arxiv.org/pdf/2203.07644v1.pdf", "conclusion": "in this work, we propose transync framework with flexible synchronization mechanisms for encoding long sequences. we demonstrate the feasibility of our method in reasoning tasks with long context, and also show its high adaptability to different scenarios. we consider our work to be valuable as an easy solution to address the long context issue in qa, and to be potentially applicable to other long sequence modeling tasks.", "Solved": NaN, "Unsolved": NaN}, "235097195": {"num": 436, "type": 1, "title": "MIMOQA: Multimodal Input Multimodal Output Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.418.pdf", "conclusion": "we presented one of the first exploration, to the best of our knowledge, of multimodal output question answering from multimodal inputs and proposed usage of publicly available textual datasets for it. we proposed strong baselines by utilizing the existing frameworks for extract textual answers and independently match them with an appropriate image. we demonstrate the value of a joint-multimodal understanding for multimodal outputs in our problem setup by developing a multimodal framework mexbert which outperformed the baselines significantly on several metrics. we also developed a proxy supervision technique in absence of labelled outputs and showed its effectiveness for improved multimodal question answering. we used some existing metrics to compare the different models and justified the usage of these metrics based on a human experiment.", "Solved": NaN, "Unsolved": NaN}, "231627885": {"num": 437, "type": 1, "title": "COARSE-GRAINED DECOMPOSITION AND FINE-GRAINED INTERACTION FOR MULTI-HOP QUESTION ANSWERING", "pdfurl": "https://arxiv.org/pdf/2101.05988v1.pdf", "conclusion": "in this paper, we propose a mutli-hop question answering model, that contains a coarse-grained decomposition strategy to divide a complex query into multiple single-hop simple queries and a fine- grained interaction strategy to better represent each word in the document and help the model find the sentences needed to answer the question. in the experiments, we show that our models significantly and consistently outperform the baseline model.", "Solved": NaN, "Unsolved": NaN}, "207852656": {"num": 438, "type": 1, "title": "Meta Answering for Machine Reading", "pdfurl": "https://arxiv.org/pdf/1911.04156v2.pdf", "conclusion": "meta-answering is a framework for qa that attempts to simulate real-world-imperfectinformation-seeking tasks, where humans look for answers in settings mediated by machines, using natural language. human meta-answerers can compete with a bert-based single system with access to full documents, by only looking at a five token window around candidates. a machine meta-answerer built on bert can improve the environment's qa system, thus proving that it is possible to investigate mr in imperfect information settings in high-performance regimes. further, the task brings to the surface, yet again but from a novel perspective, limitations of the current nlu paradigm. mma cannot use the contextual information that is effortlessly exploited by humans. thus, it might prove a suitable framework to advance on these challenges.", "Solved": NaN, "Unsolved": NaN}, "264590451": {"num": 439, "type": 1, "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter", "pdfurl": "https://export.arxiv.org/pdf/2310.18347v1.pdf", "conclusion": "in conclusion, this research successfully introduces a prca-based paradigm for reqa tasks, tackling the inherent challenges of fine-tuning llms in the retrieval-enhancement framework, especially given their vast parameter size and closed-source natures.prca innovatively distills retrieved documents via generator rewards, leading to a marked improvement in the reqa task's performance.experimental outcomes consistently demonstrate the robustness and effectiveness of prca when paired with various retrievers and generators, indicating its potential to be widely deployed as an adapter on the reqa task.", "Solved": NaN, "Unsolved": NaN}, "264406133": {"num": 440, "type": 1, "title": "Improving Question Generation with Multi-level Content Planning", "pdfurl": "https://export.arxiv.org/pdf/2310.13512v2.pdf", "conclusion": "this paper presents multifactor, a novel qg method with multi-level content planning.specifically, multifactor consists of a fa-model, which simultaneously select important phrases and generate an answer-aware summary (a full answer), and q-model, which takes the generated full answer into account for question generation.both fa-model and q-model are formalized as our simple yet effective pet.experiments on hotpotqa and squad 1.1 demonstrate the effectiveness of our method.", "Solved": NaN, "Unsolved": NaN}, "248779897": {"num": 441, "type": 1, "title": "Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.82.pdf", "conclusion": "this paper addresses answer uncertainty and unanswerability in multiple-choice mrc. measures of answer uncertainty are required to identify examples that the system may struggle to get correct and hence should abstain from answering such questions. unanswerability detection is required for when the answer cannot be deduced using the information provided. an electra prlm achieve competitive results on the default reclor dataset, achieving up to 67.1% accuracy on the evaluation split. ensemble-based predictive uncertainty measures are explored for both modes of operation: answer uncertainty for negative marking schemes and the presence of unanswerability. it is shown that uncertainty in the prediction such as expected entropy is correlated with the error rate of the mrc system allowing better than vanilla performance with an aggressive negative marking scheme for reclor and race. interestingly, it is found that expected entropy from the predictions of an implicitly trained system is competitive at unanswerability detection and is able to out-compete map decoding from an explicitly trained system that has been trained with unanswerable examples for reclor.", "Solved": NaN, "Unsolved": NaN}, "258960507": {"num": 442, "type": 1, "title": "GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking", "pdfurl": "https://export.arxiv.org/pdf/2305.18144v2.pdf", "conclusion": "in this paper, we focus on narrowing the gap between retrieval and generation for retrieval-enhanced text generation methods. we propose griprank, a novel approach to improve the passage ranking capability by distilling knowledge from a generative passage estimator to the passage ranker. we evaluate our approach on diverse knowledge-intensive language tasks, including zero-shot slot filling, open-domain question answering, and knowledge-enhanced dialogue generation. experimental results show that the proposed griprank presents advantages over previous state-of-the-art approaches. further analysis demonstrates the effectiveness of our proposed approach in narrowing the gap between passage retrieval and answer generation.", "Solved": NaN, "Unsolved": NaN}, "248218748": {"num": 443, "type": 1, "title": "Towards Fine-grained Causal Reasoning and QA", "pdfurl": "https://arxiv.org/pdf/2204.07408v1.pdf", "conclusion": "we explored the efficacy of current state-of-the-art methods for causal reasoning tasks by considering a novel fine-grained reasoning setting and developing a dataset with rich human labels. experimental results using the state-of-the-art pre-trained language models provide the evidence that there is much room for improvement on causal reasoning tasks, and a need for designing better solutions to correlation discovery related to event causality analysis and why/what-if qa tasks.", "Solved": NaN, "Unsolved": NaN}, "160009340": {"num": 444, "type": 1, "title": "Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction", "pdfurl": "https://arxiv.org/pdf/1905.08511v1.pdf", "conclusion": "we consider that the main contributions of our study are (1) the proposed qfe model that is based on a summarization model for the explainable multi-hop qa, (2) the dependency among the evidence and the coverage of the question due to the usage of the summarization model, and (3) the state-of-the-art performance in evidence extraction in both rc and rte tasks.", "Solved": NaN, "Unsolved": NaN}, "246240382": {"num": 445, "type": 1, "title": "Towards Collaborative Question Answering: A Preliminary Study", "pdfurl": "https://arxiv.org/pdf/2201.09708v1.pdf", "conclusion": "the fact that knowledge are not shared gives rise to individual diversity and motivates collaboration. we believe natural-language based collaboration system is a domain that has practical implication and holds scientific values. the collabqa task and dataset we proposed in this paper is a small step towards that direction.  figure 6 shows the structure and examples in our proposed knowledge graphs. g 1 contains a list of person entities. the value of a property of the entity is randomly generated within a reasonable range. for example, the value of a person's height is randomly sampled in the range [160cm, 200cm]. we add a series of constraints to make the kgs more realistic, such as a person who doesn't have job gets no annual income; a person cannot be a mayor and be an employee in some company at the same time; the largest company of a city must be located in that city, and so on.", "Solved": NaN, "Unsolved": NaN}, "233297024": {"num": 446, "type": 1, "title": "Constrained Language Models Yield Few-Shot Semantic Parsers", "pdfurl": "https://arxiv.org/pdf/2104.08768v2.pdf", "conclusion": "we wish to rapidly develop semantic parsers in new domains. to this end, we have demonstrated that constrained decoding of powerful language models can enable the paraphrasing of user utterances into a controlled sublanguage, which may then be mapped to a task-specific representation. with small hundreds of examples we are able to quickly bootstrap models for a variety of datasets, enabling future work that explores human in the loop interactions for iterative model refinement. ", "Solved": NaN, "Unsolved": NaN}, "233297028": {"num": 447, "type": 1, "title": "Question Decomposition with Dependency Graphs", "pdfurl": "https://arxiv.org/pdf/2104.08647v1.pdf", "conclusion": "in this work, we propose to represent qdmr structures with a dependency graph over the input tokens, and propose a graph parser and a seq2seq model that uses graph supervision as an auxiliary loss. we show that a graph parser is 16x faster than a seq2seq model, and that it exhibits better sample coplexity. moreover, using graphs as auxiliary supervision improves out-of-domain generalization and leads to better performance on questions that represent a long sequence of computational steps. last, we propose a new evaluation metric for qdmr parsing and show it better corresponds to human intuitions.", "Solved": NaN, "Unsolved": NaN}, "128345225": {"num": 448, "type": 1, "title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text", "pdfurl": "https://www.aclweb.org/anthology/D19-1242.pdf", "conclusion": "pullnet is a novel integrated qa framework for (1) learning what to retrieve from a kb and/or corpus and (2) reasoning with this heterogeneous data to find the best answer. unlike prior work, pullnet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. in each iteration, a graph cnn is used to identify subgraph nodes that should be expanded using \"pull\" operations on the corpus and/or kb. this iterative process makes it possible to retrieve a small graph that contains just the information relevant to a multi-hop question.", "Solved": NaN, "Unsolved": NaN}, "254854282": {"num": 449, "type": 1, "title": "Query-as-context Pre-training for Dense Passage Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2212.09598v3.pdf", "conclusion": "in this work, we propose query-as-context pretraining, a simple yet effective technique to alleviate the previously ignored issue of weakly correlated pairs during context-supervised pre-training.", "Solved": NaN, "Unsolved": NaN}, "235187342": {"num": 450, "type": 1, "title": "Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering *", "pdfurl": "https://arxiv.org/pdf/2105.11776v1.pdf", "conclusion": "we propose to dynamically construct amr-sg that can reflect the intrinsic relations of relevant facts leveraging amr, a graph annotation. amr-sg combines the advantages of rich textual corpus and graph structure, where we can select useful facts that completely form the reasoning chain and make fact-level modeling. experimental results show that amr-sg can maintain high explainability, and successfully couple with strong pretrained models to achieve significant improvement on openbookqa and arc-challenge over approaches leveraging additional kgs. ", "Solved": NaN, "Unsolved": NaN}, "260899983": {"num": 451, "type": 1, "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2308.07902v1.pdf", "conclusion": "this survey provides a comprehensive review of various literature for the evaluation of llms. we aggregate different works with their intended competencies. some of the competencies(reasoning, knowl-edge) already have holistic evaluation benchmarks, while others(planning, coding) still face disparate challenges. the goal of this paper is to comb the numerous work concerning llms' evaluation through the lens of the core competencies test. lighten the cognitive load for assimilating numerous evaluation works due to the various functions of llms. in doing so, we have also identified the challenge faced by each competency, looking forward to alleviating it in the future.", "Solved": NaN, "Unsolved": NaN}, "257365136": {"num": 452, "type": 1, "title": "MULTITASK PROMPT TUNING ENABLES PARAMETER-EFFICIENT TRANSFER LEARNING", "pdfurl": "https://export.arxiv.org/pdf/2303.02861v1.pdf", "conclusion": "we introduced and studied multitask prompt tuning (mpt), which learns a single transferable prompt by decomposing and distilling knowledge from multiple source tasks and their task-specific source prompts. mpt decomposes the task prompt as the hadamard product of a shared prompt matrix and a rank-one task-specific matrix. the shared component is then transferred and adapted to target tasks for further tuning. empirically we found this approach enables parameter-efficient transfer learning to target downstream tasks across diverse nlp benchmarks, even outperforming the full finetuning baseline in some cases, despite tuning much fewer task-specific parameters.", "Solved": NaN, "Unsolved": NaN}, "237513875": {"num": 453, "type": 1, "title": "Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?", "pdfurl": "https://www.aclanthology.org/2022.coling-1.139.pdf", "conclusion": "edge probing tests are the predominant method to probe for linguistic information in large language models. we use them to evaluate how the process of fine-tuning an lm for qa might change the grammatical knowledge in an encoder, and observe no significant differences between pre-trained and fine-tuned lms. more importantly, we find this phenomenon in carefully designed target tasks where the models must use the said grammatical knowledge. from similar ep test results, previous works have concluded that fine-tuning does not change the encoding of grammatical knowledge. however, our analysis provides a 'dataset bias' explanation for the consistency of the results and provides some clues as to why any representation tends to achieve very similar results for ep tests. this is different from the previous task-design criticisms of the ep tests.", "Solved": NaN, "Unsolved": NaN}, "237532313": {"num": 454, "type": 1, "title": "Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings", "pdfurl": "https://www.aclanthology.org/2021.blackboxnlp-1.3.pdf", "conclusion": "in this paper, we addressed three research questions: whether integrating external knowledge can improve explainability for nli, how effective knowledge implicitly stored in language models is for reasoning, and how humans perceive explanation quality of state-of-the-art natural language inference models. to answer these questions, we proposed different methods of integrating various knowledge sources into deep learning models. we found that fine-tuned language models reach the highest performance on e-snli as well as the highest average accuracy within the nli stress test evaluation. however, their performance can break down on numerical reasoning and negations. in addition to automatic evaluation, we conducted a large-scale human crowdsourcing evaluation and found that high differences in accuracy, bleu or bleurt scores do not reflect in significant differences in human ratings of explanation correctness, commonsense inclusion, grammar or label correctness. this highlights an alarming disconnect between automatic evaluation scores and human ratings, that puts the real-world utility of recent model improvements into question and requires to re-think automatic evaluation across the field of explainable ai.  b study interface figure 4 shows an example of the study interface used to collect human ratings as discussed in section 5. table 4 lists the annotation guidelines used to decide on low/high levels of required external knowledge as discussed in section 5.3. table 5 shows example annotations.   the entailment can be decided by matching identical parts in the premise and the hypothesis. premise: a water scene with a sunset in the background.", "Solved": NaN, "Unsolved": NaN}, "233297051": {"num": 455, "type": 1, "title": "Explaining Answers with Entailment Trees", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.585.pdf", "conclusion": "our goal is to enable machines to generate richer, more systematic explanations. to this end, we have developed a novel formulation of explanations as multistep entailment trees, and created entail-mentbank, the first large dataset of such trees.", "Solved": NaN, "Unsolved": NaN}, "258575519": {"num": 456, "type": 1, "title": "Multi-Hop Question Generation with Knowledge Graph-Enhanced Language Model", "pdfurl": NaN, "conclusion": "the present study proposes a novel approach to question generation, which leverages the benefits of a knowledge graph and mimics human reasoning. the proposed model, referred to as kgel, incorporates an answer-aware graph reasoning module to improve the ability to identify key information from the context. empirical evaluations on the hotpotqa dataset demonstrate the superiority of kgel over baseline models, particularly in terms of the completeness and answerability of generated questions.", "Solved": NaN, "Unsolved": NaN}, "231709861": {"num": 457, "type": 1, "title": "Benchmarking Machine Reading Comprehension: A Psychological Perspective", "pdfurl": "https://www.aclweb.org/anthology/2021.eacl-main.137.pdf", "conclusion": "in this paper, we outlined current issues and future directions for benchmarking machine reading comprehension. we visited the psychology study to analyze what we should ask of reading comprehension and the construct validity in psychometrics to analyze how we should correctly evaluate it. we deduced that future datasets should evaluate the capability of the situation model for understanding context-dependent situations and for grounding to non-textual information and ensure the substantive validity by creating shortcut-proof questions and designing an explanatory task formulation. ", "Solved": NaN, "Unsolved": NaN}, "260164780": {"num": 458, "type": 1, "title": "WE BAR E N A : A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS", "pdfurl": "https://export.arxiv.org/pdf/2307.13854v3.pdf", "conclusion": "we present webarena, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents.webarena includes fully functional web applications and genuine data from four major categories, providing a realistic platform for agent interaction.it further supports a wide range of tools and external knowledge bases, fostering a focus on human-like problem-solving.additionally, we curate a comprehensive benchmark consisting of 812 examples that focus on translating high-level natural language intents into specific web interactions.we also offer metrics to programmatically ascertain whether tasks have been completed according to the desired objectives.our experiments show that even gpt-4 only achieves a limited end-to-end task success rate of 14.41%, significantly lagging behind the human performance of 78.24%.these findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within webarena environment.", "Solved": NaN, "Unsolved": NaN}, "258546861": {"num": 459, "type": 1, "title": "Chain-of-Skills: A Configurable Model for Open-Domain Question Answering", "pdfurl": "https://www.aclanthology.org/2023.acl-long.89.pdf", "conclusion": "in this work, we propose a modular model chain-of-skills (cos) that learns five reusable skills for odqa via multi-task learning. to reduce task interference, we design a new parameterization for skill modules. we also show that skills learned by cos can be flexibly chained together to better fit the target task. cos can directly perform superior zero-shot retrieval using multitask self-supervision on wikipedia. when finetuned on multiple datasets, cos achieves sota results across the board. for future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (talmor et al., 2022) and biomedical retrieval (nentidis et al., 2020;zhang et al., 2022b). group at microsoft research for their helpful discussions and anonymous reviewers for their valuable suggestions on this paper.", "Solved": NaN, "Unsolved": NaN}, "264426669": {"num": 460, "type": 1, "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA", "pdfurl": "https://export.arxiv.org/pdf/2310.14393v1.pdf", "conclusion": "in this work, we study the problem of merging retrieved and llm-generated knowledge for opendomain qa.we tackle the challenge of knowledge conflicts caused by llm's hallucination with a compatibility-oriented knowledge merging framework (combo).specifically, we match llmgenerated and retrieved passages for a given question into pairs based on their compatibility and perform information fusion on the encoder side of the fid-based reader by feeding matched pairs as input.", "Solved": NaN, "Unsolved": NaN}, "261049520": {"num": 461, "type": 1, "title": "RALLE: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2308.10633v2.pdf", "conclusion": "this paper introduces ralle, an accessible framework for developing and evaluating r-llms.we also report evaluation results of several r-llms built using open-source retrievers and llms on knowledge-intensive tasks.overall, ralle offers a significant advancement in retrieval-augmented generation research, enabling efficient development, evaluation, and improvement of r-llms.we hope that ralle will contribute to the development of best practices for r-llms.", "Solved": NaN, "Unsolved": NaN}, "260611249": {"num": 462, "type": 1, "title": "RETROFORMER: RETROSPECTIVE LARGE LANGUAGE AGENTS WITH POLICY GRADIENT OPTIMIZATION", "pdfurl": "https://export.arxiv.org/pdf/2308.02151v1.pdf", "conclusion": "in this study, we present retroformer, an elegant framework for iteratively improving large language agents by learning a plug-in retrospective model. this model, through the process of policy optimization, automatically refines the prompts provided to the language agent with environmental feedback. through extensive evaluations on real-world datasets such as hotpotqa, the method has been proven to effectively improve the performances of large language agents over time both in terms of learning speed and final task completion performances.", "Solved": NaN, "Unsolved": NaN}, "236635315": {"num": 463, "type": 1, "title": "Talk2Data: High-Level Question Decomposition for Data-Oriented Question and Answering", "pdfurl": "https://arxiv.org/pdf/2107.14420v1.pdf", "conclusion": "here, we would like to report and discuss several limitations that was found during our system implementation and evaluation. scalability issue. the current implementation of the prototype system still cannot handle large datasets that contain tens of thousands of data records, where the answer extraction algorithm is the primary bottleneck. it will be more difficult to find out accurate answers from a large dataset within a fixed period of time. there are several approaches that could be applied to address the issue, which will be our future work. first, using parallel searching algorithms [45] will greatly improve the algorithm efficiency. second, using a pre-trained model such as tabert [64], to built a table-based q&a system, will also improve the system's performance. although such a system doesn't exist yet, we believe it is a promising direction, which will be our next plan.accuracy issue. although showing the relevant context is helpful for the answer interpretation, when mistake happens, the irrelevant charts could also be a distraction, which will affect users' judgments. we believe there are two methods that could be used to improve the accuracy of the system. first, we can employ knowledge bases such as wolfra-malpha 5 and knowledge graphs to guide the searching directions so that the answers could be more directly found without checking too many irrelevant candidates in the space. second, again, training a qa system based on tabert [64] could also help improve the accuracy.generalization issue. our training corpus is generated based on 26 tabular data that primarily contain marketing data records such as car sales values, and best selling books. as a result, our model could better handle high-level questions in the marking domain, but may have a lower question decomposition quality when facing a question from other domains. to overcome the issue, more datasets in various domains should be collected and more questions should be prepared to train the model and improve the generalization of the system.", "Solved": NaN, "Unsolved": NaN}, "213474484": {"num": 464, "type": 1, "title": "Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks", "pdfurl": "https://ojs.aaai.org/index.php/AAAI/article/download/6398/6254", "conclusion": "we presented quail 5 , the first multi-domain text comprehension challenge that is balanced and annotated for 9 types of verbal reasoning. quail aims to show the extent to which current models can generalize over different domains and reasoning strategies and handle questions that can be answered with the information in a given text, unanswerable questions and questions that require extra world knowledge. we hope that quail will stimulate efforts to develop generalist systems tackling different kinds of verbal reasoning, and that it will be useful in diagnostics and qualitative analysis for new qa systems.", "Solved": NaN, "Unsolved": NaN}, "240288953": {"num": 465, "type": 1, "title": "Answering Open-Domain Questions of Varying Reasoning Steps from Text", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.292.pdf", "conclusion": "in this paper, we presented iterative retriever, reader, and reranker (irrr), a system that uses a single model to perform subtasks to answer open-domain questions of arbitrary reasoning steps. irrr achieves competitive results on standard opendomain qa benchmarks, and establishes a strong baseline on b qa, the new unified benchmark we present, which features questions with mixed levels of complexity. ", "Solved": NaN, "Unsolved": NaN}, "230433978": {"num": 466, "type": 1, "title": "Few-Shot Question Answering by Pretraining Span Selection", "pdfurl": "https://www.aclanthology.org/2021.acl-long.239.pdf", "conclusion": "we explore the few-shot setting of extractive question answering, and demonstrate that existing methods, based on fine-tuning large pretrained language models, fail in this setup. we propose a new pretraining scheme and architecture for span selection that lead to dramatic improvements, reaching surprisingly good results even when only an order of a hundred examples are available. our work shows that choices that are often deemed unimportant when enough data is available, again become crucial in the few-shot setting, opening the door to new methods that take advantage of prior knowledge on the downstream task during model development. ", "Solved": NaN, "Unsolved": NaN}, "249017531": {"num": 467, "type": 1, "title": "From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering", "pdfurl": "https://arxiv.org/pdf/2205.11729v1.pdf", "conclusion": "we propose fe2h, a simple yet effective framework for multi-hop qa that divides both the document selection and question answering into two stages following an easy-to-hard manner. experimental results demonstrate that since we cannot feed all of the candidate documents to the plms at a time due to the input length limitation, taking the multi-hop reasoning nature into consideration at the document selection phase significantly improves the overall performance. as for the subsequent qa phase, thanks to the great natural language understanding ability of the plms, the performance of our simple two-stage reader is better than the stateof-the-art approaches without any graph structure and explicit reasoning chains. we hope this work could facilitate more simple yet powerful multihop qa approaches with the help of the advanced plms.", "Solved": NaN, "Unsolved": NaN}, "235125947": {"num": 468, "type": 1, "title": "Fact-driven Logical Reasoning", "pdfurl": "https://arxiv.org/pdf/2105.10334v1.pdf", "conclusion": "in this work, we propose a novel method named focal reasoner for logical reasoning in the machine reading comprehension task. our method not only better uncovers the logical structures within the context, which can be a general method for other sophisticated reasoning tasks but also better captures the logical interactions between context and options. the experimental results verify the effectiveness of our method. in the future, we intend to design more elaborate mechanisms to cope with different question types and logical types as well as combine the symbolic and neural approaches.", "Solved": NaN, "Unsolved": NaN}, "256662717": {"num": 469, "type": 1, "title": "Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories", "pdfurl": "https://export.arxiv.org/pdf/2302.03754v1.pdf", "conclusion": "in this paper we propose a new plug-in mixture-ofmemory mechanism for the retrieval augmented language models to improve their zero-shot ability on the dense retrieval task. to learn the memory mixture we develop a new joint learning approach that trains the augmentation component using the positive signals from the end task, the language model's attention scores, and  wood jr. (october 10, december 10, 1978) was an american filmmaker, actor, writer, producer, and director. hard negatives retrieved from the mixture of augmentation corpora. this leads to our final model moma (t5-ance) and moma (coco) that achieve strong zero-shot accuracy on 18 retrieval tasks included in beir. our analysis shows the importance of augmenting with diverse memory sources and in-domain information for robust generalization. we also share our observations and insights on how the model learns to leverage the augmentation information from multiple corpora during training and testing. we hope our findings and illustrations can inspire more future research in better augmenting language models, to provide other alternatives to achieve generalization ability beyond solely relying on model scale.", "Solved": NaN, "Unsolved": NaN}, "220302524": {"num": 470, "type": 1, "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval", "pdfurl": "https://arxiv.org/pdf/2007.00808v1.pdf", "conclusion": "ance fundamentally eliminates the discrepancy between the representation learning of texts and their usages in dense retrieval. our ance trained dense retrieval model, the vanilla bert-siamese, convincingly outperforms all dense retrieval and sparse retrieval baselines in our large scale document retrieval and passage retrieval experiments. it nearly matches the ranking accuracy of the state-of-theart cascade sparse retrieval and bert reranking pipeline. more importantly, all these advantages are achieved with a standard transformer encoder at a 1% online inference latency, using a simple dot-product in the ance-learned representation space.", "Solved": NaN, "Unsolved": NaN}, "226262208": {"num": 471, "type": 1, "title": "IIRC: A Dataset of Incomplete Information Reading Comprehension Questions", "pdfurl": "https://arxiv.org/pdf/2011.07127v1.pdf", "conclusion": "we introduced iirc, a new dataset of incompleteinformation reading comprehension questions. these questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways. our baseline model, built on top of state-ofthe-art models for the most closely related existing datasets, performs quite poorly in this setting, even when given oracle retrieval results, and especially when combined with other reading comprehension datasets. iirc both provides a promising new avenue for studying complex reading and retrieval problems and demonstrates that much more research is needed in this area.", "Solved": NaN, "Unsolved": NaN}, "72940739": {"num": 472, "type": 1, "title": "Fast Prototyping a Dialogue Comprehension System for Nurse-Patient Conversations on Symptom Monitoring", "pdfurl": "https://arxiv.org/pdf/1903.03530v2.pdf", "conclusion": "we formulated a dialogue comprehension task motivated by the need in telehealth settings to extract key clinical information from spoken conver-sations between nurses and patients. we analyzed linguistic characteristics of real-world humanhuman symptom checking dialogues, constructed a simulated dataset based on linguistically inspired and clinically validated templates, and prototyped a qa system. the model works effectively on a simulated test set using symptoms excluded during training and on real-world conversations between nurses and patients. we are currently improving the model's dialogue comprehension capability in complex reasoning and context understanding and also applying the qa model to summarization and virtual nurse applications.", "Solved": NaN, "Unsolved": NaN}, "248022214": {"num": 473, "type": 1, "title": "QAGAN: Adversarial Approach To Learning Domain Invariant Language Features", "pdfurl": "https://arxiv.org/pdf/2206.12388v1.pdf", "conclusion": "we presented a method for training a question-answering language model in an adversarial fashion and showed through various experiments that it helps the model generalize well to out-of-domain dataset.", "Solved": NaN, "Unsolved": NaN}, "253080775": {"num": 474, "type": 1, "title": "WIKIWHY: ANSWERING AND EXPLAINING CAUSE-AND-EFFECT QUESTIONS", "pdfurl": "https://export.arxiv.org/pdf/2210.12152v2.pdf", "conclusion": "with this paper, we release wikiwhy, a question-answering dataset enabling the analysis and improvement of llms' reasoning capability. we propose explanation between grounded cause-effect pairs to distinguish memorization of the relation from a genuine understanding of the underlying mechanics. compared to related works on explainable qa, our explanation format finds a natural middle ground that balances complexity and depth, allowing our crowdsourcing methods to produce thought-provoking examples while being highly scalable. we exploit this scalability to cover topics previously overlooked by other explanation datasets and demonstrate our proposed task to be difficult with strong baselines (our experiments feature models failing to produce satisfying explanations even under ideal conditions). finally, we motivate the development of new automatic metrics that are better able to handle the complexities of generated reasoning.", "Solved": NaN, "Unsolved": NaN}, "259212487": {"num": 475, "type": 1, "title": "CompMix: A Benchmark for Heterogeneous Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2306.12235v3.pdf", "conclusion": "we release compmix, a benchmark for heterogeneous qa that inherently requires the usage of multiple sources. answering questions in compmix requires systems to work consistently well for intents spread across five domains, and deal with a wide variety of challenging human formulations asking about rare entities. thus, our hope is that this resource can help facilitate progress in developing more robust qa models that can appropriately exploit complementary and potentially redundant sources of information. a promising direction for improvement would be to include questions that need answers of a different flavor of heterogeneity: sentences, passages, or longer lists.", "Solved": NaN, "Unsolved": NaN}, "264406215": {"num": 476, "type": 1, "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2310.13552v2.pdf", "conclusion": "in this work, we harness the capabilities of llms combined with self-prompted cots to tackle the intricate mhqa task within the open-domain context, termed as odmr.our innovative sp-cot not only sets a new benchmark by surpassing preceding cot prompting techniques but also outclasses the erstwhile sota llm-only methodologies in open-domain question-answering.a distinguishing feature of sp-cot is its proficiency in eliciting high-caliber intermediate reasoning steps, and its universal efficacy across both large and smallscale llms.we anticipate our innovative selfgeneration pipeline for odmr to not just be foundational for sp-cot, but also to pave the way for future research, catalyzing a shift towards leveraging self-generation in llms, by llms, and for llms.", "Solved": NaN, "Unsolved": NaN}, "226254024": {"num": 477, "type": 1, "title": "Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources", "pdfurl": "https://www.aclweb.org/anthology/2020.coling-main.232.pdf", "conclusion": "in this paper, we deal with the multi-choice question answering task which requires background knowledge or commonsense. we propose a novel question-answering method by exploring how to efficiently integrate multiple knowledge sources, i.e. conceptnet, wikipedia and the cambridge dictionary. firstly, we propose a novel graph-based iterative knowledge retrieval module to iteratively retrieve concepts and entities related to a given question and its choices. in addition, we propose an answer choice-aware attention mechanism to fuse all hidden representations encoded by a pre-trained language model. we conducted experiments on the commonsenseqa dataset and the experimental results show that our method significantly outperforms other competitive methods in accuracy. further ablation studies show the effectiveness of graph-based iterative knowledge retrieval module and answer choice-aware attention module in retrieving and synthesizing background knowledge from multiple knowledge sources. in the future, we will extend our method to deal with the open-domain question answering tasks that require the external background knowledge.", "Solved": NaN, "Unsolved": NaN}, "263830734": {"num": 478, "type": 1, "title": "RECOMP: IMPROVING RETRIEVAL-AUGMENTED LMS WITH COMPRESSION AND SELECTIVE AUGMENTATION", "pdfurl": "https://export.arxiv.org/pdf/2310.04408v1.pdf", "conclusion": "we introduce recomp, a method which compresses retrieved documents into textual summaries before prepending them to improve in-context retrieval augmented language models.we present two compression models -an extractive compressor and an abstractive compressor.we design a training scheme which leverages end task signals from a blackbox lm to generate useful summaries and allowing the compression models to perform selective augmentation.our experiments show that our compressors can improve the efficiency of retrieval augmented lms significantly with minimal drop in performances.", "Solved": NaN, "Unsolved": NaN}, "235293903": {"num": 479, "type": 1, "title": "Why Machine Reading Comprehension Models Learn Shortcuts?", "pdfurl": "https://arxiv.org/pdf/2106.01024v1.pdf", "conclusion": "in this work, we try to answer why many mrc models learn shortcut tricks while ignoring the pre-designed comprehension challenges that are purposely embedded in many benchmark datasets. we argue that large proportions of shortcut questions in training data push mrc models to rely on shortcut tricks excessively. to properly investigate, we first design two synthetic datasets where each instance has a shortcut version paired with a challenging one which requires paraphrasing, a complex reasoning skill, to answer, rather than performing question word matching or simple matching. with these datasets, we are able to adjust the proportion of shortcut questions in both training and testing, while maintaining other factors relatively steady. we propose two methods to examine the model training process regarding the shortcut questions, which enable us to take a closer look at the learning mechanisms of bidaf and bert under different training settings. we find that learning shortcut questions generally requires less computational resources, and mrc models usually learn the shortcut questions at their early stage of training. our findings reveal that, with larger proportions of shortcut questions for training, mrc models will learn the shortcut tricks quickly while ignoring the designed comprehension challenges, since the remaining truly challenging questions, usually limited in size, may not motivate models to explore sophisticated solutions in the later training stage. ", "Solved": NaN, "Unsolved": NaN}, "234093776": {"num": 480, "type": 1, "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.365.pdf", "conclusion": "we presented qasper, an information-seeking qa dataset over nlp research papers. with natural questions asked as follow-up to titles and abstracts, the task presented by qasper requires evidence from multiple paragraphs and/or figures and tables within the full text of the papers. our empirical quire workers to write questions grounded in those snippets.  table 5: error analysis of our best model (led from row 5 from table 2) on 55 test examples with low f 1 score (excluding those with \"yes,\" \"no,\" or \"unanswerable\" gold answers). \"quotations\" denote extractive gold answers. we note lacks domain knowledge errors are not always solved by better entity type resolution (see \u2020).", "Solved": NaN, "Unsolved": NaN}, "244346065": {"num": 481, "type": 1, "title": "The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2111.09852v3.pdf", "conclusion": "benefiting from pre-trained bert models, the field of information retrieval has seen remarkable progress in neural ir models, as exemplified by the success of vanilla bert which has become a strong, yet simple, baseline for neural ir models. to overcome the limitations of bert-based models regarding long documents, we have proposed to divide documents into blocks and to select only the most important key blocks. this is reminiscent of the way humans assess the relevance of a document for a given query: one first identifies blocks relevant to the query, blocks which are then aggregated to obtain the overall assessment of the document. in order to select blocks, we have investigated two approaches: the first one is straightforward and makes use of standard retrieval functions as tf-idf or bm25; the second one learns a single bert model used for both ranking blocks and documents. both approaches have been shown to improve over standard baselines and previous bert-based models. we have followed the same approach on another highly competitive neural ir model, namely parade, here again with improved results. all in all, selecting blocks is advantageous for the two models studied here, vanilla bert and parade. we conjecture that this selection is a way to remove passages in documents which are not relevant to the query and which are likely to bring noise when matching queries and documents.", "Solved": NaN, "Unsolved": NaN}, "204915921": {"num": 482, "type": 1, "title": "QASC: A Dataset for Question Answering via Sentence Composition", "pdfurl": "https://ojs.aaai.org/index.php/AAAI/article/download/6319/6175", "conclusion": "we present qasc, the first qa dataset for multi-hop reasoning beyond a single paragraph where two facts needed to answer a question are annotated for training, but questions cannot be easily syntactically decomposed into these facts. instead, models must learn to retrieve and compose candidate pieces of knowledge. qasc is generated via a crowdsourcing process, and further enhanced via multi-adversary distractor choice selection. state-of-the-art bert models, even with massive fine-tuning on over 100k questions from previous relevant datasets and using our proposed two-step retrieval, leave a large margin to human performance levels, thus making qasc a new challenge for the community.", "Solved": NaN, "Unsolved": NaN}, "258686160": {"num": 483, "type": 1, "title": "Parallel Context Windows for Large Language Models", "pdfurl": "https://www.aclanthology.org/2023.acl-long.352.pdf", "conclusion": "in recent years, a multitude of successful approaches have been proposed for allowing transformer-based language models to leverage large amounts of text during inference, leading to a variety of dedicated architectures. in parallel, however, the mainstream llm production line of new models with \"regular\"-up to several thousand tokens-context window sizes enjoys faster progress in the form of scaling, innovation, and data updating. this paper introduced parallel context windows (pcw): a simple approach for allowing any offthe-shelf llm to broaden the scope of text it can access during inference. we showed the effectiveness of pcw in the framework of in-context learning, where access to a context that is larger by a factor of b implies learning from b times more training examples. our results show that pcw is more effective than the vanilla single context window approach for in-context learning over a broad set of multi-class classification tasks, suggesting that pcw could improve in-context learning in tasks with diverse input or output spaces. we also showed promising signals for applying pcw for multiple retrieved document reading.", "Solved": NaN, "Unsolved": NaN}, "248366293": {"num": 484, "type": 1, "title": "Association for Computational Linguistics", "pdfurl": "https://arxiv.org/pdf/2204.10628v1.pdf", "conclusion": "in this paper we present seal, a novel retrieval system that combines an autoregressive language model with a compressed full-text substring index. such combination allows to constraint the generation of existing ngrams in a corpus and to jointly retrieve all the documents containing them. empirically, we show an improvement of more than 10 points in average passage-level r-precision on kilt, and establish new state-of-the-art downstream performance on 4 out 7 datasets when paired with a reader model. while our results show that seal could already compete with more established retrieval systems, we believe there is potential in exploring the use of existing (or yet to come) larger autoregressive models.   ", "Solved": NaN, "Unsolved": NaN}, "218516694": {"num": 485, "type": 1, "title": "MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models", "pdfurl": "https://www.aclweb.org/anthology/2021.adaptnlp-1.10.pdf", "conclusion": "in this paper, we convert eight existing qa tasks from the mrqa shared task (fisch et al., 2019b) into sentence-level retrieval tasks, by treating the sentence containing the ground-truth span as the target sentence-level answer. in additional to a new evaluation suite for sentence level retrieval, we provide strong baselines using unsupervised term-based information retrieval methods (bm25), and three neural models, off-the-self use-qa, finetuned use-qa, and bert dual encoders.", "Solved": NaN, "Unsolved": NaN}, "226262229": {"num": 486, "type": 1, "title": "Coarse-to-Fine Query Focused Multi-Document Summarization", "pdfurl": "https://www.aclweb.org/anthology/2020.emnlp-main.296.pdf", "conclusion": "in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization. we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments. experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more  gpus with 11gb memory. for the answer sentence selection model, bert was fine-tuned with a learning rate of 3 \u00d7 10 \u22126 and a batch size of 16 for 3 epochs . for span selection, we adopted a learning rate of 3 \u00d7 10 \u22125 and a batch size of 64 for 5 epochs. during inference, the confidence threshold for the relevance estimator was set to \u03b8 = 0.75 (kratzwald and feuerriegel, 2018) for both sentence and passage retrieval. for the evidence estimator, k qa was tuned on the development set. we obtained 90 and 110 evidence sentences from the sentence selection and span selection models, respectively. for the centrality estimator, the influence of the query was set to \u03c6 = 0.15 (wan, 2008;wan and zhang, 2014 ", "Solved": NaN, "Unsolved": NaN}, "235678938": {"num": 487, "type": 1, "title": "Controllable Open-ended Question Generation with A New Question Type Ontology", "pdfurl": "https://www.aclanthology.org/2021.acl-long.502.pdf", "conclusion": "we present a new question type ontology which better captures the nuances of questions to support the study of open-ended question generation. we further annotate a new dataset with 4,959 questions based on the proposed ontology. we describe a joint question focus detection and question generation framework with a novel semantic graphaugmented representation, which is directly built on large pre-trained models. based on this framework, we also enhance the controllability and diversity of generated questions by employing template exemplars or automatically generated templates. experiments on two large datasets show that questions generated by our models have better quality and higher diversity than non-trivial comparisons, with similar results rated by human judges.", "Solved": NaN, "Unsolved": NaN}, "221340891": {"num": 488, "type": 1, "title": "RELATION/ENTITY-CENTRIC READING COMPREHENSION", "pdfurl": "https://arxiv.org/pdf/2008.11940v1.pdf", "conclusion": "we presented a large-scale person-centered cloze dataset. the dataset is not anonymized, and each passage is a raw text which is not only natural but also easier to be pre-processed by syntactic and semantic parsers. in the dataset construction, we used baseline suppression,", "Solved": NaN, "Unsolved": NaN}, "263828698": {"num": 489, "type": 1, "title": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning", "pdfurl": "https://export.arxiv.org/pdf/2310.05450v1.pdf", "conclusion": "this paper provides a quantified analysis on the multi-nested boolean logic.we flag the deficiency in the state-of-the-art language models in terms of such basic capability, which will inevitably cause pitfalls in dealing with more complex reasoning tasks.for this, we propose curriculum logical reasoning, a new self-supervised learning method to empower language models with foundational logical capability.we also show that our idea can act as a cornerstone learning method for general logical reasoning.", "Solved": NaN, "Unsolved": NaN}, "222125277": {"num": 490, "type": 1, "title": "Under review AUTOREGRESSIVE ENTITY RETRIEVAL", "pdfurl": "https://arxiv.org/pdf/2010.00904v1.pdf", "conclusion": "in this work, we propose genre, a novel paradigm to addresses entity retrieval: generate entity names autoregressively. entity names have several properties that might help (even humans) retrieving them, including a compositional structure and a predictable interaction with the context. the autoregressive formulation allows us to directly capture some of these properties, leading to several advantages with respect to current solutions, including an efficient way to cross encode men-tion context and entity candidates, a much smaller memory footprint, and the ability to compute an exact softmax without the need to subsample negative data. we empirically show that these characteristics, combined with constrained decoding strategies, led to state-of-the-art performance on a plethora of entity retrieval datasets, spanning entity disambiguation, end-to-end entity linking, and page-level document retrieval, while resulting in systems with a remarkably contained memory footprint, a space reduction by a factor of twenty on average. we additionally demonstrate that new entities can be effectively considered in our system by simply appending their unambiguous name to the candidate set.", "Solved": NaN, "Unsolved": NaN}, "261276895": {"num": 491, "type": 1, "title": "MEMORY-VQ: Compression for Tractable Internet-Scale Memory", "pdfurl": "https://export.arxiv.org/pdf/2308.14903v1.pdf", "conclusion": "we introduced memory-vq, a novel approach for reducing the storage requirements of memoryaugmented language models without compromising performance. by employing vq-vae to compress token representations, we obtain a lumen model with 16x compression, denoted as lumen-vq. remarkably, lumen-vq maintains performance close to lumen and fid and benefits from lumen inference speed-ups with sharply reduced storage cost. using memory-vq, memory augmentation is a practical solution for drastic inference speedups with extensive retrieval corpora. fine-tuning during fine-tuning, we utilize the adafactor optimizer (shazeer and stern, 2018) with a constant learning rate of 0.0001, a batch size of 128, and a dropout rate of 0.1 for all tasks. when performing multi-task training, we uniformly sample from the tasks. we allocate 48 and 304 tokens for question and passage inputs, respectively. lumen-vq is using 0.999 as an ema factor for code updates.", "Solved": NaN, "Unsolved": NaN}, "257834209": {"num": 492, "type": 1, "title": "SEMI-PARAMETRIC INDUCING POINT NETWORKS AND NEURAL PROCESSES", "pdfurl": "https://export.arxiv.org/pdf/2205.11718v2.pdf", "conclusion": "in this paper, we introduce a domain-agnostic general-purpose architecture, the semi-parametric inducing point network (spin) and use it as the basis for induced point neural process (ipnps). unlike previous semi-parametric approaches whose computational cost grows quadratically with the size of the dataset, our approach scales linearly in the size and dimensionality of the data by leveraging a cross attention mechanism between datapoints and induced latents. this allows our method to scale to large datasets and enables meta learning with large contexts. we present empirical results on 10 uci datasets, a gaussian process meta learning task, and a real-world important task in genomics, genotype imputation, and show that our method can achieve competitive, if not better, performance relative to state-of-the-art methods at a fraction of the computational cost.", "Solved": NaN, "Unsolved": NaN}, "258587884": {"num": 493, "type": 1, "title": "Automatic Evaluation of Attribution by Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2305.06311v2.pdf", "conclusion": "in this paper, we investigate the important problem of automatically evaluating attribution given by llms.we begin by defining different types of attribution errors and then explore two approaches for automatic evaluation: prompting llms and fine-tuning smaller lms.we experiment with both simulated test examples and manually curated test examples from a real-life generative search engine.", "Solved": NaN, "Unsolved": NaN}, "251718892": {"num": 494, "type": 1, "title": "Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question Answering", "pdfurl": "https://www.aclanthology.org/2022.coling-1.142.pdf", "conclusion": "in this paper, we study the task of multi-hop question answering and propose to stepwise locate the single-hop supporting sentences and generate more fact-grounded single-hop questions for better interpretable multi-hop reasoning. we present a stepwise reasoning framework to incorporate both single-hop supporting sentence identification and the corresponding single-hop question generation for each intermediate step until inferring a final result. it employs a pre-trained simple question generator and takes the identified single-hop supporting sentences as base to generate the single-hop question, which obviates the necessity of constructed supervision and helps generate more fact-based single-hop questions. it utilizes a unified reader to jointly learn both intermediate hop reasoning and final hop inference for better fault tolerance. experimental results validate the general effectiveness and interpretability of our stepreasoner.", "Solved": NaN, "Unsolved": NaN}, "237552879": {"num": 495, "type": 1, "title": "Evaluation Paradigms in Question Answering", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.758.pdf", "conclusion": "we identify two core motivations for qa research over the past twenty years. we link one to the usercentered goals of the cranfield paradigm and propose the manchester paradigm to describe research working towards building human-like, intelligent qa systems. in at least the short-term, this distinction is important as it illuminates the goals of industry and academic stakeholders; ultimately, this makes it easier to ensure that both research agendas are valued. in the long term, we suspect that the best qa agents will benefit from the insights of user-oriented tasks and the longer-range efforts towards natural language understanding (bender and koller, 2020; linzen, 2020). ", "Solved": NaN, "Unsolved": NaN}, "244800755": {"num": 496, "type": 1, "title": "Case-Based Abductive Natural Language Inference", "pdfurl": "https://www.aclanthology.org/2022.coling-1.134.pdf", "conclusion": "the adopted model of explanatory power relies on the availability of human-annotated explanations with specific features (e.g., explanatory facts reused across different training instances). however, these resources might not be available in real-world scenarios and are generally costly to develop. moreover, since the explanatory power model relies on similarity measures and indicator functions, the model's ability to generalise might be sensitive to the incompleteness of the knowledge bases and the availability of representative explanations. we believe these limitations can be potentially alleviated by exploring the role of more abstract sentence representations within the cbr paradigm (bergmann and wilke, 1996).in the current implementation of cb-anli, the refine phase adopts specific assumptions to model the abstraction process required for explanation generation. this process, in fact, is performed by assuming that abstraction at the concept level translates in a correct mapping between hypotheses and central explanatory sentences. however, contextual linguistic elements can still affect the overall meaning of the specific concept being abstracted, inducing the inclusion of spurious links between sentences. while contextual elements are considered during the precedent phases through the use of contextualised embeddings and similar cases, additional work is still required to guarantee the correctness of the abstraction process.", "Solved": NaN, "Unsolved": NaN}, "199552247": {"num": 497, "type": 1, "title": "Playing log(N)-Questions over Sentences", "pdfurl": "https://arxiv.org/pdf/1908.04660v1.pdf", "conclusion": "we have proposed the game of log(n )-questions over sentences, and introduced an end-to-end system of 2 agents that are able to play the game. while our results show promise, there is work to be done on improving game and sw prediction performance simultaneously, as well as playing the game over larger sentence sets. more generally, we shows agents exhibiting reasoning and information-seeking in a text environment.", "Solved": NaN, "Unsolved": NaN}, "219559168": {"num": 498, "type": 1, "title": "Knowledge-Aided Open-Domain Question Answering", "pdfurl": "https://arxiv.org/pdf/2006.05244v1.pdf", "conclusion": "this paper investigates how the performance of open-domain question answering can be improved through enhancing document retrieval and answer reranking. the central idea is to consider both question-document and document-document relationships in the document retriever and the answer reranker. more specifically, with the aid of external knowledge resources, we first construct question-document graphs and document-document graphs using knowledge triples, and then encode such relational knowledge in the document retrieval and answer ranking components. we evaluated our model on several open-domain question answering datasets including squad-open, quasar-t and triviaqaunfiltered. we observed that our method can boost the overall performance of open-domain question answering consistently on these datasets. extensive experiments show that modeling the questiondocument and document-document relationships can contribute to the improvement consistently.", "Solved": NaN, "Unsolved": NaN}, "226262276": {"num": 499, "type": 1, "title": "Scene Restoring for Narrative Machine Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/2020.emnlp-main.247.pdf", "conclusion": "in this paper, we focus on narrative machine reading comprehension. inspired by human behaviors, we propose a novel method to restore the scene for the narrative passage. specifically, we introduce the event knowledge from atomic and build a three-dimensional graph to describe the scene. to encode the scene graph, we propose graph dimensional-iteration network (gdin). we conduct experiments on two relevant datasets, rocstories and cosmosqa. the result shows our method achieves state-of-the-art. further experimental investigation shows that (1) compared with concept knowledge, the event knowledge we choose is more suitable for narrative mrc; (2) our proposed graph models the scene more effectively than the unstructured text and the unified plane graph do; (3) our proposed gdin encodes the scene graph efficiently by iterating multiple steps.", "Solved": NaN, "Unsolved": NaN}, "212657414": {"num": 500, "type": 1, "title": "TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages", "pdfurl": "https://arxiv.org/pdf/2003.05002v1.pdf", "conclusion": "confidently making progress on multilingual models requires challenging, trustworthy evaluations. we have argued that question answering is well-suited for this purpose and that by targeting a typologically diverse set of languages, progress on the tydi qa dataset is more likely to generalize on the breadth of linguistic phenomena found throughout the world's languages. by avoiding data collection procedures reliant on translation and multilingual modeling, we greatly mitigate the risk of sampling bias. we look forward to the many ways the research community finds to improve the quality of multilingual models. 25 because we believe mt may be a fruitful research direction for tydi qa, we do not release any automatic translations. in the past, this seems to have stymied innovation around translation as applied to multilingual datasets. 26 we will happily share our annotation protocol on request.", "Solved": NaN, "Unsolved": NaN}, "202573071": {"num": 501, "type": 1, "title": "CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION", "pdfurl": "https://arxiv.org/pdf/1909.05858v2.pdf", "conclusion": "with 1.63 billion parameters, ctrl is the largest publicly released language model to date. it is trained with control codes so that text generation can be more easily controlled by human users. these codes allow users to explicitly specify domain, subdomain, entities, relationships between entities, dates, and task-specific behavior. we hope that the release of this model at https://github.com/salesforce/ctrl pushes towards more controllable, general models for natural language processing, and we encourage future discussion about artificial generation with our team by emailing ctrl-monitoring@salesforce.com.   table 3). for all the reddit data, the secondary code can be title: or text:, which is the title and text of the article, respectively.", "Solved": NaN, "Unsolved": NaN}, "258587920": {"num": 502, "type": 1, "title": "Evaluating Embedding APIs for Information Retrieval", "pdfurl": "https://www.aclanthology.org/2023.acl-industry.50.pdf", "conclusion": "the incredible capabilities of transformer-based language models at scale have attracted a handful of companies to offer access to their proprietary llms via apis. in this paper, we aim to qualitatively and quantitatively examine semantic embedding apis that can be used for information retrieval. our primary focus is to assess existing apis for domain generalization and multilingual retrieval. our findings suggest that re-ranking bm25 results is a suitable and cost-effective option for english; on the beir benchmark, openai ada2 performs the best on average. in multilingual settings, while re-ranking remains a viable technique, a hybrid approach produces the most favorable results. we hope that our insights aid practitioners and researchers in selecting appropriate apis based on their needs in this rapidly growing market.", "Solved": NaN, "Unsolved": NaN}, "211258645": {"num": 503, "type": 1, "title": "Unsupervised Question Decomposition for Question Answering", "pdfurl": "https://arxiv.org/pdf/2002.09758v1.pdf", "conclusion": "we proposed an algorithm that decomposes questions without supervision, using 3 stages: (1) learning to decompose using pseudo-decompositions without supervision, (2) answering sub-questions with an off-the-shelf qa system, and (3) answering hard questions more accurately using sub-questions and their answers as additional input. when evaluated on hotpotqa, a standard benchmark for multihop qa, our approach significantly improved accuracy over an equivalent model that did not use decompositions. our approach relies only on the final answer as supervision but works as effectively as state-of-the-art methods that rely on strong supervision, such as supporting fact labels or example decompositions. qualitatively, we found that unsupervised decomposition resulted in fluent sub-questions whose answers often match the annotated supporting facts in hotpotqa. our unsupervised decompositions are largely extractive, which is effective for compositional, multi-hop questions but not all complex questions, showing room for future work. overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems.", "Solved": NaN, "Unsolved": NaN}, "252734741": {"num": 504, "type": 1, "title": "Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt", "pdfurl": "https://export.arxiv.org/pdf/2210.03029v4.pdf", "conclusion": "in this paper, we introduce rospr, a method that efficiently enhances zero-shot generalization capabilities of a meta-trained lm by retrieving promptspecific source prompt embeddings (soft prompts) for a given target task.we accomplish this by first training the soft prompts for hard prompt of the source tasks.after training source prompt embeddings, we construct the source prompt library by storing the mean representation of training instances as keys and the corresponding prompt embeddings as values.at inference, we search for training instances stored in the library similar to sample instances from the target task, retrieve the corresponding prompt embedding, select the most frequently retrieved embedding, and append it to each of the target task instances for prediction.our results show that rospr efficiently enhances the zero-shot performance of the backbone model while introducing minimal additional parameters during inference.we additionally provide analysis of which factors attribute to the performance of rospr and find that heuristic cues such as the answer choice format are critical for generalization performance, implying that it may play a role similar to demonstrations in in-context learning.", "Solved": NaN, "Unsolved": NaN}, "248157463": {"num": 505, "type": 1, "title": "ASQA: Factoid Questions Meet Long-Form Answers", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.566.pdf", "conclusion": "in contrast to existing datasets for long-form qa, asqa admits a clear notion of correctness that we use to define an overall metric of performance (dr). our empirical evaluations demonstrate that dr correlates well with the human judgment; and there is a large gap between human performance and the strong baselines. thus, we believe that asqa is an appealing task for the qa community. our analysis suggests that strong performance on asqa is contingent upon both high-quality retrieval and summarization. these aspects constitute important directions for future work on asqa.", "Solved": NaN, "Unsolved": NaN}, "211258652": {"num": 506, "type": 1, "title": "Training Question Answering Models From Synthetic Data", "pdfurl": "https://arxiv.org/pdf/2002.09599v1.pdf", "conclusion": "we build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation. with our best models, we generate large question answering datasets from unlabeled wikipedia documents and finetune a 345 million parameter bert-style model achieving 88.4 em score. finetuning the resulting model on real squad1.1 data further boosts the em score to 89.4. this amounts to a 1.7 point improvement over our fully supervised baseline. finally, we generate synthetic text from a wikipedia-finetuned gpt-2 model, generate answer candidates and synthetic questions based on those answers, and then train a bert-large model to achieve similar question answering accuracy without directly using any real data at all. doing so required us to scale model size for our answer generators, question generators, and filtration models. we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.", "Solved": NaN, "Unsolved": NaN}, "220831004": {"num": 507, "type": 1, "title": "Big Bird: Transformers for Longer Sequences", "pdfurl": "https://arxiv.org/pdf/2007.14062v1.pdf", "conclusion": "we demonstrate a natural task which can be solved by the full attention mechanism in o(1)layers. however, under standard complexity theoretic assumptions, this problem requires \u03c9(n)-layers for any sparse attention layers with\u00f5(n) edges (not just bigbird). (here\u00f5 hides poly-logarthmic factors.)consider the simple problem of finding the corresponding furthest vector for each vector in the given sequence of length n. formally, task 1. given n unit vectors {u 1 , . . . , u n }, find f (u 1 , . . . , u n ) \u2192 (u 1 * , . . . , u n * ) where for a fixed j \u2208 [n], we define j * = arg max k u k \u2212 u j 2 2 . finding vectors that are furthest apart boils down to minimize inner product search in case of unit vectors. for a full-attention mechanism with appropriate query and keys, this task is very easy as we can evaluate all pair-wise inner products.the impossibility for sparse-attention follows from hardness results stemming from orthogonal vector conjecture(ovc) [1,2,7,97]. the ovc is a widely used assumption in fine-grained complexity. informally, it states that one cannot determine if the minimum inner product among n boolean vectors is 0 in subquadratic time. in app. c, we show a reduction using ovc to show that if a transformer g \u2208 t h=1,m=2d,q=0 d for any sparse directed graph d can evaluate the task 1, it can solve the orthogonal vector problem. proposition 1. there exists a single layer full self-attention g \u2208 t h=1,m=2d,q=0 that can evaluate task 1, i.e. g(u 1 , ..., u n ) = [u 1 * , . . . , u n * ], but for any sparse-attention graph d with o(n) edges (i.e. inner product evaluations), would require\u03c9(n 1\u2212o(1) ) layers.we give a formal proof of this fact in app. c.", "Solved": NaN, "Unsolved": NaN}, "210859295": {"num": 508, "type": 1, "title": "MANYMODALQA: Modality Disambiguation and QA over Diverse Inputs", "pdfurl": "https://arxiv.org/pdf/2001.08034v1.pdf", "conclusion": "we presented a new challenge to the community, hoping to promote research in manymodal qa. we structured our challenge in a way that encourages research in other, more general areas, such as transfer learning and end-to-end modality disambiguation + multimodal qa. we hope that this challenge will serve as a test bed for further work and that our model will inspire directions of subsequent research. we plan to continue our work by collecting data that exhibits a stronger form of multimodality, where the question can only be answered after combining multiple modalities, and by adding new modalities, such as video and audio.", "Solved": NaN, "Unsolved": NaN}, "264426784": {"num": 509, "type": 1, "title": "How Much Consistency Is Your Accuracy Worth?", "pdfurl": "https://www.aclanthology.org/2023.blackboxnlp-1.19.pdf", "conclusion": "we introduce relative consistency, which complements standard contrast consistency by allowing an accuracy and consistency score pair to be examined to determine whether a higher consistency was possible with that accuracy.this facilitates the comparison of consistencies achieved by models that achieved different levels of accuracy.we show that relative consistency enriches conclusions we make about whether a model is more consistent than another, and occasionally even leads us to different takeaways.", "Solved": NaN, "Unsolved": NaN}, "247158054": {"num": 510, "type": 1, "title": "'Tis but Thy Name: Semantic Question Answering Evaluation with 11M Names for 1M Entities", "pdfurl": "https://arxiv.org/pdf/2202.13581v1.pdf", "conclusion": "we introduce wes, an 11m example semantic entity similarity dataset for training question answering evaluation models. wes is generated by treating wikipedia link texts and target article titles as synonyms then filtering for quality. wes is targeted to question answering evaluation, independent of human annotators, and consistent with human judgment. we hope that future questionanswering datasets will implement semantic evaluation metrics in their leaderboards to encourage the development of more free-form models. in future works, link-mining similarity datasets like wes can be made more challenging by generating negative examples adversarially as described at the end of section 3, more consistent by unioning semantic clusters according to wikipedia's internal redirect pages, and more comprehensive by leveraging link-to-link pairwise synonymy within semantic clusters.", "Solved": NaN, "Unsolved": NaN}, "245437737": {"num": 511, "type": 1, "title": "Novelty Detection: A Perspective from Natural Language Processing under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics", "pdfurl": NaN, "conclusion": "textual novelty detection has an array of use-cases starting from search and retrieval on the web, nlp tasks like plagiarism detection, paraphrase detection, summarization, modeling interestingness, fake news detection, and so forth. however, less attention is paid to the document-level variant of the problem in comparison to sentence-level novelty detection. in this work, we present a comprehensive account of our experiments so far on document-level novelty detection. we study existing literature on textual novelty detection as well as our earlier explorations on the topic. here we assert that we would need to perform information assimilation from multiple premises to identify the novelty of a given text. our current approach performs better than our earlier approaches. also, we show that our method could be suitably applied to allied tasks like plagiarism detection and paraphrase detection. we point out some limitations of our approach, which we aim to explore next.", "Solved": NaN, "Unsolved": NaN}, "250264874": {"num": 512, "type": 1, "title": "Discourse-Aware Graph Networks for Textual Logical Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2207.01450v2.pdf", "conclusion": "it is studied that clause-like text spans delimited by discourse relations can be discourse units that reveal the rhetorical structure of texts [34], [41]. we further observe that such discourse units are essential logical propositions in logical reasoning, such as premise or conclusion. as the example shown in figure 3, the \"while\" in the passage indicates a comparison between the attributes of the fig. 2. the discourse-aware graph networks (dagns) pipeline mainly consists of (1) logic graph construction (2) logic representation learning. the logic graph construction module takes a logical qa data point as input and constructs logic graphs. the logic representation learning module then performs graph reasoning upon the constructed logic graphs. besides, the encoder provides fundamental embeddings for the pipeline.\"analog system\" and that of the \"digital system\". the \"because\" in the option uncovers that \"error cannot occur in the emission of digital signals\" as a premise to the conclusion \"digital systems are the best information systems\".this observation is agreed with informal logic theories [35], [36], which study uncovering logical structure from the texts and have conventional in-line logical indicators. for example, acknowledged premise indicators include \"since\", \"because\", \"given that\". conclusion indicators include \"therefore\", \"so\", \"consequently\", and so forth. most of these indicators are discourse connectives. some discourse parsers [42], [43] perform discourse unit segmentation. however, discourse parsing is still challenging, and the parsers are not general to new data, such as logical reasoning questions. for example, segbot [43] is good on the rst-dt dataset but does not work well on the standardized exam texts as in the reclor dataset. thus, we customize discourse unit delimitation strategy for logical texts.we use the penn discourse treebank (pdtb 2.0) [34] to help draw discourse connectives. pdtb 2.0 contains discourse relations that are manually annotated on the 1 million wall street journal (wsj) corpus and are broadly characterized into \"explicit\" and \"implicit\" connectives. the former ones are explicitly present in sentences such as discourse adverbial \"instead\" or subordinating conjunction \"because\", whereas the latter ones are inferred by pdtb annotators between successive pairs of text spans split by punctuation marks such as \".\" or \";\". we take all the \"explicit\" connectives as well as common punctuation marks to form our discourse-aware delimiter library, presented in table 1. each logical text is split into elementary discourse units (edus) by all the delimiters in the library. the edus are taken as graph nodes v.nodes with topic-related terms. the desired key terms are those real nouns or phrases that repeatedly appear in the text. such nouns or phrases are instantiations of logical variables in propositions. as a result, replacing such terms with abstract variables or terms in other topics does not change the process of reasoning. for example, in figure 3, the first two sentences indicate a comparison of \"signal\" between \"analog system(s)\" and \"digital system(s)\". performing abstraction by replacing \"signal\" with variable \u03b3, \"analog system(s)\" with variable \u03b1, and \"digital system(s)\" with variable \u03b2, the propositions are free from the topic of electronics, but the comparison relation is retained.we use a sliding window to collect the recurring phrases. given the input logical text, stemming is first applied to handle morphological diversity. then, the sliding window loops over ngrams and records the reoccurrence. next, all the stop words and overlapped substrings are filtered. the resulting topic-related terms are attached to the nodes according to which text segment they belong.binary node types. the text of logical reasoning qa consists of two possible structures: (passage, question, options) or (dialogue context, candidate responses). we regard passage or dialogue context as context texts that carry the main logical reasoning structure, whereas regard (question, options) or candidate responses as candidate texts that are added to the context texts and should remain their logical consistency.according to the discourse unit delimitation, the graph nodes are naturally from the context texts or the candidate texts. therefore, we define two disjoint and independent node sets: context node set v u and candidate node setv v . v u \u222av v = v and v u \u2229v v = \u2205.the interplay between the two node sets formulates logical consistency between the context and the candidate texts.", "Solved": NaN, "Unsolved": NaN}, "262044969": {"num": 513, "type": 1, "title": "Selecting which Dense Retriever to use for Zero-Shot Search", "pdfurl": "https://export.arxiv.org/pdf/2309.09403v1.pdf", "conclusion": "this paper proposes a novel research direction for zero-shot dense retrieval.while traditional information retrieval research in this area concentrates on developing universal domain-agnostic dr models, our work shifts the focus towards developing a method to rank and select pre-trained state-of-the-art dr models that are best suited for a specific target domain corpus.we acknowledge that the proposed direction does not contradict traditional research on training zero-shot dr models, but rather complements it.as newly developed dr models are likely to have varying effects on different domains, selecting the best model is still beneficial.to explore this research direction, we adapt various methods from computer vision and machine learning, along with some approaches designed for ir.we outline our reasoning and challenges with the investigated approaches and present empirical results on a popular zeroshot benchmark dataset.our findings shed light on future research avenues within this research direction.we believe that an effective method for selecting a good dr model can provide a principled way for search engine developers to identify the most suitable model for their application, ultimately enhancing user experience.", "Solved": NaN, "Unsolved": NaN}, "248562985": {"num": 514, "type": 1, "title": "KECP: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.206.pdf", "conclusion": "to bridge the gap between the pre-training and finetuning objectives, kecp views eqa as an answer generation task. in kecp, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query. the span-level contrastive learning objective is proposed to improve the performance of eqa. experiments on multiple benchmarks in both instance-level and task-level few-shot scenarios show that our framework consistently outperforms the state-of-the-art methods.", "Solved": NaN, "Unsolved": NaN}, "225068329": {"num": 515, "type": 1, "title": "Measuring Association Between Labels and Free-Text Rationales", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.804.pdf", "conclusion": "after demonstrating the weaknesses that pipeline models exhibit for free-text rationalization tasks, we propose two measurements of label-rationale association in self-rationalizing models. we find that on three free-text rationalization datasets for commonsenseqa and snli, models based on t5 exhibit high robustness equivalence and feature importance agreement, demonstrating that they pass a necessary sanity check for generating faithful free-text rationales.", "Solved": NaN, "Unsolved": NaN}, "263310649": {"num": 516, "type": 1, "title": "Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback", "pdfurl": "https://export.arxiv.org/pdf/2309.17078v1.pdf", "conclusion": "in this work, we propose a novel framework that leverages contrastive feedback to optimize large language models through reinforcement learning, namely rlcf. the capabilities of llm could be aligned with the context of information retrieval through the proposed rlcf. specifically, we first construct a group of similar documents by a dense retrieval model. subsequently, documents in the same group are fed into a llm to be optimized. the responses are obtained by the llm for these similar documents. the contrastive feedback is obtained from these responses generated by llm with respect to corresponding documents. the contrastive feedback is calculated by dense retrieval model. formally, we employ a novel reward function, batched-mrr, as the contrastive feedback, which is a variant of mrr. then, the contrastive feedback could be utilized to optimized llm through ppo algorithm, which is a widely used reinforcement learning method. we conduct experiments on two tasks of information retrieval, demonstrating the effectiveness of our proposed rlcf. the rlcf-optimized llm could generates specific queries for data augmentation, achieving promising performance on few-shot dense retrieval. besides, we introduce a brand-new setting of document summarization, which is under the context of information retrieval. to be specific, the summarizes should be specific to each document among similar documents, which is desired for users to filter out target document. to evaluate the effectiveness of summarization in the proposed setting, we introduce rouge-diff, a variant of rouge score, which is calculated in the group level. in future work, we suggest exploring more domains which could use the rlcf for optimization, such as style transfer, harmless alignment, helpfulness alignment and etc.", "Solved": NaN, "Unsolved": NaN}, "250264890": {"num": 517, "type": 1, "title": "Rationale-Augmented Ensembles in Language Models", "pdfurl": "https://arxiv.org/pdf/2207.00747v1.pdf", "conclusion": "in this paper, we have presented a unified framework for rationale-augmented ensembles, and found that rationale sampling in the output space is a key component for achieving improved performance in natural language processing tasks. by sampling diverse rationales and ensembling the results, we have shown that rational-ensembling methods in the proposed framework can reliably outperform standard prompting and rationale-based few-shot prompting, across a wide range of natural language tasks and alternative language models. overall, rationale-augmented ensembling appears to be a reliable way to shift from the paradigm of (input \u2192 output) pairs to (input, rationale \u2192 output) pairs to achieve more accurate and interpretable natural language processing.", "Solved": NaN, "Unsolved": NaN}, "247762238": {"num": 518, "type": 1, "title": "Lite Unified Modeling for Discriminative Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2022.acl-long.594.pdf", "conclusion": "in this work, we propose pos-enhanced iterative co-attention network (poi-net), as a lightweight unified modeling for multiple subcategories of discriminative mrc. poi-net utilizes pos embedding to encode pos attributes for the preciseness of answer boundary, and iterative co-attention mechanism with integration strategy is employed to highlight and integrate critical information at decoding aspect, with almost no additional parameter. as the first effective and unified modeling with pertinence for different types of discriminative mrc, evaluation results on four extractive and multi-choice mrc benchmarks consistently indicate the general effectiveness and applicability of our model.", "Solved": NaN, "Unsolved": NaN}, "233219392": {"num": 519, "type": 1, "title": "Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.368.pdf", "conclusion": "we introduce a simple yet effective multi-step dense retrieval method, beamdr. by conducting beam search and globally refreshing negative chains during training, beamdr finds reasoning chains in dense space. beamdr is competitive to more complex sota systems albeit not using semi-structured information.", "Solved": NaN, "Unsolved": NaN}, "248986946": {"num": 520, "type": 1, "title": "Interpretable Proof Generation via Iterative Backward Reasoning", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.216.pdf", "conclusion": "this paper presents ibr, a proof generation model via iterative backward reasoning for rule-based qa tasks. we equip the reasoning procedure with detailed hidden state tracking by predicting nodes and edges in the proof path iteratively backward from the question, and allow the model to reason on the elaborate representations of nodes and his- tory paths. our model is more interpretable than previous at-once models, and is also more effective and efficient than former iterative models. experiments also demonstrate the superiority of ibr to various baselines on proof generation under various settings.  we implement our model based on pytorch along with huggingface-transformers toolkit 6 . we use roberta large model 7 as our backbone encoder to generate token-level representations. table 10 shows the implementation details of ibr, including learning rates for different modules. all linear layers used in our model have one layer. the model trained after 8 epochs will be used in the evaluation. we remove functional words without lexical meaning like \"a\" and \"the\" from facts, rules, and questions to shorten the input length, so each training epoch takes about 2 hours. we select these hyper-parameters according to tuning them empirically based on the performance. all experiments are run on nvidia tesla-v100 gpus. the main experiment performance of ibr fluctuates by one point.", "Solved": NaN, "Unsolved": NaN}, "226262339": {"num": 521, "type": 1, "title": "Generating Fact Checking Briefs", "pdfurl": "https://arxiv.org/pdf/2011.05448v1.pdf", "conclusion": "we propose the concept of fact checking briefs, to be read before performing a fact check. crucially, we develop qabriefer and release the accompanying qabriefdataset, to create qabriefs. we show in extensive empirical studies with crowdworkers and volunteers that qabriefs can improve accuracy and efficiency of fact checking. ", "Solved": NaN, "Unsolved": NaN}, "249926985": {"num": 522, "type": 1, "title": "Questions Are All You Need to Train a Dense Passage Retriever", "pdfurl": "https://www.aclanthology.org/2023.tacl-1.35.pdf", "conclusion": "we introduced art, a novel approach to train a dense passage retriever using only questions. art does not require question-passage pairs or hardnegative examples for training and yet achieves state-of-the-art results. the key to making art work is to optimize the retriever to select relevant passages such that conditioning on them, the question generation likelihood computed using a large pre-trained language model iteratively improves. despite requiring much less supervision, art substantially outperforms dpr when evaluated on multiple qa datasets and also generalizes better on out-of-distribution questions.", "Solved": NaN, "Unsolved": NaN}, "219573577": {"num": 523, "type": 1, "title": "Multi-hop Reading Comprehension across Documents with Path-based Graph Convolutional Network", "pdfurl": "https://arxiv.org/pdf/2006.06478v2.pdf", "conclusion": "in this paper, we propose a novel approach for multi-hop reading comprehension across documents. our approach extends the entity graph by introducing reasoning entities, which can form the reasoning path from question to candidates. in addition, our approach incorporates the question in the multi-hop reasoning through a new gate mechanism to regulate how much useful information propagating from neighbors to the node. experiments show that our approach achieves state-of-the-art accuracy both for single and ensemble models.", "Solved": NaN, "Unsolved": NaN}, "253384010": {"num": 524, "type": 1, "title": "NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2211.03462v2.pdf", "conclusion": "hybrid tabular-textual question answering (qa) requires reasoning from heterogeneous information, and numerical reasoning is its key challenge compared to extractive qa.to address the severe exposure bias issue of current autoregressive methods when program generation performance is far from good, we present a non-autoregressive program generation (napg) framework for numerical reasoning, which facilitates program generation in parallel.our framework independently generates complete program tuples containing both the operator and its operands.compared to previous autoregressive decoding methods, napg does not suffer from exposure bias, and can significantly boost program generation speed.", "Solved": NaN, "Unsolved": NaN}, "202558795": {"num": 525, "type": 1, "title": "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations", "pdfurl": "https://arxiv.org/pdf/1909.04925v1.pdf", "conclusion": "our work reveals important findings about the inner functioning of transformer networks. the impact of these findings and how future work can build upon them is described in the following:", "Solved": NaN, "Unsolved": NaN}, "237737294": {"num": 526, "type": 1, "title": "Dual-Channel Reasoning Model for Complex Question Answering", "pdfurl": NaN, "conclusion": "in this paper, we propose a dual-channel reasoning architecture for complex question answering. e dual-channel reasoning architecture is applied to the feature interaction framework and graph-based models to verify its general applicability. in the experiments, we show that our models   complexity significantly and consistently outperform the baseline model, especially in supporting fact prediction tasks. after more detailed experimental analysis, it is proved that the dual-channel reasoning structure has stronger step-by-step reasoning ability than the single-channel reasoning structure. in the future, we believe that the following issue will be worth studying. for the dual-channel reasoning architecture, the interaction strategy between the two channels, such as the soft parameter sharing of the homogeneous neural network components of the two channels, is worthy of further study.", "Solved": NaN, "Unsolved": NaN}, "238583118": {"num": 527, "type": 1, "title": "A Few More Examples May Be Worth Billions of Parameters", "pdfurl": "https://arxiv.org/pdf/2110.04374v1.pdf", "conclusion": "in this work, we present an empirical investigation on the relationships between (1) a task's format, (2) the number of labeled examples available for said task, and (3) the number of parameters the model tackling the task has. through our extensive experiments, we determine that task format greatly affects the relative performance improvement that can be expected from increased training set size and parameter count. for tasks that do not require the recollection of specific external information -i.e. classification, multiple choice, and extractive qa -we find that more labeled data and larger models both reliably improve performance. in fact, for some of these tasks, adding a few hundred labeled examples is more beneficial than scaling up the model size by billions of parameters. it seems then, from a practitioner's perspective, that for many tasks where data is very sparse, the tried-and-true strategy of simply collecting more training data will often be a more effective strategy than attempting to scale to larger, more computationally-demanding models. however, the picture is very different for open qa tasks; for such tasks, we find that increasing the size of the training data barely improves performance, leaving parameter inflation as the only reliable approach to improve accuracy. finally, we provide a hypothesis to explain these results and conclude with a practical corollary -when possible, changing the format from open qa into a more \"self-contained\" one will allow labeled data to bridge performance gaps between moderately-sized models and much larger ones.  table 2 provides the results from our main experiment (section 3, figure 2) in tabular form.  ", "Solved": NaN, "Unsolved": NaN}, "261049680": {"num": 528, "type": 1, "title": "Large Language Models as Zero-Shot Conversational Recommenders ACM Reference Format", "pdfurl": "https://export.arxiv.org/pdf/2308.10053v1.pdf", "conclusion": "finding 10 -llm recommendations suffer from popularity bias in crs. popularity bias refers to a phenomenon that popular items are recommended even more frequently than their popularity would warrant [8]. figure 8 shows the popularity bias in llm recommendations, though it may not be biased to the popular items in the target datasets. on redial, the most popular movies such as avengers: infinity war appear around 2% of the time over all ground-truth items; on reddit, the most popular movies such as everything everywhere all at once appears less than 0.3% of the time over ground-truth items. but for the generated recommendations from gpt-4 (other llms share a similar trend), the most popular items such as the shawshank redemption appear around 5% times on redial and around 1.5% times on reddit. compared to the target datasets, llms recommendations are more concentrated on popular items, which may cause further issues like the bias amplification loop [8]. moreover, the recommended popular items are similar across different datasets, which may reflect the item popularity in the pre-training corpus of llms.finding 11 -recommendation performance of llms is sensitive to geographical regions. despite the effectiveness in general, it is unclear whether llms can be good recommenders across various cultures and regions. specifically, pre-trained language models' strong open-domain ability can be attributed to pre-training from massive data [5]. but it also leads to llms' sensitivity to data distribution. to investigate llms recommendation abilities for various regions, we take test instances from the reddit dataset and obtain the production region of 7,476 movies from a publicly available movie dataset 13 by exact title matching, then report the recall@1 for the linked movies grouped by region. we only report regions with more than 300 data points available to ensure enough data to support the result. as shown in figure 9 the current best model, gpt-4's performance on recommendation is higher for movies produced in english-speaking regions. this could be due to bias in the training data -the left of figure 9 show item on reddit forums are dominated by movies from english-speaking regions. such a result highlights large language model's recommendation performance varies by region and culture and demonstrates the importance of cross-regional analysis and evaluation for language model-based conversational recommendation models.", "Solved": NaN, "Unsolved": NaN}, "53116244": {"num": 529, "type": 1, "title": "Re Co CoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/1810.12885v1.pdf", "conclusion": "we introduced record, a large-scale reading comprehension dataset requiring commonsense reasoning. unlike existing machine reading comprehension (mrc) datasets, record contains a large portion of queries that require commonsense reasoning to be answered. our baselines, including top performers on existing mrc datasets, are no match for human competence on record. we hope that record will spur more research in mrc with commonsense reasoning.", "Solved": NaN, "Unsolved": NaN}, "259980634": {"num": 530, "type": 1, "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2307.11768v2.pdf", "conclusion": "overall, our results from the reasoning perturbation experiments suggest that question decomposition leads to more faithful model-generated reasoning. factored decomposition generates the most faithful reasoning, whereas cot decomposition generates less faithful reasoning than factored decomposition but more faithful reasoning than cot prompting. this is shown by the early answering experiments, which find comparable faithfulness between cot decomposition and cot prompting, and the adding mistakes experiments, which find cot decomposition has intermediate faithfulness.", "Solved": NaN, "Unsolved": NaN}, "249062748": {"num": 531, "type": 1, "title": "Generating Natural Language Proofs with Verifier-Guided Search", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.7.pdf", "conclusion": "we have introduced nlproofs for stepwise proof generation in natural language. it learns to generate relevant proof steps conditioning on the hypothesis. to prevent hallucination, nlproofs searches for proofs that maximize a validity score judged by a verifier. our method has achieved state-of-the-art performance on en-tailmentbank and ruletaker, demonstrating the promise of stepwise proof generation for human-authored proofs. in the future, we hope to see increasing applications of verifiers and proof search in various reasoning tasks.", "Solved": NaN, "Unsolved": NaN}, "254854493": {"num": 532, "type": 1, "title": "Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation", "pdfurl": "https://export.arxiv.org/pdf/2212.08841v1.pdf", "conclusion": "in this study, a series of scalable augmentation techniques are proposed to produce surrogate queries for training dense retrievers without using any annotated pairs. we achieve state-of-the-art performance on two collections of widely used benchmarks (beir and six odqa datasets), demonstrating that the inductive bias of the synthetic querydoc pairs is effective for training dense retrievers, greatly bridging the gap between unsupervised dense models and bm25 and inspiring us to rethink the necessity of using real queries.", "Solved": NaN, "Unsolved": NaN}, "202558815": {"num": 533, "type": 1, "title": "A Discrete Hard EM Approach for Weakly Supervised Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-1284.pdf", "conclusion": "in this paper, we demonstrated that, for many qa tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. we showed that this approach significantly outperforms previous approaches on six qa tasks including reading comprehension, opendomain qa, discrete reasoning task and semantic parsing, achieving absolute gains of 2-10% and setting the new state-of-the-art on five wellstudied datasets.", "Solved": NaN, "Unsolved": NaN}, "226262367": {"num": 534, "type": 1, "title": "EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering", "pdfurl": "https://arxiv.org/pdf/2011.03080v1.pdf", "conclusion": "we presented e\u03c7\u03b1\u00b5s, a new challenging crosslingual and multilingual benchmark for science qa in 16 languages and 24 subjects from high school examinations.", "Solved": NaN, "Unsolved": NaN}, "244909409": {"num": 535, "type": 1, "title": "Hybrid Autoregressive Inference for Scalable Multi-hop Explanation Regeneration", "pdfurl": "https://arxiv.org/pdf/2107.11879v2.pdf", "conclusion": "this work presented scar, a hybrid autoregressive architecture for scalable explanation regeneration. an extensive evaluation demonstrated that scar achieves performance comparable with that of state-of-the-art crossencoders while being \u2248 50 times faster and intrinsically scalable, and confirmed the impact of the hybridisation on semantic drift and question answering. this work demonstrated the effectiveness of hybrid architectures for explainable inference at scale, opening the way for future research at the intersection of latent and explicit models. as a future work, we plan to investigate the integration of relevance and explanatory power in an end-to-end differentiable architecture, and explore the applicability of the hybrid framework on additional natural language and scientific reasoning tasks, with a focus on real-world scientific inference problems.", "Solved": NaN, "Unsolved": NaN}, "259317090": {"num": 536, "type": 1, "title": "Single Sequence Prediction over Reasoning Graphs for Multi-hop QA", "pdfurl": "https://www.aclanthology.org/2023.acl-long.642.pdf", "conclusion": "in this paper, we propose seqgraph, an approach that utilizes the structured relationship between passages in the context of multi-hop questions to reduce disconnected reasoning. we construct a localized entity-passage graph using wikipedia hyperlinks, encode it using a gnn, and fuse the structured representations with the text encoder for predicting a reasoning path. our approach results in strong performance gains in terms of both answer and support em/f1 on hotpot-qa and reduces disconnected reasoning measured using dire score. we also obtain state-of-the-art performance on the more challenging musique benchmark with a 17-point improvement in answer f1 over the current best end-to-end(e2e) model. experimenting with sophisticated methods of encoding the graph structure and fusing the text and graph representations can be explored in future work.", "Solved": NaN, "Unsolved": NaN}, "259949923": {"num": 537, "type": 1, "title": "SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval Lexical Inverted Index CPU for Interpret- System Matching for Inference Inference ability", "pdfurl": "https://export.arxiv.org/pdf/2307.10488v1.pdf", "conclusion": "in this work, we presented sprint, a unified python toolkit focused on sparse neural retrieval. the toolkit extends the evaluation of several neural sparse retrievers on a common interface and easily allows practitioners to search their custom datasets using sparse retrieval. evaluation of a custom dataset using our toolkit is straightforward, as we effectively use an inference pipeline to unify evaluation across all different sparse retrievers.", "Solved": NaN, "Unsolved": NaN}, "237563234": {"num": 538, "type": 1, "title": "Slot Filling for Biomedical Information Extraction", "pdfurl": "https://www.aclanthology.org/2022.bionlp-1.7.pdf", "conclusion": "in this work we formulated the task of biomedical information extraction as a slot filling problem. this approach aims to forgo the need for entity and relation type specific training data, which is scarce and costly to annotate in the biomedical domain. additionally, this formulation allows to deal with the addition of new relation types, without needing to re-train the relevant models. additionally, we have introduced a new biomedical slot filling benchmark and used it to train a biomedical dpr model, a dual bert-based encoder for retrieval, as well as a biomedical slot filling reader based on biobert. in a series of experiments our approach outperforms significantly a number of general domain baselines as well as the simpler bm25 retriever. furthermore, our results illustrate the importance of in-domain, taskspecific training data, in line with findings from recent works (glass et al., 2021;maillard et al., 2021).", "Solved": NaN, "Unsolved": NaN}, "252089700": {"num": 539, "type": 0, "title": "A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension", "pdfurl": "https://export.arxiv.org/pdf/2209.01824v2.pdf", "conclusion": "we have covered the shortcut identification and mitigation landscape in mrc. the presence of shortcuts can be made clear through a variety of methods, and most researchers are aware of this issue. mitigation methods are varied and have some degree of success, but a lot more work is needed before we can achieve models mostly free of shortcut biases. efforts should be made to improve mrc shortcut debiasing techniques by incorporating those found in other fields such as computer vision and nli, as well as finding methods with lower human and/or computation costs.", "Solved": NaN, "Unsolved": NaN}, "189898081": {"num": 540, "type": 1, "title": "DocRED: A Large-Scale Document-Level Relation Extraction Dataset", "pdfurl": "https://www.aclweb.org/anthology/P19-1074.pdf", "conclusion": "to promote re systems from sentence level to document level, we present docred, a large-scale document-level re dataset that features the data size, the requirement for reading and reasoning over multiple sentences, and the distantly supervised data offered for facilitating the development of weakly supervised document-level re. experiments show that human performance is significantly higher than re baseline models, which suggests ample opportunity for future improvement.", "Solved": NaN, "Unsolved": NaN}, "257813864": {"num": 541, "type": 1, "title": "qaaskeR + : a novel testing method for question answering software via asking recursive questions", "pdfurl": NaN, "conclusion": "question answering (qa) software has been widely used in our daily life. in this paper, we propose a novel recursive metamorphic testing method qaasker + with five novel recursive metamorphic relations. qaasker + tests qa software by checking its behaviors on multiple recursively asked questions that are relevant to the same or some further enriched knowledge. it cuts off the reliance on the preannotated labels of test cases, thus enables both the flexible just-in-time test during usage and the extensible test with massive unlabeled data for qa software, which cannot be supported by the current reference-based test paradigm. we evaluate the effectiveness of qaasker + by using it to test four representative state-of-the-art qa software that covers two mainstream types of qa software, as well as a popular reallife qa application, the google search service. comprehensive results demonstrate that qaasker + can reveal quantities of valid violations that depict diverse answering issues for various kinds of mainstream qa software. besides, we also found that our recursive mrs have a better fault detection effectiveness than two representative non-recursive mrs and can even help to fix the revealed issues.", "Solved": NaN, "Unsolved": NaN}, "232035689": {"num": 542, "type": 1, "title": "Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing", "pdfurl": "https://arxiv.org/pdf/2102.12060v4.pdf", "conclusion": "we have presented a review of existing datasets for exnlp research, highlighted discrepancies in data collection that can have downstream modeling effects, and synthesized the literature both inside and outside exnlp into a set of recommendations for future data collection.", "Solved": NaN, "Unsolved": NaN}, "245218668": {"num": 543, "type": 1, "title": "Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering", "pdfurl": "https://arxiv.org/pdf/2112.08777v1.pdf", "conclusion": "in this work, we propose an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts. we demonstrate consistent improvement when using our approach on three different models over two different benchmarks. in future work, we would like to explore variations of our proposed supervised loss on other long-context tasks, such as long document summarization.", "Solved": NaN, "Unsolved": NaN}, "259501422": {"num": 544, "type": 1, "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation", "pdfurl": "https://export.arxiv.org/pdf/2307.03987v2.pdf", "conclusion": "in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models. through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average. we also demonstrated the individual efficacy of our detection and mitigation techniques. specifically, our detection technique achieves a high recall and the mitigation technique successfully mitigates a large fraction of the correctly detected hallucinations. notably, the mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. we further demonstrated the effectiveness and wide applicability of our approach and presented several interesting studies including evaluation with another llm (vicuna) and answering multi-hop and false premise questions. overall, our work addresses the llms' hallucination problem and thus contributes to improving their reliability and trustworthiness, a crucial step en route to enabling their widespread adoption in real-world applications.  table 6 shows the instructional prompts used for different steps of our approach. we note that these techniques are the preferred techniques as they do not require calling an external task-specific tool to achieve the corresponding objectives.", "Solved": NaN, "Unsolved": NaN}, "251135345": {"num": 545, "type": 1, "title": "Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation", "pdfurl": "https://export.arxiv.org/pdf/2207.14000v1.pdf", "conclusion": "we provide insights into an rnn-based iterative memory model that incorporates gate attention on multi-step reasoning over natural language. instead of using the original gru and dotproduct attention, we integrate gate attention to update hidden states. the experiment results show the model with gate attention achieves generally better performance than the original rnnbased iterative-memory model with dot-product attention and other rnn-based models. the performance of our model is comparable or better than the much larger and pretrained roberta-large in some scenarios. furthermore, our model shows better out-of-distribution generalisation performance than the pretained roberta. to address the issue of depth-imbalance in the existing datasets on multi-step reasoning over natural language, we develop a large-scale multi-step reasoning dataset called pararule-plus, with more examples of deep reasoning depths than previous datasets. we find that the performance of the models in our experiments improves when we add pararule-plus in the training, especially on examples that require deeper reasoning depths and extra out-of-distribution examples. table 4 we use glove [16] as the word vector representation. we use pararules with all depths as the training set for all models and then test them on examples with different reasoning depths (d). comparison among our ima-glove-ga, ima-glove, mac-glove, dmn-glove, imasm-glove, lstm-glove, and roberta-large on pararules test sets with different reasoning depths. ", "Solved": NaN, "Unsolved": NaN}, "237450610": {"num": 546, "type": 1, "title": "Flexible Generation of Natural Language Deductions", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.506.pdf", "conclusion": "building systems that use natural language as a medium for reasoning will require operations to logically combine and transform natural language statements. in this work, we present parapattern, a method for creating such models with minimal manual effort by finetuning pretrained sequence-to-sequence language models on data generated through a three-step process of syntactic retrieval, template expansion, and automatic paraphrasing. our experimental results show that parapattern yields operation models capable of generating consistent logical transformations over a diverse range of natural language inputs, matching the performance of models trained with in-domain human supervision.", "Solved": NaN, "Unsolved": NaN}, "248563058": {"num": 547, "type": 1, "title": "Long Context Question Answering via Supervised Contrastive Learning", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.207.pdf", "conclusion": "in this work, we proposed an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts. we demonstrate consistent improvement when using our approach on three different models over two different benchmarks. for future work, we propose exploring variations of our proposed supervised loss on other long-context tasks, such as longdocument and multi-document summarization, and integrating our method into information retrieval re-ranker models. depends on the size of the model). in order to determine the temperature hyperparameter \u03c4 , we searched over {0.2, 0.4, 0.6, 0.8, 1.0} per question type (if applicable). we also applied dropout with a rate of p = 0.1 over the linear projections, which consistently improved the results over all the benchmarks. finally, we searched for the best performing \u03bb hyperparameter over the values of {0.2, 0.4, 0.6, 0.8, 1.0}.", "Solved": NaN, "Unsolved": NaN}, "263830898": {"num": 548, "type": 1, "title": "RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS", "pdfurl": "https://export.arxiv.org/pdf/2310.05149v1.pdf", "conclusion": "in this paper, we present itrg, which is an iterative retrievalgeneration synergy framework, containing two important steps: generation-augmented retrieval and retrieval-augmented generation.they form a closed loop, and can improve each other via multiple iterations.we propose a simple and effective generation-augmented retrieval strategy and two retrieval-augmented generation strategies.empirical results show our approach significantly exceeds several strong baselines, including gpt 3.5, on four open domain question answering datasets, which indicates that our method can significantly improve the reasoning ability of large language models.", "Solved": NaN, "Unsolved": NaN}, "246015349": {"num": 549, "type": 1, "title": "Natural Language Deduction through Search over Statement Compositions", "pdfurl": "https://export.arxiv.org/pdf/2201.06028v2.pdf", "conclusion": "in this work, we propose a system that performs natural language reasoning through generative deduction and heuristic-guided search.", "Solved": NaN, "Unsolved": NaN}, "211258744": {"num": 550, "type": 1, "title": "Do Multi-Hop Question Answering Systems Know How to Answer the Single-Hop Sub-Questions?", "pdfurl": "https://arxiv.org/pdf/2002.09919v1.pdf", "conclusion": "we propose a new way to evaluate whether multi-hop qa systems have learned the ability to perform reasoning over multiple documents by asking sub-questions. an automatic approach is designed to generate sub-questions for a multihop question. on a human-verified test set, we show that all three existing top models give worse performance on the subquestions compared to our proposed model with an explicit question type classification component and a single-hop qa component. as an initial step towards a more explainable qa system, we hope our work could motivate the construction of multi-hop qa datasets with explicit reasoning paths annotated and the development of better multi-hop qa models.", "Solved": NaN, "Unsolved": NaN}, "233219449": {"num": 551, "type": 1, "title": "What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.646.pdf", "conclusion": "we show that training multiple heads on top of a pre-trained language model creates a steering effect, where the target head influences the behaviour of another head, steering it towards capabilities beyond its training objective. in three multi-task settings, we find that without any dedicated training, the steered head often outputs explanations for the model predictions. moreover, modifying the input representation based on the outputs of the steered head can lead to predictable changes in the target head predictions.", "Solved": NaN, "Unsolved": NaN}, "253398396": {"num": 552, "type": 1, "title": "ROBUSTLR: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.653.pdf", "conclusion": "in this paper, we proposed robustlr, a diagnostic benchmark to test the logical robustness of deductive reasoning models. in robustlr, we propose two evaluation sets, logical contrast and logical equivalence, each probing different logical reasoning abilities. overall, we find that fine-tuning lms such as roberta and t5 on deductive reasoning datasets is not sufficient to learn the semantics of the logical operators conjunction, disjunction, and negation. although well-aligned training dataset improves model performance, the models still find it challenging to understand negations, both in logical contrast and logical equivalence sets. we demonstrate some interesting shortcoming of lms designed for logical reasoning, that can eventually enable building better reasoning models.", "Solved": NaN, "Unsolved": NaN}, "258461053": {"num": 553, "type": 1, "title": "Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA", "pdfurl": "https://www.aclanthology.org/2023.acl-long.55.pdf", "conclusion": "in this work, we formulated 'post-abstention', a task that allows re-attempting the abstained instances of the given selective prediction system with the aim of increasing its coverage without significantly sacrificing the accuracy. we also explored several baseline methods for this task. through comprehensive experiments on 11 qa datasets, we showed that these methods lead to considerable performance improvements in both in-domain and out-of-domain settings. we further performed a thorough analysis that resulted in several interesting findings.", "Solved": NaN, "Unsolved": NaN}, "262217090": {"num": 554, "type": 1, "title": "Automatic Answerability Evaluation for Question Generation", "pdfurl": "https://export.arxiv.org/pdf/2309.12546v1.pdf", "conclusion": "this paper highlights an urgent issue in the field of question generation: the absence of an effective automatic evaluation metric to assess whether the generated questions are answerable.to address this issue, we propose a prompting-based metric on answerability (pman), leveraging chatgpt to assess answerability through chain-of-thought (cot) prompting.experiments with both manually created and model-generated samples demonstrate its reliability and strong alignment with human evaluations.applying our metric to evaluate qg models further indicates its potential to complement conventional metrics and guide future research in qg toward the generation of more answerable questions.however, pman currently fails to assess \"yes/no\" type questions with high accuracy, which remains as a direction for future research.", "Solved": NaN, "Unsolved": NaN}, "202583429": {"num": 555, "type": 1, "title": "Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-5806.pdf", "conclusion": "this paper introduces an important sub-problem of bridge reasoning for the task of multi-hop qa in the open-domain setting. we propose a bridge reasoner that utilizes multiple types of evidence to derive the passages that cover the answers. the reasoner significantly improves the coverage of answer passages than ir methods. with the predicted passages, we show that a standard reading comprehension model is able to achieve similar performance as the state-of-the-art method that requires bert in multiple modules.", "Solved": NaN, "Unsolved": NaN}, "207756678": {"num": 556, "type": 1, "title": "Do Multi-hop Readers Dream of Reasoning Chains?", "pdfurl": "https://www.aclweb.org/anthology/D19-5813.pdf", "conclusion": "in this paper, we analyze qa models' capability in multi-hop reasoning by assessing if the reasoning chain could help existing multi-hop readers. we observed the general weakness of stat-or-the-art models in multi-hop reasoning and proposed a comatching based method to mitigate. despite the fact that co-matching is designed to encode only three input sequences to achieve limited multi-hop reasoning, we consider this as the most promising one that demonstrates the concrete reasoning capability and has the potential for real multi-hop reasoning.", "Solved": NaN, "Unsolved": NaN}, "202583433": {"num": 557, "type": 1, "title": "Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-5816.pdf", "conclusion": "we introduce an entity-centric approach to ir that finds relevant evidence required to answer multihop questions from a corpus containing millions of paragraphs leading to significant improvement to an existing qa system.", "Solved": NaN, "Unsolved": NaN}, "207852944": {"num": 558, "type": 1, "title": "Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering", "pdfurl": "https://arxiv.org/pdf/1911.03868v2.pdf", "conclusion": "we proposed a general approach for open-domain question answering (qa) that models interactions between paragraphs using structural information from a knowledge base. unlike standard approaches where a model retrieves and reads a set of passages, we integrate graph structure at every stage to construct, retrieve and read a graph of passages. our approach consistently outperforms competitive baselines in three open-domain qa datasets, webquestions, natural ques-tions and triviaqa, and we also include a detailed qualitative analysis to illustrate where the cross paragraph reading contributes the most to the overall system performance.", "Solved": NaN, "Unsolved": NaN}, "236477844": {"num": 559, "type": 1, "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2108.06027v2.pdf", "conclusion": "this paper presented a novel dense passage retrieval approach that leverages both query-centric and passage-centric similarity relations for capturing more comprehensive semantic relations. to implement our approach, we made three important technical contributions in the loss formulation, training data augmentation and effective training procedure. extensive results demonstrated the effectiveness of our approach. to our knowledge, it is the first time that passage-centric similarity relation has been considered for dense passage retrieval. we believe such an idea itself is worth exploring in designing new ranking mechanism. in future work, we will design more principle ranking functions and apply current retrieval approach to downstream tasks such as question answering and passage re-ranking.", "Solved": NaN, "Unsolved": NaN}, "222080412": {"num": 560, "type": 1, "title": "KPQA: A Metric for Generative Question Answering Using Keyphrase Weights", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.170.pdf", "conclusion": "in this paper, we create high-quality human judgments on two genqa datasets, ms-marco and avsd, and show that previous evaluation metrics are poorly correlated with human judgments in terms of the correctness of an answer. we propose kpqa-metric, which uses the pre-trained model that can predict the importance weights of words in answers to a given question to be integrated with existing metrics. our approach has a dramatically higher correlation with human judgments than existing metrics, showing that our model-based importance weighting is critical to measure the correctness of a generated answer in genqa.", "Solved": NaN, "Unsolved": NaN}, "241583517": {"num": 561, "type": 1, "title": "Team Papelo at FEVEROUS: Multi-hop Evidence Pursuit", "pdfurl": "https://www.aclanthology.org/2021.fever-1.5.pdf", "conclusion": "team papelo's system for feverous achieves .281 feverous score on the development set, with .658 label accuracy and .348 evidence recall. the largest increase in performance over the baseline comes from the label classifier, which uses a different model architecture and is trained on extracted evidence chains including irrelevant evidence. we also achieve better evidence recall through our table cell ranking module, which was trained with a multiple choice cross entropy loss similar to dpr. additional gains are achieved by our multi-hop evidence retrieval. these modules can only be effective when given good representations of the context of sentences, list items and table cells, which we have carefully constructed.", "Solved": NaN, "Unsolved": NaN}, "254854559": {"num": 562, "type": 1, "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning", "pdfurl": "https://www.aclanthology.org/2023.acl-long.347.pdf", "conclusion": "in this paper, we proposed apollo, an adaptive pre-trained language model with logical reasoning abilities. we use a subset of wikipedia sentences for continued pretraining of the model using two self-supervised loss functions. the choice of the training dataset and loss functions are guided by the goal to include more reasoning-related sentences and training signals, respectively. through experiments on two logical reasoning datasets and ablation studies, we demonstrate the effectiveness of our proposed approach. overall, we show that apollo is a generalized solution to improving logical reasoning in language models.", "Solved": NaN, "Unsolved": NaN}, "261891487": {"num": 563, "type": 1, "title": "Cross-Lingual Knowledge Editing in Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2309.08952v1.pdf", "conclusion": "in this paper, we first explore the cross-lingual effect of knowledge editing. to achieve that, we automatically construct bi-zsre dataset by translating the previous zsre dataset from english to chinese. based on bi-zsre, we conduct experiments on various knowledge editing methods and multilingual llms, and study the cross-lingual effect from english to chinese and vice versa. our results indicate that (1) the language modeling gaps of different languages might influence the efficiency of knowledge editing in different languages; (2) it is still hard for existing knowledge editing methods to transfer the edited knowledge from one language to another in a multi-lingual llm; (3) when editing llms in a language, the locality in the other languages could also be influenced. we also analyze the inconsistent behaviors of the edited models and discuss their specific challenges to provide a deeper understanding of the cross-lingual effect in knowledge editing.", "Solved": NaN, "Unsolved": NaN}, "235097509": {"num": 564, "type": 1, "title": "Breadth First Reasoning Graph for Multi-hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.464.pdf", "conclusion": "in this paper, we proposed a novel gnn model of bfr-graph. specifically, the reasoning message starts from the question node and passes to the next sentences node hop by hop until all the edges have been passed. we also construct the reasoning graph as a weighted graph and present a more interpretable way to aggregate scores of different levels from gnn. on hotpotqa leaderboard, bfr-graph achieved state-of-the-art on answer span prediction.", "Solved": NaN, "Unsolved": NaN}, "235212203": {"num": 565, "type": 1, "title": "Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance", "pdfurl": "https://arxiv.org/pdf/2105.12969v2.pdf", "conclusion": "in this work, we propose qfs-bart, an abstractive summarization model for query focused summarization. we use a generalizing qa model to make explicit answer relevance scores for all words in the document and combine them to the encoder-decoder attention. we also leverage pretrained model (e.g. bart) and two-stage finetuning method which further improve the summarization performance significantly. experimental results show the proposed model achieves state-ofthe-art performance on debatepedia dataset and outperforms several comparable baselines on duc 2006-7 datasets. hal daum\u00e9 iii and daniel marcu. 2006  in this paper, we introduce a two-step architecture: 1) retrieve answer-related sentences given the query, rank them by the confidence score (generated from equation 4) and concatenate them. 2) use our qfs-bart to produce an abstractive summary.", "Solved": NaN, "Unsolved": NaN}, "153312687": {"num": 566, "type": 1, "title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale", "pdfurl": "https://export.arxiv.org/pdf/1905.05460v2.pdf", "conclusion": "we present a new framework cogqa to tackle multi-hop machine reading problem at scale.the reasoning process is organized as cognitive graph, reaching unprecedented entity-level explainability.our implementation based on bert and gnn obtains state-of-art results on hotpotqa dataset, which shows the efficacy of our framework.", "Solved": NaN, "Unsolved": NaN}, "233219505": {"num": 567, "type": 1, "title": "Evaluating Pre-Trained Models for User Feedback Analysis in Software Engineering: A Study on Classification of App-Reviews", "pdfurl": "https://arxiv.org/pdf/2104.05861v3.pdf", "conclusion": "we conducted an extensive exploratory study comparing app issue classification tools and pre-trained transformer-based models in various settings. we conducted the experiments on six available datasets and a highly imbalanced dataset, which is a combination of the six datasets. domain-specific ptms were trained using different sizes of app review data we collected from google play and these customized ptms were also studied here. our results confirm that ptms are achieving higher scores in binary and multi-class classification compared to prior approaches, but the over-the-shelf ptms are not always the best models to be used in all scenarios. instead, cptms have the highest scores and are able to perform better than other models in all settings. moreover, incorporating app specific data in the pre-training of ptms reduces the prediction time. one of the future directions of this research is assessing domain-specific ptms in other areas of app reviews and exploring ways to increase performance in zero-shot setting.            ", "Solved": NaN, "Unsolved": NaN}, "221978039": {"num": 568, "type": 1, "title": "FORECASTQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data", "pdfurl": "https://www.aclanthology.org/2021.acl-long.357.pdf", "conclusion": "forecasting is a difficult task that requires every possible advantage to do well. it would be wise to harness this pool of unstructured data for training automatic event forecasting agents. to utilize this form of data for forecasting, we proposed a question-answering task that requires forecasting skills to solve forecastqa, and provided the accompanying dataset. various baseline methods did not perform well, but this is not surprising given the inherent difficulty of forecasting. our benchmark dataset can benefit future research beyond natural language understanding and hope forecasting performance will be significantly improved.  ", "Solved": NaN, "Unsolved": NaN}, "204823992": {"num": 569, "type": 1, "title": "MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension", "pdfurl": "https://www.aclweb.org/anthology/D19-5801.pdf", "conclusion": "we have presented the mrqa 2019 shared task, which focused on testing whether reading comprehension systems can generalize to examples outside of their training domain. many submissions improved significantly over our baseline, and investigated a wide range of techniques.", "Solved": NaN, "Unsolved": NaN}, "248798649": {"num": 570, "type": 1, "title": "TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.132.pdf", "conclusion": "in this paper, we proposed a tag-level qa model called tie to better understand the topological information contained in the structured web pages. our model explicitly captures two of the most informative topological structures of the web pages, logical and spatial structures, by dom trees and npr graphs, respectively. with the proposed twostage pipeline, we conduct extensive experiments on the websrc dataset. our tie successfully achieves sota performances and the contributions of its key components are validated.", "Solved": NaN, "Unsolved": NaN}, "263699899": {"num": 571, "type": 1, "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation", "pdfurl": "https://export.arxiv.org/pdf/2307.03987v2.pdf", "conclusion": "in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models.through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average.we also demonstrated the individual efficacy of our dete", "Solved": NaN, "Unsolved": NaN}, "238198206": {"num": 572, "type": 1, "title": "Single-dataset Experts for Multi-dataset Question Answering", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.495.pdf", "conclusion": "made combines the benefits of single-and multidataset training, resulting in better in-domain accuracy and transfer performance than either multidataset models or ensembles of single-dataset models, especially in low resource settings. for future work we plan to explore ensembling methods for better zero-shot prediction and interpolating made weights for better transfer learning. ", "Solved": NaN, "Unsolved": NaN}, "232168895": {"num": 573, "type": 1, "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering", "pdfurl": "https://arxiv.org/pdf/2103.05568v3.pdf", "conclusion": "the widely used benchmark okvqa dataset [25] consists of over 14000 question-image pairs, with 9000 training examples and 5000 examples in the test set.we identify two broad issues with it.first, significant overlap exists between answers in the train and test folds.recall from section 1.1 that 48.9% of answers in the test set are present in the training set.existing systems leverage this limitation to boost their accuracy by limiting their test answers to the most frequent answers in the training set.second, unlike webquestions [6] or complexwebquestions [42], okvqa questions, even when grouped into some categories (see below) have no clear pattern of reasoning.18% (type-1) of the questions require detecting objects and subsequent reasoning over an external knowledge source to arrive at the answer.7% of the questions (type-2) require reading text from the image (ocr) (and no other information) to answer.12% of the questions (type-3) are based on personal opinion or speculation.the remaining questions (rest) can perhaps be described best through figure 2d.we provide an example of each type in figure 2.we found that several queries of type-1 have a structural similarity to the bridging queries in complexwebquestions [42].there, each query has exactly two clauses.the first clause, when issued to a web search engine, returns (via rc) an entity which plugs into the second clause, which is again sent to the web search engine, fetching the overall answer.we found that type-1 questions can be reformulated, with the help of the scene graph, to a query that can be answered directly using web search.inspired by complex-webquestions, we next develop our challenge data set.", "Solved": NaN, "Unsolved": NaN}, "235097535": {"num": 574, "type": 1, "title": "MULTIPROVER: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.287.pdf", "conclusion": "we proposed multilabel-multiprover and iterative-multiprover, two variants of a proof-set generation model where the former performs implicit conditioning between the proofs to generate them in parallel while the latter generates a proof-set through explicit conditioning on the previously generated proofs. both models obtain strong proof f1 improvements on synthetic and humanparaphrased datasets and iterative-multiprover also obtains state-of-the-art proof f1 on a zero-shot dataset with single proofs. multiprover's modeling is fairly generic and similar methods can be used in generating a set of structured explanations for other nlp tasks like multi-hop qa.", "Solved": NaN, "Unsolved": NaN}, "234790338": {"num": 575, "type": 1, "title": "KLUE: Korean Language Understanding Evaluation Seungwon Do", "pdfurl": "https://arxiv.org/pdf/2105.09680v4.pdf", "conclusion": "we introduce ynat, the first korean topic classification benchmark. the benchmark includes 63,892 news headlines classified to a single hand-labeled topic among 7 categories. we assume each headline has only a single topic, but it could be formulated as multi-label classification. we thus open the second and third relevant topic annotations. also, urls for each headlines are accompanied for future work if metadata is needed. if some of them requires permission to use, one should contact to the agency. we expect ynat to serve as a simple and basic nlu task compared to others in klue.", "Solved": NaN, "Unsolved": NaN}, "231847375": {"num": 576, "type": 1, "title": "Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question Answering", "pdfurl": "https://arxiv.org/pdf/2102.03741v1.pdf", "conclusion": "in this paper, to tackle the multi-hop information retrieval challenge, we introduce an architecture that models a set of paragraphs as sequential data and iteratively identifies them. specifically, we propose gated memory flow to iterative read and memorize reasoning required information without noise information interference. we evaluate our method on both full wiki and distractor settings on the hotpotqa dataset and the method outperforms previous works by a large margin. in the future, we will attempt to design a more complicated model to improve retrieval performance and explore more about the effect of training data with different data distribution for multi-hop information retrieval.", "Solved": NaN, "Unsolved": NaN}, "207756753": {"num": 577, "type": 1, "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.441.pdf", "conclusion": "in this work, we used a human-and-model-in-theloop training method to collect a new benchmark for natural language understanding. the benchmark is designed to be challenging to current stateof-the-art models. annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into misclassifying, but that another person would correctly classify. we found that non-expert annotators, in this gamified setting and with appropriate incentives, are remarkably creative at finding and exploiting weaknesses. we collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult. training on this new data yielded the state of the art on existing nli benchmarks.", "Solved": NaN, "Unsolved": NaN}, "264146386": {"num": 578, "type": 1, "title": "EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification", "pdfurl": "https://export.arxiv.org/pdf/2310.09754v1.pdf", "conclusion": "we recognize the challenge of generating highquality explanations in a realistic setting, especially in terms of no post-hoc methods and multi-hop reasoning.however, there is still significant potential for further research in leveraging the capabilities of large language models (llms) for fact-checking systems.overcoming these challenges is essential for advancing fact-checking as a whole, improving reliability, trustworthiness, and facilitating better decision-making across different domains.", "Solved": NaN, "Unsolved": NaN}, "238259667": {"num": 579, "type": 1, "title": "Trustworthy AI: From Principles to Practices", "pdfurl": "https://arxiv.org/pdf/2110.01167v2.pdf", "conclusion": "in this survey, we outlined the key aspects of trustworthiness that we think are essential to ai systems. we introduced how ai systems can be evaluated and assessed on each of these aspects, and reviewed current efforts in this direction in the industry. we further proposed a systematic approach to consider these aspects of trustworthiness in the entire lifecycle of real-world ai systems, which offers recommendations for every step of the development and use of these systems. we recognize that fully adopting this systematic approach to build trustworthy ai systems requires that practitioners embrace the concepts underlying the key aspects that we have identified. more importantly, it requires a shift of focus from performance-driven ai to trust-driven ai. in the short run, this shift will inevitably involve side-effects, such as longer learning time, slowed development, and/or increased cost to build ai systems. however, we encourage practitioners to focus on the long-term benefits of gaining the trust of all stakeholders for the sustained use and development of these systems. in this section, we conclude by discussing some of the open challenges and potential opportunities in the future development of trustworthy ai.", "Solved": NaN, "Unsolved": NaN}, "246652372": {"num": 580, "type": 1, "title": "Survey of Hallucination in Natural Language Generation", "pdfurl": "https://export.arxiv.org/pdf/2202.03629v5.pdf", "conclusion": "in this survey, we provide the first comprehensive overview of the hallucination problem in nlg, summarizing existing evaluation metrics, mitigation methods, and the remaining challenges for future research. hallucination is an artifact of neural-based nlg and is of concern because they appear fluent and can therefore be misleading to users. in some scenarios and tasks, hallucination can cause harm. we survey various contributors to hallucination, ranging from noisy data, erroneous parametric knowledge, incorrect attention mechanism, inappropriate training strategy, to inference exposure bias, etc. we show that there are two categories of hallucinations, namely intrinsic hallucination and extrinsic hallucination, and they need to be treated differently with diverse mitigation strategies. hallucination is relatively easy to detect in abstractive summarization and in nmt against the evidence in the source. for dialogue systems, it is important to balance diversity vs consistency in dialogue responses. hallucination in gqa and vl tasks is detrimental to the performance, but research on mitigation methods is still very preliminary in these areas. for datato-text generation, hallucination arises from the discrepancy between the input and output format. most methods to mitigate hallucinations in nmt either aim to reduce dataset noise or alleviate exposure bias. in the vl domain, models also generate unfaithful output given the visual scene, and recent works have mainly focused on the object hallucination problem. there remain many challenges ahead in identifying and mitigating hallucinations in nlg, and we hope research in this area can benefit from this survey.", "Solved": NaN, "Unsolved": NaN}, "235097557": {"num": 581, "type": 1, "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.463.pdf", "conclusion": "in this paper, we propose a novel method for accelerating bert inference, called tr-bert, which prunes bert at token-level granularity. specifically, tr-bert utilizes reinforcement learning to learn a token selection policy, which is able to select general meaningful tokens in the bottom layers and select task-relevant tokens in the top layers. experiments on eleven nlp tasks demonstrate the effectiveness of tr-bert as it accelerates bert inference by 2-5 times for various performance demand. besides, tr-bert achieves a better quality and speed trade-off on long-text tasks, which shows its potential to process large amounts of information in the real-world applications.", "Solved": NaN, "Unsolved": NaN}, "102352338": {"num": 582, "type": 1, "title": "Tracking Discrete and Continuous Entity State for Process Understanding", "pdfurl": "https://www.aclweb.org/anthology/W19-1502.pdf", "conclusion": "in this paper, we present a structured architecture for entity tracking which leverages both the discrete and continuous characterization of the entity evolution. we use a neural crf approach to model our discrete constraints while tracking entities and locations recurrently. our model achieves state of the art results on the propara dataset.", "Solved": NaN, "Unsolved": NaN}, "219792855": {"num": 583, "type": 1, "title": "Pre-trained Language Models as Symbolic Reasoners over Knowledge?", "pdfurl": "https://arxiv.org/pdf/2006.10413v1.pdf", "conclusion": "this work is a first study towards understanding bert's ability to capture knowledge seen during pre-training by investigating it's reasoning and memorization capabilities. we identified factors influencing what knowledge is stored and what is forgotten and what is learnable beyond knowledge explicitly seen during training. we saw that theoretically bert is able to infer facts not explicitly seen during training via symbolic rules. future work should investigate how to enable bert during pre-training to use this capability. we see the need to incentivize plms to capture symbolic rules and factual knowledge as this could potentially improve plm's performance also on downstream tasks where reasoning capabilities or implicit knowledge leverage is needed.", "Solved": NaN, "Unsolved": NaN}, "249062872": {"num": 584, "type": 1, "title": "Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts", "pdfurl": "https://arxiv.org/pdf/2205.12701v1.pdf", "conclusion": "in this paper, based on the observation that transformer models trained by massive tasks have better ability to generalize to unseen tasks, we hope to provide a new sight on exploring how this cross-task generalization ability is achieved and reused. inspired by the way that humans sparsely recall learned skills to solve new tasks, we explicitly model this process by resorting to a task-level mixture-of-expert model, where each expert represent different skills and tasks are routed by a router network based on the task property. we empirically investigate several importance design choices, i.e., routing models, expert selection strategies, task representations to exploring their influence on final model. secondly, by conducting a detailed analysis on the final routing decisions, we find it has a strongly correlation with human-defined task ontology (e.g., classification) and task characteristics (e.g., extractive, linguistic) even without any prior knowledge. we believe the result is valuable and promising in understanding the skills learned behind the black-box transformer models. model's parameters as the task embedding. the fim provides a measure of the information a particular parameter learns about the loss corresponding to the probe model, so it is disable to represent task (wang et al., 2021,vu et al., 2020b. given probe model m \u03b8 , for example bart-base, we first calculate the loglikelihood with respect to the model parameters(\u03b8) as: p \u03b8 = log m \u03b8 (y | x), then fim is calculated as the covariance of gradients of the loglikelihood:", "Solved": NaN, "Unsolved": NaN}, "236486104": {"num": 585, "type": 1, "title": "Team JARS: DialDoc Subtask 1 -Improved Knowledge Identification with Supervised Out-of-Domain Pretraining", "pdfurl": "https://www.aclanthology.org/2021.dialdoc-1.13.pdf", "conclusion": "our submission to the dialdoc subtask 1 performs continual pretraining of a transformer-based encoder on out-of-domain qa datasets. experiments with different qa datasets suggest that conversational qa datasets like coqa and quac are highly beneficial as their setup is substantially similar to doc2dial, the downstream dataset of interest. our final submission ensembles two albert-xl models independently pretrained on coqa and quac and achieves an f1-score of 70.9% and em-score of 53.5% on the competition test-set.", "Solved": NaN, "Unsolved": NaN}, "248227284": {"num": 586, "type": 1, "title": "TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.169.pdf", "conclusion": "we introduce a method to train bi-encoders on unstructured text and knowledge graph types through a type-enforced contrastive loss. our loss can improve retrieval of rare entities for ambiguous mentions, while maintaining strong overall performance on open-domain nlp tasks. we hope our work inspires future work on integrating structured data into pretrained models. s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon. any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of nih, onr, or the u.s. government.", "Solved": NaN, "Unsolved": NaN}, "231632353": {"num": 587, "type": 1, "title": "ComQA:Compositional Question Answering via Hierarchi-cal Graph Neural Networks", "pdfurl": "https://arxiv.org/pdf/2101.06400v1.pdf", "conclusion": "in this paper, we study the compositional question answering where the answer is composed of discontiguous segments in the document. we present a large scale chinese comqa dataset containing more than 120k human-labeled questions. the data construction process has undergone rigid inspections to ensure high quality. to solve the comqa problem, we propose a hierarchical graph neural networks that incorporate document graph structure to the model. we also devise two novel tasks, i.e., question selection and node selection, to pre-train the model. the proposed methods achieve significant improvement over previous methods. we also conduct several ablation studies to demonstrate the superiority of the proposed pre-training tasks and the graph structure. however, there is still a large gap between our model with human performance, suggesting that there is still room for improvement in comqa. figure 8: a snapshot of the annotation interface. each row is a single node that could be served as the final answer component. the question is the page title in the top. note that the image or table could also be selected.", "Solved": NaN, "Unsolved": NaN}, "259145189": {"num": 588, "type": 1, "title": "Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard", "pdfurl": "https://export.arxiv.org/pdf/2306.07471v1.pdf", "conclusion": "the beir benchmark provides an important instrument for evaluating the cross-domain robustness of retrieval models and has gained traction due to the growing recognition of retrieval as a form of representation learning. the efforts described in this paper address the two shortcomings that we have identified with beir: challenges in reproducibility and in the sharing of results. reproducible reference implementations in the pyserini ir toolkit tackle the first challenge. an official self-service leaderboard and best practices for sharing results target the second challenge.", "Solved": NaN, "Unsolved": NaN}, "67855846": {"num": 589, "type": 1, "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "pdfurl": "https://arxiv.org/pdf/1903.00161v1.pdf", "conclusion": "we have presented drop, a dataset of complex reading comprehension questions that require discrete reasoning over paragraphs. this dataset is substantially more challenging than existing datasets, with the best baseline achieving only 32.7% f1, while humans achieve 96%. we hope this dataset will spur research into more compre-hensive analysis of paragraphs, and into methods that combine distributed representations with symbolic reasoning. we have additionally presented initial work in this direction, with a model that augments qanet with limited numerical reasoning capability, achieving 47% f1 on drop.", "Solved": NaN, "Unsolved": NaN}, "202712552": {"num": 590, "type": 1, "title": "What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-1281.pdf", "conclusion": "we focus on the task of question answering under partial knowledge: a novel task that lies inbetween open-domain qa and reading comprehension. we identify classes of knowledge gaps when reasoning under partial knowledge and collect a dataset targeting one common class of knowledge gaps. we demonstrate that identifying the knowledge gap first and then reasoning by filling this gap outperforms previous approaches on the openbookqa task, with and even without additional missing fact annotation. this work opens up the possibility of focusing on other kinds of knowledge gaps and extending this approach to other datasets and tasks (e.g., span prediction).  boiling point means temperature above which a liquid boils figure 10: visualization of the models behavior with the predicted span, top predicted relation, and the top fact used by model. the heat map shows the confidence of the model for all the relations for each input sentence (first five) and conceptnet sentencized tuple (last but one) and the back-off tuple (last one) to capture the knowledge in the embeddings.", "Solved": NaN, "Unsolved": NaN}, "259376626": {"num": 591, "type": 1, "title": "A Question Answering Benchmark Database for Hungarian", "pdfurl": "https://www.aclanthology.org/2023.law-1.19.pdf", "conclusion": "we presented a new qa benchmark database in hungarian, that in several aspects, goes beyond squad-type datasets: it is not limited to single contiguous short extractive answer spans, contains yes/no questions, non-contiguous multispan short answers, long answers, questions requiring arith-model short answers long answers with multispan no multispan with multispan no multispan  table 5: performance of extractive reader models on short and long answer spans with and without multispan answers. metic reasoning, and other questions where the answer cannot be simply copied from the text. the annotation was created using a customized label-studio-based annotation platform. the annotators were encouraged to get actively involved in selecting the texts to be annotated and to abandon annotation of uninteresting or low quality texts in order to make the annotation task less boring and demotivating. we also trained and evaluated baseline models for document retrieval and reader models for answer span extraction. cross-lingual knowledge transfer naturally facilitated by multilingual transformer models was found to be beneficial for the quality of the trained models.", "Solved": NaN, "Unsolved": NaN}, "248780279": {"num": 592, "type": 1, "title": "Modeling Multi-hop Question Answering as Single Sequence Prediction", "pdfurl": "https://www.aclanthology.org/2022.acl-long.69.pdf", "conclusion": "in this work, we propose a generative question answering (qa) approach that models multi-hop qa as a single sequence prediction task. it learns to generate an answer along with a reasoning path to improve its capability of multi-hop reasoning. our experiments on prominent multi-hop qa benchmarks, hotpotqa and iirc, validate the promise and effectiveness of our proposed method path-fid and its extension pathfid+. future work will explore (1) our pathfid approach more closely with text retrieval models in open-domain qa scenarios and (2) more explicit grounding on the input information to make our approach even more interpretable and controllable.   in figure 4 and 5, we visualize the correlation between supporting evidence and answer prediction performances for comparison and bridge question types, respectively. to obtain these plots, we first split the examples into 10 buckets where n-th bucket contains the examples with support-f1 score in (10 * (n \u2212 1), 10 * n] percentile for n = {1, 2, . . . , 10}. then, we take the average answer prediction accuracy (both em and f1) over these examples for each bucket, and report this number on the y-axis of the plot at the corresponding support-f1 bucket on the x-axis, while dropping the empty buckets. note that x = 0 corresponds to examples with support-f1 score of 0. also note that the size of a data point on the figure reflects the number of examples in the corresponding bucket as also indicated by the legend. from figures 4 and 5, we can observe that the accuracy of the generated answers is significantly lower, 30% for bridge and 10% for comparison, for the first bucket with zero support-f1 compared to buckets with positive support-f1 score. this suggests that the model has a difficult time figuring out the an-swer when the supporting evidence prediction is poor. another observation that holds for both categories is the general trend of increased answer quality as the supporting fact prediction improves. combining these two points provide additional evidence (in addition to table 2 in the main paper) implicitly supporting the answer generation process of pathfid being grounded on the generated supporting facts, which is generated as the prefix of the answer segment in the full decoded reasoning path sequence during inference. ", "Solved": NaN, "Unsolved": NaN}, "258999803": {"num": 593, "type": 1, "title": "Explanation Graph Generation via Generative Pre-training over Synthetic Graphs", "pdfurl": "https://export.arxiv.org/pdf/2306.00652v1.pdf", "conclusion": "in this paper, we propose a pre-training framework eg 3 p for a structured explanation generation task. distinct from existing pre-training tasks based on natural language text, eg 3 p focuses more on training mapping between natural language and graphs. meanwhile, due to the high cost of manually tagging, we construct queries from the synthetic graph automatically to get a large-scale corpus to support the pre-training process. using explagraph as a main benchmark, experimental results show that eg 3 p could significantly improve the ability of the model to generate explanations. in addition, on the other dataset, the results of the model after pre-training also showed a considerable improvement. our approach offers a new possibility for addressing the challenges of limited labeled data in natural language processing tasks. in the future, the ability of the model to generate explanation graphs will benefit from more datasets released with labels and more and more objective evaluation indicators put forward. additionally, while our current approach processes graphs as strings, utilizing a model architecture that is more suitable for graph generation may further enhance the model's graph generation ability.", "Solved": NaN, "Unsolved": NaN}, "248665596": {"num": 594, "type": 1, "title": "From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective", "pdfurl": "https://arxiv.org/pdf/2205.04733v2.pdf", "conclusion": "in this paper, we have built on the splade model, and studied to which extent it is able to take advantage of training improvements like distillation and hard negative mining, as well as better suited plm initialization: combined altogether, the resulting model reaches state-of-the-art performance on both in-domain and zeroshot evaluation. we also investigated the link between effectiveness and efficiency -induced by the degree of regularization -highlighting that more expressive models are better at generalization. ", "Solved": NaN, "Unsolved": NaN}, "235097600": {"num": 595, "type": 1, "title": "TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration", "pdfurl": NaN, "conclusion": "the 2021 edition of the shared task on multi-hop inference for explanation regeneration was a success, with 4 participating teams each substantially improving performance over the baseline model. the best performing team, deepblueai, produced a system that improves absolute performance by 32%, up to 0.820 ndcg, bringing overall state-ofthe-art performance at this relevancy ranking aspect of multi-hop inference to a moderate level. we hope that future systems for many-hop multi-hop inference that aim to build large detailed explanations for question answering will be able to leverage these results to build strong relevancy retrieval subcomponents to augment their compositional inference algorithms. award #1815948, \"explainable natural language inference\"). this edition of the shared task would not have been possible without the hard work of a number of relevance annotators, and their generous offer to anonymously use their data while their work is under review. a special thanks to andr\u00e9 freitas for the helpful discussions. additionally, we would like to thank the computational shared facility of the university of manchester for providing the infrastructure to run our experiments.", "Solved": NaN, "Unsolved": NaN}, "238419458": {"num": 596, "type": 1, "title": "TOWARDS CONTINUAL KNOWLEDGE LEARNING OF LANGUAGE MODELS", "pdfurl": "https://arxiv.org/pdf/2110.03215v4.pdf", "conclusion": "in this paper, we propose continual knowledge learning (ckl), where we establish benchmark datasets and metrics, and explore methodologies towards continual knowledge learning of an ever-changing lm. we find that parameter-expansion methods show the most robust performance throughout all of the experimental settings, which nevertheless has severe memory inefficiency and that seeing the same data often is a critical cause of forgetting. we also discuss several other interesting results of which we leave further exploration to future studies. to this end, we suggest the community to explore ckl for the better design of an ever-changing lm. ", "Solved": NaN, "Unsolved": NaN}, "258588167": {"num": 597, "type": 1, "title": "Unsupervised Dense Re-trieval Training with Web Anchors", "pdfurl": "https://export.arxiv.org/pdf/2305.05834v1.pdf", "conclusion": "we train an unsupervised dense retrieval model, anchor-dr, leveraging the rich web anchors. in particular, we design a contrastive learning task: anchor-document prediction to continuously pretrain anchor-dr. additionally, we apply predefined rules and train a query classifier to filter out uninformative anchors. experiments on two public datasets: msmarco and beir show that anchor-dr significantly outperforms the state-of-the-art dense retrievers on unsupervised retrieval. our analyses provide a further comparison of the patterns of information contained in our contrastive learning pairs and query-document pairs in test datasets.", "Solved": NaN, "Unsolved": NaN}, "247594506": {"num": 598, "type": 1, "title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language", "pdfurl": "https://www.aclanthology.org/2022.acl-long.77.pdf", "conclusion": "in this paper, we proposed fairr, a faithful and robust deductive reasoning model based on three modular components: rule selection, fact selection, and knowledge composition. fairr ensures causality from proof generation to entailment prediction by design. we established the effectiveness of our approach through experiments on testing robustness to language variations and demonstrating the interpretability of the errors made by our model. we also show that fairr is faster and more precise at deductive reasoning than prior baselines.  ", "Solved": NaN, "Unsolved": NaN}, "250729995": {"num": 599, "type": 1, "title": "ScienceQA: a novel resource for question answering on scholarly articles", "pdfurl": NaN, "conclusion": "in this paper, we present scienceqa, a novel dataset for benchmark evaluation of methods in the mrc (qa and qg in particular) task on scholarly articles. the dataset is created semi-automatically, consisting of over 100k triples of context-question-answer. the developed qa system could provide valuable evidence in managing the vast number of scholarly submissions. we offer a baseline and two more models, viz., (i). vanilla bert, (ii). science bert (i.e., scib-ert), and (iii). combination of bert and bi-daf. our proposed models are competitive compared to the existing state-of-the-art models. our future works would include:", "Solved": NaN, "Unsolved": NaN}, "248780313": {"num": 600, "type": 1, "title": "Predicting Difficulty and Discrimination of Natural Language Questions", "pdfurl": "https://www.aclanthology.org/2022.acl-short.15.pdf", "conclusion": "in this paper, we explored qa datasets through the lens of item response theory. we have demonstrated a way to build regression models that can describe the difficulty and discrimination of a question. we note that our work is limited in two important ways: firstly, we only use the dfgn model in our artificial crowd, which may have introduced a bias in which some factors that make questions difficult/discriminatory are only applicable to this model. secondly, we only explore the hotpotqa dataset, which may further limit our analysis to only be applicable to hotpotqa or similar datasets. future work could incorporate multiple models and datasets to explore a more easily generalizable difficulty/discrimination prediction pipeline. we also note that our analysis here focused on qa. however, there are many nlp tasks in which the difficulty or discrimination of an item may be important. our work here could naturally extend to these domains. finally, automatically predicting these traits without relying on user responses can engender a host of creative educational applications. future work can also leverage such predictive models to explore more efficient strategies for learning and evaluation.", "Solved": NaN, "Unsolved": NaN}, "258987552": {"num": 601, "type": 1, "title": "UKP-SQuARE: An Interactive Tool for Teaching Question Answering", "pdfurl": "https://www.aclanthology.org/2023.bea-1.17.pdf", "conclusion": "in this paper, we present a novel method to teach question-answering to postgraduate nlp students following the learner-centered method of flipped classrooms. we propose to provide reading materials to the students before the class and use the ukp-square platform as a driving tool to conduct the class. this platform integrates the most popular qa pipelines and an ecosystem of tools to analyze the available models. these tools include explainability methods, behavioral tests, adversarial attacks, and graph visualizations. we provide a series of use cases for teaching based on the provided models and methods by ukp-square, showing that classes can become much more interactive by using ukp-square than in conventional lectures.", "Solved": NaN, "Unsolved": NaN}, "252818979": {"num": 602, "type": 1, "title": "Hierarchical Representation-based Dynamic Reasoning Network for Biomedical Question Answering", "pdfurl": "https://www.aclanthology.org/2022.coling-1.127.pdf", "conclusion": "this paper proposes hdrn, a novel model for representation learning and reasoning for biomedical question answering. first, we construct hierarchical representations to obtain a deep understanding of the biomedical evidences. then, we perform multi-step dynamic reasoning to solve complex biomedical questions. we evaluate our model on three bioqa datasets and achieve new state-of-theart performances.", "Solved": NaN, "Unsolved": NaN}, "219401765": {"num": 603, "type": 1, "title": "GMAT: Global Memory Augmentation for Transformers", "pdfurl": "https://arxiv.org/pdf/2006.03274v1.pdf", "conclusion": "in this work, we proposed gmat, a simple extension to the transformer architecture that allows a better trade-off between compute and performance and can be naturally used for sequence compression. our approach can be seamlessly integrated with the increasingly-popular sparse transformer variants.", "Solved": NaN, "Unsolved": NaN}, "263620134": {"num": 604, "type": 1, "title": "RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE MODELS", "pdfurl": "https://export.arxiv.org/pdf/2310.03025v1.pdf", "conclusion": "in this work, we systematically study the retrieval-augmentation versus long context extension using the state-of-the-art llms after instruction tuning for various long context qa and querybased summarization tasks.after study, we have the following interesting findings: i) retrieval largely boosts the performance of both 4k short context llm and 16k/32k long context llms.", "Solved": NaN, "Unsolved": NaN}, "252624423": {"num": 605, "type": 1, "title": "eRock at Qur'an QA 2022: Contemporary Deep Neural Networks for Qur'an based Reading Comprehension Question Answers", "pdfurl": "https://www.aclanthology.org/2022.osact-1.11.pdf", "conclusion": "we attempted to solve quranqa shared task using bert (devlin et al., 2018) from scratch as well as fine-tuned over two different pre-trained variants. moreover we opted for data augmentation and weight-decay regularization techniques to improve performance over the task.", "Solved": NaN, "Unsolved": NaN}, "247158312": {"num": 606, "type": 1, "title": "Combining Modular Skills in Multitask Learning", "pdfurl": "https://arxiv.org/pdf/2202.13914v2.pdf", "conclusion": "in this work, we argued that a modular design is crucial to ensure that neural networks can learn from a few examples and generalise robustly across tasks by recombining autonomous facets of knowledge. to this end, we proposed a model where a subset of latent, discrete skills from a fixed inventory is allocated to each task in an end-to-end fashion. the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters. we evaluate the sample efficiency of our model on multitask instruction following through reinforcement learning and its few-shot adaptability on multitask text-to-text generation through supervised learning. in both experiments, we surpass competitive baselines where parameters are fully shared, task-specific, combined according to expert knowledge, or generated conditionally on the task. finally, we show that our model facilitates interpretability by learning an explicit hierarchy of tasks based on the skills they require. ", "Solved": NaN, "Unsolved": NaN}, "264451629": {"num": 607, "type": 1, "title": "Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs", "pdfurl": "https://export.arxiv.org/pdf/2310.16605v2.pdf", "conclusion": "in this paper, we propose web-dro, which is an efficient grouplevel data clustering and reweighting strategy for unsupervised dense retrieval model training.we use structures of the web graph to train a link prediction style embedding model for clustering and utilize groupdro to reweight the clusters during training.experiments on ms marco and beir show that web-dro achieves significant improvements over baseline models, especially on difficult tasks.further analysis shows that group weights learned by", "Solved": NaN, "Unsolved": NaN}, "263605809": {"num": 608, "type": 1, "title": "MERGE, THEN COMPRESS: DEMYSTIFY EFFICIENT SMOE WITH HINTS FROM ITS ROUTING POLICY", "pdfurl": "https://export.arxiv.org/pdf/2310.01334v1.pdf", "conclusion": "sparse mixture-of-experts (smoe) is a promising framework to scale up the model capacity, which enjoys roughly unchanged training and inference flops at the cost of significantly increased memory overheads.the memory requirements and expert redundancy highly limit its practical usage.", "Solved": NaN, "Unsolved": NaN}, "257642038": {"num": 609, "type": 1, "title": "Parameter-Efficient Sparse Retrievers and Rerankers using Adapters", "pdfurl": "https://export.arxiv.org/pdf/2303.13220v1.pdf", "conclusion": "retrieval models, based on plm, require finetuning millions of parameters which makes them memory inefficient and non-scalable for out-of-domain adaptation. this motivates the need for efficient methods to adapt them to information retrieval tasks. in this paper, we examine adapters for sparse retrieval models. we show that with approximately 2% of training parameters, adapters can be successfully employed for splade models with comparable or even better effectiveness on benchmark ir datasets such as ms marco and trec. we further analyze adapter layer ablation and see a further reduction in training parameters to 1.8% retains effectiveness of full finetuning. for domain adaptation, adapters are more stable and outperform finetuning, which is prone to overfitting, on tripclick dataset, adapters outperform on precision metrics torso and tail queries and performs comparably on head queries. we explore knowledge transfer between first stage rankers and rerankers as a final study. adapters underperform full finetuning when trying to reuse sparse model to rerankers. dense first stage rankers perform similarly for adapters and finetuning while sparse first stage rankers is less effective compared to finetuning. we leave this as future work. as memory-efficient adapters are effective for splade, we leave for future studying larger sparse models and their generalizability. finally, an interesting scenario could also be to tackle unsupervised domain adaptation with adapters.", "Solved": NaN, "Unsolved": NaN}, "254926391": {"num": 610, "type": 1, "title": "Multi-hop Evidence Retrieval for Cross-document Relation Extraction", "pdfurl": "https://export.arxiv.org/pdf/2212.10786v2.pdf", "conclusion": "we study efficient and effective ways to extract multi-hop evidence for cross-document re and propose mr.cod. mr.cod extracts evidence paths from an open set of documents and ranks them with adapted dense retrievers as scorers. to overcome the gap between retrieval in odqa and evidence retrieval for re, we develop a contextual dpr that augments sparse queries with passage context. extensive experiments show high-quality evidence retrieved by mr.cod boosts end-to-end cross-document re performance. future works include extending mr.cod to more retrieval methods, such as generative dense retrievers.", "Solved": NaN, "Unsolved": NaN}, "212747830": {"num": 611, "type": 0, "title": "Invited Review . Pre-trained Models for Natural Language Processing: A Survey", "pdfurl": "https://arxiv.org/pdf/2003.08271v4.pdf", "conclusion": "in this survey, we conduct a comprehensive overview of ptms for nlp, including background knowledge, model architecture, pre-training tasks, various extensions, adaption approaches, related resources, and applications. based on current ptms, we propose a new taxonomy of ptms from four different perspectives. we also suggest several possible future research directions for ptms.", "Solved": NaN, "Unsolved": NaN}, "218502712": {"num": 612, "type": 1, "title": "Query Reformulation using Query History for Passage Retrieval in Conversational Search", "pdfurl": "https://arxiv.org/pdf/2005.02230v1.pdf", "conclusion": "we present hqe and ntr, both conversational query reformulation methods stacked on a successful multi-stage ir pipeline. the effectiveness of our methods are attested by experiments on the cast benchmark dataset, the results of which suggest that the two methods have different advantages in fusing context information into conversational user utterances for downstream ir models. finally, this work elevates the state of the art in cast benchmarks and provides simple but effectives baselines for future research.", "Solved": NaN, "Unsolved": NaN}, "253255229": {"num": 613, "type": 1, "title": "MULTI-VECTOR RETRIEVAL AS SPARSE ALIGNMENT", "pdfurl": "https://export.arxiv.org/pdf/2211.01267v1.pdf", "conclusion": "in this paper, we introduce aligner, a novel sparse alignment method for multi-vector document retrieval. we first formulate different retrieval models with token-level sparse alignments and propose aligner to tackle the limitations of existing models. specifically, aligner uses pairwise alignments and unary saliences that allow us to adapt to different tasks and prune unimportant tokens, respectively. as a result, we achieve strong performance on both zero-shot and few-shot document retrieval tasks while drastically improving the run-time and storage complexity of multi-vector retrieval. with its interpretable alignments and better performance with large language models, we envision that our multi-vector retrieval model can serve as a strong standalone retriever in the future.", "Solved": NaN, "Unsolved": NaN}, "218973757": {"num": 614, "type": 1, "title": "Why Attention is Not Explanation: Surgical Intervention and Causal Reasoning about Neural Models", "pdfurl": "https://www.aclweb.org/anthology/2020.lrec-1.220.pdf", "conclusion": "while our study of explanation is based in philosophy, our contributions are based in epistemology, not in ethics. many adjacent subfields of philosophy of science exist and only occasional interactions between computer scientists and philosophers have taken place to date (miller, 2018;zerilli et al., 2018). in this work, we have examined whether researchers or users are being given a true explanation. but a good explanation does not mean that an algorithm has made a good decision. explainability research frequently studies tasks with high stakes, including notoriously biased tasks like recidivism prediction, financial risk modeling, and facial recognition for surveillance. our work does not absolve researchers from a broader social responsibility: the presence of a successful explanation will not help if a loan is denied because of race (fuster et al., 2018), if an accused criminal is wrongly identified because of their gender presentation (buolamwini and gebru, 2018), or if algorithms persecute ethnic groups (wang et al., 2016) or misdiagnose mental health (bennett and keyes, 2019).to build algorithmic decision-making in a truly socially responsible way, our work must be a component piece, incorporated into a broader foundation that accounts not only for explanation but also for ethical software development. this work provides a vocabulary for computer scientists struggling to unify the informal language that proliferates across research today, and will allow nlp researchers to improve the quality and rigor of their explanations, and provide a sure footing for the field.", "Solved": NaN, "Unsolved": NaN}, "235368003": {"num": 615, "type": 1, "title": "Adversarial Training for Machine Reading Comprehension with Virtual Embeddings", "pdfurl": "https://www.aclanthology.org/2021.starsem-1.30.pdf", "conclusion": "we have applied adversarial training on a wide range of mrc tasks, including span-based extractive rc and multiple-choice rc. especially, we have proposed a novel adversarial training method pqat, which uses virtual p/q-embedding matrices to generate global and role-aware perturbations that consider the characteristics of mrc tasks. our experiments demonstrate that adversarial training improves the mrc model performance universally and consistently, even over the strong pre-trained model baseline. the pqat method further improves the model performance over the standard at on both normal datasets and adversarial datasets.", "Solved": NaN, "Unsolved": NaN}, "237940293": {"num": 616, "type": 1, "title": "Sorting through the noise: Testing robustness of information processing in pre-trained language models", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.119.pdf", "conclusion": "we have presented results manipulating inputs of pre-trained lms, to test the ability of such models to represent and retain information conveyed by input text. our results show that though models may appear to handle information correctly in simple settings, these correct predictions are easily broken by insertion of distracting material in the context. systematic manipulation of the distracting content further indicates key roles for semantic similarity and relative word position in models' selection of relevant contextual cues for prediction. overall, the results suggest that lm predictions are driven more by coarse-grained superficial cues than by extraction of robust meaning information from context. the results serve as a reality check for considerations of the extent to which lms \"understand\" their input, and lay groundwork to understand the mechanisms that do drive predictions in these models.   ", "Solved": NaN, "Unsolved": NaN}, "249431623": {"num": 617, "type": 1, "title": "No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2206.02873v5.pdf", "conclusion": "in this work we studied how distillation and parameter count influence the zero-shot effectiveness of neural retrievers. we begin by showing that in-domain effectiveness, i.e., when retrievers are finetuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al. [14] and gupta et al. [9] and zhan et al [46]. furthermore, we show that a distilled reranker has better zeroshot effectiveness than much larger non-distilled rerankers, which is an important and desirable feature of deployed models. however, our largest reranker significantly outperforms smaller rerankers and achieves a new state of the art across almost all datasets used in our zero-shot experiments. this suggests that a large number of parameters may play a significant role in the generalization capability of pretrained language models.", "Solved": NaN, "Unsolved": NaN}, "264820296": {"num": 618, "type": 1, "title": "M 4 LE: A MULTI-ABILITY MULTI-RANGE MULTI-TASK MULTI-DOMAIN LONG-CONTEXT EVALUATION BENCHMARK FOR LARGE LANGUAGE MODELS", "pdfurl": "https://export.arxiv.org/pdf/2310.19240v1.pdf", "conclusion": "in this paper, we propose a benchmark m 4 le for llms assessing their capability of long-context understanding.to establish a benchmark with diverse nlp tasks, rather than just those that are inherently lengthy, we propose a systematic method to convert short nlp task instances into long context inputs, encompassing five distinct abilities.we collect and construct in total of 36 tasks from different sources and domains covering multiple length ranges to maximize the diversity of the tasks in benchmark, with our customized construction methods which enable flexibility to extend arbitrary context lengths.we evaluate 11 well-known llms with our benchmark and find that current models struggle to understand long-context inputs and the corresponding performance related to ability types, data used when fine-tuning and positions of the relevant information.a.1.1 mnds news mnds news (petukhova & fachada, 2023) is an english hierarchical news category classification dataset comprising 10,917 news articles from 260 sources.we only use the 17 first-level categories as the labels for this study.for multiple retrieval tasks, we randomly sample a class label that appears in the instance.", "Solved": NaN, "Unsolved": NaN}, "259096138": {"num": 619, "type": 1, "title": "Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation", "pdfurl": "https://export.arxiv.org/pdf/2306.04101v1.pdf", "conclusion": "in this work, we propose to incorporate the cloze task to improve neural machine question answering with a few training examples. the key idea is to identify and mask the informative entities in the passage and make the model predict them correctly. through empirical experimental studies on various qa benchmarks and different few-shot settings, we show that the cloze task indeed benefits the qa task due to its commonalities. we find different ways of incorporating the cloze task improve the qa task while prompt-tuning brings the most. looking forward, it is of interest to explore qa-dedicated pre-training and ways of pipelining pretraining and prompt-tuning for downstream few-shot qa needs.", "Solved": NaN, "Unsolved": NaN}, "220483148": {"num": 620, "type": 1, "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning", "pdfurl": "https://arxiv.org/pdf/2007.08124v1.pdf", "conclusion": "we have presented logiqa, a large-scale logical reasoning reading comprehension dataset. in addition to testing reasoning capacities of machine reading, our dataset can also serve as a benchmark for re-examining the long pursued research of logical ai in the deep learning nlp era. results show that the state-of-the-art machine readers still fall far behind human performance, making our dataset one of the most challenging test for reading comprehension.", "Solved": NaN, "Unsolved": NaN}, "233219660": {"num": 621, "type": 1, "title": "SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.364.pdf", "conclusion": "spatial reasoning is an important problem in natural language understanding. we propose the first human-created qa benchmark on spatial reasoning, and experiments show that state-of-the-art pretrained language models (lm) do not have the capability to solve this task given limited training data, while humans can solve those spatial reasoning questions reliably. to improve lms' capability on this task, we propose to use hand-crafted grammar and spatial reasoning rules to automatically generate a large corpus of spatial descriptions and corresponding question-answer annotations; further pretraining lms on this distant supervision dataset significantly enhances their spatial language understanding and reasoning. we also show that a spatially-improved lm can have better results on two extrinsic datasets (babi and boolq).  table 7 shows the templates used to create questions in spartqa-auto. the \"<object>\" is a variable replaced by objects from the story (using choose-objects and describe-objects modules), and the \"<relation>\" variable can be replaced by the chosen relations between objects (using findall-relations module). the articles and the indefinite pronouns in each template play an essential role in understanding the question's objective. for example, \"are all blue circles near to a triangle?\" is different from \"are there any blue circles near to a triangle?\", and \"are there any blue circles near to all triangles?\". therefore, we check the uniqueness of the object definition, using \"a\" or \"the\" in proper places and randomly place the terms \"any\" or \"all\" in the yn questions to generate different questions. table 8 shows the percentage of correct labels in train and test sets. in multi-choice q-types, more than one label can be true. table 10 shows some generated sentences in spartqa-auto with some specific features that challenge models to understand different forms of relation description in spatial language.", "Solved": NaN, "Unsolved": NaN}, "266163790": {"num": 622, "type": 1, "title": "Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering", "pdfurl": "https://aclanthology.org/2023.emnlp-main.803.pdf", "conclusion": "in this paper, we attempt to improve the robustness of qa models by testing time adaptation (tta) but find that tta causes the models collapse.we thoroughly investigate why previous tta methods cause the model collapse and find that the imbalanced label distribution is the main reason.we address this problem by adding constraints between the source and adapted model during the tta process.we also design an efficient side block to speed up the inference time.sufficient experimental results show that our proposed method is effective and efficient, making tta a big step closer to being applied in real-world scenarios.", "Solved": NaN, "Unsolved": NaN}, "239885904": {"num": 623, "type": 1, "title": "Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable", "pdfurl": "https://arxiv.org/pdf/2110.13472v1.pdf", "conclusion": "we propose a three-stage framework of relation extractor-reader and comparator (rerc), which solves the multi-hop qa task through the idea of complex question decomposition, and obtains the state-of-the-art results in the 2wikimultihopqa dataset, which is close to human performance. our rerc framework can also provide faithful evidence with excellent interpretability.", "Solved": NaN, "Unsolved": NaN}, "247518803": {"num": 624, "type": 1, "title": "Synthetic Question Value Estimation for Domain Adaptation of Question Answering", "pdfurl": "https://www.aclanthology.org/2022.acl-long.95.pdf", "conclusion": "we propose a question value estimator to estimate the usefulness of synthetic questions and select useful ones for improving target-domain qa train- 7 we treat it as a binary classification problem here: if a question is selected, the prediction is 1; 0 otherwise.", "Solved": NaN, "Unsolved": NaN}, "249062996": {"num": 625, "type": 1, "title": "LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA", "pdfurl": "https://arxiv.org/pdf/2205.12650v1.pdf", "conclusion": "this work introduces lepus, a method to perform unsupervised re-ranking of multi-document paths for question answering based on large language models. given a question, the document path is encoded into a prompt and the document path is scored as the probability of generating the question given the prompt. experiments on a standard multihop qa benchmark show the strong performance of lepus in the zero-shot setting, displaying comparable performance to fully-supervised retrievers. we also analyze our approach showing the utility of using multi-hop prompts as opposed to singlehop ones. lastly, our work shows that language models can indeed function as strong unsupervised re-rankers for multi-hop question answering.", "Solved": NaN, "Unsolved": NaN}, "258436694": {"num": 626, "type": 1, "title": "Explainable Conversational Question Answering over Heterogeneous Sources via Iterative Graph Neural Networks", "pdfurl": "https://export.arxiv.org/pdf/2305.01548v2.pdf", "conclusion": "there are three main takeaways for a broader audience from this work. first, at a time when large language models (llms) like chat-gpt are used as a one-stop shop for most nlp tasks including convqa, our method explaignn stands out by providing traceable provenance of its answer predictions. next, explainability for graph neural networks is an unsolved concern: we propose an iterative model that sequentially reduces the graph size as a medium for offering causal insights into the prediction process. finally, for several systems in ir and nlp, performance, efficiency, and explainability are seen as trade-offs. through our highly configurable solution, we show that in certain use cases, it is actually possible to find configurations that lie at the sweet spots of all the three factors. a natural future work would be to generalize these insights to other problems, like graph-based neural recommendation.", "Solved": NaN, "Unsolved": NaN}, "248780378": {"num": 627, "type": 1, "title": "Query Generation with External Knowledge for Dense Retrieval", "pdfurl": "https://www.aclanthology.org/2022.deelio-1.3.pdf", "conclusion": "we presented a novel query generation method, qgek, that generates synthetic queries in a form more similar to human labeled queries by using external knowledge. in order to use unprocessed external knowledge, we convert a query into a tripletbased template, which can include information of subjects and answers. remarkably, when dense retrieval models are trained with the queries generated from qgek, the performance has improved much compared to using the queries without external knowledge. also, we have shown that including external knowledge give rises to the distribution of the unique words similar to that of the human labeled queries. we believe that qgek can also be applied to the other generation methods by orthogonally adding some external knowledge processing modules. for future work, we plan to generate queries both close to human labeled ones and optimized for ir tasks and to allow the template to accept more general logical forms for diverse highquality queries. the code and data will be made available for public access.", "Solved": NaN, "Unsolved": NaN}, "234335834": {"num": 628, "type": 1, "title": "Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.127.pdf", "conclusion": "in this paper, we focus on the task of logical reasoning of text. following a three-step logical reasoning paradigm, we first propose a neuro-symbolic logicdriven context extension framework. it identifies logical expressions as elementary units of logical inference and symbolically deduces the implicitly mentioned expressions, and verbalizes them as an extended context into a pre-trained model to match the answer. we also introduce a logic-driven data augmentation algorithm, which augments literally similar but logically different instances and employs contrastive learning to help our model better capture logical information. experimental results confirm the general effectiveness of our lreasoner, and it even surpasses human performance on the reclor dataset. in the future, we will explore to model different logical reasoning types and directly incorporate symbolic logic into the model structure.", "Solved": NaN, "Unsolved": NaN}, "214802013": {"num": 629, "type": 1, "title": "Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games", "pdfurl": "https://arxiv.org/pdf/2004.02393v1.pdf", "conclusion": "in this paper we propose the problem of recovering reasoning chains in multi-hop qa from weak supervision signals. our model adopts an cooperative game approach where a ranker and a reasoner cooperate to select the most confident chains. experiments on the hotpotqa and medhop benchmarks show the effectiveness of the proposed approach. ", "Solved": NaN, "Unsolved": NaN}, "244954717": {"num": 630, "type": 1, "title": "Does Structure Matter? Leveraging Data-to-Text Generation for Answering Complex Information Needs", "pdfurl": "https://arxiv.org/pdf/2112.04344v1.pdf", "conclusion": "traditionally, ir approaches solving complex information needs focused on leveraging multi-turn interactions to provide optimal rankings of candidate documents at each turn. in this paper we have suggested alternative retrieval models that do not rely on the interactive updating of queries and document rankings as answers. we suggest one such alternative approach can be found using datato-text generation models to generate in a single-turn, a natural language and structured answer. experimental evaluation of a planning-based dtt model using the trec car dataset shows the potential of our intuition. we believe that our work opens up novel areas of investigation including answer generation and explanation in conversational systems for ir.", "Solved": NaN, "Unsolved": NaN}, "259096157": {"num": 631, "type": 1, "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations", "pdfurl": "https://export.arxiv.org/pdf/2306.04618v2.pdf", "conclusion": "we revisit ood robustness research in nlp, identifying deficiencies in benchmarks and evaluation.correspondingly, a benchmark construction protocol and an ood robustness evaluation suite are proposed to facilitate future research.the correlation between ood and id performance, the effectiveness of existing methods, and the challenges faced by llms are investigated.", "Solved": NaN, "Unsolved": NaN}, "248987232": {"num": 632, "type": 1, "title": "Domain Adaptation for Memory-Efficient Dense Retrieval", "pdfurl": "https://arxiv.org/pdf/2205.11498v1.pdf", "conclusion": "supervised dense compression algorithms have been popular and effective in-domain in recent times, however can have difficulties to generalize well to unseen domains. the algorithms are memory efficient, but lack in performance when evaluated in specialized domains which contain no training data. in order to adapt these compression algorithms under severe domain shifts, in this paper we propose a solution to jointly optimize domainadaption algorithms along with vector compression. the recent technique, gpl in combination with bpr and jpq provide a boost of 19.3 and 11.6 ndcg@10 points respectively.", "Solved": NaN, "Unsolved": NaN}, "236428903": {"num": 633, "type": 1, "title": "Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy", "pdfurl": "https://arxiv.org/pdf/2107.11823v1.pdf", "conclusion": "this paper proposes a novel \"select-to-guide\" model (s2g) for multi-hop reading comprehension in more effective and convenient way. as an alternative of the existing graph modeling, the proposed graph-free s2g model consists of an evidence paragraph retrieval module which selects evidence paragraphs in a step-by-step multihop manner, and a multi-task module that simultaneously extracts evidence sentences and answer spans.", "Solved": NaN, "Unsolved": NaN}, "264306280": {"num": 634, "type": 1, "title": "Knowledge-Augmented Language Model Verification", "pdfurl": "https://export.arxiv.org/pdf/2310.12836v1.pdf", "conclusion": "in this work, we proposed knowledge-augmented language model verification (kalmv), which identifies not only the relevance of the retrieved knowledge to the input query but also the faithfulness of the reflection of knowledge in the generated answers, in order to prevent incorrect answer generations with knowledge-augmented lms.to this end, we developed a verifier that can detect errors in both the knowledge retrieval and answer generation stages by instruction-finetuning lms.further, during inference, we proposed to rectify errors by re-retrieving knowledge and re-generating answers if our kalmv detects errors, and also perform an ensemble over multiple verification outputs from different instructions, to improve the efficacy of the verifier.we validated kalmv on two question answering tasks and showed its effectiveness in significantly reducing hallucinations.we believe that kalmv will bring substantial practical impact in improving the reliability of lm-based systems, especially since it is a plug-and-play module.", "Solved": NaN, "Unsolved": NaN}, "252819049": {"num": 635, "type": 1, "title": "To What Extent Do Natural Language Understanding Datasets Correlate to Logical Reasoning? A Method for Diagnosing Logical Reasoning", "pdfurl": "https://www.aclanthology.org/2022.coling-1.147.pdf", "conclusion": "in this work, we present a novel framework, which can diagnose the correlation between the nlu dataset and a specific skill and we probe a fundamental reasoning skill, logical reasoning, on 11 nlu datasets. our framework involves a logical probe to conduct diagnosis and defines a qualitative process and a quantitative process to calculate two indicators. from the results, we observe that 1) most nli datasets have a relatively strong correlation with logical reasoning. 2) the correlations between type 1 mrc datasets and logical reasoning are moderate because logical reasoning is not the only dominant skill in these datasets. 3) the dependences of type 2 mrc datasets are not always exactly consistent with their intended purpose. based on the analysis, although there are several limitations in our proposed method, this work is still a reasonable attempt to deeply understand the relationship between the dataset and a specific nlu skill. in future works, we will focus on: 1) exploring the solution to the limitations of the proposed method; 2) build associations for different datasets that require the same nlu capabilities.", "Solved": NaN, "Unsolved": NaN}, "236447339": {"num": 636, "type": 1, "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension", "pdfurl": "https://export.arxiv.org/pdf/2107.12708v2.pdf", "conclusion": "the number of qa/rc datasets produced by the nlp community is large and growing rapidly. we have presented the most extensive survey of the field to date, identifying the key dimensions along which the current datasets vary.", "Solved": NaN, "Unsolved": NaN}, "252819056": {"num": 637, "type": 1, "title": "Evaluating Diversity of Multiword Expressions in Annotated Text", "pdfurl": NaN, "conclusion": "in this paper, we borrowed the formalization of the notion of diversity from the literature in ecology. we focused on two out of the three main aspects of diversity, namely variety and balance. our contribution is to apply these measures to assess intralinguistic diversity, focusing on the particular phenomenon of multiword expressions. we not only formalize variety and balance measures in this context but we also put forward methods for selecting those variants of these measure which fit the nature of the mwe phenomenon. this validation methodology is based on corpus sampling with variable sample size. as a result, we retain richness and the e 2,1 evenness as the optimal variety and balance measures for mwes (among those studied by us). we apply these measures to the corpora and system results in the parseme shared task on automatic identification of mwes. the results show that richness of the correct annotations produced by the systems is roughly consistent with their f-measure performances. however, their balance is much less correlated with more traditional measures. we also display the limits of the richness and balance measures, when calculated on automatically annotated data, due to incorrect approximation of types under improper lemmatization in a morphologically rich language.", "Solved": NaN, "Unsolved": NaN}, "198229624": {"num": 638, "type": 1, "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "pdfurl": "https://www.aclweb.org/anthology/2020.tacl-1.5.pdf", "conclusion": "we presented a new method for span-based pretraining which extends bert by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. together, our pretraining process yields models that outperform all bert baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.", "Solved": NaN, "Unsolved": NaN}, "255186555": {"num": 639, "type": 1, "title": "DEMONSTRATE-SEARCH-PREDICT: Composing retrieval and language models for knowledge-intensive NLP", "pdfurl": "https://export.arxiv.org/pdf/2212.14024v2.pdf", "conclusion": "for a long time, the dominant paradigm for building models in ai has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration. however, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems.", "Solved": NaN, "Unsolved": NaN}, "258418300": {"num": 640, "type": 1, "title": "A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning", "pdfurl": "https://export.arxiv.org/pdf/2304.14856v1.pdf", "conclusion": "we have proposed ugr, a novel unified generative retriever, which can robustly serve different retrieval tasks for knowledge-intensive language tasks. to unify retrieval tasks, we formulated the retrieval problem as a conditional generation problem and introduced an n-gram-based identifier for relevant contexts at different levels of granularity. to learn different retrieval tasks with a single model, we mapped the descriptions of tasks to a few prompt tokens for keeping task specifications. empirical results on the kilt benchmark demonstrated the superiority of the proposed method. efficiently integrating knowledge from different retrieval tasks in ugr has the potential to save significant time and computational resources in both academic and industrial environments. however, ugr needs a complex scoring function to solve the identifier repetition problem; we encourage future work that explores other effective and efficient semantic identifiers for generative retrieval. beyond kilt, training a more general unified generative retrieval model to serve different retrieval applications under multiple corpora and modalities seems a promising future direction.", "Solved": NaN, "Unsolved": NaN}, "243986045": {"num": 641, "type": 1, "title": "Recent Advances in Automated Question Answering In Biomedical Domain", "pdfurl": "https://arxiv.org/pdf/2111.05937v1.pdf", "conclusion": "in this review, we focused on the recent advances in biomedical question answering. we provided a review of general domain question answering using knowledge bases, texts or both, before moving on to biomedical question answering systems. we explored current state of the art biomedical qa systems and discussed their limitations. we finally provided an overall analysis of limitations of bqa systems along with ways to overcome them. we finally explore the potential areas of focus for further research.", "Solved": NaN, "Unsolved": NaN}, "258461315": {"num": 642, "type": 1, "title": "Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks", "pdfurl": "https://export.arxiv.org/pdf/2304.14732v6.pdf", "conclusion": "in this paper, we point out the challenges that should be considered in introducing ir into llm from perspectives of reasoning and knowledge.then, we propose a novel framework called searchain for enabling ir and llm to interact with each other effectively.searchain not only stimulates the knowledge-reasoning ability of llm but also uses ir to provide the knowledge that llm really needs based on the external knowledge base, which improves accuracy and credibility.besides, searchain can mark references to supporting documents for the knowledge involved in the generated contents, which improves the traceability of the contents.in addition, the interaction between ir and llm in searchain transforms the topology of reasoning from chain to tree, which enables llm to dynamically modify reasoning direction.experimental results on complex knowledge-intensive tasks show searchain performs better than all baselines.in future work, we will consider how to improve the efficiency of the framework and how to introduce more tools to interact with llm for more tasks.", "Solved": NaN, "Unsolved": NaN}, "234776197": {"num": 643, "type": 1, "title": "If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.363.pdf", "conclusion": "we introduced a simple unsupervised approach for retrieving candidate evidence chains that after reranking achieves state-of-the-art evidence retrieval performance on two multi-hop qa datasets: qasc and multirc. we highlight the importance of generating and feeding candidate evidence chains by showing several benefits over the widely followed approach that retrieves evidence sentences individually. further, we introduced few attention and embedding analyses demonstrating that jointly retrieving and reranking chains assist in learning compositional information, which is also beneficial to the downstream qa task. overall, our work highlights the strengths and potential of joint retrieval+reranking approaches for future works.", "Solved": NaN, "Unsolved": NaN}, "235742855": {"num": 644, "type": 1, "title": "Probabilistic Graph Reasoning for Natural Proof Generation", "pdfurl": "https://arxiv.org/pdf/2107.02418v1.pdf", "conclusion": "in this work, we propose probr, a novel probabilistic graph reasoning framework for joint question answering and proof generation. probr defines a joint distribution over all possible answers and proofs, which can directly characterize the interaction between answers and proofs. experiments prove the effectiveness of proposed probr.        1. in all ablation experiments, probr achieved the best qa performance, demonstrating that probr can capture critical information for question answering in a variety of settings. however, since some of the dataset are artificially synthesized, it is difficult to guarantee that probr will work in the real dataset as well. we leave it as future work.", "Solved": NaN, "Unsolved": NaN}, "247518855": {"num": 645, "type": 1, "title": "AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2022.acl-long.494.pdf", "conclusion": "to meet the challenge of reasoning-based mrc, we presented a neural-symbolic approach where neural and symbolic reasoning mutually and iteratively reinforce each other via our new adalogn model. we also enhanced graph-based neural reasoning with a novel subgraph-to-node message passing mechanism. since these ideas are quite general, we believe they have great potential for a variety of applications beyond mrc, e.g., link prediction.", "Solved": NaN, "Unsolved": NaN}, "222133899": {"num": 646, "type": 1, "title": "Towards Interpretable Reasoning over Paragraph Effects in Situation", "pdfurl": "https://arxiv.org/pdf/2010.01272v1.pdf", "conclusion": "in this paper, we aim to answer ropes questions in an interpretable way by leveraging five neural network modules. these modules are trained in an end-to-end manner and each module provides transparent intermediate outputs. experimental results demonstrate the effectiveness of each module, and analysis on intermediate outputs presents good interpretability for the inference process in contrasted with \"black box\" models. moreover, we find that with explicitly designed compositional modeling of inference process, our approach with a few training examples achieves similar accuracy to strong baselines with full-size training data which indicates a better generalization capability. meanwhile, extending these models to a larger scope of question types or more complex scenarios is still a challenge, and we will further investigate the trade-off between explainability and scalability.   table 8 shows one labelled example, and the process of adding auxiliary supervision label contain the following steps: table 7: detailed parameters used in answer prediction, we provide search bounds for each hyperparameter and list out the hyperparameters combination for out best model and baseline model. other unmentioned parameters keep same as the one used in bert.", "Solved": NaN, "Unsolved": NaN}, "236486285": {"num": 647, "type": 1, "title": "Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering", "pdfurl": "https://www.aclanthology.org/2021.dialdoc-1.9.pdf", "conclusion": "in this paper, the application of transfer learning by utilizing transformer models like roberta, al-bert and electra for question answering span prediction task was explored. we also experimented with pretrained models on several other datasets prior to fine-tuning it on the dialdoc21 dataset, provided as part of the dialdoc21 shared task. maximum confidence score based ensemble techniques were employed to combine various base transformer models to further boost the per-formance. we plan to extend our approach and experiment with other ensembling techniques for further enhancing the performance and also explore avenues for improved scalability when applied to larger datasets.", "Solved": NaN, "Unsolved": NaN}, "243865235": {"num": 648, "type": 1, "title": "Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.303.pdf", "conclusion": "in this paper, we propose a diagnostic method to diagnose lms' fol reasoning ability. this method introduces a novel proposed benchmark, logicnli, that disentangles the fol reasoning from commonsense inference. specifically, it includes four evaluations to measure the fol reasoning ability from different perspectives. results on three lms show that although some lms (roberta) own a certain interpretable fol reasoning ability, they still cannot make sensible fol reasoning like humans. detailed analysis motivates us to enhance specific reasoning abilities or explore new methods to make neural models understand more refined logic.", "Solved": NaN, "Unsolved": NaN}, "207982228": {"num": 649, "type": 1, "title": "An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering", "pdfurl": "https://www.aclweb.org/anthology/D19-5829.pdf", "conclusion": "this paper describes experiments on various competitive pre-trained models (bert, xlnet), domain sampling strategies, negative sampling, data augmentation via back-translation, and active learning. we determine which of these strategies help and hurt multi-domain generalization, finding ultimately that some of the simplest techniques offer surprising improvements. the most significant benefits came from sampling no answer segments, which proved to be particularly important for training extractive models on long sequences. in combination these findings culminated in the second ranked submission on the mrqa-19 shared task.", "Solved": NaN, "Unsolved": NaN}, "215768725": {"num": 650, "type": 1, "title": "A Simple Yet Strong Pipeline for HotpotQA", "pdfurl": "https://www.aclweb.org/anthology/2020.emnlp-main.711.pdf", "conclusion": "our work shows that on the hotpotqa tasks, a simple pipeline model can do as well as or better than more complex solutions, such as graph networks, cross-document attention, or ner. powerful pre-trained models allow us to score sentences one at a time, without looking at other paragraphs. by operating jointly over these sentences chosen from multiple paragraphs, we arrive at answers and supporting sentences on par with state-of-theart approaches. this result shows that supporting sentence identification in hotpotqa is itself not a multi-hop problem, and suggests focusing on other multi-hop datasets to demonstrate the value of more complex retrieval techniques.", "Solved": NaN, "Unsolved": NaN}, "216868500": {"num": 651, "type": 1, "title": "Look at the First Sentence: Position Bias in Question Answering", "pdfurl": "https://arxiv.org/pdf/2004.14602v1.pdf", "conclusion": "most qa studies frequently utilize start and end positions of answers as training targets without much considerations. our study shows that most qa models fail to generalize over different positions when trained on datasets having answers in a specific position. we introduce several de-biasing methods to make models to ignore the spurious positional cues, and find out that the sentence-level answer prior is very useful. our findings also generalize to different positions and different datasets. one limitation of our approach is that our method and analysis are based on a single paragraph setting which should be extended to a multiple paragraph setting to be more practically useful.", "Solved": NaN, "Unsolved": NaN}, "258309779": {"num": 652, "type": 1, "title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought", "pdfurl": "https://export.arxiv.org/pdf/2304.13007v3.pdf", "conclusion": "this work introduces mcr for meta-reasoning over multiple reasoning chains.we evaluate mcr on 7 datasets for multi-hop qa that require both implicit and explicit reasoning in an open-domain setting and show that it outperforms previous approaches on all evaluation benchmarks.", "Solved": NaN, "Unsolved": NaN}, "254877343": {"num": 653, "type": 1, "title": "HYRR: Hybrid Infused Reranking for Passage Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2212.10528v1.pdf", "conclusion": "we proposed a generic training framework for rerankers based on a hybrid retriever. while the hybrid retriever is composed of term-based and neural models, the reranker is a neural cross-attention model which learns from negatives examples generated by the hybrid retriever. the proposed approach is robust and outperforms several strong baselines on ms marco passage ranking task and beir benchmark dataset, which demonstrates that it is practical and generalized. we observe that a model trained with robust training instances (in this case, from the hybrid retriever) produces a reranker that outperforms matched-training rerankers for termbased or neural retrievers.", "Solved": NaN, "Unsolved": NaN}, "235364000": {"num": 654, "type": 1, "title": "Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study", "pdfurl": "https://arxiv.org/pdf/2106.03826v1.pdf", "conclusion": "we conduct a comprehensive analysis on the book qa task, taking the representative narrativeqa dataset as an example. firstly, we design the book qa techniques by borrowing the wisdom from the cutting-edge open-domain qa research and demonstrate through extensive experiments that (1) evidence retrieval in book qa is difficult even with the state-of-the-art pre-trained lms, due to the factors of rich writing style, recurrent book plots and characters, and the requirement of high-level story understanding; (2) our proposed approaches that adapt pre-trained lms to books, especially the prereading technique for the reader training, are consistently helpful.", "Solved": NaN, "Unsolved": NaN}, "258959010": {"num": 655, "type": 1, "title": "Answering Unanswered Questions through Semantic Reformulations in Spoken QA", "pdfurl": "https://www.aclanthology.org/2023.acl-industry.70.pdf", "conclusion": "we tackled the problem of improving spoken qa, and analyzed questions from live data to identify key challenges that could be addressed with reformulation. based on this we proposed surf with novel linguistically-motivated reformulation operators to solve the identified challenges. offline experiments show the effectiveness of our novel root transformation and generalization operations, with up to 24% of unanswered questions being answered via reformulations with high answer relevance. live deployment in a leading voice assistant has positively impacted millions of requests.", "Solved": NaN, "Unsolved": NaN}, "247475874": {"num": 656, "type": 1, "title": "E-KAR : A Benchmark for Rationalizing Natural Language Analogical Reasoning", "pdfurl": "https://www.aclanthology.org/2022.findings-acl.311.pdf", "conclusion": "in this work, we propose a first-of-its-kind benchmark e-kar (in both chinese and english) for explainable analogical reasoning, which sets a concrete playground and evaluation benchmark to boost the development of human-like analogical reasoning algorithms. the e-kar benchmark is featured by its rich coverage in knowledge and welldesigned free-text explanations to rationalize the analogical reasoning process. preliminary experiments show that this benchmark provides a rather difficult challenge for prevailing language models.", "Solved": NaN, "Unsolved": NaN}, "263909028": {"num": 657, "type": 1, "title": "FORMALLY SPECIFYING THE HIGH-LEVEL BEHAVIOR OF LLM-BASED AGENTS", "pdfurl": "https://export.arxiv.org/pdf/2310.08535v1.pdf", "conclusion": "in this work, we introduced a framework for defining agents using ltl in a task-agnostic, domain general fashion.the ltl formulation of an agent can be used to construct a constrained decoder, which controls the high-level behavior of an agent at runtime.we demonstrated the utility of our framework with experiments on three datasets, where we found that the hard constraints imposed on generation by our approach can lead to an increase in agent performance.", "Solved": NaN, "Unsolved": NaN}, "245329570": {"num": 658, "type": 1, "title": "Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering", "pdfurl": "https://arxiv.org/pdf/2112.09658v1.pdf", "conclusion": "in this work, we propose a reasoning chain based adversarial attack for multi-hop qa. by formulating the multihop reasoning process with a reasoning chain, we can identify different reasoning types and customize adversary design for each type. our method allows to attack any certain hop by identifying different hop spans of the question, and making modification on relational words. the hop-targeted attack can inspect models' error-prone parts during the reasoning process specifically. three qa models under evaluation both exhibit poor performance in face of adversaries, suggesting that they are not robust enough and have limited interpretability of conducting multi-hop reasoning. our adversarial evaluation can be utilized to improve models' performance by adversarial retraining as well as motivate new model development according to the weakness detected.", "Solved": NaN, "Unsolved": NaN}, "245218982": {"num": 659, "type": 1, "title": "QuALITY: Question Answering with Long Input Texts, Yes!", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.391.pdf", "conclusion": "we introduce the long-document qa dataset quality. this dataset was crowdsourced and validated by humans to ensure that the questions are answerable, unambiguous, and challenging. the quality-hard subset, comprising half the dataset, consists of questions that are unanswerable by annotators working under tight time constraints, helping ensure that skimming and simple search do not yield high performance.", "Solved": NaN, "Unsolved": NaN}, "239998631": {"num": 660, "type": 1, "title": "How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.582.pdf", "conclusion": "our realfp dataset includes only one explanation program to a given fp whereas in practice, there can be multiple creative decompositions that lead to the correct answer. to encourage models that are capable of capturing this diversity in the output space, it would be interesting to (a) collect alternative solutions similar to say, image captioning datasets where it is the norm to train and evaluate against multiple ground truth candidates and (b) increasing the number of templates in the synthfp dataset, thereby biasing the model towards exploring multiple solutions by pre-training on a richer synthetic dataset. further, the work doesn't include other variants of fps -e.g. binary yes/no questions, comparisons, or fps involving probability and risk quantification. finally, note that our real-world dataset, by virtue of how it is collected, has a high us-centric bias, both in terms of cultural context and vocabulary.", "Solved": NaN, "Unsolved": NaN}, "104292014": {"num": 661, "type": 1, "title": "Quizbowl: The Case for Incremental Question Answering", "pdfurl": "https://arxiv.org/pdf/1904.04792v1.pdf", "conclusion": "this article introduces and argues for quizbowl: an incremental question answering task. solving quizbowl questions requires sophisticated nlp such as resolving complex coreference, multi-hop reasoning, and understanding the relationships between a gigantic menagerie of entities that could be answers. fundamental to answering quizbowl questions is that the questions are incremental; this is both fun and good for research. it is fun because it allows for live, engaging competitions between humans and computers. this format-the product of refining human question answering competitions over decades-is also good for research because it allows for fair, comprehensive comparison of systems and iterative improvement as systems answer questions earlier and earlier.", "Solved": NaN, "Unsolved": NaN}, "258959022": {"num": 662, "type": 1, "title": "Plug-and-Play Document Modules for Pre-trained Models 5 Beijing Key Laboratory of Big Data Management and Analysis Methods", "pdfurl": "https://www.aclanthology.org/2023.acl-long.875.pdf", "conclusion": "in this paper, we explore a new paradigm, which aims to represent documents as pluggable modules for ptms. in this setting, we can get rid of encoding the same document multiple times for different tasks. the extensive experiments prove that our proposed plugd can significantly reduce the computational cost and effectively inject document knowledge into ptms to improve performance. in the future, we will explore more effective plugin learning tasks and further attempt to represent knowledge graphs, and figures as plugins to provide knowledge for ptms.", "Solved": NaN, "Unsolved": NaN}, "254125744": {"num": 663, "type": 1, "title": "NIR-Prompt: A Multi-task Generalized Neural Information Retrieval Training Framework", "pdfurl": "https://export.arxiv.org/pdf/2212.00229v3.pdf", "conclusion": "in this paper, we point out that although there are some differences among the various information retrieval tasks, there are still essential matching signals shared by the various tasks, such as exact matching, semantic matching, and inference matching.if the model can capture and exploit these signals, the generalization ability of the model across tasks and domains will be improved.with this intuition, we propose a neural information retrieval training framework called nir-prompt consisting of essential matching module (emm) and matching description module (mdm) based on the idea of decoupling the process of signal capturing and signal combination.mdm uses the method of prompt learning to obtain the description of different tasks in the pre-trained language model.emm is trained on diverse mixed datasets and combined with the guidance from the task descriptions in mdm to capture essential matching signals and adapt these signals to different tasks.based on this, a generalized neural information retrieval pipeline consisting of retrieval and reranking is constructed.the experimental results on eighteen public datasets and a heterogeneous benchmark for testing the generalization ability of retrieval models show that our method yields better in-domain multi-task, out-of-domain multi-task, and new task adaptation performance for dense retrieval, reranking, and the entire neural information retrieval pipeline compared to the traditional fine-tuning paradigm.", "Solved": NaN, "Unsolved": NaN}, "258418354": {"num": 664, "type": 1, "title": "Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks", "pdfurl": "https://export.arxiv.org/pdf/2304.14732v1.pdf", "conclusion": "in this paper, we propose a novel method to improve the accuracy, credibility and traceability of large language models (llm) for complex knowledge-intensive tasks called searchain, which is a framework for deep interaction between llm and information retrieval (ir). in searchain, when llm faces complex questions, it constructs a chain called chain-of-query (coq), each node of the coq is an ir-oriented query and llm-generated answer. ir interacts with each node on the chain, judges whether the answer is correct, and provides llm with its unknown knowledge. llm generates a new coq according to the feedback of ir. ir interacts with llm for multiple rounds, gradually helping llm generate the correct coq, and finally solve the complex question. besides, the contents returned to the user include not only the final answer but also the reasoning process for the question, that is, the coq and the supporting documents retrieved by ir for each node of the coq, which improves the credibility and traceability of the contents generated by llm. experiments on multi-hop question answering datasets show that searchain not only has strong knowledge-reasoning ability when faced with complex questions but also can effectively exploit the interaction with ir to supplement and correct its knowledge, so as to improve the accuracy and credibility of the generated contents. besides, searchain effectively decouples the knowledge of llm and ir, which avoids the misleading of llm and can also accurately explain whether the knowledge involved in solving knowledge-intensive tasks comes from parameters or external ir.", "Solved": NaN, "Unsolved": NaN}, "245218995": {"num": 665, "type": 1, "title": "Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.275.pdf", "conclusion": "in this work, we introduce generative annotation assistants (gaas) and investigate their potential to aid crowdworkers with creating more effective training data more efficiently. we perform a thorough analysis of how gaas can be used for improving qa dataset annotation in different settings, including different generative model training data, sampling strategies, and whether to also provide annotators with answer suggestions. we find that gaas are beneficial in both the standard and adversarial data collection settings. in the standard data collection setting, and under the assumption of no access to adversarially-collected data, gaas with prompts sampled based on likelihood provide annotation speed-ups, while prompts sampled by adversarial performance or uncertainty metrics provide benefits to both the model error rates on the collected data as well as subsequent downstream qa performance. we find that while gaas are effective for improving standard data collection, we still do not approach the performance obtained when using adversarial data collection.", "Solved": NaN, "Unsolved": NaN}, "227231411": {"num": 666, "type": 1, "title": "Answer-driven Deep Question Generation based on Reinforcement Learning", "pdfurl": "https://www.aclweb.org/anthology/2020.coling-main.452.pdf", "conclusion": "deep question generation aims to generate complex questions that require reasoning over multiple pieces of information. in this paper, we propose an answer-driven end-to-end deep question generation model (addqg) based on reinforcement learning. an answer-aware initialization module with a gated connection layer and a semantic-rich fusion attention mechanism are designed to incorporate document and answer information into the generation process. reinforcement learning is further applied to integrate both syntactic and semantic metrics as the reward to enhance the training of addqg. experiments show that addqg outperforms the state-of-the-art systems on the challenging dqg dataset. ablation studies have demonstrated the effectiveness of our designs, and human evaluations show that our model can produce more coherent and answer-focused questions.", "Solved": NaN, "Unsolved": NaN}, "248780469": {"num": 667, "type": 1, "title": "MULTIHIERTT: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data", "pdfurl": "https://www.aclanthology.org/2022.acl-long.454.pdf", "conclusion": "although the proposed mt2net model outperforms other baseline models, it still performs significantly worse than human experts, which reflects the challenge of multihiertt. primarily, we find that models do not perform well on certain types of questions: 1) questions requiring reasoning across multiple tables; 2) questions requiring multi-step reasoning; 3) questions requiring reasoning over tables with complex hierarchical structures; and 4) questions requiring external financial knowledge.to deal with these challenges, we believe that four main directions of work may be workable: 1) designing a specialized module to handle multitable reasoning; 2) decomposing a complex question requiring multi-step reasoning into several simpler sub-questions that qa models can handle (perez et al., 2020;chen et al., 2020); 3) applying a more advanced table-encoding method. for example, a pre-trained model with specialized table structure-aware mechanisms cheng et al., 2021a;yang et al., 2022) can be utilized in the facts retrieving module to better understand hierarchical tables; and 4) leveraging structured knowledge (xie et al., 2022) to inject external financial knowledge to models.", "Solved": NaN, "Unsolved": NaN}, "254125751": {"num": 668, "type": 1, "title": "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.446.pdf", "conclusion": "we present a new parameter-effluent tuning method attempt, which learns to produce instance-wise prompts by interpolating multiple reusable soft prompts trained on source tasks and a new taskspecific prompt, while keeping the original lm frozen. our large-scale experiments demonstrate that attempt achieves a great trade-off between task performance and efficiency, introducing an interpretable and modular task transfer.", "Solved": NaN, "Unsolved": NaN}, "256827065": {"num": 669, "type": 1, "title": "Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering", "pdfurl": "https://www.aclanthology.org/2023.findings-eacl.87.pdf", "conclusion": "we analyze the effectiveness of the underlying reasoning tasks using two multi-hop datasets: 2wiki and hotpotqa-small. the results reveal that the underlying reasoning tasks can improve qa performance. using four debiased sets, we demonstrate that the underlying reasoning tasks can reduce the reasoning shortcuts of the qa task. the results also reveal that the underlying reasoning tasks do not make the models more robust on adversarial examples, such as sub-questions and inverted questions. we encourage future studies to investigate the effectiveness of the entity-level reasoning task in the form of sub-questions.", "Solved": NaN, "Unsolved": NaN}, "243865275": {"num": 670, "type": 1, "title": "Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation", "pdfurl": "https://arxiv.org/pdf/2110.04889v1.pdf", "conclusion": "we present distdr, a distantly-supervised odqa system that improves over a weak retriever by iter-atively finding evidence from a corpus, and using the evidence as distant supervision for model training.without using any evidence labels, distdr matches the fully-supervised sota approaches on both multi-hop and single-hop qa benchmarks.", "Solved": NaN, "Unsolved": NaN}, "215768766": {"num": 671, "type": 1, "title": "Coreferential Reasoning Learning for Language Representation", "pdfurl": "https://arxiv.org/pdf/2004.06870v1.pdf", "conclusion": "in this paper, we present a language representation model named corefbert, which is trained on a novel task, mention reference prediction, for strengthening the coreferential reasoning ability of bert. experimental results on several downstream nlp tasks show that our corefbert significantly outperforms bert by considering the coreference information within the text. in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mention reference prediction training task. it is a feasible approach to introducing the coreferential signal to language representation models, but the automatic labeling mechanism inevitably accompanies with the wrong labeling problem. until now, mitigating noise in ds data is still an open question.", "Solved": NaN, "Unsolved": NaN}, "261875390": {"num": 672, "type": 1, "title": "Reward Engineering for Generating Semi-structured Explanation", "pdfurl": "https://export.arxiv.org/pdf/2309.08347v1.pdf", "conclusion": "in this work, we focused on the semi-structured explanation generation task and proposed to use train a single model with sft+rl to generate both answers and structured explanations.we highlight the inadequacy of sft in performing this complex task, and proposed a carefully designed reward engineering method in rl to better address this problem.we investigated different reward aggregation methods and conduct extensive experiments under different settings to better highlight the dynamic of the rl objective function and reward model choices.our method achieves the new sota results on two seg benchmarks, explagraph and copa-sse.", "Solved": NaN, "Unsolved": NaN}, "245769925": {"num": 673, "type": 1, "title": "Does Entity Abstraction Help Generative Transformers Rea- son?", "pdfurl": "https://export.arxiv.org/pdf/2201.01787v2.pdf", "conclusion": "conclusion. we presented various ways to incorporate abstract knowledge into transformer language models. focusing on entity types, this work evaluated model performance on reasoning tasks requiring compositional generalization and multi-hop reasoning. overall our results demonstrate three things: (i) incorporating abstract knowledge significantly improves reasoning and compositional generalization in both interpolation and extrapolation when the environment is formally defined in a logical reasoning setting; (ii) different ways to incorporate abstraction yields different performance boosts: enc-sum and dec-loss are generally performing better than others; (iii) abstraction is not beneficial when the task at hand is more natural, less procedural, and not requiring long reasoning chains. this last result is due to the noisy entity tagging from \"off-the-shelf\" taggers, and due to the nature of the task at hand.", "Solved": NaN, "Unsolved": NaN}, "263605962": {"num": 674, "type": 1, "title": "RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUC-TION TUNING", "pdfurl": "https://export.arxiv.org/pdf/2310.01352v2.pdf", "conclusion": "in this paper, we propose ra-dit, a lightweight retrieval-augmented dual instruction tuning framework that can effectively retrofit any pre-trained llm with retrieval capabilities.ra-dit updates the llm with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information.it also fine-tunes the retriever with supervision from the llm to retrieve texts that can better help the llm generate correct outputs.ra-dit achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context ralm approaches such as replug and compete effectively against methods that require extensive pre-training such as atlas.", "Solved": NaN, "Unsolved": NaN}, "248377036": {"num": 675, "type": 1, "title": "Evaluating Extrapolation Performance of Dense Retrieval", "pdfurl": "https://arxiv.org/pdf/2204.11447v1.pdf", "conclusion": "in this paper, we propose a simple yet effective method that evaluates the extrapolation performance of dr models, i.e., how dr models perform on queries that are distinct from the training queries. with the proposed evaluation method, we first revisit how existing dr models perform in the extrapolation regimes. results lead to several non-trivial findings that have been concealed by the existing evaluation protocol. concretely, dr is substantially more vulnerable to extrapolation than the interaction-based deep neural ranking models, and pretraining is a more effective method to improve the extrapolation ability of dr than finetuning techniques. then we further interpret our extrapolation performance by investigating its relationship with the domain transfer ability. results suggest that the extrapolation performance is a potential indicator of the domain transfer ability, further highlighting the feasibility of our methods to evaluate the generalization ability of dr models. although this paper focuses on evaluating how dr models extrapolate, the methodologies can also be used for other models in the future.", "Solved": NaN, "Unsolved": NaN}, "258436815": {"num": 676, "type": 1, "title": "Huatuo-26M, a Large-scale Chinese Medical QA Dataset", "pdfurl": "https://export.arxiv.org/pdf/2305.01526v1.pdf", "conclusion": "in this paper, we propose the largest chinese medical qa dataset to date, consisting of 26 million medical qa pairs, expanding the size of existing datasets by more than 2 orders of magnitude. at the same time, we benchmark many existing works based on the data set and found that these methods still have a lot of room for improvement in medical qa scenarios. we also demonstrate the possible uses of the dataset in practice. the experimental results show that the dataset contains rich medical knowledge that can be very helpful to existing datasets and tasks. we hope that the huatuo-26m dataset can not only help promote the research of medical qa, but also practically help doctors and patients.", "Solved": NaN, "Unsolved": NaN}, "215238353": {"num": 677, "type": 1, "title": "Is Graph Structure Necessary for Multi-hop Reasoning?", "pdfurl": "https://arxiv.org/pdf/2004.03096v1.pdf", "conclusion": "this study set out to investigate whether graph structure is necessary for multi-hop reasoning tasks and what role it plays. we established that with the proper use of pre-trained models, graph structure may not be necessary for multi-hop reasoning. in addition, we point out that the adjacency matrix and graph structure can be regarded as some kind of task-related prior knowledge. we find both graph-attention and graph structure can be replaced by self-attention or transformer.", "Solved": NaN, "Unsolved": NaN}, "248266450": {"num": 678, "type": 0, "title": "A Survey on Multi-hop Question Answering and Generation", "pdfurl": "https://arxiv.org/pdf/2204.09140v1.pdf", "conclusion": "multi-hop qa has been researched quite frequently in the recent years with multiple diverse models proposed that aim to model the multi-step retrieval-reasoning process and achieve promising improvements on existing datasets and benchmarks. such systems capable of performing multistep reasoning have a variety of applications ranging from chat-bot assistants that are capable of interactive conversations, to search engines that are capable to retrieve results that may be relevant but not reachable directly from the query text. at the same time the task of mhqa is significantly more challenging than its single hop counterpart. since paragraphs multiple hops away from the question could share few common words and little semantic relation with the question [34], the task to retrieve such contexts is challenging and suffers from semantic drift. the ability of current models to combine multiple contexts for reasoning is also limited. further challenges for solving mhqa is the difficult process of creating datasets that require the models to perform multi-hop reasoning, as well as the task of evaluating the models' abilities to do so without any hacks. some challenging benchmarks and evaluation methods have been recently proposed that bring out some surprising and interesting observations. these results point out to several limitations of existing systems and call for further research.", "Solved": NaN, "Unsolved": NaN}, "234778323": {"num": 679, "type": 1, "title": "Answering Any-hop Open-domain Questions with Iterative Document Reranking", "pdfurl": "https://export.arxiv.org/pdf/2009.07465v5.pdf", "conclusion": "we present a qa framework that can answer any-hop open-domain questions, which iteratively retrieves, reranks and filters documents with a graph-based reranking model, and adaptively decides how many steps of retrieval and reranking are needed for a multi-hop question. our method consistently achieves promising performance on both single-and multi-hop open-domain qa datasets.", "Solved": NaN, "Unsolved": NaN}, "221845203": {"num": 680, "type": 1, "title": "ETC: Encoding Long and Structured Inputs in Transformers", "pdfurl": "https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf", "conclusion": "this paper introduced the extended transformer construction (etc), an architecture designed to (1) scale up the input length (linearly with input), and (2) encode structured inputs. etc allows lifting weights from existing bert models, improving results significantly. the key ideas are a new globallocal attention mechanism, coupled with relative position encodings and a cpc pre-training task.", "Solved": NaN, "Unsolved": NaN}, "258987734": {"num": 681, "type": 1, "title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language", "pdfurl": "https://export.arxiv.org/pdf/2305.19840v1.pdf", "conclusion": "information retrieval in the polish language is still developing and is an intensive research area. therefore, there is a great need for resources to enable further training and more accurate evaluation of existing and new deep neural ir models. in this work, we introduced the translated beir-pl benchmark and showed the results of a broad family of ir baseline models. we would like to encourage other researchers to participate in further development of polish and multilingual ir models using our new resource.", "Solved": NaN, "Unsolved": NaN}, "248405719": {"num": 682, "type": 1, "title": "A Thorough Examination on Zero-shot Dense Retrieval", "pdfurl": "https://export.arxiv.org/pdf/2204.12755v2.pdf", "conclusion": "in this paper, we thoroughly examine the zeroshot capability of dr models. we conduct empirical analysis by extensively studying the effect of various factors on the retrieval performance. in particular, we find that the factors of vocabulary overlap, query type distribution, and data scale are likely to affect the zero-shot performance of dense retriever. besides, the performance between bm25 and dr models varies significantly on different target datasets, where the dataset bias (e.g., a dataset is created based on exact match) is likely to make such comparison unfair. overall, we find that the zero-shot performance of dense retrieval models still has room to improve and deserves further study.", "Solved": NaN, "Unsolved": NaN}, "214802134": {"num": 683, "type": 1, "title": "Graph Sequential Network for Reasoning over Sequences", "pdfurl": "https://arxiv.org/pdf/2004.02001v1.pdf", "conclusion": "this paper proposes graph sequential network as a novel neural architecture to facilitate reasoning over graphs with sequential data on the nodes. we develop a new message passing algorithm based on co-attention between two sequences on graph nodes. the scheme avoids the information loss inherent in the pooling based early summarization of existing gnn-based models, and improve the reasoning ability on sentence level. through experiments on hotpotqa and fever, both of which require the model to perform multi-hop reasoning, we show that our proposed gsn attains better performance than existing gnns on different types of tasks. for future work we would like to apply gsn to other applications in nlp that require complex reasoning.", "Solved": NaN, "Unsolved": NaN}, "215238360": {"num": 684, "type": 1, "title": "Query Focused Multi-Document Summarization with Distant Supervision", "pdfurl": "https://arxiv.org/pdf/2004.03027v1.pdf", "conclusion": "in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization. we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments. experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more relevant and less redundant. we have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query. in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019;chu and liu, 2019) and investigate how recent advances in open domain qa qi et al., 2019) can be adapted for query focused summarization.", "Solved": NaN, "Unsolved": NaN}, "222090978": {"num": 685, "type": 0, "title": "A Survey on Explainability in Machine Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2010.00389v1.pdf", "conclusion": "this survey has proposed a systematic categorisation of benchmarks and approaches for explainability in mrc. lastly, we outline a set of open research questions for future work:", "Solved": NaN, "Unsolved": NaN}, "247779047": {"num": 686, "type": 1, "title": "Understanding Questions that Arise When Working with Business Documents", "pdfurl": "https://export.arxiv.org/pdf/2203.15073v2.pdf", "conclusion": "in this work we focused only on word documents as a common document type where users can author, copy-edit, or read content. indeed, the organization where we deployed our studies primarily uses word for business documents. the focus on word rather than e.g., pdfs or web pages also allowed us to obtain questions about documents at various stages of development, not just finalized manuscripts. future work can investigate the types of questions that arise when users interact with other file types such as pdf, excel, and powerpoint. because each file type is used for different purposes (e.g., excel documents for long-term book-keeping [27]) and possibly containing content at different levels of abstraction, the extent to which question answering in these documents can be automated and the kinds of expertise knowledge workers need may be different from the word documents in our study.because all questions had to first pass through the knowledge workers' system, the majority of the responses that participants received from the q&a system had a delay. therefore, it is possible that participants would have asked different questions if the responses had been provided instantaneously. another property that could have conceivably impacted the types of questions users asked is the quality of the answers that they received. to understand if the types of questions by a user changed over time as they gained familiarity with the system, we examined the questions that were posted by the same user both across different documents or on the same document. we observed that some users posted multiple questions in succession and close together in time, e.g., in the span of a few minutes. although these users would realize that the system does not provide answers instantaneously, their cluster of initial questions would not be impacted by the expectation of a long delay, their perceived capabilities of the system, or the quality of the answers. in fact, although we had specified in the consent form that there may be a delay in the responses that participants would receive from the q&a system, some end-of-study survey responses indicated that a number of participants had in fact not noticed this point and had asked their first few questions expecting instantaneous answers-\"i was initially confused by the delay of asking the question in the document and then waiting for an email that told me to go back to the document. it seemed a bit redundant to get an email about it vs. just a notification in the word doc itself and telling me it was working on it or something. for a plug-in, however, i would expect less of a delay. \" (p-2-18). therefore, the first few questions from these participants could help with the generalizability of our results.the delay or the quality of answers however, may have influenced those users who submitted questions after receiving answers from the system. upon examination, we found that in many instances, when users received answers to their previous questions, they explored increasingly more sophisticated questions of various types, e.g., content-related, concerning metadata, or seeking external information. for instance, one participant started by asking simple metadata questions (\"who is the author?\") and proceeded to ask questions on a scanned document that needed not only complex reasoning, but also optical character recognition (ocr): \"what is the total score?\" on a document where handwritten scores were given to each question. this finding suggests that the answers may have in fact encouraged users to be liberal with the types of questions they wished to subsequently ask. this exploration could be due to users gaining confidence that the system can in fact handle the types of questions with which they need support or could be because they wished to test the limits of its abilities.nevertheless, the characterization that we present in the paper also includes the questions users submitted in the experience sampling phase, where participants could imagine a sophisticated system with any or no delay. the set of questions in that phase of the study could further help with generalizability of our results, e.g., to settings where not all questions necessarily experience a delay.it is conceivable that the types of questions about a document may vary with the document type. in phase 2 where we had access to the documents, we observed that the document distribution was in fact very varied and included project proposals and timelines, value propositions, design specifications, service instructions, management training, protocols, faqs, whitepaper reports, strategy planning, customer feedback, research findings, etc. from various domains. with this diverse set of document types, investigating the relationship between types of questions and types of documents would require collecting far more questions by running the study for a long time.the setup of our q&a system was such that users submitted one-shot questions as there were no affordances for following up on previously asked questions. future work should investigate whether the types of questions that users ask a document q&a system differ if the system provides the users with the means for following up on their previous questions or multi-round conversations with the assistant.another area for future work would be to explore the context of user needs including when users need different types of assistance with their documents, what they do before the seek help, and what they do after they receive answers. to minimize concerns about the confidentiality of the business data, the contextual information that we collected in our study concerned the metadata of the document (e.g., file size, last modified date, etc.) and not the content. for the same reason, we did not track a user's modification of the document before or after the user posted a question; the metadata was collected upon the user's submission of a question. therefore, given the data we collected, we cannot examine the context of user needs. future work can study this question through interviews and user-produced logs.", "Solved": NaN, "Unsolved": NaN}, "207853300": {"num": 687, "type": 1, "title": "Hierarchical Graph Network for Multi-hop Question Answering", "pdfurl": "https://arxiv.org/pdf/1911.03631v1.pdf", "conclusion": "in this paper, we propose a new approach, hierarchical graph network (hgn), for multi-hop question answering. to capture clues from different granularity levels, our hgn model weaves heterogeneous nodes into a single unified graph. experiments with detailed analysis demonstrate the effectiveness of our proposed model, which achieves state-of-the-art performance on hotpotqa benchmark. currently, in the fullwiki setting, an off-theshelf paragraph retriever is adopted for selecting relevant context from large corpus of text. future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.", "Solved": NaN, "Unsolved": NaN}, "214802166": {"num": 688, "type": 1, "title": "Prerequisites for Explainable Machine Reading Comprehension: A Position Paper", "pdfurl": "https://arxiv.org/pdf/2004.01912v1.pdf", "conclusion": "in this position paper, we overviewed issues and future directions of mrc. we focused specifically on the situation model in psychology for what we should ask of reading comprehension and the substantive validity in psychometrics for how we should correctly evaluate it. we conclude that future datasets should (i) evaluate the capability of the situation model for understanding contextdependent situations and for grounding to nontextual information and (ii) ensure the substantive validity by improving the question quality and designing a white-box task formulation. ", "Solved": NaN, "Unsolved": NaN}, "215828216": {"num": 689, "type": 1, "title": "ETC: Encoding Long and Structured Data in Transformers", "pdfurl": "https://arxiv.org/pdf/2004.08483v2.pdf", "conclusion": "this paper introduced the extended transformer construction, or etc, a novel extension of the original transformer model designed specifically to (1) scale up the input length to sequences longer than 512 tokens (scaling linearly in the size of the input), and (2) allow ingesting structured inputs. etc also allows lifting weights from existing bert models, saving significant computational resources while training. the key ideas that enable etc to achieve these are a new global-local attention mechanism, coupled with relative position encodings.", "Solved": NaN, "Unsolved": NaN}, "232320760": {"num": 690, "type": 1, "title": "Mitigating False-Negative Contexts in Multi-Document Question Answering with Retrieval Marginalization", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.497.pdf", "conclusion": "we proposed a new probabilistic model for retrieving set-valued contexts for multi-document qa and show that training the qa model with marginalization over this set can help mitigate the false negatives in evidence annotations. experiments on iirc and hotpotqa fullwiki show that our proposed framework can learn to retrieve unlabeled alternative contexts and improves qa f1 by 5.5 on iirc and 8.9 on hotpotqa.", "Solved": NaN, "Unsolved": NaN}, "216553210": {"num": 691, "type": 1, "title": "Semantic Graphs for Generating Deep Questions", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.135.pdf", "conclusion": "we propose the problem of dqg to generate questions that requires reasoning over multiple disjoint pieces of information. to this end, we propose a novel framework which incorporates semantic graphs to enhance the input document representations and generate questions by jointly training with the task of content selection. experiments on the hotpotqa dataset demonstrate that introducing semantic graph significantly reduces the semantic errors, and content selection benefits the selection and reasoning over disjoint relevant contents, leading to questions with better quality.", "Solved": NaN, "Unsolved": NaN}, "232380161": {"num": 692, "type": 1, "title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.467.pdf", "conclusion": "in this paper, we introduce a discourse-aware graph network (dagn) to addressing logical reasoning qa tasks. we first treat elementary discourse units (edus) that are split by discourse relations as basic reasoning units. we then build discourse-based logic graphs with edus as nodes and discourse relations as edges. dagn then learns the discourse-based features and enhances them with contextual token embeddings. dagn reaches competitive performances on two recent logical reasoning datasets reclor and logiqa.  ", "Solved": NaN, "Unsolved": NaN}, "262217474": {"num": 693, "type": 1, "title": "FURTHEST REASONING WITH PLAN ASSESSMENT: STABLE REASONING PATH WITH RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS", "pdfurl": "https://export.arxiv.org/pdf/2309.12767v1.pdf", "conclusion": "based on the experiments and results presented above, it is evident that furthest reasoning with plan assessment exhibits exceptional performance in addressing multi-hop questions.firstly, the furthest reasoning framework significantly contributes not only to the reasoning process but also to the direct answering process.in complex and long-term problem-solving scenarios such as multi-hop questions, furepa is designed to generate more effective plans, avoiding the influence of convoluted previous reasoning processes.moreover, the plan assessment component plays a crucial role in selecting the most suitable query from the llm-generated candidates, leading to more relevant evidence retrieval by the information retriever (ir).this enhancement in query selection contributes to a more effective and accurate multi-hop question-solving process.", "Solved": NaN, "Unsolved": NaN}, "236478211": {"num": 694, "type": 1, "title": "GCRC: A New MRC Dataset from Gaokao Chinese for Explainable Evaluation", "pdfurl": "https://www.aclanthology.org/2021.findings-acl.113.pdf", "conclusion": "in this paper, we present a new challenging machine reading comprehension dataset (gcrc), collected from gaokao chinese, consisting of 8,719 high-level comprehensive multiple-choice questions. to the best of our knowledge, this is currently the most comprehensive, challenging, and high-quality dataset in mrc domain. in addition, we spend considerable effort to label three types of information, including sentence-level sfs, ers of a distractor, and reasoning skills required for qa, aiming to comprehensively evaluate systems in an explainable way. through experiments, we observe gcrc is very challenging data set for existing models, and we hope it can inspire innovative machine learning and reasoning approach to tackle the challenging problem and make mrc as an enabling technology for many real-world applications.", "Solved": NaN, "Unsolved": NaN}, "226254596": {"num": 695, "type": 1, "title": "Context-Aware Answer Extraction in Question Answering", "pdfurl": "https://arxiv.org/pdf/2011.02687v1.pdf", "conclusion": "in this paper, we showed the importance of predicting an answer with the correct context of a given question. we proposed blanc with two novel ideas: context word prediction task and a block attention method that identifies an answer within the context of a given question. the context words prediction task labels latent context words with the labeled answer-span and is used in a multi-task learning manner. block attention models the latent context words with negligible extra parameters and training/inference time. we showed that blanc increases reading comprehension performance, and we verify that the performance gain increases for complex examples (i.e., when the answer occurs two or more times in the passage). also, we showed the generalizability of blanc and its contextaware performance with the zero-shot supporting fact prediction task on the hotpotqa dataset. ", "Solved": NaN, "Unsolved": NaN}, "230770437": {"num": 696, "type": 1, "title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2021.eacl-demos.2.pdf", "conclusion": "in conclusion, this paper presents sf-qa, a novel evaluation framework to make open-domain qa research simple and fair. this framework fixes the gap among researchers from different fields, and make the open-domain qa more accessible. we show the robustness of this framework by successfully reproducing several existing models in opendomain qa research. we hope that sf-qa can make the open-domain qa research more accessible and make the evaluation easier. we expect to further improve our framework by including more models in both ranker and reader side, and encourage community contributions to the project as well.", "Solved": NaN, "Unsolved": NaN}, "225066758": {"num": 697, "type": 1, "title": "Unsupervised Multi-hop Question Answering by Question Generation", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.469.pdf", "conclusion": "in this work, we study unsupervised multi-hop qa and propose a novel framework mqa-qg to generate multi-hop questions via composing reasoning graphs built upon basic operators. the experiments show that our model can generate human-like questions that help to train a well-performing multi-hop qa model in both the unsupervised and the fewshot learning setting. further work is required to include more flexible paraphrasing at the fusion stage. we can also design more reasoning graphs and operators to generate more complex questions and support more input modalities.", "Solved": NaN, "Unsolved": NaN}, "248780551": {"num": 698, "type": 1, "title": "CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation", "pdfurl": "https://www.aclanthology.org/2022.acl-long.475.pdf", "conclusion": "the mqg task is more challenging and worthy of exploration compared with conventional shallow qg. to address the complexity control problem of mqg, we propose a simple control framework cqg, which consists of a gat-based key entity extractor and a controlled generated. cqg greatly improves the performance and we hope our model will help researchers to study the mqg task.", "Solved": NaN, "Unsolved": NaN}, "233219849": {"num": 699, "type": 1, "title": "MULTIMODALQA: COMPLEX QUESTION ANSWERING OVER TEXT, TABLES AND IMAGES", "pdfurl": "https://arxiv.org/pdf/2104.06039v1.pdf", "conclusion": "we present mmqa, a new qa dataset that contains 29,918 examples, 35.7% of which require crossmodality reasoning. we describe a novel framework for generating complex multimodal questions at scale, and showcase the diversity and multimodal properties of the resulting dataset. we evaluate mmqa using a variety of models, and confirm that the best model exploits the multimodality of the dataset and takes into account multi-hop reasoning via implicit decomposition. however, human performance substantially exceeds the best model, establishing the need for further research involving multiple modalities in question answering systems, which we hope that our work will drive.", "Solved": NaN, "Unsolved": NaN}, "225040142": {"num": 700, "type": 1, "title": "Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval", "pdfurl": "https://www.aclanthology.org/2021.acl-long.118.pdf", "conclusion": "we provide the first in-depth analysis on information-seeking qa datasets to inspect where unanswerability arises and quantify the remaining modeling challenges. our controlled experiments identifies two remaining headrooms, answerability prediction and paragraph selection. observing a large percentage of questions are unanswerable, we provide manual analysis studying why questions are unanswerable and make suggestions to improve answer coverage: (1) going beyond wikipedia textual information as the only source of information, (2) addressing ambiguous queries instead of simply marking and leaving the questions as is, (3) enable accessing multiple documents and introducing abstractive answers for non-factoid questions. together, our work shed light on future work for information-seeking qa, both for modeling and dataset design.", "Solved": NaN, "Unsolved": NaN}, "243756815": {"num": 701, "type": 1, "title": "Extracting a Knowledge Base of COVID-19 Events from Social Media", "pdfurl": "https://export.arxiv.org/pdf/2006.02567v4.pdf", "conclusion": "in this paper, we presented a corpus of 10,000 tweets annotated with 5 types of events and 28 slots. we showed that our corpus supports automatic extraction of covid-19 events using supervised learning. by aggregating extractions over millions of tweets, our approach can accurately answer a range of structured queries about events that are publicly reported in real-time on twitter. our knowledge base could be a useful tool for epidemiologists, journalists and policymakers to more efficiently track the spread of this new disease. this work also presents a case-study on how an information extraction system can be rapidly developed for a new domain in response to an emerging crisis. for example, our methodology could be applied to develop knowledge bases for natural disasters (spiliopoulou et al., 2020) or future disease outbreaks.", "Solved": NaN, "Unsolved": NaN}, "221970190": {"num": 702, "type": 1, "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams", "pdfurl": "https://arxiv.org/pdf/2009.13081v1.pdf", "conclusion": "we present the first open-domain multiple-choice question answering dataset for solving medical problems, medqa, collected from the real-world professional examinations, requiring extensive and advanced domain knowledge to answer questions. this dataset covers three languages: english, simplified chinese, and traditional chinese. together with the question data, we also collect and release a largescale corpus from medical textbooks from which the reading comprehension models can obtain necessary knowledge for answering the questions. we implement several state-of-theart methods as baselines to this dataset by cascading two components: document retrieval and reading comprehension. and experimental results demonstrate that even current best approach cannot achieve good performance on these data. we anticipate more research efforts from the community can be devoted to this dataset so that future openqa models can be strong enough to solve such real-world complex problems.", "Solved": NaN, "Unsolved": NaN}, "252407569": {"num": 703, "type": 1, "title": "Dynamic Relevance Graph Network for Knowledge-Aware Question Answering", "pdfurl": "https://www.aclanthology.org/2022.coling-1.116.pdf", "conclusion": "in this paper, we propose a novel dynamic relevance graph network (drgn) architecture for commonsense question answering given an external source of knowledge in the form of a knowledge graph. our model learns the graph node representation while a) exploits the existing relations in kg, b) re-scales the importance of the neighbor nodes in the graph based on training a dynamic relevance matrix, c) establishes direct connections between graph nodes based on measuring the relevance scores of the nodes dynamically during training. the dynamic relevance edges help in finding the chain of reasoning when there are missing edges in the original kg. our quantitative and qualitative analysis shows that the proposed approach facilitates answering the complex questions that need multiple hops of reasoning. furthermore, since drgn uses the relevance between the question node and graph entities, it exploits the richer semantic context of the question in graph reasoning which leads to improvements in the performance on the negative questions. our proposed approach shows competitive performance on two qa benchmarks, including commonsenseqa and openbookqa.", "Solved": NaN, "Unsolved": NaN}, "244799249": {"num": 704, "type": 1, "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.272.pdf", "conclusion": "we introduced colbertv2, a retriever that advances the quality and space efficiency of multivector representations. we hypothesized that cluster centroids capture context-aware semantics of the token-level representations and proposed a residual representation that leverages these patterns to dramatically reduce the footprint of multi-vector systems off-the-shelf. we then explored improved supervision for multi-vector retrieval and found that their quality improves considerably upon distillation from a cross-encoder system. the proposed colbertv2 considerably outperforms existing retrievers in within-domain and out-of-domain evaluations, which we conducted extensively across 28 datasets, establishing state-of-the-art quality while exhibiting competitive space footprint.", "Solved": NaN, "Unsolved": NaN}, "256231186": {"num": 705, "type": 1, "title": "An Experimental Study on Pretraining Transformers from Scratch for IR", "pdfurl": "https://export.arxiv.org/pdf/2301.10444v1.pdf", "conclusion": "foundation models come with the promise to be highly general and modular. it is believed that they contain a wide \"knowledge\" due to their pretraining on a large collection, which is then believed to be the source of their improved performance. we have examined how this pretraining collection influence the performance of ir models. our research question was to assess how much of this implicit knowledge, beneficial to the final performance, comes from pretraining on a large external collection. this is why we have experimented on a variety of collections, domains and languages to study how pretraining from scratch actually performed compared to their de facto approach of simple finetuning. while we were expecting the standard pretrained models to work better, we surprisingly revealed that pretraining from scratch works better for first-stage retrieval on msmarco, tripclick and several non-english languages on the mr.  tydi benchmark. in particular, the flops regularization played a critical role in those results, suggesting that regularization or better pretraining techniques could further improve the results. furthermore, pretrained models from scratch also behave well in the zero shot scenario for sparse models such as splade. nevertheless, pretraining from a large collection has a slight advantage when training rerankers. overall, these results, specific to ir, challenge the foundation model hypothesis for small models, ie that a more general model encapsulating the world knowledge would be better than a smaller one in a specific domain application. furthermore, our study makes a contribution to the debate between general purpose and specific purpose models. in a way, our experiments showed that less is more. in addition, pretrained language models come also with many challenges such as the societal bias in the data they have been trained on. we hope that our study could convince practitioners, both from industry and academia, to reconsider specific purpose models by pretraining from scratch. last but not least, doing so enable to better control efficiency, data bias and replicability, which are key research questions for the ir community.", "Solved": NaN, "Unsolved": NaN}, "252819220": {"num": 706, "type": 1, "title": "KHANQ: A Dataset for Generating Deep Questions in Education", "pdfurl": "https://www.aclanthology.org/2022.coling-1.518.pdf", "conclusion": "in this paper, we propose khanq, a dataset for generating in-depth educational questions. each sample in khanq is carefully annotated as context, prompt, and question to form a clean dataset. we evaluate the performance of state-of-the-art question generation models on khanq. we find that although it is feasible for the model to generate fluent and complex questions, the ability to understand and reason over the context and the prompt is still far from reaching the human level.", "Solved": NaN, "Unsolved": NaN}, "249018129": {"num": 707, "type": 1, "title": "Semi-Parametric Deep Neural Networks in Linear Time and Memory", "pdfurl": "https://arxiv.org/pdf/2205.11718v1.pdf", "conclusion": "the spin model achieves linear complexity via advanced self-attention mechanisms; this also introduces additional hyper-parameters into the model, potentially increasing tuning time. the resulting architecture remains overparametrized even after with small numbers of inducing points h, f and may overfit.the primary source of expressivity is attention between datapoints, which has a query dimension he.highly expressive models may learn to ignore the training set and operate in a fully-parametric mode; this failure mode is best avoided via regularization and large datasets. interestingly, our approach benefits from big data, while classical non-parametric models work best on small datasets due to their computational complexity. regularization via small h, f , dropout, and feature masking control overfitting; we will explore more compact architectures in future work.", "Solved": NaN, "Unsolved": NaN}, "239009558": {"num": 708, "type": 1, "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer", "pdfurl": "https://arxiv.org/pdf/2110.07904v2.pdf", "conclusion": "as other parameter-efficient adaptation methods (see \u00a74) may outperform prompttuning in specific situations, it would be interesting to test whether an approach similar to spot could extend successfully to these methods. at the same time, we believe that prompttuning has its own merit. as pre-trained language models become larger and larger, some advantages of prompttuning over other methods are: (1) among current methods with learnable parameters, prompttuning is the most parameter efficient, requiring less than 0.01% task-specific parameters for most model sizes.(2) prompttuning is simpler than other methods, as it does not modify the internal model architecture (cf. the prefix-tuning method of li and liang (2021), which adds a prefix to each layer of both the transformer encoder and decoder); as such, prompttuning allows mixed-task inference and facilitates transfer learning between tasks.(3) as model capacity increases, prompttuning becomes more competitive with modeltuning; to the best of our knowledge, this has not been shown for other methods. (4) soft prompts could possibly be interpreted as natural language instructions.additionally, since our prompt-based task embedding approach does not capture all of the factors that influence task transferability, we leave further exploration of other task embedding methods to future work.", "Solved": NaN, "Unsolved": NaN}, "247158553": {"num": 709, "type": 1, "title": "KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models", "pdfurl": "https://arxiv.org/pdf/2202.13529v1.pdf", "conclusion": "this paper introduces kmir benchmark for evaluating knowledge memorization, identification, and reasoning abilities. kmir includes a systematic evaluation syllabus to summarize the knowledgerelated abilities of plms, and has 184,348 questions involving 4 types of questions covering 3 types of knowledge. we also find many interesting phenomena through extensive experiments: 1) the memorization ability of plms depends more on the number of parameters than training schemes. 2) current plms are struggling to robustly remember the facts. 3) model compression technology retains the amount of knowledge well, but hurts the identification and reasoning ability, etc.", "Solved": NaN, "Unsolved": NaN}, "237940507": {"num": 710, "type": 0, "title": "More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering", "pdfurl": "https://arxiv.org/pdf/2109.12264v2.pdf", "conclusion": "in this survey, we reviewed 47 textual qa benchmark datasets and their corresponding evaluation metrics. a novel taxonomy of textual qa tasks is provided from an application scenario point of view. a detailed description is provided for each dataset, which covers the task definition, contraction method, statistics, and evaluation measures. key features and detailed statistics among the benchmark datasets are summarized and compared in the form of tables. detailed description and distribution analysis of evaluation metrics are provided. finally, we summarized the trends of the recent textual qa benchmark contraction methods and give our opinions on the future directions of the textual qa benchmark research. we hope this work serves as a good introduction to textual qa tasks.", "Solved": NaN, "Unsolved": NaN}, "233219869": {"num": 711, "type": 1, "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.45.pdf", "conclusion": "we presented qa-gnn, an end-to-end question answering model that leverages lms and kgs.", "Solved": NaN, "Unsolved": NaN}, "257279774": {"num": 712, "type": 1, "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers", "pdfurl": "https://export.arxiv.org/pdf/2303.00807v3.pdf", "conclusion": "we present udapdr, a novel strategy for adapting retrieval models to new domains.udapdr uses synthetic queries created using generative models, such as gpt-3 and flan-t5 xxl, to train multiple passage rerankers on queries for target domain passages.these passage rerankers are then distilled into colbertv2 to boost retrieval accuracy while keeping query latency competitive as compared to other retrieval systems.we validate our approach across the lotte, beir, nq, and squad datasets.additionally, we explore various model configurations that alter the generative models, prompting strategies, retriever, and passage rerankers used in our approach.we find that udapdr can boost zero-shot retrieval accuracy on new domains without the use of labeled training examples.we also discuss several directions for future work.", "Solved": NaN, "Unsolved": NaN}, "248780575": {"num": 713, "type": 1, "title": "Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking", "pdfurl": "https://www.aclanthology.org/2022.acl-long.165.pdf", "conclusion": "we introduce an effective dicos-dst that dynamically selects the relevant dialogue contents corresponding to each slot from a combination of three perspectives. the dialogue collaborative selector module performs a comprehensive selection for each turn dialogue based on its relation to the slot name, its connection to the current turn dialogue, and the implicit mention oriented reasoning. then only the selected dialogue contents are fed into state generator, which explicitly minimizes the distracting information passed to the downstream state prediction. our dicos-dst model achieves new state-of-the-art performance on the multiwoz benchmark, and achieves competitive performance on most other dst benchmark datasets. the potential relationship among the above perspectives is a promising research direction, and we will explore it for more than dialogue selection in the future. ", "Solved": NaN, "Unsolved": NaN}, "202542881": {"num": 714, "type": 1, "title": "Span Selection Pre-training for Question Answering", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.247.pdf", "conclusion": "span selection pre-training is effective in improving reading comprehension across four diverse datasets, including both generated and natural questions, and with provided contexts of passages, documents and even passage sets. this style of pretraining focuses the model on finding semantic connections between two sequences, and supports a style of cloze that can train deep semantic understanding without demanding memorization of specific knowledge in the model. the span selection task is suitable for pre-training on any domain, since it makes no assumptions about document structure or availability of summary/article pairs. this allows pre-training of language understanding models in a very generalizable way.", "Solved": NaN, "Unsolved": NaN}, "244119330": {"num": 715, "type": 1, "title": "What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension", "pdfurl": "https://aclanthology.org/2021.findings-emnlp.202.pdf", "conclusion": "this paper aims at addressing the newly discovered difficulty of the boundary ambiguity between sentences and sub-sentences, which exists in many languages to different extents and essentially limits the performance of span extraction mrc models, especially in chinese environment. we apply explicit span-sentence predication (esp) to enhance model's ability of precisely locating sentences containing the target span. our proposed model design is evaluated on chinese span extraction mrc benchmark, cmrc 2018. the experimental results show that our model significantly improves both em and f1 scores compared with strong baselines and helps achieve a new state-of-the-art performance. our method also shows generality and potential in dealing with other languages. this work highlights the research line of further improving challenging mrc by analyzing specific linguistics phenomena.", "Solved": NaN, "Unsolved": NaN}, "252090148": {"num": 716, "type": 1, "title": "Interactive Question Answering Systems: Literature Review", "pdfurl": "https://export.arxiv.org/pdf/2209.01621v1.pdf", "conclusion": "in conclusion, we have reviewed a substantial collection of interactive question answering systems (iqass)-related literature published during the past decade. we discovered the literature to be diverse, beginning with adopted methodologies for addressing multiple qa tasks and concluding with a vast array of diverse resources (i.e. knowledge source, and datasets) that are typically utilized to create and evaluate question answering systems (qass). despite the fact that the state-of-the-art is defined by several types of qa solutions, we were able to determine the characteristics shared by the suggested systems that constitute a shared framework. to the best of our knowledge, we are the first to present a unified and comprehensive design that emphasizes the fundamental components and functions of iqass.", "Solved": NaN, "Unsolved": NaN}, "264490789": {"num": 717, "type": 1, "title": "Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks", "pdfurl": "https://export.arxiv.org/pdf/2310.16955v1.pdf", "conclusion": "if we put our new more robust models to use, human adversaries may adapt to them as well.checking whether crowd-sourcing fresh attacks is indeed more difficult on the new models is beyond the scope of this work.also, we benefit from having a fair number of human adversarial examples (16.9k in anli, and 10k per round in hate speech).our methods may be less successful in a scenario with very fewer examples (\u223c 10).on the flip side, we have also not evaluated these methods on classifiers with access to even larger real adversarial datasets.finally, our methods work on datasets with the notion of an original example, and a perturbed adversarial example as is the norm for adversarial robustness literature (madry et al., 2017).in the new paradigm of larger more capable nlp models, adversarial datasets may increasingly not involve a perturbation (ganguli et al., 2022b).risks: our techniques can enhance robustness given a set of observed adversarial examples.the new classifier we trained with generated data from di and ice may still be vulnerayble to future human attacks that are able to adapt to the new model (in this paper future attack rounds are known a priori from past model-in-the-loop work).this would require extensive crowd-sourcing efforts to evaluate.we also run the risk of over-fitting to the new human generated adversarial data.this may come at the cost of lower performance on future attacks generated by a different mechanism (say textfooler instead of future anli rounds), and comes at the cost of degrading accuracy on original tasks such as mnli and snli (table 19 in appendix).as a separate concern, any technique that betters generative text modeling brings the risk that humans may struggle to distinguish machine generated text.this can have negative consequences for disinformation and misinformation, which is an active area of research (pu et al., 2023).", "Solved": NaN, "Unsolved": NaN}, "123758373": {"num": 718, "type": 1, "title": "Understanding Dataset Design Choices for Multi-hop Reasoning", "pdfurl": "https://arxiv.org/pdf/1904.12106v1.pdf", "conclusion": "there exist several other multi-hop reasoning datasets including worldtree , openbookqa (mihaylov et al., 2018), and mul-tirc (khashabi et al., 2018). these datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content. we leave a detailed investigation of these for future work.", "Solved": NaN, "Unsolved": NaN}, "209063721": {"num": 719, "type": 1, "title": "Memory Graph Networks for Explainable Memory-grounded Question Answering", "pdfurl": "https://www.aclweb.org/anthology/K19-1068.pdf", "conclusion": "we introduce episodic memory qa, the task of answering personal user questions grounded on memory graph (mg). the dataset is generated with synthetic memory graphs with simulated attributes, and accompanied with 100k qa pairs composed via bootstrapped scripts and manual annotations. several novel model components are proposed for unique challenges for the episodic memory qa: 1) memory graph networks (mgn) extends the conventional memory networks by enabling dynamic expansion of memory slots through graph traversals, which also naturally allows for explainable predictions. 2) several neural module networks are proposed for the proposed task, each of which takes queries and memory graphs as input to infer answers. 3) the main episodic memory qa net aggregates answer prediction from each neural module to generate final answer candidates. the empirical results demonstrate the efficacy of the proposed model in the memory qa reasoning.", "Solved": NaN, "Unsolved": NaN}, "236459821": {"num": 720, "type": 1, "title": "AND does not mean OR: Using Formal Languages to Study Language Models' Representations", "pdfurl": "https://aclanthology.org/2021.acl-short.21.pdf", "conclusion": "using propositional logic corpora to simulate a controlled language modeling setting, we ask: 1) do properties of the training corpus affect lms' abilities to differentiate the meanings of logical operators? and 2) do any training corpora lead to models that differentiate these meanings to a satisfactory degree? our results imply a positive answer to (1): models trained on corpora generated with different constraints appear to perform differently at the task of separating \u2227 from \u2228. however, these differences are a function of both data and model. for example, the transformer architecture seems better able to learn from weaker signal (corpora generated only with a truthfulness constraint), while lstms require more explicit signal (direct access to truth values). on question (2), our results are largely negative for the syntactically similar operators. even the most semantically transparent training data did not enable models to separate the representations of symbols with similar form but different meaning. only the small transformer trained on the explicit grounding condition can perfectly differentiate \u2227 from \u2228 at the lexical level, despite the task's controlled nature. however, every model did separate \u00ac from both \u2227 and \u2228, illustrating how syntactic differences can support differentiation of meaning.", "Solved": NaN, "Unsolved": NaN}, "247793456": {"num": 721, "type": 1, "title": "LinkBERT: Pretraining Language Models with Document Links", "pdfurl": "https://www.aclanthology.org/2022.acl-long.551.pdf", "conclusion": "we presented linkbert, a new language model (lm) pretraining method that incorporates document link knowledge such as hyperlinks. in both the general domain (pretrained on wikipedia with hyperlinks) and biomedical domain (pretrained on pubmed with citation links), linkbert outperforms previous bert models across a wide range of downstream tasks. the gains are notably large for multi-hop reasoning, multi-document understanding and few-shot question answering, suggesting that linkbert effectively internalizes salient knowledge through document links. our results suggest that linkbert can be a strong pretrained lm to be applied to various knowledge-intensive tasks. ", "Solved": NaN, "Unsolved": NaN}, "263671602": {"num": 722, "type": 1, "title": "EVALUATING HALLUCINATIONS IN CHINESE LARGE LANGUAGE MODELS", "pdfurl": "https://export.arxiv.org/pdf/2310.03368v4.pdf", "conclusion": "in this work, we create a chinese hallucination question-answering dataset named halluqa to evaluate hallucinations in chinese large language models.questions in halluqa can be used to measure imitative falsehoods and factual errors.we design a llm-based automated evaluation method and verify its effectiveness.we conduct extensive experiments on 24 large language models.all models achieve less than a 70% non-hallucination rate on halluqa, which proves the challenging nature of our dataset.according to the experimental results, we further analyze the primary hallucinations types of different models and discuss the types that different models need to prioritize and address.we hope that halluqa can help reduce hallucinations problems in chinese large language models and enhance the credibility of the models.the prompt used for evaluation is shown in figure 10.we utilize the conversational format of gpt-4.in the first turn, we include the evaluation criteria for hallucinations and evaluation guidance, and concatenate a response from the assistant indicating affirmation.in the second turn, we provide the question to be evaluated, the response, examples of correct answers, and an instruction to output \"yes\" or \"no\" as the evaluation reulst.during the evaluation, we need to replace the red placeholders with the question, examples of the correct answer, and the model's response to this question.", "Solved": NaN, "Unsolved": NaN}, "258236218": {"num": 723, "type": 1, "title": "Why Does ChatGPT Fall Short in Answering Questions Faithfully?", "pdfurl": "https://export.arxiv.org/pdf/2304.10513v1.pdf", "conclusion": "our paper investigates the common failures of chat-gpt in complex open-domain question answering. we identify four types of errors: comprehension, factualness, specificity, and inference. we also examine the key abilities knowledge memorization, knowledge association, and knowledge reasoning, which are critical to these failures. additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on association, and the effect of decomposition on reasoning. finally, we suggest several techniques to help users more effectively use chat-gpt as a question-answering tool and enable system builders to develop better qa systems. our research contributes to the understanding of what influencing the faithfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more efficient and reliable language models.", "Solved": NaN, "Unsolved": NaN}, "254877499": {"num": 724, "type": 1, "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "pdfurl": "https://www.aclanthology.org/2023.acl-long.557.pdf", "conclusion": "chain-of-thought prompting has significantly improved llms' ability to perform multi-step reason-  [1984][1985][1986][1987][1988][1989] was performed by jane siberry. jane siberry was born in toronto. the castle in toronto is the casa loma. so the answer is: casa loma. table 2: example cots generated by gpt3 with different methods. since nor relies on parametric knowledge, it often makes a factual error in the first sentence derailing the full cot. oner can retrieve relevant information closest to the question and is less likely to make such errors early on, but it still makes errors later in the cot. as ircot performs retrieval after each step, it is often able to prevent such errors in each step. more examples are in app. d.", "Solved": NaN, "Unsolved": NaN}, "247594811": {"num": 725, "type": 1, "title": "Simulating Bandit Learning from User Feedback for Extractive Question Answering", "pdfurl": "https://www.aclanthology.org/2022.acl-long.355.pdf", "conclusion": "we present a simulation study of learning from user feedback for extractive qa. we formulate the problem as contextual bandit learning. we conduct experiments to show the effectiveness of such feedback, the robustness to feedback noise, the impact of initial model performance, the trade-offs between online and offline learning, and the potential for domain adaptation. our study design emphasizes the potential for reducing annotation costs by annotating few examples or by utilizing existing datasets for new domains.", "Solved": NaN, "Unsolved": NaN}, "259991357": {"num": 726, "type": 1, "title": "Thrust: Adaptively Propels Large Language Models with External Knowledge", "pdfurl": "https://export.arxiv.org/pdf/2307.10442v1.pdf", "conclusion": "in this work, we propose instance-level adaptive propulsion of external knowledge (iapek) as a solution to propel model performance with external knowledge. accordingly, we propose a simple and effective instance-level metric, thrust, to perform the adaptive knowledge injection. extensive experiments show that thrust is a good indicator of models' knowledgeability and can improve the performance of utilizing external knowledge under various settings. understanding the delicate usage of potentially noisy knowledge for ptlms can further enable the models to conduct inference beyond the limitation of internal knowledge.", "Solved": NaN, "Unsolved": NaN}, "264935229": {"num": 727, "type": 1, "title": "IMPLICIT CHAIN OF THOUGHT REASONING VIA KNOWLEDGE DISTILLATION", "pdfurl": "https://export.arxiv.org/pdf/2311.01460v1.pdf", "conclusion": "lack of transparency and interpretability one of the main advantages of explicit cot is its inherent transparency: the intermediate steps allow for easy interpretation of the model's reasoning process.in contrast, implicit cot, by virtue of its internal processing within hidden states, lacks this transparency.while it achieves compactness and efficiency in generation, it sacrifices human interpretability, making it challenging to understand how the model arrives at its conclusions.reliance on the teacher's thought process our current three-step strategy is, at a high level, trying to distill the teacher model's horizontal reasoning process into the vertical reasoning process of the student and the emulator.while the ultimate goal of implicit reasoning is to allow models to develop their own unique trajectories of reasoning, our initial approach still relies heavily on the teacher's thought processes for a starting point.performance discrepancies our current results of implicit cot still lag behind the performance of explicit cot.however, this work is just a first step towards building implicit cot, and there exists ample room for further optimization.", "Solved": NaN, "Unsolved": NaN}, "234334015": {"num": 728, "type": 1, "title": "ExpMRC: explainability evaluation for machine reading comprehension", "pdfurl": "https://arxiv.org/pdf/2105.04126v1.pdf", "conclusion": "in this paper, we propose a comprehensive benchmark for evaluating the explainability of mrc systems. the proposed expmrc benchmark contains four datasets, including squad, cmrc 2018, race + , c 3 , covering span-extraction mrc and multiple-choice mrc in both en- glish and chinese. expmrc aims to evaluate the mrc system to give not only correct predictions on the final answer but also extract correct evidence for the answer. we set up several baseline systems to thoroughly evaluate the difficulties of expmrc. the experimental results show that both traditional and state-of-the-art pre-trained language models still underperform human performance by a large margin on most of the subsets, indicating that more efforts should be made on designing an effective approach for evidence extraction. we hope the release of the dataset will further accelerate the research on the explainability and interpretability of mrc systems, especially for the unsupervised approaches.", "Solved": NaN, "Unsolved": NaN}, "262053695": {"num": 729, "type": 1, "title": "MINT: EVALUATING LLMS IN MULTI-TURN INTER-ACTION WITH TOOLS AND LANGUAGE FEEDBACK", "pdfurl": "https://export.arxiv.org/pdf/2309.10691v2.pdf", "conclusion": "in this work, we present mint, an evaluation benchmark designed to evaluate llm's task-solving ability in multi-turn interaction by using tools and leveraging natural language feedback, which we", "Solved": NaN, "Unsolved": NaN}, "245219136": {"num": 730, "type": 1, "title": "Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.162.pdf", "conclusion": "augmenting pre-trained generation models with retrievers has shown to be effective in many knowledge-intensive tasks; however, they often rely on spurious cues or generate hallucinations during inference. we introduce a multi-task learning objective the combines answer generation and evidentiality prediction. we propose task-agnostic data mining techniques to obtain silver evidentiality labels to enable this auxiliary training. our experiments across five datasets show large performance improvements over baselines and our evidentialityguided generator advances the state-of-the-art performance on faviq-ambig, fever and wow. our analysis shows that multi-task learning and silver evidentiality mining both contribute to the performance improvements by helping the model learn to focus on and generate answers from more relevant passages. ", "Solved": NaN, "Unsolved": NaN}, "208000835": {"num": 731, "type": 1, "title": "Generalizing Question Answering System with Pre-trained Language Model Fine-tuning", "pdfurl": "https://www.aclweb.org/anthology/D19-5827.pdf", "conclusion": "in this paper, we propose a multi-task framework to improve the generalization ability of question answering systems by leveraging large pre-trained language models. experimental results indicate the effectiveness of our methods on broader qa tasks, with an average exact match score of 56.59 and an average f1 score of 68.98, which are significantly higher than the bert-large baseline results by 8.39 and 7.22, respectively.", "Solved": NaN, "Unsolved": NaN}, "246904646": {"num": 732, "type": 0, "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models", "pdfurl": "https://arxiv.org/pdf/2202.08772v1.pdf", "conclusion": "we comprehensively survey existing works about knowledgeintensive nlp with pre-trained language models and summarize the current progress in terms of the three critical components in plmkes: knowledge sources, knowledge-intensive nlp tasks, and knowledge fusion methods. based on the discussion about the three components, we further pose several challenges that would be influential in the practical usage and propose the related future directions in response to the challenges. we hope that this paper could provide nlp practitioners with a clear picture on the topic and boost the development of the current knowledge-intensive nlp technologies.", "Solved": NaN, "Unsolved": NaN}, "231718729": {"num": 733, "type": 1, "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.468.pdf", "conclusion": "in this work, we propose pre-finetuning, a stage after pre-training to further refine representations before end-task finetuning. we show that we can effectively learn more robust representations through multi-task learning (mtl) at scale. our mtl models outperform their vanilla pre-trained counterparts across several tasks. our analysis shows that properly scaling mtl with heterogeneous batches and loss scaling is critical to leveraging better representations. we also show a critical point regarding the number of tasks when doing multi-task learning, where fewer tasks degrade representations compared to the pre-trained model, but more tasks than this point improve representations.", "Solved": NaN, "Unsolved": NaN}, "252819276": {"num": 734, "type": 1, "title": "Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2022.coling-1.126.pdf", "conclusion": "this paper presents a novel method to guide the mrc model to better perform logical reasoning tasks. we propose a holistic graph-based system to model hierarchical logical reasoning chains. to our best knowledge, we are the first to deal with context at both discourse level and phrase level as the basis for logical reasoning. to decouple the interaction between the node features and type features, we apply hierarchical interaction mechanism to yield the appropriate representation for reading comprehension. on the logical qa benchmarks (reclor, logiqa) and natural language inference benchmarks (snli and anli), our proposed model has been shown effective by significantly outperforming the strong baselines.", "Solved": NaN, "Unsolved": NaN}, "258676557": {"num": 735, "type": 1, "title": "Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning", "pdfurl": "https://export.arxiv.org/pdf/2305.07475v1.pdf", "conclusion": "in this paper, we propose three solution program centric auxiliary pretraining tasks at both the whole program level and sub-program level. at the wholeprogram level, we propose the variable integrity ranking pretraining task, which guides the model to distinguish required and irrelevant variables in the noisy input. to further enhance the model's ability to learn the underlying reasoning process, we propose two additional pretraining tasks: variable operator prediction and variable keyphrase masking. these tasks help the model perform accurate sub-program construction. experimental results demonstrate the effectiveness of our method. variable integrity ranking achieves the most improvement on both the retriever and program solver. the sub-program level tasks substantially improve results on plms of different scales. our approach achieves 3.56% execution accuracy and 3.74% program accuracy improvement on the competitive roberta-large baseline.", "Solved": NaN, "Unsolved": NaN}, "218974030": {"num": 736, "type": 1, "title": "Project PIAF: Building a Native French Question-Answering Dataset", "pdfurl": "https://www.aclweb.org/anthology/2020.lrec-1.673.pdf", "conclusion": "motivated by the scarcity of non-english data, we described our ongoing effort towards gathering native qa samples for the french language, using a participatory approach. rather than a transactional approach to data collection, as usually adopted in crowd-sourcing efforts, we experiment with a comparatively slower and more engaging process, focusing on quality over quantity. amongst desirable sideeffects of our approach, we highlight the educational aspects, e.g. introducing a wider audience to ai concepts and methodologies during our annothathons.  table 3: n = 191 randomly sampled triplets were manually assigned into one or more of the above categories. words relevant to the corresponding reasoning type are in bold, and the annotated answer is underlined.  ", "Solved": NaN, "Unsolved": NaN}, "211572557": {"num": 737, "type": 1, "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-demos.2.pdf", "conclusion": "in this paper, we present textbrewer, a flexible pytorch-based distillation toolkit for nlp research and applications. textbrewer provides rich customization options for users to compare different distillation methods and build their strategies. we have conducted a series of experiments. the results show that the distilled models can achieve state-of-the-art results with simple settings. textbrewer also has its limitations. for example, its usability in generation tasks such as machine translation has not been tested. we will keep adding more examples and tests to expand textbrewer's scope of application.", "Solved": NaN, "Unsolved": NaN}, "254246737": {"num": 738, "type": 1, "title": "What is Not in the Context? Evaluation of Few-shot Learners with Informative Demonstrations", "pdfurl": "https://export.arxiv.org/pdf/2212.01692v1.pdf", "conclusion": "this work introduces a task of conceptual few-shot learning, that reflects on language models' ability to learn in-context and apply a specific, possibly novel reasoning concept. we find that current incontext few-shot learners are largely insensitive to the concepts presented in demonstrations, suggesting that their ability to learn in context might be conditioned by other factors, such as their memo-rization capacity, rather than their level of task comprehension. conceptual few-shot learning poses a challenge of more controllable and scalable fewshot learning to future few-shot learners.", "Solved": NaN, "Unsolved": NaN}, "263888725": {"num": 739, "type": 1, "title": "ViQuAE, a Dataset for Knowledge-based Visual Question Answering about Named Entities a Dataset for Knowledge-based Visual Question Answering about Named", "pdfurl": "https://hal-universite-paris-saclay.archives-ouvertes.fr/hal-03650618/document", "conclusion": "despite the improvement brought by dpr and the multimodal fusion, there is still a lot of room for improvement, which highlights the difficulty of the task, especially for questions about non-human entities. this can be explained by the specialized image representation of arcface, whereas imagenet-resnet and clip are more general. it also highlights the need to study visual representation of non-human entities, as exemplified in section 1.", "Solved": NaN, "Unsolved": NaN}, "235755349": {"num": 740, "type": 1, "title": "Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training", "pdfurl": "https://www.aclanthology.org/2021.acl-long.476.pdf", "conclusion": "in this paper, we propose a new approach to train multi-hop qa models, not to take reasoning shortcuts of guessing right answers without sufficient evidences. we do not require annotations and generate pseudo-evidentiality instead, by regularizing qa model from being overconfident when evidences are insufficient. our experimental results show that our method outperforms baselines on hotpotqa and has the effectiveness to distinguish between evidence-positive and negative set.", "Solved": NaN, "Unsolved": NaN}, "251320151": {"num": 741, "type": 1, "title": "Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models", "pdfurl": "https://export.arxiv.org/pdf/2204.11447v2.pdf", "conclusion": "in this paper, we propose to separately evaluate interpolation and extrapolation capabilities of neural retrieval models. considering the dynamics of queries in web search, we define them based on whether the training and test queries are similar or not. based on the definition, we investigate the bias in popular benchmarks, design associated evaluation methods, and revisit existing neural ranking models. we observe that the popular benchmarks are biased towards interpolation and thus may not reflect how models extrapolate.", "Solved": NaN, "Unsolved": NaN}, "264439639": {"num": 742, "type": 1, "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models", "pdfurl": "https://export.arxiv.org/pdf/2310.15372v1.pdf", "conclusion": "in this paper, we presented the epik-eval benchmark, a tool designed specifically to evaluate the proficiency of lms in consolidating their knowledge for problem-solving tasks.our findings underscore the limitations of current lms, which appear to mostly maintain a disjoint knowledge state of training observations.further, we note a significant performance gap and an increased rate of hallucinations for models trained on segmented narratives compared to those trained on unsegmented ones.we attribute these discrepancies to the training objectives of the models, which underscores the need to more effectively model the dependencies within the training corpus.by highlighting current limitations and opportunities for improving lms, these results delineate paths for future research, hopefully enabling the growth of language models beyond simple knowledge bases.", "Solved": NaN, "Unsolved": NaN}, "252280663": {"num": 743, "type": 1, "title": "Prompt-based Conservation Learning for Multi-hop Question Answering", "pdfurl": "https://www.aclanthology.org/2022.coling-1.154.pdf", "conclusion": "in this paper, we introduce a novel prompt-based conservation learning framework for multi-hop qa -a framework that retains knowledge from previous component tasks -able to answer questions in a principled way that matches human expectations by answering sub-questions and integrating the answers. by developing soft prompts related to reasoning types during training, we also show that we can condition plms to stimulate and apply the reasoning knowledge required for specific multihop questions. experimental results on multiple multi-hop qa datasets demonstrate the improved performance of pcl over previous multi-hop qa models in multi-hop qa.", "Solved": NaN, "Unsolved": NaN}, "256597851": {"num": 744, "type": 1, "title": "LIQUID: A Framework for List Question Answering Dataset Generation", "pdfurl": "https://export.arxiv.org/pdf/2302.01691v2.pdf", "conclusion": "herein, we introduced liquid, a framework that automatically generates list qa datasets from unlabeled corpora to alleviate the data scarcity problem in this field. our synthetic data significantly improved the performance of the current supervised models on five benchmark datasets. we thoroughly analyzed the effect of each component in liq-uid and generated data quantitatively and qualitatively.   table 10: distribution of answer types for the synthetic and bioasq 9b data. table 9 presents the distribution of the number of answers. similar to the results in the general domain (table 5), the synthetic data were more skewed toward smaller numbers of answers than the labeled data, but some answers (14.2%) had four or more spans.", "Solved": NaN, "Unsolved": NaN}, "234479457": {"num": 745, "type": 1, "title": "A Multilingual Modeling Method for Span-Extraction Reading Comprehension CCS CONCEPTS \u2022 Computing methodologies \u2192 Natural language processing; Multilingual modeling KEYWORDS multilingual modeling, span-extraction reading comprehension, multilingual attention, self-adaptive attention", "pdfurl": "https://arxiv.org/pdf/2105.14880v1.pdf", "conclusion": "despite tremendous advances have been made in the field of spanextraction reading comprehension in recent years, large-scale highquality extractive qa datasets in languages other than english remain scarce, and collecting such a sufficient amount of training data for each language is costly, and even impossible, making training reading comprehension systems in other languages challenging. to make full use of all existing extractive training datasets in various languages and to learn the rich hidden semantic knowledge from different language families, we propose xlrc, a multilingual extractive reading comprehension method, to simultaneously model the existing extractive reading comprehension training data in a multilingual environment by using multilingual bert and multilingual attention. experimental results demonstrate the effectiveness of our proposed multilingual modelling by transferring the semantic knowledge learned from various existing datasets in different languages.", "Solved": NaN, "Unsolved": NaN}, "236459873": {"num": 746, "type": 1, "title": "Explanations for CommonsenseQA: New Dataset and Models", "pdfurl": "https://aclanthology.org/2021.acl-long.238.pdf", "conclusion": "we have presented desiderata of what constitutes an explanation in the case of common-sense qa. based on it, we generated a human-annotated explanation dataset ecqa for commonsenseqa. we have also proposed models to retrieve and generate common-sense facts required to justify the answer choice. we have publicly released our crowdsourced ecqa dataset and code/models. in future work, we plan to explore directions to design rlbased schemes for joint training of property ranker and property selector components in the xr system and joint training of xgp and xgf-ii to generate free-flow explanation. another direction is to improve the accuracy and interpretability of the existing models for commonsenseqa using the ecqa dataset.", "Solved": NaN, "Unsolved": NaN}, "222177127": {"num": 747, "type": 1, "title": "Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment", "pdfurl": "https://arxiv.org/pdf/2010.03249v1.pdf", "conclusion": "we propose a novel ea model (attrgnn) and contribute a hard experimental setting for practical evaluation. attrgnn can integrate both attribute and relation triples with varying importance for better performance. experimental results under the regular and hard settings present significant improvements of our proposed model, and the severe dataset bias can be effectively alleviated in our proposed hard setting.", "Solved": NaN, "Unsolved": NaN}, "220403560": {"num": 748, "type": 1, "title": "KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base", "pdfurl": "https://arxiv.org/pdf/2007.03875v1.pdf", "conclusion": "we introduce kqa pro, a new benchmark of complex kbqa with following features: 1) with explicit reasoning process, including sparqls and programs; 2) large-scale, with about 120k natural questions; 3) with rich kinds of knowledge, including relational, literal, and high-level. we create a unified codebase to implement the baselines and state-of-the-arts of complex kbqa. extensive experiments reveal a huge gap between machines and humans, demonstrating that kqa pro is very challenging. kqa pro is the first kbqa benchmark that provides the explicit reasoning process for complex questions. we hope that these additional information can help machines develop the compositional reasoning ability and learn to tackle complex questions like a human.", "Solved": NaN, "Unsolved": NaN}, "253237103": {"num": 749, "type": 1, "title": "RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees", "pdfurl": "https://www.aclanthology.org/2022.emnlp-main.483.pdf", "conclusion": "we presented rlet, a rl-based entailment tree generation framework, which contains sentences selection and deduction generation modules and can be trained with cumulative signals across the entire reasoning tree. experiments show that rlet outperforms existing baselines on structure correctness and is applicable in practical scenarios. future directions include applying rl framework on other stepwise methods with more stable and sophisticated rl algorithms.", "Solved": NaN, "Unsolved": NaN}, "256900985": {"num": 750, "type": 1, "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?", "pdfurl": "https://www.aclanthology.org/2023.acl-long.659.pdf", "conclusion": "in this paper, we have introduced meta prompt tuning (mpt), which learns to initialize the prompt embeddings for adapting to a target task. we have identified key research questions and systematically studied where and how meta learning can improve cross-task generalization in prompt tuning. we have empirically analyzed a representative set of meta learning methods in a variety of adaptation settings on a large, diverse collection of few-shot tasks. extensive experimental results and analysis verify the effectiveness of mpt. given the findings, in the future, we would like to explore more advanced meta learning algorithms which can consistently outperform multi-task learning.", "Solved": NaN, "Unsolved": NaN}, "252907386": {"num": 751, "type": 1, "title": "Can language representation models think in bets?", "pdfurl": "https://export.arxiv.org/pdf/2210.07519v1.pdf", "conclusion": "modern lrms, based on transformer neural networks, have rapidly exceeded the previous state-of-theart on a range of natural language understanding tasks, including question answering, text summarization and information extraction [104][105][106]. in this article, we addressed the question of whether such lrms can be adapted for (approximately) rational decision-making and preference elicitation. in the cognitive science literature, such decision-making is often evaluated using bets. given the near human-like performance of lrms on language-based problems, we formulated a set of rqs to specifically test whether: (i) lrms have a distinct preference for high-value items over lowvalue items, especially when the items were not seen during training, and after stratifying by the format of the questions, (ii) lrms can make, or be taught to make, (approximately rational) bets in a generalizable manner, including when an lrm has been fine-tuned on one 'modality' of bet but is evaluated on another modality.", "Solved": NaN, "Unsolved": NaN}, "252819325": {"num": 752, "type": 1, "title": "CMQA: A Dataset of Conditional Question Answering with Multiple-Span Answers", "pdfurl": "https://www.aclanthology.org/2022.coling-1.146.pdf", "conclusion": "in this paper, we propose a new challenge: conditional question answering with hierarchical multispan answers, which might be widespread in multispan qa in real-world scenarios. moreover, we introduce cmqa, which contains conditional and hierarchical samples to study the new proposed task. data analysis and experimental results show the main characteristics and challenges of cmqa, and the poor model performance demonstrates that the proposed task is challenging for the community to solve. we believe cmqa can serve as a benchmark to study the new proposed task and help build more reliable and sophisticated qa systems. figure 4: the amount of labels in cmqa. c-a refers to condition-answer and c-f refers to coarse-fine.", "Solved": NaN, "Unsolved": NaN}, "221970302": {"num": 753, "type": 1, "title": "ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS WITH MULTI-HOP DENSE RETRIEVAL", "pdfurl": "https://arxiv.org/pdf/2009.12756v1.pdf", "conclusion": "in this work, we generalized the recently proposed successful dense retrieval methods by extending them to the multi-hop setting. this allowed us to handle complex multi-hop queries with much better accuracy and efficiency than the previous best methods. we demonstrated the versatility of our approach by applying it to two different tasks, using a variety of downstream modules. in addition, the simplicity of the framework and the fact that it does not depend on a corpus-dependent graph structure opens the possibility of applying such multi-hop retrieval methods more easily and broadly cross different domains and settings.", "Solved": NaN, "Unsolved": NaN}, "259252098": {"num": 754, "type": 1, "title": "SciMRC: Multi-perspective Scientific Machine Reading Comprehension", "pdfurl": "https://export.arxiv.org/pdf/2306.14149v1.pdf", "conclusion": "in this paper, we proposed a novel multiperspective scientific machine (smrc) reading comprehension dataset, called scimrc, with different perspectives of readers, including begin-ners, students and experts. extensive experimental results suggest the inner relations and differences among different perspectives, suggesting the importance of analyzing perspectives. the extensive results suggest that scimrc could serve as a testbed for evaluating smrc research.", "Solved": NaN, "Unsolved": NaN}, "248496003": {"num": 755, "type": 1, "title": "Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning", "pdfurl": "https://arxiv.org/pdf/2205.00731v2.pdf", "conclusion": "we propose a two-branch graph transformer network for logical reasoning of text, which is named as logiformer. firstly, we introduce two different strategies to construct the logical graph and syntax graph respectively. especially for the logical graph, we are the first to model both causal relations and negations in the logical reasoning task. secondly, we feed the extracted node sequences to the fully connected graph transformer for each graph. the topology of the graph is utilized to form the attention bias for the self-attention layers. thirdly, a dynamic gate mechanism is applied to make a fusion of the features from two branches. to improve the awareness of different question types, the question feature is updated based on the self-attention module. finally, the concatenated text sequence is passed through the feed forward layer and obtains the answer prediction. the whole reasoning process provides the interpretability, reflected by logical units with explicit relations and the visualization of the attention maps.", "Solved": NaN, "Unsolved": NaN}, "225040262": {"num": 756, "type": 1, "title": "Stronger Transformers for Neural Multi-Hop Question Generation", "pdfurl": "https://arxiv.org/pdf/2010.11374v1.pdf", "conclusion": "in this work, we propose a series of strong transformer models for multi-hop qg. to effectively encode the context documents and the answer, we introduce answer type embeddings and a new sublayer to incorporate the extracted entity-centric graph. we also propose an auxiliary contrastive objective to identify the supporting facts and a data filtering approach to balance the training-test distribution mismatch. experiments on the hotpotqa dataset show that our models outperform the current best approaches by a substantial margin of 5 bleu points. our analysis further reveals that graph-based components may not be the most critical in improving the performance, but can render complementary strengths to the transformer.  ", "Solved": NaN, "Unsolved": NaN}, "218521479": {"num": 757, "type": 1, "title": "VisBERT: Hidden-State Visualizations for Transformers", "pdfurl": "https://arxiv.org/pdf/2011.04507v1.pdf", "conclusion": "visbert establishes a novel method to analyze the behavior of bert models, in particular regarding the question answering task. our method allows a fine-grained analysis of each of the bert layers and depicts how each input token changes in each step. additionally, visbert reveals four phases in bert's transformations that are common to all of the datasets we examined and that mirror the traditional nlp pipeline, cf. [10]. we establish this behaviour on three diverse question answering datasets and make all three models available for users to make their own analyses on their own data, as well as the code to reproduce this visualization.", "Solved": NaN, "Unsolved": NaN}, "218613640": {"num": 758, "type": 1, "title": "Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond", "pdfurl": "https://arxiv.org/pdf/2005.06249v1.pdf", "conclusion": "this work comprehensively reviews the studies of mrc in the scopes of background, definition, development, influence, datasets, technical and benchmark highlights, trends, and opportunities. we first briefly introduced the history of mrc and the background of contextualized language models. then, we discussed the role of contextualized language models and the influence of mrc to the nlp community. the previous technical advances were summarized in the framework of encoder to decoder. after going through the mechanisms of mrc systems, we showed the highlights in different stages of mrc studies. finally, we summarized the trends and opportunities. the basic views we have arrived at are that 1) mrc boosts the progress from language processing to understanding; 2) the rapid improvement of mrc systems greatly benefits from the progress of clms; 3) the theme of mrc is gradually moving from shallow text matching to cognitive reasoning.", "Solved": NaN, "Unsolved": NaN}, "258557836": {"num": 759, "type": 1, "title": "A Frustratingly Easy Improvement for Position Embeddings via Random Padding", "pdfurl": "https://export.arxiv.org/pdf/2305.04859v1.pdf", "conclusion": "in this work, we propose a simple strategy, random padding, to improve the performance of extractive qa models, especially when they are trained on short contexts but evaluated on longer contexts. our method only re-organizes the input token sequences when fine-tuning, without any modifications to the architectures of plms. experiments reveal that our simple method can effectively enhance qa models when predicting answers at the rear positions, where the position embeddings may not be sufficiently updated without random padding. we also show that our simple strategy can improve the performance of plm components in more benchmarks and tasks where accurate local context representations over longer context are necessary.", "Solved": NaN, "Unsolved": NaN}, "254877586": {"num": 760, "type": 1, "title": "When Do Decompositions Help for Machine Reading?", "pdfurl": "https://export.arxiv.org/pdf/2212.10019v1.pdf", "conclusion": "our work explored when decompositions are helpful for machine reading. we showed that decompositions are helpful when there is limited data available, or when parameters cannot be tuned. however, when enough data exists (empirically around a few hundred instances) and parameters can be fine-tuned, it is best to let the model learn the decompositions implicitly through end-to-end training. we hope that our work will help to inform readers as they create new datasets and select methods to use for complex question answering.", "Solved": NaN, "Unsolved": NaN}, "227231636": {"num": 761, "type": 1, "title": "Towards building a Robust Industry-scale Question Answering System", "pdfurl": "https://www.aclweb.org/anthology/2020.coling-industry.9.pdf", "conclusion": "although large pre-trained language models have shown super-human performance on benchmark datasets like squad, we show that there is plenty of room to make improvements on top of bert qa . specifically, we outline prior strategies that do not work on a real benchmark consisting of \"natural questions\" showing the difficulty of the dataset and need for better algorithms. we introduce gaama and outline several strategies that are broadly classified under attention and data augmentation and show how effective it can be to attain competitive performance on nq compared to other industry baselines. we also outline gaama's ootb zero-shot transfer on two unseen datsets and show optimistic performance. our future work will involve adding larger pre-trained language models like t5 and also exploring multi-lingual qa.", "Solved": NaN, "Unsolved": NaN}, "263671701": {"num": 762, "type": 1, "title": "DSPY: COMPILING DECLARATIVE LANGUAGE MODEL CALLS INTO SELF-IMPROVING PIPELINES", "pdfurl": "https://export.arxiv.org/pdf/2310.03714v1.pdf", "conclusion": "this paper introduced dspy, a new programming model for designing ai systems using pipelines of pretrained lms and other tools.we presented three new concepts introduced in this abstraction (dspy signatures, modules, and teleprompters), and showed in two very different case studies that it supports rapid development of highly effective systems that use relatively small lms.we have maintained open-source versions of this framework for close to a year.in this period, we have seen and created a large number of programs that were compiled to high-quality systems by dspy, spanning tasks from information extraction to low-resource synthetic data generation.in the interest of space and to maintain reasonable scope in this paper, we leave reporting on such tasks under controlled experimental conditions to future work.while in-context learning has proved transformative over the past 2-3 years of lm research, we argue that the true expressive power in this emerging paradigm is in building sophisticated text transformation graphs in which composable modules and optimizers (teleprompters) come together to leverage lms in more systematic and reliable ways.", "Solved": NaN, "Unsolved": NaN}, "248227734": {"num": 763, "type": 1, "title": "Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes", "pdfurl": "https://export.arxiv.org/pdf/2204.07693v2.pdf", "conclusion": "we have demonstrated the utility of question decomposition as an effective means to probe pretrained multi-hop question-answering models for supporting evidence. through simulatability experiments, we show the effectiveness of this explanation form at allowing humans to predict model behavior, a sign that it helps humans to form an accurate mental model of the machine learning system (jacovi et al., 2022). this ability to predict system performance occurs at the instance level instead of a sense of trust of the overall system, which can be important if the accuracy of the system is variable based on the question.", "Solved": NaN, "Unsolved": NaN}, "252819353": {"num": 764, "type": 1, "title": "Question Generation Based on Grammar Knowledge and Fine-grained Classification", "pdfurl": "https://www.aclanthology.org/2022.coling-1.562.pdf", "conclusion": "to solve the problem of mismatch between question types and answers in question generation, this paper constructs a question type classifier and a question generator. we classify questions into finegrained classification and integrate grammar knowledge into question type classifier to improve the accuracy of question types, then, the prediction results of the classifier are fused into the question generator to improve the performance of question generation. to verify the effectiveness of our model, we perform an upper bound analysis on it, we integrate grammar knowledge into the question generator to provide accurate question types to guide the model to generate questions. the final experimental results show that the method proposed in this paper not only improves the accuracy of question words in the generated question, but also improves the quality of the generated question.", "Solved": NaN, "Unsolved": NaN}, "258615193": {"num": 765, "type": 1, "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models", "pdfurl": "https://www.aclanthology.org/2023.acl-long.307.pdf", "conclusion": "despite the simplicity and ubiquity of lexical matching as an evaluation metric in open-domain qa, it is unnecessarily rigid because plausible candidate answers are likely not to appear in the list of gold answers. this flaw has been long known, but the efforts to circumvent it have been mostly artisanal. in this paper, we report a systematic study of lexical matching by manually judging answers generated by several prominent open-domain qa models. we found that llms achieve stateof-the-art on nq-open. the accuracy of models is severely underestimated, with most em failure cases stemming from syntactical variations of answers. moreover, a zero-shot prompting method can be a reasonable substitute for human evaluation although it cannot detect unattributability in long-form answers. our insights and analysis in this paper will hopefully underpin the development of solid evaluation techniques in open-domain qa.", "Solved": NaN, "Unsolved": NaN}, "258959258": {"num": 766, "type": 1, "title": "Do Large Language Models Know What They Don't Know?", "pdfurl": "https://export.arxiv.org/pdf/2305.18153v2.pdf", "conclusion": "this study investigates the self-knowledge of llms by evaluating their ability to identify unanswerable questions. through the introduction of a novel dataset and an automated method for detecting uncertainty in the models' responses, we are able to accurately measure the self-knowledge of llms such as gpt-3, instructgpt and llama. our results reveal that while these models possess a certain degree of self-knowledge, there is still an apparent disparity in comparison to human selfknowledge. this highlights the need for further research in this area to enhance the ability of llms to understand their own limitations on the unknows. such efforts will lead to more accurate and reliable responses from llms, which will have a positive impact on their applications in diverse fields.", "Solved": NaN, "Unsolved": NaN}, "235485084": {"num": 767, "type": 1, "title": "Weakly Supervised Pre-Training for Multi-Hop Retriever", "pdfurl": "https://arxiv.org/pdf/2106.09983v1.pdf", "conclusion": "answering complex questions includes reasoning across multiple documents. recent studies have found that reasoning requires learning sub-question detection and relevant document retrieval to predict n correct answer with supporting facts. however, building such datasets requires costly human annotation and has limited scalability. to address this issue, we proposed a weakly supervised pre-training method for multi-hop retriever, louvre. our pretraining method contains three elements: \"next document prediction\" task, \"bridge entity re-phrasing\", and a model. we demonstrated the efficacy of louvre and its robustness on few-shot settings with extensive experiments on supporting document retrieval task and end-to-end multi-hop qa task. we also showed that our method performs very well at a much lower inference cost.   calculate score of each paths/documents by jointly encode each document with the given question. as a result, reranking takes a huge portion of computation time of the end-to-end multi-hop qa pipeline. semanticretrievalmrs (nie et al., 2019) propose the document reranking model that takes output of sparse retrievers such as tf-idf. since the model outputs documents not a list of supporting documents, we use the same document rearranging method as tf-idf above. pathretriever (asai et al., 2020) and", "Solved": NaN, "Unsolved": NaN}, "250390431": {"num": 768, "type": 1, "title": "Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding", "pdfurl": "https://www.aclanthology.org/2022.naacl-main.28.pdf", "conclusion": "temporal reading comprehension plays a critical role in natural language understanding. in this paper, we propose a precise question understanding method to tackle the trc problem. specifically, we encode temporal ordering questions into repre-sentations of referred events and concerned temporal relations, based on which candidate answers are evaluated in terms of their temporal relations to the referred events. in addition, a contrastive loss is employed to empower the model to capture essential differences among temporal relations. experimental results based on four pre-trained models verify the effectiveness of our proposed approach. in the future, we will investigate general approaches to handle more diverse temporal relation understanding problems and improve the passage understanding capability for temporal reading comprehension.", "Solved": NaN, "Unsolved": NaN}, "260865960": {"num": 769, "type": 1, "title": "BOLAA: BENCHMARKING AND ORCHESTRATING LLM-AUGMENTED AUTONOMOUS AGENTS", "pdfurl": "https://export.arxiv.org/pdf/2308.05960v1.pdf", "conclusion": "in this paper, we systematically investigate the performances of various laa architecture paired with different llm backbones. we also provide one novel orchestrating method for multiple agents, i.e. bolaa. the benchmarking results provide experimental justification for the laa investigation and verify the potential benefits of bolaa architecture. during the investigation, we also identify the challenge of designing bolaa architecture for environments with compounding actions. in the future, we will explore whether we can harness llms in the controller such that selection and communication with labor agents is also fully autonomous. we will continue developing more laa architectures and include more llms and environments for evaluations.", "Solved": NaN, "Unsolved": NaN}, "256827305": {"num": 770, "type": 1, "title": "Improving Out-of-Distribution Generalization of Neural Rerankers with Contextualized Late Interaction", "pdfurl": "https://export.arxiv.org/pdf/2302.06589v1.pdf", "conclusion": "in this work, we presented our finding that adding late interaction to existing rerankers brings visible improvement to out-of-distribution capacity without any degradation on in-domain effectiveness, even though the reranker already processes the token interaction via the attention mechanism at previous layers. extensive experiments on different model sizes and first-stage retrievers show that this improvement is consistent, and according to our analysis, the improvement is more prominent on longer queries. our findings suggest that boiling all information into the [cls] token may not be the optimal choice for neural rerankers, and more studies are required to better explore its capacity.", "Solved": NaN, "Unsolved": NaN}, "264490922": {"num": 771, "type": 1, "title": "What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems", "pdfurl": "https://export.arxiv.org/pdf/2305.14331v2.pdf", "conclusion": "large general-purpose language models, such as the gpt family of models (brown et al., 2020;openai, 2023), lamda (thoppilan et al., 2022), palm (chowdhery et al., 2022;anil et al., 2023), and others, have propagated into informationseeking workflows of a general audience.a vast host of existing and ongoing work in nlp examines the deficiencies of these language models, ranging from hallucinated generations (bang et al., 2023;ji et al., 2023), lack of transparency (weld and bansal, 2019;lipton, 2018), reasoning gaps (bang et al., 2023;press et al., 2022), and more.however, close examination of the other piece of the puzzle-the user-is fairly sparse.literature in explainable nlp considers how nlp systems, and their underlying reasoning process, can be made available to the user to allow for better, more informed use of these systems and their predictions (ribeiro et al., 2016;wang and yin, 2021).extending this view of explainability, we considered the question of aiding users in their decision-making process, not by explaining the model prediction but by providing them with relevant pieces of information to assess the prediction.as argued by the contemporary work by fok and weld (2023), explanations in the form of the model's internal reasoning are rarely useful for supporting human decision-making (bansal et al., 2021;bu\u00e7inca et al., 2021;zhang et al., 2020).instead, explanations should aim to help the users assess the model prediction.our study takes a step in this direction by analyzing users' trust and reliance on qa models by providing information external to the model's input to enable verification of model predictions.", "Solved": NaN, "Unsolved": NaN}, "247476141": {"num": 772, "type": 1, "title": "Hyperdecoders: Instance-specific decoders for multi-task NLP", "pdfurl": "https://export.arxiv.org/pdf/2203.08304v3.pdf", "conclusion": "we propose a novel method for generating adapters conditioned on a model's input and show that this improves performance in multi-task settings across a variety of tasks. we explore the effectiveness of our approach for sequence classification, qa, and summarisation tasks, and find that it often outperforms strong parameter-efficient baselines. future work could examine applying our approach to other architectures (e.g. decoder-only models) or explore the tradeoffs between shared and generated parameters across different layers. an analysis of our approach suggests the primary benefits come from improved control of the encoder over the decoder, enhancing the effects of positive transfer from the shared encoder. this allows our approach to efficiently adapt a pretrained language model to multiple tasks unseen during pretraining while still benefiting strongly from positive transfer.", "Solved": NaN, "Unsolved": NaN}, "252383151": {"num": 773, "type": 1, "title": "Activity report analysis with automatic single or multi span answer extraction", "pdfurl": "https://export.arxiv.org/pdf/2209.09316v1.pdf", "conclusion": "we present a simple multispan architecture, mseqa, for multi type question answering by classifying each sentence as a probable answer candidate. we show that when a combination of single/multi-span classifier with multispan tagging is used, the model provides robust answers for multi-span tasks without degrading its performance on single-span questions. as future work, we would like to further process selected sentences from the multi-span tagger and consolidate them into one fluent answer. we also plan to explore ways to put this question answering capability onto edge devices for various applications.  ", "Solved": NaN, "Unsolved": NaN}, "251594672": {"num": 774, "type": 1, "title": "CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks", "pdfurl": "https://export.arxiv.org/pdf/2208.07652v1.pdf", "conclusion": "in this paper, we have proposed corpusbrain, a novel pre-trained generative retrieval model to encode all information about the corpus into its parameters. to train such a strong generative model, we delicately devised a set of pre-training tasks to emphasize different aspects of semantics between queries and documents. the key idea is to sample a context from one document as a pseudo query and generate the document identifiers of source or destination documents based on hyperlinks. corpusbrain just needs to pre-train one model and could be then adapted to improve a diversity of downstream kilt tasks without the need of constructing additional index. through experiments on the kilt benchmark in terms of the retrieval task, corpusbrain achieved significant improvements over strong baseline approaches. we also showed that corpusbrain can achieve strong performance under both the zero-and low-resource settings.", "Solved": NaN, "Unsolved": NaN}, "259833782": {"num": 775, "type": 1, "title": "Can In-context Learners Learn a Reasoning Concept from Demonstrations?", "pdfurl": "https://www.aclanthology.org/2023.nlrse-1.8.pdf", "conclusion": "this work introduces a task of conceptual few-shot learning that reflects on in-context learners' ability to learn to apply a specific reasoning concept that can be informative for prediction. we assess a set of recent in-context learners for this ability over a set of concepts extracted from human explanations.", "Solved": NaN, "Unsolved": NaN}, "246863799": {"num": 776, "type": 1, "title": "MuLD: The Multitask Long Document Benchmark", "pdfurl": "https://arxiv.org/pdf/2202.07362v1.pdf", "conclusion": "to enable the evaluation of long document models, we introduce muld: a benchmark of varied nlp tasks where each document consists of more than 10,000 tokens. the six tasks in our benchmark are created by filtering, extending, or modifying existing nlp tasks and are designed to require a long context for high performance. we evaluate simple chunking-based baselines, and find that the longformer model is able to outperform the t5 model suggesting our benchmark is a good test for the ability of models to make use of longer contexts. we believe that the technique explored in this work of augmenting and extending existing 'short document' datasets, can be applied to many other nlp tasks. as the performance of efficient transformers improves, we anticipate the need to update this benchmark with more challenging tasks. while we are focused on creating a benchmark which tests a model's ability to solve real-world long document tasks, we also expect improvements in the efficiencies of the models themselves which may make datasets with more than 100,000 tokens necessary which may require a fundamentally different approach to creating long document datasets. we leave for future work both the development of improved chunking methods, and more efficient transformers which make such methods unnecessary. we hope that the muld benchmark will encourage this further research into efficient models for long document nlp. to this end we provide the data, baseline models, and other code at www.github.com/ ghomashudson/muld.", "Solved": NaN, "Unsolved": NaN}, "263829433": {"num": 777, "type": 1, "title": "DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries", "pdfurl": "https://export.arxiv.org/pdf/2310.04678v3.pdf", "conclusion": "this paper introduces a novel task, scientific document retrieval using multi-level aspect-based queries (doris-mae), aimed at modeling the process of information retrieval in the context of scientific research.we also present a dataset for doris-mae generated using the anno-gpt framework.", "Solved": NaN, "Unsolved": NaN}, "252846009": {"num": 778, "type": 1, "title": "PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers", "pdfurl": "https://www.aclanthology.org/2023.eacl-main.223.pdf", "conclusion": "in the half decade since (poliak et al., 2018) single sentence relation leakage bias has proven to remain a difficult issue. efforts to debias nli have led to datasets that merely exhibit different kinds of bias than those shown before, or less saturated benchmarks that continue to exhibit cheating features. future work must prioritize reducing observable bias directly using a model-driven approach.", "Solved": NaN, "Unsolved": NaN}, "221749191": {"num": 779, "type": 1, "title": "Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning", "pdfurl": "https://www.aclweb.org/anthology/2020.emnlp-main.712.pdf", "conclusion": "progress in multi-hop qa under the reading comprehension setting relies on understanding and quantifying the types of undesirable reasoning current models may perform. this work introduced a formalization of disconnected reasoning, a form of bad reasoning prevalent in multi-hop models. it showed that a large portion of current progress in multifact reasoning can be attributed to disconnected reasoning. using a notion of contrastive sufficiency, it showed how to automatically transform existing support-annotated multi-hop datasets to create a more difficult and less cheatable dataset that results in reduced disconnected reasoning.", "Solved": NaN, "Unsolved": NaN}, "253708231": {"num": 780, "type": 1, "title": "CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval", "pdfurl": "https://www.aclanthology.org/2023.acl-long.663.pdf", "conclusion": "this paper proposes a novel multi-vector retrieval method that achieves state-of-the-art performance on several benchmark datasets while being 40\u00d7 faster than colbert-v2 and 17\u00d7 faster than the most efficient multi-vector retrieval library to date, plaid, on gpus. by jointly optimizing for the token index size and load balancing, our new dynamic lexical routing scheme greatly reduces the redundancy in the all-to-all token interaction of col-bert while bridging the word-mismatch problem in coil. experiments on both in-domain and outof-domain datasets demonstrate the effectiveness and efficiency of our model.", "Solved": NaN, "Unsolved": NaN}, "262459341": {"num": 781, "type": 1, "title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models", "pdfurl": "https://export.arxiv.org/pdf/2309.13345v1.pdf", "conclusion": "we propose bamboo, a benchmark for comprehensively evaluating the long text modeling capabilities of llms.bamboo consists of five tasks with two length levels, enabling the evaluation of llms' main capacities across various dimensions and domains.based on the evaluation of several long context models on bamboo, we give an overall analysis of the performances of different models and tasks.we have discussed key questions of long text models, provide a qualitative analysis of long text modeling tasks, and suggest directions for improving long context modeling abilities.we believe bamboo can be employed to analyze the extensive capacities and advance the long text modeling proficiency of llms in the future.", "Solved": NaN, "Unsolved": NaN}, "86611921": {"num": 782, "type": 1, "title": "Natural Questions: A Benchmark for Question Answering Research", "pdfurl": "https://www.aclweb.org/anthology/Q19-1026.pdf", "conclusion": "we argue that progress on qa has been hindered by a lack of appropriate training and test data. to address this, we present the natural questions corpus. this is the first large publicly available data set to pair real user queries with high-quality annotations of answers in documents. we also present metrics to be used with nq, for the purposes of evaluating the performance of question answering systems. we demonstrate a high upper bound on these metrics and show that existing methods do not approach this upper bound. we argue that for them to do so will require significant advances in nlu. figure 5 shows example questions from the data set. figure 6 shows example question/answer pairs from the data set, together with expert judgments and statistics from the 25-way annotations.", "Solved": NaN, "Unsolved": NaN}, "246996947": {"num": 783, "type": 1, "title": "SGPT: GPT Sentence Embeddings for Semantic Search", "pdfurl": "https://export.arxiv.org/pdf/2202.08904v5.pdf", "conclusion": "this work presented sgpt. building on sbert, we proposed modifications to gpt models to use them as cross-or bi-encoders for semantic search.", "Solved": NaN, "Unsolved": NaN}, "259360724": {"num": 784, "type": 1, "title": "Text Alignment Is An Efficient Unified Model for Massive NLP Tasks", "pdfurl": "https://export.arxiv.org/pdf/2307.02729v2.pdf", "conclusion": "we propose to unify diverse language tasks into a text pair alignment problem.this framework yields an alignment model (align) that, despite being less versatile than llms, solves a wide range of language problems efficiently with superior performance.we show that align outperforms task-specific models finetuned on several nlu tasks while having performance comparable to llms that are orders of magnitude larger.additionally, align excels in factual consistency evaluation, and can be used as an add-on to augment llms in qa tasks by identifying unanswerable questions.", "Solved": NaN, "Unsolved": NaN}, "207847382": {"num": 785, "type": 1, "title": "Ask to Learn: A Study on Curiosity-driven Question Generation", "pdfurl": "https://www.aclweb.org/anthology/2020.coling-main.202.pdf", "conclusion": "asking inquisitive questions allows humans to learn from each other and increase their knowledge. we thus proposed a new task: curiosity-driven question generation, which attempts to address such a key component for several human-machine interaction scenarios. in absence of data directly usable for this task, we proposed an automatic method to derive it from conversational qa datasets. further, recognizing that the great majority of qa datasets are not conversational, we also extended the method to standard qa data. our experiments, which include learning strategies such as pretraining and reinforcement, show promising results under both automatic and human evaluation. in future works, we plan to extend the approach to conditional generation of curiosity-driven questions.", "Solved": NaN, "Unsolved": NaN}, "254017497": {"num": 786, "type": 1, "title": "Solving math word problems with process- and outcome-based feedback", "pdfurl": "https://export.arxiv.org/pdf/2211.14275v1.pdf", "conclusion": "we generally expect process-based and outcome-based feedback to align more closely for math compared to other domains. for math problems, incorrect traces are typically harmful for reaching correct final answers. this matches our earlier finding that outcome-supervised rms approximate process-based feedback. in contrast, in other domains, undesirable behaviors may be helpful for highly-rated outcomes, e.g., manipulation may increase reported user satisfaction. as a result, we believe optimizing for outcomes (final-answer correctness) for math problems has a stronger effect on inducing a correct process than it would in other domains.", "Solved": NaN, "Unsolved": NaN}, "243865561": {"num": 787, "type": 1, "title": "Neural Natural Logic Inference for Interpretable Question Answering", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.298.pdf", "conclusion": "in this work, we explore the feasibility of combining natural logic with neural networks for interpretable question answering. we present an end-toend differentiable method for learning the parameters as well as the structure of natural logical rules, which is capable of considering the contextual information while conducting natural logic-based reasoning. experimental results on the regents science exam of the aristo dataset show that our proposed model could bring improvements over baseline methods.", "Solved": NaN, "Unsolved": NaN}, "232478685": {"num": 788, "type": 1, "title": "FeTaQA: Free-form Table Question Answering", "pdfurl": "https://www.aclanthology.org/2022.tacl-1.3.pdf", "conclusion": "in this paper, we introduced the task of generative table question answering with fetaqa, a table qa dataset consisting of complex questions that require free-form, elaborate answers. we also proposed two modeling approaches: (1) a pipeline model that incorporates a table semantic parser and a data-to-text generator, and (2) an end-to-end model that integrates query comprehension, reasoning and text generation. our experimental results indicate that the end-to-end model with a simple table encoding strategy achieves much higher scores than the pipeline model that requires table semantic parsing. furthermore, we show that fetaqa reveals the challenging nature of the table question answering task and calls for innovative model designs in the future.  ", "Solved": NaN, "Unsolved": NaN}, "221448158": {"num": 789, "type": 1, "title": "Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models", "pdfurl": "https://www.aclweb.org/anthology/2021.naacl-main.99.pdf", "conclusion": "we introduced text modular networks, which provide a general-purpose framework that casts complex tasks as textual interaction between existing, simpler qa modules. based on this conceptual framework, we built modularqa, an instantiation of tmns that can perform multi-hop and discrete numeric reasoning. empirically, modu-larqa is on-par with other modular approaches (which are dataset-specific) and outperforms a stateof-the-art model in a limited data setting and on expert-generated perturbations. importantly, mod-ularqa provides easy-to-interpret explanations of its reasoning. it is the first system that decomposes drop questions into textual sub-questions and can be applied to both drop and hotpotqa.", "Solved": NaN, "Unsolved": NaN}, "249847776": {"num": 790, "type": 1, "title": "Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering", "pdfurl": "https://export.arxiv.org/pdf/2206.08486v1.pdf", "conclusion": "we have demonstrated use of question decomposition based on the amr semantic representation for multi-hop qa, using an intrinsically interpretable framework to incorporate interpretability directly into the system structure. the complex task of multi-hop question interpretation is delegated to amr parsers. these parsers produce amr graphs to which two segmentation methods are applied, i.e., unknownbased and path-based graph segmentation, to achieve question decomposition. to generate a well-formed sub-question, we perform both amr parsing and amr-to-text generation with the same architecture, which uses a fully graphisomorphic linearization technique to complete the transformation from graph to a sequence of symbols without losing adjacency information. experimental results demonstrate that our qdamr system outperforms baseline question decomposition methods, both in performance of multi-hop qa and in the quality of generated sub-questions. since our proposed graph segmentation methods are based on predicateargument relations and parallel conditions/entities respectively, they could in principle be generalized to an unknown number of hops by identifying multiple predicate nodes or capturing multiple parallel conditions/entities. while, as noted, an aim of the qdamr design is to provide inherent interpretability, the effectiveness with which its outputs serve as explanations for human users remains to be evaluated in future work.", "Solved": NaN, "Unsolved": NaN}, "218486753": {"num": 791, "type": 1, "title": "R 4 C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.602.pdf", "conclusion": "towards evaluating rc systems' internal reasoning, we have proposed r 4 c that requires systems not only to output answers but also to give their derivations. for scalability, we have carefully developed a crowdsourced framework for annotating existing rc datasets with derivations. our experiments have demonstrated that our framework produces high-quality derivations, and that automatic evaluation metrics using multiple reference derivations can reliably capture oracle derivations. the experiments using two simple baseline models highlight the nature of r 4 c, namely that the derivation generation task is not simply the sf detection task. we make the dataset, automatic evaluation script, and baseline systems publicly available at https://naoya-i.github.io/r4c/.", "Solved": NaN, "Unsolved": NaN}, "233289699": {"num": 792, "type": 1, "title": "What to Pre-Train on? Efficient Intermediate Task Selection", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.827.pdf", "conclusion": "in this work we have established that intermediate pre-training can yield gains in adapter-based setups, however, around 44% of all transfer combinations result in decreased performances. we have consolidated several existing and new methods for efficiently identifying beneficial intermediate tasks.", "Solved": NaN, "Unsolved": NaN}, "259833830": {"num": 793, "type": 1, "title": "Enhancing text comprehension for Question Answering with Contrastive Learning", "pdfurl": "https://www.aclanthology.org/2023.repl4nlp-1.7.pdf", "conclusion": "in this work, we proposed a novel contrastiveqa to alleviate the problem of confusion in answer extraction using contrastive learning. the proposed model contrastiveqa consists of three tasks: 1) candidate answer extraction, 2) positive and negative sampling, 3) contrastiveqa using contrastive learning. we demonstrated the effectiveness of the proposed model by outperforming the performance of the baseline models on four benchmark datasets. in the future, we would like to adopt a method to specify the number of samples required in contrastive learning dynamically. this approach created an adaptive mechanism to use the number of samples for each input. this method could improve the overall performance of the model.", "Solved": NaN, "Unsolved": NaN}, "248571884": {"num": 794, "type": 1, "title": "Better Retrieval May Not Lead to Better Question Answering", "pdfurl": "https://arxiv.org/pdf/2205.03685v1.pdf", "conclusion": "in this work we investigate the impact of evidence quality on question answering on two open-domain qa datasets, strategyqa and hotpotqa. results show that (1) strategyqa is less sensitive to the evidence quality than hotpotqa and (2) boolean questions are less sensitive to the evidence quality than extractive questions. further study shows this is mainly because the model does not yield a sufficiently high score on those examples with perfect evidence retrieval. finally, we recommend that for the implicit decomposition open-domain question answering problems with boolean questions such as strategyqa, researchers start by improving qa performance given the gold paragraphs rather than improving evidence quality.", "Solved": NaN, "Unsolved": NaN}, "251224044": {"num": 795, "type": 1, "title": "Masked Autoencoders As The Unified Learners For Pre-Trained Sentence Representation", "pdfurl": "https://export.arxiv.org/pdf/2208.00231v1.pdf", "conclusion": "in this paper, a unified framework is presented for the pre-training of sentence representation. on top of the consecutive mae style pre-training on generic and domain-specific data, and with proper collaboration with contrastive learning, the generated model may support a wide variety of sentence representation tasks, including zero-shot retrieval, in-domain retrieval, and sentence embeddings. the experiment studies demonstrate that the proposed framework helps to achieve strong performances on benchmarks, like beir, ms marco, natural questions, sts and transfer tasks in senteval. for future works, we'll make extensions by scaling up the size of the encoding network and having it pre-trained on more unsupervised data, which will push to the empirical limit of the current method. we'll also explore its effectiveness in other languages other than english, and its impact to more applications beyond dense retrieval and nli.", "Solved": NaN, "Unsolved": NaN}, "218486765": {"num": 796, "type": 1, "title": "Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset", "pdfurl": "https://www.aclweb.org/anthology/2020.acl-main.410.pdf", "conclusion": "we study the clinical reading comprehension (clinirc) task with the recently created emrqa dataset. our qualitative and quantitative analysis as well as exploration of the two desired aspects of clinirc systems show that future clinical qa datasets should not only be large-scale but also less noisy and more diverse. moreover, questions that involve complex relations and are across different domains should be included, and then more advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated.  ", "Solved": NaN, "Unsolved": NaN}, "230799347": {"num": 797, "type": 1, "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies", "pdfurl": "https://arxiv.org/pdf/2101.02235v1.pdf", "conclusion": "we present strategyqa, the first dataset of implicit multi-step questions requiring a wide-range of reasoning skills. to build strategyqa, we introduced a novel annotation pipeline for eliciting creative questions that use simple language, but cover a challenging range of diverse strategies. questions in strategyqa are annotated with decomposition into reasoning steps and evidence paragraphs, to guide the ongoing research towards addressing implicit multi-hop reasoning.", "Solved": NaN, "Unsolved": NaN}, "231698419": {"num": 798, "type": 1, "title": "WebSRC: A Dataset for Web-Based Structural Reading Comprehension", "pdfurl": "https://www.aclanthology.org/2021.emnlp-main.343.pdf", "conclusion": "in this paper, we introduce websrc, a multi-modal dataset for web-based structural reading comprehension with both html documents and screenshots. the task is to answer questions about the web pages. we evaluate several baselines on our dataset, and the results showed that incorporating layout features with textual contents is crucial to web understanding, but how to utilize such structural information requires further investigation. we hope this work can push the research on web-based structural reading comprehension forward. in the future, we will go beyond web pages to explore more structural reading comprehension tasks.", "Solved": NaN, "Unsolved": NaN}, "254535672": {"num": 799, "type": 1, "title": "From Clozing to Comprehending: Retrofitting Pre-trained Language Model to Pre-trained Machine Reader *", "pdfurl": "https://export.arxiv.org/pdf/2212.04755v1.pdf", "conclusion": "this paper presents a novel mrc-sytle pre-training model, namely pmr. pmr can fully resolve the learning objective and data format gaps that frequently appear in fine-tuning existing plms. ex-perimental results over multiple dimensions, including effectiveness in solving few-shot tasks, outof-domain generalization, demonstrate its strong benefits in bridge the pretrain-finetune gaps. pmr also shows encouraging potential in explaining the sequence classification process in an extractiondriven mrc framework.  (trischler et al., 2017), searchqa (dunn et al., 2017, hot-potqa (yang et al., 2018), bioasq (tsatsaronis et al., 2015), drop (dua et al., 2019), duorc (saha et al., 2018), race (lai et al., 2017), re-lationextraction (levy et al., 2017), textbookqa (kembhavi et al., 2017. eqa has always been treated as an mrc problem, where the question serves as the mrc query, and the passage containing the answers serves as the mrc context. for ner, we follow li et al. (2020) to formulate ner into the mrc paradigm, where the entity label together with its description serves as the mrc query, and the input text serves as the mrc context. the goal is to extract the corresponding entities as answers. we use the eq. 4 as the learning objective, where y ext i,j indicates that the input span x i:j is an answer/entity.", "Solved": NaN, "Unsolved": NaN}, "247476217": {"num": 800, "type": 1, "title": "Tracing Origins: Coreference-aware Machine Reading Comprehension", "pdfurl": "https://arxiv.org/pdf/2110.07961v2.pdf", "conclusion": "in this paper, we present intuitive methods to solve coreference-intensive machine reading comprehension tasks by following the reading process of human in which people connect the anaphoric expressions with explicit instructions. we demonstrate that all our three fine-tuning methods, including coref gnn , coref addatt and coref multiatt , are superior to the pre-trained language models that incorporate the coreference information in the pretraining stage, such as corefroberta large . as the fine-tuning methods rely on the coreference resolution models supplied by other researchers, their performance is also constrained by the accuracy of those coreference resolution models. in addition, the questions that require multistep reasoning, span multiple entities or contain multiple answer items also pose the challenges to our models. in the future, with more in-depth study on human reasoning in reading comprehension and more progress in graph neural networks, the gnnbased coreference graph can be enriched with more edge types and diverse structures to leverage more linguistic knowledge and gain better performance.", "Solved": NaN, "Unsolved": NaN}, "251224058": {"num": 801, "type": 1, "title": "Efficient Long-Text Understanding with Short-Text Models", "pdfurl": "https://export.arxiv.org/pdf/2208.00748v3.pdf", "conclusion": "we present sled as a simple and effective method to extend the capabilities of pretrained short-text models to long-text tasks. despite its impressive empirical performance on scrolls, sled suffers from two disadvantages which may limit its applicability to some long-range tasks.long output to obtain linear complexity, sled assumes the output length k is constant. this is since the decoder uses quadratic selfattention over the output, on top of o(nk) crossattention between the output and input. while most current long-text tasks follow this assumption, future tasks, such as academic reports or script writing, may require long text generation. this limitation is not unique to sled and affects other long-range transformers including longt5 and led. aside from finetuning, this also affects pretraining models on long inputs with selfsupervised losses such as span-corruption (raffel et al., 2020b) or denoising (lewis et al., 2020), which require the decoder to process an output that is linear in the length of the input.co-reference resolution and fact retention an assumption at the heart of sled is the locality of information assumption. when the input text is long, this assumption may break if distant entity resolution or factual knowledge are required. for example, a chapter in a book may mention \"they were walking into the room\" when knowledge of what room or who walked is located a few chapters back. in such cases, the encoder used by sled will not be able to access this information, moving more responsibility to the decoder and reducing the effectiveness of the contextual encoding. similarly, in multi-hop questions (yang et al., 2018), attending to one part of the context is necessary in order to fully understand the question and encode a second piece of information correctly. as the encoder will not have access to the first context that leads to better question understanding, here as well more responsibility is delegated to the decoder.", "Solved": NaN, "Unsolved": NaN}, "221640701": {"num": 802, "type": 1, "title": "Accelerating Real-Time Question Answering via Question Generation", "pdfurl": "https://arxiv.org/pdf/2009.05167v2.pdf", "conclusion": "in this paper, we present ocean-q, a novel method that uses qg to reduce the computational cost of rtqa, and address the bottleneck of encoding coming question which cannot be cached offline. experiments on squad-open demonstrate that ocean-q is able to accelerate the fastest rtqa system by 4 times, while only losing the accuracy by 3+%. to further improve qg quality, we introduce a new data augmentation method and leverage multi-task learning and diverse beam search to boost rtqa performance. currently, there still exists a gap between ocean-q and the state-of-the-art rtqa models. future work that includes designing a better qg model may help closing the gap.  data augmentation, s min , s max and n are set to 0.5, 1.0 and 2 respectively. for multi-task learning, we use squad, hotpotqa and newsqa dataset, with mixture ratio 0.4 by searching from 0.1 to 0.9. for diverse beam search, the diverse strength rate \u03b3 is set to 4.0. for rtqa, we generate 20 different questions with each answer.", "Solved": NaN, "Unsolved": NaN}, "247748606": {"num": 803, "type": 1, "title": "UKP-SQUARE: An Online Platform for Question Answering Research", "pdfurl": "https://www.aclanthology.org/2022.acl-demo.2.pdf", "conclusion": "we introduce the ukp-square platform that enables researchers and developers to study and compare qa pipelines, i.e., skills, that comprises a selection of datastores, retrieval mechanisms and reader models. the platform enables querying ex-isting public skills, as well as implementing custom ones using ukp-square's microservices and utility functions that support a large collection of model types and datastores. furthermore, users can simultaneously query multiple skills, and analyze them through integrated behavioural tests. our architecture is scalable and flexible to incorporate most of the latest developments in the qa domain. future versions will include automated deployment of custom models and datastores, automated skill selection by incorporating previous works (puerto et al., 2021;geigle et al., 2021) and increasing the number of supported datastores (e.g., wikidata, vrande\u010di\u0107 and kr\u00f6tzsch, 2014). we also plan to incorporate specialized models (e.g., using graph encoders, ribeiro et al., 2021), structured reasoning approaches (yasunaga et al., 2021) and interpretability techniques such as saliency maps .", "Solved": NaN, "Unsolved": NaN}}