{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import ast\n",
        "import pickle\n",
        "import glob\n",
        "from time import process_time, sleep\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, download_url\n",
        "import networkx as nx\n",
        "from networkx.algorithms import community\n",
        "from torch_geometric.utils import to_networkx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from pyvis.network import Network\n",
        "import string\n",
        "import colorama\n",
        "from colorama import Fore\n",
        "import termtables as tt\n",
        "string.punctuation\n",
        "import openai\n",
        "import time\n",
        "import os\n",
        "import seaborn as sns\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.read_csv('./Data/Insight_label_pred.csv',index_col=0)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Insight_dataset=pd.read_csv('./Data/Insight_dataset.csv',index_col=0)\n",
        "Insight_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add summarization to insight dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(Insight_dataset)):\n",
        "    s_solved=[]\n",
        "    s_unsolved=[]\n",
        "    for j in range(len(df)):\n",
        "        if Insight_dataset['corpusid'][i] == df['corpusid'][j] and df['Label'][j] == 'ReSolved':\n",
        "            s_solved.append(df['Sentence'][j])\n",
        "        if Insight_dataset['corpusid'][i] == df['corpusid'][j] and df['Label'][j] == 'Finding':\n",
        "            s_unsolved.append(df['Sentence'][j])\n",
        "    Insight_dataset['ReSolved'][i] = ''.join(s_solved)\n",
        "    Insight_dataset['Finding'][i] = ''.join(s_unsolved)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Insight_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Citation net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# columns = ['sourceNodeId','targetNodeId','cite_info','cite_text','relationshipType','flag']\n",
        "columns = ['sourceNodeId','targetNodeId','cite_info','relationshipType','flag']\n",
        "d_edges=pd.DataFrame(columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count=0\n",
        "for i in range(len(Insight_dataset)):\n",
        "    try:\n",
        "        List=eval(Insight_dataset['ref_paper_id'][i])\n",
        "        # cite_dic=eval(Insight_dataset['cite_text'][i])\n",
        "        count=count+1\n",
        "    except:\n",
        "        continue\n",
        "    for j in List:\n",
        "        if j in list(Insight_dataset['corpusid']):\n",
        "            # d_edges_append=pd.DataFrame(data=[[Insight_dataset['corpusid'][i],j,str(List),cite_dic[j].replace('\\n',''),'cites',count]], columns=columns)\n",
        "            d_edges_append=pd.DataFrame(data=[[Insight_dataset['corpusid'][i],j,str(List),'cites',count]], columns=columns)\n",
        "            d_edges=pd.concat([d_edges, d_edges_append], ignore_index=True, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d_edges.to_csv('./Data/d_edges.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_id=list(set(d_edges['sourceNodeId']))\n",
        "total_id.extend(list(set(d_edges['targetNodeId'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(set(total_id)))\n",
        "total_id=list(set(total_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#id -> index -> content\n",
        "re_num={}\n",
        "for i in range (len(total_id)):\n",
        "    content={}\n",
        "    content['num']=i\n",
        "    content['type']=1\n",
        "    for j in range(len(Insight_dataset)):\n",
        "        if Insight_dataset['corpusid'][j]==total_id[i]:\n",
        "            # content['author-date']=total[total_id[i]]\n",
        "            content['title']=Insight_dataset['paper-title'][j]\n",
        "            content['pdfurl']=Insight_dataset['pdfurl'][j]\n",
        "            content['conclusion']=Insight_dataset['Content'][j]\n",
        "            content['ReSolved']=Insight_dataset['ReSolved'][j]\n",
        "            content['Finding']=Insight_dataset['Finding'][j]\n",
        "\n",
        "            if 'a survey' in content['title'] or 'A survey' in content['title'] or 'a Survey' in content['title'] or 'A Survey' in content['title']:\n",
        "                content['type']=0\n",
        "            else:\n",
        "                content['type']=1\n",
        "            \n",
        "            # print('true')\n",
        "            break\n",
        "        \n",
        "    re_num[total_id[i]]=content\n",
        "re_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_source=[]\n",
        "list_target=[]\n",
        "for i in range(len(d_edges)):\n",
        "    if d_edges['sourceNodeId'][i]==d_edges['targetNodeId'][i]:\n",
        "        continue\n",
        "   \n",
        "    list_source.append(re_num[d_edges['sourceNodeId'][i]]['num'])\n",
        "    list_target.append(re_num[d_edges['targetNodeId'][i]]['num'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Demo_Data(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "    #file\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "   \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "    # download from website\n",
        "    # def download(self):\n",
        "    #     # Download to `self.raw_dir`.\n",
        "    #     download_url(url, self.raw_dir)\n",
        "        ...\n",
        "    #data generate\n",
        "    def process(self):\n",
        "        # Read data into huge `Data` list.\n",
        "        # Read data into huge `Data` list.\n",
        "        # building\n",
        "        \n",
        "        start=0\n",
        "        end=len(list_source)\n",
        "        \n",
        "        Edge_index = torch.tensor([list_source,\n",
        "                                   list_target], dtype=torch.long)\n",
        "        \n",
        "        # s=list_source[start:end].copy()\n",
        "        # s.extend(list_target[start:end])\n",
        "        print(len(re_num))\n",
        "        \n",
        "        # node_num=len(set(s))\n",
        "\n",
        "        # node feature\n",
        "        sample_feature=[0]*len(re_num)\n",
        "        X = torch.tensor(np.array(sample_feature).reshape(-1, 1).tolist(), dtype=torch.float)\n",
        "        # node label\n",
        "        # Y = torch.tensor([0,1,0],dtype=torch.float)\n",
        "        \n",
        "        #building\n",
        "        # data = Data(x=X, edge_index=Edge_index, y=Y)\n",
        "        data = Data(x=X,edge_index=Edge_index)\n",
        "        \n",
        "        # put in datalist\n",
        "        data_list = [data]\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        data.num_nodes = len(re_num)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Demo_Data(\"Whole_Network\")\n",
        "data=dataset[0]\n",
        "dataset.get_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "G = to_networkx(data, to_undirected=False)\n",
        "degrees = [val for (node, val) in G.degree()]\n",
        "display(pd.DataFrame(pd.Series(degrees).describe()).transpose().round(2))\n",
        "print(len(degrees))\n",
        "print(sum(degrees))\n",
        "plt.figure(figsize=(10, 6))\n",
        "# plt.hist(degrees, bins=100)\n",
        "# plt.hist(degrees, range(0,10))\n",
        "plt.hist(degrees, range(0,50),rwidth=0.5,align=\"left\")\n",
        "\n",
        "plt.xlabel(\"node degree\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ancestors={}\n",
        "ancestors_num=[]\n",
        "for i in G.nodes:\n",
        "    # print(i)\n",
        "    # print(nx.ancestors(G, i))\n",
        "    ancestors[i]=len(nx.ancestors(G, i))\n",
        "    ancestors_num.append(len(nx.ancestors(G, i)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# degrees = [val for (node, val) in G.degree()]\n",
        "i=1\n",
        "r_node_list=[]\n",
        "while i<=10: \n",
        "    print('rank:',i)\n",
        "    r_node=ancestors_num.index(sorted(ancestors_num,reverse=True)[0])\n",
        "    print('high_contribution_node_num:',r_node)\n",
        "    r_node_list.append(r_node)\n",
        "    print('number of cited:',len(nx.ancestors(G, r_node)))\n",
        "    print('cited_by:',nx.ancestors(G, r_node))\n",
        "    G.nodes[r_node]['color']='deeppink'\n",
        "    G.nodes[r_node]['shape']='triangle'\n",
        "    G.nodes[r_node]['size']=35\n",
        "    ancestors_num[r_node]=0 # clear\n",
        "    i=i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#make graph - total\n",
        "word_node_num=len(total_id)\n",
        "for i in re_num:\n",
        "    #paper node shape,word,edge\n",
        "    if re_num[i]['type']==0:\n",
        "        G.nodes[re_num[i]['num']]['shape']='star'\n",
        "        G.nodes[re_num[i]['num']]['color']='chocolate'\n",
        "        # G.nodes[re_num[i]['num']]['label']='S'+str(re_num[i]['num'])\n",
        "        G.nodes[re_num[i]['num']]['size']=50\n",
        "\n",
        "    #paper label title\n",
        "    try:\n",
        "        # G.nodes[re_num[i]['num']]['label']=re_num[i]['author-date']\n",
        "        G.nodes[re_num[i]['num']]['title']=re_num[i]['title']+'\\n'+re_num[i]['pdfurl']+'\\n'+str(re_num[i]['num'])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    word_node_num=word_node_num+4\n",
        "\n",
        "net = Network(notebook=True, cdn_resources='in_line',directed=True,bgcolor='#fffefe')\n",
        "net.from_nx(G)\n",
        "net.toggle_physics(False)\n",
        "net.show_buttons()\n",
        "net.show('./Output/Cite_net_total.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#nltk_word\n",
        "def word_div(strings):\n",
        "    strings=strings.lower()\n",
        "    strings=strings.translate(str.maketrans('', '', string.punctuation))\n",
        "    token=nltk.word_tokenize(strings)\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens=[]\n",
        "    for i_2 in range(len(token)):\n",
        "        if token[i_2] not in stop_words:\n",
        "            tokens.append(token[i_2])   \n",
        "    filter_word=['from', 'subject', 're', 'edu', 'use','introduction','conclusion','evaluation','figure','table','survey','et','al','research','we','use','work','issue','also','future','however','finally','task','tasks','paper','nlp','recent','natural','language','processing','nevertheless']\n",
        "    clean_token=[]\n",
        "    for item in tokens:\n",
        "        item=lemmatizer.lemmatize(item, pos=\"n\")\n",
        "        if item not in filter_word and re.search(r'^\\d+$',item) is None:\n",
        "            # clean_token.append(st.stem(item))\n",
        "            clean_token.append(item)\n",
        "    return clean_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "st = LancasterStemmer()\n",
        "model = SentenceTransformer('allenai/scibert_scivocab_uncased')\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
        "sentences=[]\n",
        "strings_total=''\n",
        "branch=2\n",
        "pair_list=[]\n",
        "pair_list_total=[]\n",
        "for item in re_num:\n",
        "    if 'conclusion' not in re_num[item].keys():\n",
        "        # print(re_num[item]['num'])\n",
        "        continue\n",
        "    \n",
        "    sentences.append(re_num[item]['conclusion'])\n",
        "    strings_total=strings_total+re_num[item]['conclusion']\n",
        "    \n",
        "\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i_1 in range(len(cosine_scores)-1):\n",
        "    for j_1 in range(i_1+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i_1, j_1], 'score': cosine_scores[i_1][j_1]})\n",
        "\n",
        "#Sort scores in decreasing order\n",
        "pairs_dec = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "#Simliarity of each article (n-branch)\n",
        "start=0\n",
        "for i in range(1,len(pairs)):\n",
        "    strings_pair=''\n",
        "    flag=pairs[i-1]['index'][0]\n",
        "    i_1, j_1 = pairs[i]['index']\n",
        "    if i_1!=flag:\n",
        "        pair=pairs[start:i]\n",
        "        pair = sorted(pair, key=lambda x: x['score'], reverse=True)\n",
        "        # for building tree\n",
        "        for j in range(branch):\n",
        "            pair[j]['s1']=sentences[pair[j]['index'][0]]\n",
        "            pair[j]['s2']=sentences[pair[j]['index'][1]]\n",
        "            strings_pair=sentences[pair[j]['index'][0]]+sentences[pair[j]['index'][1]]\n",
        "            # print(Fore.BLACK + \"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[pair[j]['index'][0]], sentences[pair[j]['index'][1]], pair[j]['score']))\n",
        "\n",
        "            #nltk->上位語,top:10\n",
        "            clean_token=word_div(strings_pair)\n",
        "            freq_dist = nltk.FreqDist(clean_token)\n",
        "            # print(Fore.BLUE + 'hypernym:',freq_dist.most_common(10))\n",
        "            pair[j]['hypernym']=freq_dist.most_common(10)\n",
        "\n",
        "            #nltk ->共通語\n",
        "            S_0=word_div(sentences[pair[j]['index'][0]])\n",
        "            S_1=word_div(sentences[pair[j]['index'][1]])\n",
        "            and_list=set(S_0)&set(S_1)\n",
        "            # print(Fore.MAGENTA + 'common_word:',and_list)\n",
        "            pair[j]['common_word']=and_list\n",
        "\n",
        "            #add data to list\n",
        "            pair_list.append(pair[j])\n",
        "\n",
        "        start=i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for j in range(len(pairs)):\n",
        "    pairs[j]['s1']=sentences[pairs[j]['index'][0]]\n",
        "    pairs[j]['s2']=sentences[pairs[j]['index'][1]]\n",
        "    strings_pair=sentences[pairs[j]['index'][0]]+sentences[pairs[j]['index'][1]]\n",
        "    # print(Fore.BLACK + \"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[pair[j]['index'][0]], sentences[pair[j]['index'][1]], pair[j]['score']))\n",
        "\n",
        "    #nltk->Hypernym,top:10\n",
        "    clean_token=word_div(strings_pair)\n",
        "    freq_dist = nltk.FreqDist(clean_token)\n",
        "    # print(Fore.BLUE + 'hypernym:',freq_dist.most_common(10))\n",
        "    pairs[j]['hypernym']=freq_dist.most_common(10)\n",
        "\n",
        "    #Common_word\n",
        "    S_0=word_div(sentences[pairs[j]['index'][0]])\n",
        "    S_1=word_div(sentences[pairs[j]['index'][1]])\n",
        "    and_list=set(S_0)&set(S_1)\n",
        "    # print(Fore.MAGENTA + 'common_word:',and_list)\n",
        "    pairs[j]['common_word']=and_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pair_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Subgraph - Inheritance tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Copy\n",
        "G_ext=G.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Number of citations in whole net\n",
        "ancestors={}\n",
        "ancestors_num=[]\n",
        "for i in G_ext.nodes:\n",
        "    # print(i)\n",
        "    # print(nx.ancestors(G_ext, i))\n",
        "    ancestors[i]=len(nx.ancestors(G_ext, i))\n",
        "    ancestors_num.append(len(nx.ancestors(G_ext, i)))\n",
        "\n",
        "ancestors_org=ancestors_num.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#node-select-tree\n",
        "root_max=10 # init \n",
        "layer_max=5\n",
        "leaf_lim=2 # number of leaf\n",
        "tree_list=[] # node be selected\n",
        "node_his=[]\n",
        "for n in range(root_max):# Topn cited-root\n",
        "     # history\n",
        "    root_list=[] # roor list   \n",
        "    print(f'path{n}:')\n",
        "    path_dict={}\n",
        "    ancestors_num=ancestors_org.copy()\n",
        "    i=1 #layer_num\n",
        "    # print('rank:',i)\n",
        "    for m in range(len(G_ext.nodes)):\n",
        "        if ancestors_num.index(sorted(ancestors_num,reverse=True)[m]) not in node_his:\n",
        "            root=ancestors_num.index(sorted(ancestors_num,reverse=True)[m])\n",
        "            print('root:',root)\n",
        "            break\n",
        "    \n",
        "    G_ext.nodes[root]['color']='deeppink'\n",
        "    G_ext.nodes[root]['shape']='star'\n",
        "    ancestors_num[root]=0 \n",
        "    node_his.append(root)\n",
        "    root_list.append(root)\n",
        "\n",
        "    while True:\n",
        "        if root_list==[]:\n",
        "            break\n",
        "        # if ancestors_num.index(sorted(ancestors_num,reverse=True)[0])==0:\n",
        "        #     break\n",
        "        print('Current_node:',root_list[0])\n",
        "        print('number of cited:',len(nx.ancestors(G_ext, root_list[0])))\n",
        "        print('cited_by:',nx.ancestors(G_ext, root_list[0]))\n",
        "        # G_ext.nodes[root]['label']=f'layer{i}'\n",
        "        \n",
        "    \n",
        "        for l in range(leaf_lim):\n",
        "            count=0\n",
        "            while count<len(G):\n",
        "                candidate=ancestors_num.index(sorted(ancestors_num,reverse=True)[0])\n",
        "                if candidate in list(nx.ancestors(G_ext, root_list[0])) and candidate not in node_his:\n",
        "                    i=i+1\n",
        "                    path_dict.setdefault(root_list[0],[]).append(candidate)\n",
        "                    print('candidate:',candidate)\n",
        "                    root_list.append(candidate)\n",
        "                    node_his.append(candidate)\n",
        "                    break\n",
        "                ancestors_num[candidate]=0 \n",
        "                count=count+1\n",
        "        \n",
        "        root_list.pop(0)\n",
        "        print('root_list:',root_list)\n",
        "                \n",
        "    tree_list.append(path_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strings_list=[]\n",
        "for i in range(len(tree_list)):\n",
        "    strings=''\n",
        "    nodes=list(set(tree_list[i]))\n",
        "    for node in nodes:\n",
        "        for item in list(re_num.values()):\n",
        "            if node == item['num']:\n",
        "                key = [k for k, v in re_num.items() if v['num'] == node][0]\n",
        "                break\n",
        "        strings=strings+re_num[key]['conclusion']\n",
        "    \n",
        "    strings_list.append(strings)\n",
        "strings_list\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from textblob import TextBlob as tb\n",
        "\n",
        "def tf(word, blob):\n",
        "    return blob.words.count(word) / len(blob.words)\n",
        "\n",
        "def n_containing(word, bloblist):\n",
        "    return sum(1 for blob in bloblist if word in blob.words)\n",
        "\n",
        "def idf(word, bloblist):\n",
        "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
        "\n",
        "def tfidf(word, blob, bloblist):\n",
        "    return tf(word, blob) * idf(word, bloblist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import stem\n",
        "def imp_word(d1,d2):\n",
        "    words_list=[]\n",
        "    document1=tb(d1)\n",
        "    document2=tb(d2)\n",
        "    bloblist = [document1, document2]\n",
        "\n",
        "    stemmer = stem.LancasterStemmer()\n",
        "    stop_words = stopwords.words('english')\n",
        "    \n",
        "    for i, blob in enumerate(bloblist):\n",
        "        # print(\"Top words in document {}\".format(i + 1))\n",
        "        scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
        "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        for word, score in sorted_words[:7]:\n",
        "            if word in stop_words:\n",
        "                continue\n",
        "\n",
        "            clean_word=stemmer.stem(word)\n",
        "            # print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
        "            words_list.append(str(word))\n",
        "        break\n",
        "\n",
        "    return words_list\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "color_list = ['red',\n",
        "           'goldenrod',\n",
        "           'springgreen',\n",
        "           'limegreen',\n",
        "           'green',\n",
        "           'salmon',\n",
        "           'violet',\n",
        "           'pink',\n",
        "           'sienna',\n",
        "           'palegreen',\n",
        "           'darkcyan',\n",
        "           'orchid',\n",
        "           'magenta',\n",
        "           'slategrey',\n",
        "           'olive',\n",
        "           'indigo',\n",
        "           'blueviolet',\n",
        "           'gold',\n",
        "           'aquamarine',\n",
        "           'lightcoral',\n",
        "           'teal',\n",
        "           'fuchsia',\n",
        "           '#03f5dd']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# build-graph : Tree structure\n",
        "inv=100\n",
        "inv_title=35\n",
        "# color_list=['Pink','Red','Blue','Yellow','Brown','Pink','Green']\n",
        "G_generate= Network(notebook=True, \n",
        "cdn_resources='in_line',\n",
        "directed=True,\n",
        "height='600px',\n",
        "width='100%',\n",
        "font_color=\"blue\",\n",
        "heading='Inheritance tree'\n",
        ")\n",
        "history=[]\n",
        "for i in range(len(tree_list)):\n",
        "    color=color_list[i]\n",
        "    flag=0\n",
        "    for j in tree_list[i]:\n",
        "        for item in list(re_num.values()):\n",
        "            if j == item['num']:\n",
        "                key_j = [k for k, v in re_num.items() if v['num'] == j][0]\n",
        "                words_list=imp_word(re_num[key_j]['conclusion'],strings_list[i])\n",
        "                try:\n",
        "                    G_generate.add_node(j, color=color,label='\\n'.join([re_num[key_j]['title'][i:i+inv_title] for i in range(0, len(re_num[key_j]['title']),inv_title)]),\n",
        "                    title='- Keyword: '+', '.join(words_list)+'\\n\\n'+\n",
        "                '- ReSolved: '+'\\n'.join([re_num[key_j]['ReSolved'][i:i+inv] for i in range(0, len(re_num[key_j]['ReSolved']), inv)])+'\\n\\n'+\n",
        "                '- Finding: '+'\\n'.join([re_num[key_j]['Finding'][i:i+inv] for i in range(0, len(re_num[key_j]['Finding']), inv)])+'\\n\\n'+'- Url: '+re_num[key_j]['pdfurl'],\n",
        "                shape='box',\n",
        "                size=60,\n",
        "                group=i)\n",
        "                except:\n",
        "                    G_generate.add_node(j, color=color,label=re_num[key_j]['title'],title='None',group=i)\n",
        "            \n",
        "        if flag==0:\n",
        "            G_generate.nodes[G_generate.node_ids.index(j)]['shape']='ellipse'\n",
        "            flag=1\n",
        "            \n",
        "        for z in tree_list[i][j]:\n",
        "            key_z = [k for k, v in re_num.items() if v['num'] == z][0]\n",
        "            words_list=imp_word(re_num[key_z]['conclusion'],strings_list[i])\n",
        "            try:\n",
        "                G_generate.add_node(z, color=color,label='\\n'.join([re_num[key_z]['title'][i:i+inv_title] for i in range(0, len(re_num[key_z]['title']),inv_title)]),\n",
        "                title='- Keyword: '+', '.join(words_list)+'\\n\\n'+\n",
        "                '- ReSolved: '+'\\n'.join([re_num[key_z]['ReSolved'][i:i+inv] for i in range(0, len(re_num[key_z]['ReSolved']), inv)])+'\\n\\n'+\n",
        "                '- Finding: '+'\\n'.join([re_num[key_z]['Finding'][i:i+inv] for i in range(0, len(re_num[key_z]['Finding']), inv)])+'\\n\\n'+'- Url: '+re_num[key_z]['pdfurl'],\n",
        "                shape='box',\n",
        "                size=40,\n",
        "                group=i)\n",
        "            except:\n",
        "                G_generate.add_node(z, color=color,label=str(z),title='None',group=i)\n",
        "\n",
        "            if [j, z] in history:\n",
        "                continue\n",
        "\n",
        "            # add edge(thickness,length)\n",
        "            flag=0\n",
        "            ## find corpus id\n",
        "          \n",
        "            for c in range(len(d_edges)):\n",
        "                if d_edges['targetNodeId'][c] == key_j and d_edges['sourceNodeId'][c] == key_z:\n",
        "                    # cite_t = d_edges['cite_text'][c]\n",
        "                    break\n",
        "\n",
        "            for p in pairs:\n",
        "                if p['index']==[j,z] or p['index']==[z,j]:\n",
        "                    G_generate.add_edge(j, z,\n",
        "                    gravity=-8000, \n",
        "                    color=color,\n",
        "                    label='\\n\\n'.join(list(p['common_word'])[0:10]),\n",
        "                    title='Similarity: '+str(round(float(p['score']),3)),\n",
        "                    spring_length=10000,\n",
        "                    width=float(p['score'])*5,\n",
        "                    spring_strength=0.001,\n",
        "                    overlap=1,\n",
        "                    )\n",
        "                    flag=1\n",
        "                    break\n",
        "            # if flag==0:\n",
        "            #     G_generate.add_edge(j, z, color=color,width=0.1)\n",
        "\n",
        "            # G_generate.add_edge(j, z,\n",
        "            # label='edge',\n",
        "            # gravity=-10,\n",
        "            # central_gravity=0.3,\n",
        "            # spring_length=z,\n",
        "            # spring_strength=0.001,\n",
        "            # overlap=1)\n",
        "            \n",
        "            history.append([j,z])\n",
        "     \n",
        "\n",
        "G_generate.bgcolor=\"#ffffee\"\n",
        "G_generate.barnes_hut()\n",
        "\n",
        "options = \"\"\"const options = {\n",
        "  \"layout\": {\n",
        "    \"hierarchical\": {\n",
        "      \"enabled\": true,\n",
        "      \"levelSeparation\": 320,\n",
        "      \"nodeSpacing\": 155,\n",
        "      \"sortMethod\": \"directed\"\n",
        "    }\n",
        "  },\n",
        "  \"physics\": {\n",
        "    \"hierarchicalRepulsion\": {\n",
        "      \"centralGravity\": 0,\n",
        "      \"nodeDistance\": 180,\n",
        "      \"damping\": 0.46,\n",
        "      \"avoidOverlap\": 1\n",
        "    },\n",
        "    \"minVelocity\": 0.75,\n",
        "    \"solver\": \"hierarchicalRepulsion\"\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "G_generate.set_options(options)\n",
        "# G_generate.set_edge_smooth('dynamic')\n",
        "\n",
        "# G_generate.show_buttons(filter_=['layout','physics'])\n",
        "# G_generate.show_buttons()\n",
        "G_generate.show('./Output/Inheritance_tree.html',notebook=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sub-net\n",
        "sub_net=[]\n",
        "for i in range(len(node_his)):\n",
        "    sub_net.append(node_his[i])\n",
        "    # sub_net.extend([n for n in G_ext.neighbors(cluster_list[i])])\n",
        "\n",
        "#Sub-Grpah\n",
        "# nx.draw(G.subgraph(sub_net),with_labels=True)\n",
        "net = Network(notebook=True, cdn_resources='in_line',directed=True,bgcolor='#fffefe')\n",
        "net.from_nx(G_ext.subgraph(sub_net))\n",
        "net.show_buttons()\n",
        "net.show('./Output/Subnet_paper_in_inheritance_tree.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Subgraph - Relevance-tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FW_list=[]\n",
        "Solved_list=[]\n",
        "sentences_fw=[]\n",
        "sentences_solved=[]\n",
        "for key in re_num:\n",
        "    if re_num[key]['Finding']!='':\n",
        "        FW_list.append(re_num[key]['num'])\n",
        "        sentences_fw.append(re_num[key]['Finding'])\n",
        "    if re_num[key]['ReSolved']!='':\n",
        "        Solved_list.append(re_num[key]['num'])\n",
        "        sentences_solved.append(re_num[key]['ReSolved'])\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings_FW = model.encode(sentences_fw, convert_to_tensor=True)\n",
        "embeddings_Solved = model.encode(sentences_solved, convert_to_tensor=True)\n",
        "cosine_scores_novelty = util.cos_sim(embeddings_FW, embeddings_Solved)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s_c=20\n",
        "\n",
        "avg_list=[]\n",
        "max_list_index=[]\n",
        "\n",
        "for i in range(len(cosine_scores_novelty)):\n",
        "    l=[]\n",
        "    for z in range(s_c):\n",
        "        l.append(Solved_list[int(np.argsort(cosine_scores_novelty[i])[-(z+s_c)])])\n",
        "\n",
        "    avg_list.append(sum(cosine_scores_novelty[i])/len(cosine_scores_novelty[i]))\n",
        "    max_list_index.append(l)\n",
        "\n",
        "avg_list_index_org=list(np.argsort(avg_list)[::-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#adjustment\n",
        "avg_list_index=[]\n",
        "for i in range(len(avg_list_index_org)):\n",
        "    avg_list_index.append(FW_list[avg_list_index_org[i]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#node-select-tree\n",
        "root_max=10 # init \n",
        "layer_max=5\n",
        "leaf_lim=2 # number of leaf\n",
        "tree_list=[] # node be selected\n",
        "node_his=[]\n",
        "for n in range(root_max):# Topn cited-root\n",
        "     # history\n",
        "    root_list=[] # root list   \n",
        "    print(f'path{n}:')\n",
        "    path_dict={}\n",
        "    i=1\n",
        "    avg_list=avg_list_index.copy()\n",
        "    # print('rank:',i)\n",
        "    root=int(avg_list_index[n])\n",
        "    print(root)\n",
        "    \n",
        "    G_ext.nodes[root]['color']='deeppink'\n",
        "    G_ext.nodes[root]['shape']='star'\n",
        "    node_his.append(root)\n",
        "    root_list.append(root)\n",
        "\n",
        "    while True:\n",
        "        if i>layer_max:\n",
        "            break\n",
        "        if root_list==[]:\n",
        "            break\n",
        "        # if ancestors_num.index(sorted(ancestors_num,reverse=True)[0])==0:\n",
        "        #     break\n",
        "        print('Current_node:',root_list[0])\n",
        "        # G_ext.nodes[root]['label']=f'layer{i}'\n",
        "        \n",
        "    \n",
        "        for l in range(leaf_lim):\n",
        "            for s in range(s_c):\n",
        "                candidate=max_list_index[FW_list.index(root)][s]\n",
        "                if candidate not in node_his:\n",
        "                    path_dict.setdefault(root_list[0],[]).append(candidate)\n",
        "                    print('candidate:',candidate)\n",
        "                    root_list.append(candidate)\n",
        "                    node_his.append(candidate)\n",
        "                    break\n",
        "\n",
        "        i=i+1\n",
        "        root_list.pop(0)\n",
        "        print('root_list:',root_list)\n",
        "                \n",
        "    tree_list.append(path_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strings_list=[]\n",
        "for i in range(len(tree_list)):\n",
        "    strings=''\n",
        "    nodes=list(set(tree_list[i]))\n",
        "    for node in nodes:\n",
        "        for item in list(re_num.values()):\n",
        "            if node == item['num']:\n",
        "                key = [k for k, v in re_num.items() if v['num'] == node][0]\n",
        "                break\n",
        "        strings=strings+re_num[key]['conclusion']\n",
        "    \n",
        "    strings_list.append(strings)\n",
        "strings_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# build-graph : Tree structure\n",
        "inv=100\n",
        "inv_title=35\n",
        "# color_list=['Pink','Red','Blue','Yellow','Brown','Pink','Green']\n",
        "G_Novelty= Network(notebook=True, \n",
        "cdn_resources='in_line',\n",
        "directed=True,\n",
        "height='600px',\n",
        "width='100%',\n",
        "font_color=\"blue\",\n",
        "heading='Relevance-tree'\n",
        ")\n",
        "history=[]\n",
        "for i in range(len(tree_list)):\n",
        "    color=color_list[i]\n",
        "    flag=0\n",
        "    for j in tree_list[i]:\n",
        "        for item in list(re_num.values()):\n",
        "            if j == item['num']:\n",
        "                key_j = [k for k, v in re_num.items() if v['num'] == j][0]\n",
        "                words_list=imp_word(re_num[key_j]['conclusion'],strings_list[i])\n",
        "                try:\n",
        "                    G_Novelty.add_node(j, color=color,label='\\n'.join([re_num[key_j]['title'][i:i+inv_title] for i in range(0, len(re_num[key_j]['title']),inv_title)]),\n",
        "                    title='- ReSolved: '+'\\n'.join([re_num[key_j]['ReSolved'][i:i+inv] for i in range(0, len(re_num[key_j]['ReSolved']), inv)])+'\\n\\n'+\n",
        "                    '- Finding: '+'\\n'.join([re_num[key_j]['Finding'][i:i+inv] for i in range(0, len(re_num[key_j]['Finding']), inv)])+'\\n\\n'+'- Url: '+re_num[key_j]['pdfurl'],\n",
        "                    shape='box',\n",
        "                    size=60,\n",
        "                    group=i)\n",
        "                except:\n",
        "                    G_Novelty.add_node(j, color=color,label=str(j),title='None',group=i)\n",
        "            \n",
        "        if flag==0:\n",
        "            # G_Novelty.nodes[G_Novelty.node_ids.index(j)]['shape']='ellipse'\n",
        "            flag=1\n",
        "            \n",
        "        for z in tree_list[i][j]:\n",
        "            key_z = [k for k, v in re_num.items() if v['num'] == z][0]\n",
        "            words_list=imp_word(re_num[key_z]['conclusion'],strings_list[i])\n",
        "            try:\n",
        "                G_Novelty.add_node(z, color=color,label='\\n'.join([re_num[key_z]['title'][i:i+inv_title] for i in range(0, len(re_num[key_z]['title']),inv_title)]),\n",
        "                title='- Keyword: '+', '.join(words_list)+'\\n\\n'+\n",
        "                '- ReSolved: '+'\\n'.join([re_num[key_z]['ReSolved'][i:i+inv] for i in range(0, len(re_num[key_z]['ReSolved']), inv)])+'\\n\\n'+\n",
        "                '- Finding: '+'\\n'.join([re_num[key_z]['Finding'][i:i+inv] for i in range(0, len(re_num[key_z]['Finding']), inv)])+'\\n\\n'+'- Url: '+re_num[key_z]['pdfurl'],\n",
        "                shape='box',\n",
        "                size=40,\n",
        "                group=i)\n",
        "            except:\n",
        "                G_Novelty.add_node(z, color=color,label=str(z),title='None',group=i)\n",
        "\n",
        "            if [j, z] in history:\n",
        "                continue\n",
        "\n",
        "            # add edge(thickness,length)\n",
        "            flag=0\n",
        "            ## find corpus id\n",
        "            cite_r='False'\n",
        "            for c in range(len(d_edges)):\n",
        "                if d_edges['targetNodeId'][c] == key_j and d_edges['sourceNodeId'][c] == key_z:\n",
        "                    cite_r='True'\n",
        "                    break\n",
        "                if d_edges['sourceNodeId'][c] == key_j and d_edges['targetNodeId'][c] == key_z:\n",
        "                    cite_r='True'\n",
        "                    break\n",
        "              \n",
        "\n",
        "            for p in pairs:\n",
        "                if p['index']==[j,z] or p['index']==[z,j]:\n",
        "                    G_Novelty.add_edge(j, z,\n",
        "                    gravity=-8000, \n",
        "                    color=color,\n",
        "                    label='\\n\\n'.join(list(p['common_word'])[0:10]),\n",
        "                    title='Similarity: '+str(round(float(p['score']),3))+'\\n\\n'+'Cite_relationship: '+cite_r,\n",
        "                    spring_length=10000,\n",
        "                    width=float(p['score'])*5,\n",
        "                    spring_strength=0.001,\n",
        "                    overlap=1,\n",
        "                    )\n",
        "                    flag=1\n",
        "                    break\n",
        "            # if flag==0:\n",
        "            #     G_generate.add_edge(j, z, color=color,width=0.1)\n",
        "\n",
        "            # G_generate.add_edge(j, z,\n",
        "            # label='edge',\n",
        "            # gravity=-10,\n",
        "            # central_gravity=0.3,\n",
        "            # spring_length=z,\n",
        "            # spring_strength=0.001,\n",
        "            # overlap=1)\n",
        "            \n",
        "            history.append([j,z])\n",
        "     \n",
        "\n",
        "G_Novelty.bgcolor=\"#ffffee\"\n",
        "G_Novelty.barnes_hut()\n",
        "\n",
        "options = \"\"\"const options = {\n",
        "  \"layout\": {\n",
        "    \"hierarchical\": {\n",
        "      \"enabled\": true,\n",
        "      \"levelSeparation\": 320,\n",
        "      \"nodeSpacing\": 155,\n",
        "      \"sortMethod\": \"directed\"\n",
        "    }\n",
        "  },\n",
        "  \"physics\": {\n",
        "    \"hierarchicalRepulsion\": {\n",
        "      \"centralGravity\": 0,\n",
        "      \"nodeDistance\": 180,\n",
        "      \"damping\": 0.46,\n",
        "      \"avoidOverlap\": 1\n",
        "    },\n",
        "    \"minVelocity\": 0.75,\n",
        "    \"solver\": \"hierarchicalRepulsion\"\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "G_Novelty.set_options(options)\n",
        "# G_generate.set_edge_smooth('dynamic')\n",
        "\n",
        "# G_generate.show_buttons(filter_=['layout','physics'])\n",
        "# G_generate.show_buttons()\n",
        "G_Novelty.show('./Output/Relevance_tree.html',notebook=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gnn_citations",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
