{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from colorama import Fore\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#HotpotQa paper_content from S2orc\n",
        "df=pd.read_csv('all_paper_content.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Get offset of text\n",
        "def get_offset(dict,i):\n",
        "    start=dic[i]['start']\n",
        "    end=dic[i]['end']\n",
        "    return start,end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Extract text of paper-title by using annotation and offset  \n",
        "title_list=[]\n",
        "for i in range(len(df)):\n",
        "    a=eval(df['content'][i])\n",
        "    if a['annotations']['title']==None:\n",
        "        title_list.append('None')\n",
        "    else:\n",
        "        dic=eval(a['annotations']['title'])\n",
        "        start,end = get_offset(dic,0)\n",
        "        title_list.append(a['text'][start:end])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpusid</th>\n",
              "      <th>title</th>\n",
              "      <th>externalids</th>\n",
              "      <th>content</th>\n",
              "      <th>updated</th>\n",
              "      <th>ref_paper_id</th>\n",
              "      <th>cite_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>263908862</td>\n",
              "      <td>Idea2Img: Iterative Self-Refinement with GPT-4...</td>\n",
              "      <td>{'arxiv': '2310.08541', 'mag': None, 'acl': No...</td>\n",
              "      <td>{'source': {'pdfurls': ['https://export.arxiv....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>248665596</td>\n",
              "      <td>From Distillation to Hard Negative Sampling: M...</td>\n",
              "      <td>{'arxiv': None, 'mag': None, 'acl': None, 'pub...</td>\n",
              "      <td>{'source': {'pdfurls': ['https://arxiv.org/pdf...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{220302524: '[30]'}</td>\n",
              "      <td>{220302524: '\\n\\n\\nResults and discussion\\n\\nW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6729528</td>\n",
              "      <td>ACO Implementation for Sequence Alignment with...</td>\n",
              "      <td>{'arxiv': '1406.0930', 'mag': '2296706549', 'a...</td>\n",
              "      <td>{'source': {'pdfurls': ['https://arxiv.org/pdf...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264490922</td>\n",
              "      <td>What Else Do I Need to Know? The Effect of Bac...</td>\n",
              "      <td>{'arxiv': None, 'mag': None, 'acl': None, 'pub...</td>\n",
              "      <td>{'source': {'pdfurls': ['https://export.arxiv....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{123758373: 'Chen and Durrett, 2019;', 2466523...</td>\n",
              "      <td>{123758373: ', 2019;', 246652372: ', 2023), la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14155615</td>\n",
              "      <td>Supplemental Material to</td>\n",
              "      <td>{'arxiv': None, 'mag': '2065435633', 'acl': No...</td>\n",
              "      <td>{'source': {'pdfurls': None, 'pdfsha': '8cd7af...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>236459873</td>\n",
              "      <td>Explanations for CommonsenseQA: New Dataset an...</td>\n",
              "      <td>{'arxiv': None, 'mag': None, 'acl': '2021.acl-...</td>\n",
              "      <td>{'source': {'pdfurls': ['https://aclanthology....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{218487313: 'Yadav et al., 2020', 52822214: 'Y...</td>\n",
              "      <td>{218487313: '\\n\\nAIR ', 52822214: ', 2017;'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>2724512</td>\n",
              "      <td>Phylogenetic Comparison of F-Box (FBX) Gene Su...</td>\n",
              "      <td>{'arxiv': None, 'mag': '2021858754', 'acl': No...</td>\n",
              "      <td>{'source': {'pdfurls': None, 'pdfsha': 'fb186d...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{9060527: '[41]'}</td>\n",
              "      <td>{9060527: ' remanei, which were estimated to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1536</th>\n",
              "      <td>202565945</td>\n",
              "      <td>Self-Assembling Modular Networks for Interpret...</td>\n",
              "      <td>{'arxiv': '1909.05803', 'mag': '2972378157', '...</td>\n",
              "      <td>{'source': {'pdfurls': ['https://www.aclweb.or...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{139103297: 'Chen and Durrett, 2019;', 1899278...</td>\n",
              "      <td>{139103297: '\\n\\n\\nAdversarial Evaluation\\n\\nM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1537</th>\n",
              "      <td>248476403</td>\n",
              "      <td>Inferring Implicit Relations with Language Models</td>\n",
              "      <td>{'arxiv': None, 'mag': None, 'acl': None, 'pub...</td>\n",
              "      <td>{'source': {'pdfurls': ['https://arxiv.org/pdf...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{230799347: 'Geva et al., 2021', 225075843: 'L...</td>\n",
              "      <td>{230799347: '\\n\\n\\nData Collection\\n\\nWe curat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538</th>\n",
              "      <td>13269303</td>\n",
              "      <td>BMC Genomics The repertoire of equine intestin...</td>\n",
              "      <td>{'arxiv': None, 'mag': '2017331012', 'acl': No...</td>\n",
              "      <td>{'source': {'pdfurls': None, 'pdfsha': '884cb4...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1539 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       corpusid                                              title  \\\n",
              "0     263908862  Idea2Img: Iterative Self-Refinement with GPT-4...   \n",
              "1     248665596  From Distillation to Hard Negative Sampling: M...   \n",
              "2       6729528  ACO Implementation for Sequence Alignment with...   \n",
              "3     264490922  What Else Do I Need to Know? The Effect of Bac...   \n",
              "4      14155615                           Supplemental Material to   \n",
              "...         ...                                                ...   \n",
              "1534  236459873  Explanations for CommonsenseQA: New Dataset an...   \n",
              "1535    2724512  Phylogenetic Comparison of F-Box (FBX) Gene Su...   \n",
              "1536  202565945  Self-Assembling Modular Networks for Interpret...   \n",
              "1537  248476403  Inferring Implicit Relations with Language Models   \n",
              "1538   13269303  BMC Genomics The repertoire of equine intestin...   \n",
              "\n",
              "                                            externalids  \\\n",
              "0     {'arxiv': '2310.08541', 'mag': None, 'acl': No...   \n",
              "1     {'arxiv': None, 'mag': None, 'acl': None, 'pub...   \n",
              "2     {'arxiv': '1406.0930', 'mag': '2296706549', 'a...   \n",
              "3     {'arxiv': None, 'mag': None, 'acl': None, 'pub...   \n",
              "4     {'arxiv': None, 'mag': '2065435633', 'acl': No...   \n",
              "...                                                 ...   \n",
              "1534  {'arxiv': None, 'mag': None, 'acl': '2021.acl-...   \n",
              "1535  {'arxiv': None, 'mag': '2021858754', 'acl': No...   \n",
              "1536  {'arxiv': '1909.05803', 'mag': '2972378157', '...   \n",
              "1537  {'arxiv': None, 'mag': None, 'acl': None, 'pub...   \n",
              "1538  {'arxiv': None, 'mag': '2017331012', 'acl': No...   \n",
              "\n",
              "                                                content  updated  \\\n",
              "0     {'source': {'pdfurls': ['https://export.arxiv....      NaN   \n",
              "1     {'source': {'pdfurls': ['https://arxiv.org/pdf...      NaN   \n",
              "2     {'source': {'pdfurls': ['https://arxiv.org/pdf...      NaN   \n",
              "3     {'source': {'pdfurls': ['https://export.arxiv....      NaN   \n",
              "4     {'source': {'pdfurls': None, 'pdfsha': '8cd7af...      NaN   \n",
              "...                                                 ...      ...   \n",
              "1534  {'source': {'pdfurls': ['https://aclanthology....      NaN   \n",
              "1535  {'source': {'pdfurls': None, 'pdfsha': 'fb186d...      NaN   \n",
              "1536  {'source': {'pdfurls': ['https://www.aclweb.or...      NaN   \n",
              "1537  {'source': {'pdfurls': ['https://arxiv.org/pdf...      NaN   \n",
              "1538  {'source': {'pdfurls': None, 'pdfsha': '884cb4...      NaN   \n",
              "\n",
              "                                           ref_paper_id  \\\n",
              "0                                                    {}   \n",
              "1                                   {220302524: '[30]'}   \n",
              "2                                                    {}   \n",
              "3     {123758373: 'Chen and Durrett, 2019;', 2466523...   \n",
              "4                                                   NaN   \n",
              "...                                                 ...   \n",
              "1534  {218487313: 'Yadav et al., 2020', 52822214: 'Y...   \n",
              "1535                                  {9060527: '[41]'}   \n",
              "1536  {139103297: 'Chen and Durrett, 2019;', 1899278...   \n",
              "1537  {230799347: 'Geva et al., 2021', 225075843: 'L...   \n",
              "1538                                                 {}   \n",
              "\n",
              "                                              cite_text  \n",
              "0                                                    {}  \n",
              "1     {220302524: '\\n\\n\\nResults and discussion\\n\\nW...  \n",
              "2                                                    {}  \n",
              "3     {123758373: ', 2019;', 246652372: ', 2023), la...  \n",
              "4                                                   NaN  \n",
              "...                                                 ...  \n",
              "1534       {218487313: '\\n\\nAIR ', 52822214: ', 2017;'}  \n",
              "1535  {9060527: ' remanei, which were estimated to h...  \n",
              "1536  {139103297: '\\n\\n\\nAdversarial Evaluation\\n\\nM...  \n",
              "1537  {230799347: '\\n\\n\\nData Collection\\n\\nWe curat...  \n",
              "1538                                                 {}  \n",
              "\n",
              "[1539 rows x 7 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Insert paper-title column\n",
        "df.insert(1,'title',title_list)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Idea2Img Framework\n",
            "\u001b[35mSection_title: 4.  Experiments\n",
            "\u001b[35mSection_title: 4.1.  Experiment Settings\n",
            "\u001b[35mSection_title: 4.2.  Image Generation Results\n",
            "\u001b[35mSection_title: 4.3.  LMM Feedback, Revision, and Selection\n",
            "\u001b[32mlimitation: tasks beyond image generation. idea2img explores the emergent ability of multimodal self-refinement in lmmbased systems, through the image design and generation task. specifically, idea2img views the t2i model to use as an unknown multimodal environment to explore, and iteratively refines t2i prompts to find its optimal usage. this concept mirrors the intrinsic human approach of iterative problem-solving when faced with unknown environments or complex challenges. we leave its extension to other intriguing tasks [48], e.g., gui navigation, embodied agents, and complicated visual reasoning, for future exploration. from a single image generation model to multiple tools. idea2img explores the usage of a single image generation model, including a text-to-image model [34] or a textconditioned image-to-image model [24]. when needed, other specialized generative models like controlnet [56], inpainting [4], region-controlled t2i generation [21,51], customized generation [8,35], and video generation [42,53] can be seamlessly switched as replacements. that is, idea2img could broadly boost different image generation models with diverse specialties by exploring their optimal text description or instruction prompts.beyond a single generation model, idea2img can also be used to allocate multiple tools as in multimodal agent studies [45,49]. in this case, idea2img isn't limited to optimizing the use of individual tools but also investigates their effective collaboration when used together, such as generator selection and multi-step visual generation. consolidating explored knowledge. we have shown the effectiveness of lmm iterative self-refinement in automatic image design and generation. going beyond merely preserving the probed t2i model's properties in memory, idea2img can also help to consolidate or distill the explored knowledge into t2i model parameters, such that no inference-time iterative refinement is needed when encountering similar generation scenarios. for example, one could collect a dataset using idea2img for a scenario of interest, and fine-tune a t2i model with the explored selfrefinement trajectory. storing the probed knowledge as sample-agnostic prompt for each image generation model is another promising direction [14,32,57].\n",
            "\u001b[35mSection_title: 5.  Limitation and Discussion\n",
            "\u001b[32mconclusion: we have presented idea2img, a multimodal iterative selfrefinement framework that leverages gpt-4v(ision) for image design and generation. idea2img delves into the emergent capabilities of iterative self-refinement in lmm-based systems, showcasing its effectiveness in improving, assessing, and verifying the generated multimodal content. the user preference study demonstrates idea2img's capability in assisting humans to find the optimal usage of a t2i model for automatic image design and generation.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 3  SPLADE AND METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  SPLADE\n",
            "\u001b[35mSection_title: 3.2  Distillation, hard negative mining and PLM initialization\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS AND EVALUATION\n",
            "\u001b[35mSection_title: 4.1  Training and evaluation\n",
            "\u001b[35mSection_title: 4.2  Results and discussion\n",
            "\u001b[32mconclusion: in this paper, we have built on the splade model, and studied to which extent it is able to take advantage of training improvements like distillation and hard negative mining, as well as better suited plm initialization: combined altogether, the resulting model reaches state-of-the-art performance on both in-domain and zeroshot evaluation. we also investigated the link between effectiveness and efficiency -induced by the degree of regularization -highlighting that more expressive models are better at generalization. \n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Research Questions\n",
            "\u001b[35mSection_title: 3.1  Study design\n",
            "\u001b[35mSection_title: 3.2  Measures\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: large general-purpose language models, such as the gpt family of models (brown et al., 2020;openai, 2023), lamda (thoppilan et al., 2022), palm (chowdhery et al., 2022;anil et al., 2023), and others, have propagated into informationseeking workflows of a general audience.a vast host of existing and ongoing work in nlp examines the deficiencies of these language models, ranging from hallucinated generations (bang et al., 2023;ji et al., 2023), lack of transparency (weld and bansal, 2019;lipton, 2018), reasoning gaps (bang et al., 2023;press et al., 2022), and more.however, close examination of the other piece of the puzzle-the user-is fairly sparse.literature in explainable nlp considers how nlp systems, and their underlying reasoning process, can be made available to the user to allow for better, more informed use of these systems and their predictions (ribeiro et al., 2016;wang and yin, 2021).extending this view of explainability, we considered the question of aiding users in their decision-making process, not by explaining the model prediction but by providing them with relevant pieces of information to assess the prediction.as argued by the contemporary work by fok and weld (2023), explanations in the form of the model's internal reasoning are rarely useful for supporting human decision-making (bansal et al., 2021;buçinca et al., 2021;zhang et al., 2020).instead, explanations should aim to help the users assess the model prediction.our study takes a step in this direction by analyzing users' trust and reliance on qa models by providing information external to the model's input to enable verification of model predictions.\n",
            "\u001b[35mSection_title: 6  Discussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Compositionality in Question Answering\n",
            "\u001b[35mSection_title: 2.3  Curriculum Learning (CL)\n",
            "\u001b[35mSection_title: 3  Problem Definition\n",
            "\u001b[35mSection_title: 3.1  A Challenging Task for Pretrained LMs\n",
            "\u001b[35mSection_title: 4  Compositional Fine-Tuning (CFT)\n",
            "\u001b[35mSection_title: 4.1  Decompose\n",
            "\u001b[35mSection_title: 4.2  Demonstrate\n",
            "\u001b[35mSection_title: 4.3  Fine-Tune\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.2  Evaluation\n",
            "\u001b[35mSection_title: 5.3  Experiment 1: The Role of Components\n",
            "\u001b[35mSection_title: 5.4  Experiment 2: Attribute Transfer\n",
            "\u001b[35mSection_title: 5.5  Experiments 3 & 4: Comparison to Chain of Thought Prompting\n",
            "\u001b[35mSection_title: 5.5.1  Recommendation Tasks\n",
            "\u001b[35mSection_title: 5.5.2  Sports Understanding\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: in this work, we proposed cft as an improvement upon end-to-end learning. to enable research on this topic, we developed a new schema for generating recommendation datasets, which we instantiated in two domains. we showed that cft indeed consistently outperforms end-to-end learning, as much as 32% for local dining. furthermore, we found evidence suggesting that more component tasks can be beneficial for cft. finally, instantiating chain of thought prompting in our dataset and cft in sports understanding, we found cft to be as good or better with lms only 7.4% of the size.\n",
            "\u001b[35mSection_title: 7  Conclusion & Future Work\n",
            "\u001b[32mlimitation: this work focuses on testing if cft outperforms end-to-end learning and chain of thought prompting in two very different domains. despite the positive evidence, it remains to be seen: (i) if task decomposition can be fully automated, and (ii) if different decompositions-in the case of tasks that allow for multiple decompositions-yield similar results. both are second-order research questions that can be pursued once compositionality has been confirmed to improve performance. importantly, both questions have been left open in the initial chain of thought work as well. we hope that our results will add to theirs in attracting more attention to these questions in the future.another limitation of this work is that cft is not applicable to several decomposition datasets that have been proposed. for example, a dataset focused on compositional generalization may include many different types of questions, each requiring different types of intermediate steps. cft is not designed for intermediate steps that carry out very heterogeneous logic. nonetheless, as shown in the recommendation tasks, cft is still relevant for a substantial family of tasks with real-world applicability.lastly, this work is limited by its focus on the english language, and by the use of gpt-3 for its unique range of model sizes. for example, when we discuss that cft on a 13b parameter model (curie) is a much cheaper alternative to chain of thought prompting on a 175b parameter model (davinci), the finding is limited to this setting. it is important to replicate this work on other languages and models, which we plan to do as these become available.comprises two attributes. for world cities, we have a = {temperature, population} where average city temperatures are obtained from wikipedia 4 and city populations from sim-plemaps 2019. 5 after merging items from both sources, we end with 347 well-known cities (>50k inhabitans) from around the globe, such that d c = (i f ull c , {temperature, population}) and |i f ull c | = 347. for local restaurants, we randomly sample 240 restaurants from the city with most restaurants in the yelp dataset 6 , toronto. we have a = {price, distance} where restaurant prices are obtained from yelp and distances to a hypothetical location are randomly generated, thus limiting the lm's access to prior knowledge in this scenario. with that, we have d r = (i f ull r , {price, distance}) and |i f ull r | = 240.in terms of component tasks, we have 694 factual statements for the cities domain and 480 for restaurants, covering two attributes per item. whenever factual statements are provided in cft, they always cover i f ull entirely in order to give the lm full knowledge of the attribute values.\n",
            "\u001b[35mSection_title: 8  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Background\n",
            "\u001b[35mSection_title: 2.  Results\n",
            "\u001b[35mSection_title: 2.1  Quality control of SELDI analysis\n",
            "\u001b[35mSection_title: 2.2  Protein peak detection and data preprocessing\n",
            "\u001b[35mSection_title: 2.3  Validation of the serum proteomic profiles in testing set\n",
            "\u001b[35mSection_title: 2.4  Biomarker purification and identification\n",
            "\u001b[35mSection_title: 2.5  Immunohistochemistry of NAP-2\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[32mconclusion: in summary, we have identified a set of protein peaks that could discriminate hcc from healthy controls. from the protein peaks specific to hcc disease, we identified and characterized neutrophil-activating peptide-2 as a potential proteomic biomarker of hcc. further studies with larger sample sizes will be needed to verify this specific protein marker and to address its efficacy, especially with regard to discriminating histologic types of hcc and disease stages. nevertheless, our study demonstrates a rational approach for identifying hcc biomarkers that could be used for detection and monitoring hcc by proteomic techniques.\n",
            "\u001b[35mSection_title: 4.  Conclusion\n",
            "\u001b[35mSection_title: 5.  Materials and methods\n",
            "\u001b[35mSection_title: 5.1  Patients and samples\n",
            "\u001b[35mSection_title: 5.2  SELDI-TOF-MS analysis of serum protein profiles\n",
            "\u001b[35mSection_title: 5.3  Bioinformatics and biostatistics\n",
            "\u001b[35mSection_title: 5.4  Biomarker purification and identification\n",
            "\u001b[35mSection_title: 5.5  Immunohistochemistry (IHC)\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Question Answering\n",
            "\u001b[35mSection_title: 2.3  Multi-task Learning\n",
            "\u001b[35mSection_title: 3.1  Baseline\n",
            "\u001b[35mSection_title: 3.2  XLNet\n",
            "\u001b[35mSection_title: 3.3  Attention-over-Attention\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Preprocessing\n",
            "\u001b[35mSection_title: 4.2  Data Analysis\n",
            "\u001b[35mSection_title: 4.3  Data Feeding Methods\n",
            "\u001b[35mSection_title: 4.4  Fine-tuning Methods\n",
            "\u001b[35mSection_title: 4.5  Results\n",
            "\u001b[32mconclusion: in this paper, we propose a multi-task framework to improve the generalization ability of question answering systems by leveraging large pre-trained language models. experimental results indicate the effectiveness of our methods on broader qa tasks, with an average exact match score of 56.59 and an average f1 score of 68.98, which are significantly higher than the bert-large baseline results by 8.39 and 7.22, respectively.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND AND RELATED WORK\n",
            "\u001b[35mSection_title: 3  CAN LARGE LANGUAGE MODELS SELF-CORRECT REASONING?\n",
            "\u001b[35mSection_title: 3.1  SELF-CORRECTION WITH ORACLE FEEDBACK\n",
            "\u001b[35mSection_title: 3.1.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.1.2  RESULTS\n",
            "\u001b[35mSection_title: 3.1.3  REFLECTION\n",
            "\u001b[35mSection_title: 3.2  INTRINSIC SELF-CORRECTION\n",
            "\u001b[35mSection_title: 3.2.1  WHY DOES THE PERFORMANCE NOT INCREASE, BUT INSTEAD DECREASE?\n",
            "\u001b[35mSection_title: 3.2.2  RESULTS OF OTHER PROMPTS AND RELATED STUDIES\n",
            "\u001b[35mSection_title: 3.3  MULTI-AGENT DEBATE AND SELF-CONSISTENCY\n",
            "\u001b[35mSection_title: 4  SELF-CORRECTION AS POST-HOC PROMPTING\n",
            "\u001b[35mSection_title: 5  DISCUSSION\n",
            "\u001b[32mconclusion: our research shows that llms are not yet capable of self-correcting their reasoning.this implies that expecting these models to inherently recognize and rectify their inaccuracies might be overly optimistic, at least with the current state of technology.more broadly, this underscores the need for ongoing improvement and a multi-faceted approach when deploying llms in real-world applications.in light of these findings, it is imperative for researchers and practitioners to approach the concept of self-correction with a discerning perspective, acknowledging its potential and recognizing its boundaries.by doing so, we can better equip this technique to address the limitations of llms, steering their evolution towards enhanced accuracy and reliability.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\u001b[32mlimitation: although our exploration spans a variety of self-correction strategies, prompts, and tasks, as we mentioned earlier, it is plausible that there exist specific prompts or strategies that could enhance the reasoning performance of models for particular benchmarks.however, searching such prompts or strategies may inadvertently rely on external feedback, either from human insights or training data.additionally, such search efforts can be equally applied to the pre-hoc prompts (zhou et al., 2022;yang et al., 2023).a potential outcome of our study might be that it discourages research into self-correction for certain applications.nonetheless, we are optimistic that our findings will steer future research towards more practical applications of self-correction, especially those where tangible benefits can be derived, potentially by leveraging external feedback.furthermore, several related works have already presented findings consistent with our observation: that self-correction, when devoid of external feedback, serves as a relatively weak baseline (gou et al., 2023;zhou et al., 2023a).despite this, we have identified a prevailing ambiguity in the wider community, with even domain experts being uncertain about the intricacies of when and how selfcorrection operates.some existing literature may inadvertently contribute to this confusion, either by relegating crucial details about label usage to less prominent sections or by failing to clarify that their designed self-correction strategies actually incorporate external feedback.our intention in this paper is to amplify these concerns and offer a comprehensive overview of the state of \"selfcorrection\" in llms.the title, \"large language models cannot self-correct reasoning yet\", is not an outright dismissal of self-correction techniques.instead, it serves as a call to action, urging researchers to approach this domain with a discerning and critical perspective.furthermore, it encourages future research to explore approaches that can genuinely enhance reasoning.for the sake of clarity, we recommend omitting the term \"self\" from concepts like self-correction or selfcritique when the context does not strictly adhere to an intrinsic self-correction setting and adopting less confusing terms like \"correction with external tools/feedback\".finally, in light of our findings that llms struggle to self-correct reasoning based purely on their inherent capabilities, we urge a more circumspect view on any unwarranted optimism (or fear) regarding the autonomous evolution of llms (and ai systems) through self-improvement.\n",
            "\u001b[35mSection_title: 7  LIMITATIONS AND BROADER IMPACT\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Summary\n",
            "\u001b[35mSection_title: 2  Source Corpora\n",
            "\u001b[35mSection_title: 2.1  Corpora Selection Criteria\n",
            "\u001b[35mSection_title: 2.2  Selected Corpora\n",
            "\u001b[35mSection_title: 2.2.1  Potential Concerns\n",
            "\u001b[35mSection_title: 2.3  Preprocessing\n",
            "\u001b[35mSection_title: 2.4  Task Assignment\n",
            "\u001b[35mSection_title: 3  KLUE Benchmark\n",
            "\u001b[35mSection_title: 3.1  Topic Classification (TC)\n",
            "\u001b[35mSection_title: 3.1.1  Dataset Construction\n",
            "\u001b[35mSection_title: 3.1.2  Evaluation Metric\n",
            "\u001b[35mSection_title: 3.1.3  Related Work\n",
            "\u001b[32mconclusion: we introduce ynat, the first korean topic classification benchmark. the benchmark includes 63,892 news headlines classified to a single hand-labeled topic among 7 categories. we assume each headline has only a single topic, but it could be formulated as multi-label classification. we thus open the second and third relevant topic annotations. also, urls for each headlines are accompanied for future work if metadata is needed. if some of them requires permission to use, one should contact to the agency. we expect ynat to serve as a simple and basic nlu task compared to others in klue.\n",
            "\u001b[35mSection_title: 3.1.4  Conclusion\n",
            "\u001b[35mSection_title: 3.2  Semantic Textual Similarity (STS)\n",
            "\u001b[35mSection_title: 3.2.1  Dataset Construction\n",
            "\u001b[35mSection_title: 3.2.2  Evaluation Metrics\n",
            "\u001b[32mconclusion: we create the first human-annotated korean sts benchmark, klue-sts, that covers multiple domains and styles with free accessibility to everyone. the similarity score annotation process is specially designed to capture the characteristics of the korean language. covering the expressions from various domains, our benchmark is expected to be a useful resource for further research, beyond serving as a benchmark. our benchmark helps to develop numerous models established on sts resources, such as sentencebert [115].\n",
            "\u001b[35mSection_title: 3.2.4  Conclusion\n",
            "\u001b[35mSection_title: 3.3  Natural Language Inference (NLI)\n",
            "\u001b[35mSection_title: 3.3.1  Dataset Construction\n",
            "\u001b[35mSection_title: 3.3.2  Evaluation Metric\n",
            "\u001b[35mSection_title: 3.3.3  Related Work\n",
            "\u001b[32mconclusion: our new dataset, klue-nli, is the first resource constructed upon naturally occurring korean sentences. klue-nli represents diverse linguistic phenomena, writing style, degree of formality and contents that are most natural and suitable for korean. the premise sentences of our dataset come from six korean corpora, and the hypothesis sentences are written by well-trained workers.\n",
            "\u001b[35mSection_title: 3.3.4  Conclusion\n",
            "\u001b[35mSection_title: 3.4  Named Entity Recognition (NER)\n",
            "\u001b[35mSection_title: 3.4.1  Dataset Construction\n",
            "\u001b[35mSection_title: 3.4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.4.3  Related Work\n",
            "\u001b[32mconclusion: we construct a new korean ner benchmark that covers broad domains and styles, which is freely accessible to anyone. the entity types are annotated so that a model has to use both morphological and contextual cues. the character-level entity tagging and evaluation method reflects the characteristics of korean morphology. since klue-ner dataset covers both formal news articles and informal user-generated web texts, we hope that our benchmark helps develop ner models that can be used in a wide a range of domains, and serve as a resource for developing advanced models for information extraction.\n",
            "\u001b[35mSection_title: 3.4.4  Conclusion\n",
            "\u001b[35mSection_title: 3.5  Relation Extraction (RE)\n",
            "\u001b[35mSection_title: 3.5.1  Data Construction\n",
            "\u001b[35mSection_title: 1.  Collect Candidate Sentences\n",
            "\u001b[35mSection_title: 2.  Define Relation Schema\n",
            "\u001b[35mSection_title: 3.  Detect Entities\n",
            "\u001b[35mSection_title: 4.  Select Entity Pairs\n",
            "\u001b[35mSection_title: 5.  Annotate Relations\n",
            "\u001b[35mSection_title: 3.5.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.5.3  Related Work\n",
            "\u001b[32mconclusion: we propose klue-re, a large-scale human-annotated re benchmark for korean. to overcome the lack of large-scale and up-to-date korean kbs, we design an efficient candidate collection method, coupled with an effective annotation scheme. klue-re can not only be used for online information extraction but also contribute to building a large-scale knowledge graph from unstructured texts. we therefore expect klue-re to be a starting point for building a large-scale, ever-growing public kb in korean, as well as a valuable korean nlu benchmark.\n",
            "\u001b[35mSection_title: 3.5.4  Conclusion\n",
            "\u001b[35mSection_title: 3.6  Dependency Parsing (DP)\n",
            "\u001b[35mSection_title: 3.6.1  Dataset Construction\n",
            "\u001b[35mSection_title: 3.6.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.6.3  Related Work\n",
            "\u001b[32mconclusion: we build a korean dp benchmark klue-dp consisting of formal news and informal user-generated web data. klue-dp is helpful for developing a dp model that can be used in multiple domains. pos tagging is performed together to improve dp performance, and the tagset and guideline for dp and pos tagging are applied by revising the existing tta dataset. this guideline is customized to reflect the characteristics of korean (agglutinative, free word order, etc.), and it also tackles omission of predicates in web data or errors in spacing. we hope that our benchmarks will help in the development of korean dp models and other natural language processing. 40 https://universaldependencies.org\n",
            "\u001b[35mSection_title: 3.6.4  Conclusion\n",
            "\u001b[35mSection_title: 3.7  Machine Reading Comprehension (MRC)\n",
            "\u001b[35mSection_title: 3.7.1  Dataset Construction\n",
            "\u001b[35mSection_title: 1.  Common Guidelines\n",
            "\u001b[35mSection_title: 2.  Type-Specific Guidelines\n",
            "\u001b[35mSection_title: 2.1.  Question Paraphrasing (Type 1)\n",
            "\u001b[35mSection_title: 2.2.  Multiple-Sentence Reasoning (Type 2)\n",
            "\u001b[35mSection_title: 2.3.  Unanswerable Questions (Type 3)\n",
            "\u001b[35mSection_title: 3.7.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.7.3  Analysis\n",
            "\u001b[35mSection_title: 3.7.4  Related Work\n",
            "\u001b[32mconclusion: we create a new challenging korean mrc benchmark named (klue-mrc). in order to evaluate different aspects of mrc capabilities, klue-mrc includes multi-domain passages and three types of questions: paraphrase, multisentence reasoning, and unanswerable. klue-mrc shows improvements in question type diversity, difficulty, and lexical overlap compared to existing korean mrc datasets.\n",
            "\u001b[35mSection_title: 3.7.5  Conclusion\n",
            "\u001b[35mSection_title: 3.8  Dialogue State Tracking (DST)\n",
            "\u001b[35mSection_title: 3.8.1  Dataset Construction\n",
            "\u001b[35mSection_title: 2.  Creating Knowledge Base\n",
            "\u001b[35mSection_title: 3.  Designing the Annotation System\n",
            "\u001b[35mSection_title: 3.1.  User Side\n",
            "\u001b[35mSection_title: 3.2.  System Side\n",
            "\u001b[35mSection_title: 4.  Dataset Construction\n",
            "\u001b[35mSection_title: 3.8.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.8.3  Analysis\n",
            "\u001b[35mSection_title: 3.8.4  Related Work\n",
            "\u001b[32mconclusion: we introduce wizard-of-seoul (wos), the first large-scale korean multi-domain task-oriented dialogue dataset that simulates conversations between seoul tourists and travel agents. we adapt 'self-dialog' for efficiently scaling up of dialogue collection scheme. in addition, consideration on annotation interfaces (drop-down menu and turn-switching) mitigates erroneous cases and diverse goal instructions including counterfactual ones promote each conversation to be more natural and challenging. we hope that wos sparks various future dialogue research in korean and also offers valuable insights to pushing forward end-to-end dialogue modeling.\n",
            "\u001b[35mSection_title: 3.8.5  Conclusion\n",
            "\u001b[35mSection_title: 4  Pretrained Language Models\n",
            "\u001b[35mSection_title: 4.1  Language Models\n",
            "\u001b[35mSection_title: 4.2  Existing Language Models\n",
            "\u001b[35mSection_title: 5.1.1  Single Sentence Classification\n",
            "\u001b[35mSection_title: 5.1.2  Sentence Pair Classification / Regression\n",
            "\u001b[35mSection_title: 5.1.3  Multiple-Sentence Slot-Value Prediction\n",
            "\u001b[35mSection_title: 5.1.4  Sequence Tagging\n",
            "\u001b[35mSection_title: 5.2  Fine-Tuning Configurations\n",
            "\u001b[35mSection_title: 5.3  Evaluation Results\n",
            "\u001b[35mSection_title: 5.4  Analysis of Models\n",
            "\u001b[35mSection_title: 6  Ethical Considerations\n",
            "\u001b[35mSection_title: 6.1  Copyright and Accessibility\n",
            "\u001b[35mSection_title: 6.2  Toxic Content\n",
            "\u001b[35mSection_title: 6.3  Personally Identifiable Information\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\u001b[32mconclusion: we present klue, a suite of korean nlu benchmarks that includes diverse tasks. we open klue to everyone, and we also provide korean language models trained to outperform multilingual models and other existing open-sourced korean language models. we set high standards from the outset, as we built the benchmark and trained the models from scratch. we designed the benchmark datasets and trained the annotators rigorously to consider potential ethical issues including private information and hate speech. we documented in detail all of the benchmark construction and testing processes. we also discussed broader impacts and limitations of klue and our models. despite the limitations, klue and the accompanying language models will facilitate future korean nlp research by setting a valuable precedent describing how datasets and language models should be created and spread to a wider community.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Objectives and Methodology\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\u001b[35mSection_title: 3.3.  Transform\n",
            "\u001b[32mconclusion: the proposed intelligent healthcare data management system dynamically assembles the nanosensor information of the sick person or patient and operates it on the smart phone.this system also transforms raw data of the user from nanosensors to hl7 format and transfers the encrypted hl7 data to a remote healthcare server.\n",
            "\u001b[35mSection_title: 4.  Conclusions and Future Works\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Graph Neural Networks\n",
            "\u001b[35mSection_title: 3  Dynamic Relevance Graph Network\n",
            "\u001b[35mSection_title: 3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.2  Model Description\n",
            "\u001b[35mSection_title: 3.3  Language Context Encoder\n",
            "\u001b[35mSection_title: 3.4  KG Subgraph Construction\n",
            "\u001b[35mSection_title: 3.5  Graph Neural Network Module\n",
            "\u001b[35mSection_title: 3.6  Answer Prediction\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Baseline Description\n",
            "\u001b[35mSection_title: 4.4  Result Comparison\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Effects on Finding the Line of Reasoning\n",
            "\u001b[35mSection_title: 5.2  Effects on Semantic Context\n",
            "\u001b[35mSection_title: 5.3  Effects of Number of Graph Layers\n",
            "\u001b[35mSection_title: 5.4  Ablation of DRGN Modules\n",
            "\u001b[32mconclusion: in this paper, we propose a novel dynamic relevance graph network (drgn) architecture for commonsense question answering given an external source of knowledge in the form of a knowledge graph. our model learns the graph node representation while a) exploits the existing relations in kg, b) re-scales the importance of the neighbor nodes in the graph based on training a dynamic relevance matrix, c) establishes direct connections between graph nodes based on measuring the relevance scores of the nodes dynamically during training. the dynamic relevance edges help in finding the chain of reasoning when there are missing edges in the original kg. our quantitative and qualitative analysis shows that the proposed approach facilitates answering the complex questions that need multiple hops of reasoning. furthermore, since drgn uses the relevance between the question node and graph entities, it exploits the richer semantic context of the question in graph reasoning which leads to improvements in the performance on the negative questions. our proposed approach shows competitive performance on two qa benchmarks, including commonsenseqa and openbookqa.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  TEK-enriched Representations\n",
            "\u001b[35mSection_title: 2.1  TEK-Enriched Question Answering\n",
            "\u001b[35mSection_title: 2.2  TEK-enriched Pretraining\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Baselines\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 5  Ablation Studies TEK vs. Context-only Pretraining\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we presented a method to build text representations by jointly contextualizing the input with dynamically retrieved textual encyclopedic knowledge. we showed consistent improvements, in-and out-of-domain, across multiple reading comprehension benchmarks that require factual reasoning and knowledge well represented in the background collection.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Ethics\n",
            "\u001b[35mSection_title: 2.2  Ethics in NLP\n",
            "\u001b[35mSection_title: 3  Deontological Ethics\n",
            "\u001b[35mSection_title: 3.1  Generalization Principle\n",
            "\u001b[35mSection_title: 3.2  Respect for Autonomy\n",
            "\u001b[35mSection_title: 4  Applying Ethics to NLP systems\n",
            "\u001b[35mSection_title: 4.1  Question-Answering Systems\n",
            "\u001b[35mSection_title: 4.2  Detecting Objectionable Content\n",
            "\u001b[35mSection_title: 4.3  Machine Translation Systems\n",
            "\u001b[35mSection_title: 4.4  Dialogue Systems\n",
            "\u001b[35mSection_title: 5  Ethical Decision Making with NLP\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: two principles of deontological ethics-namely the generalization principle and respect for autonomy via informed consent-can be used to decide if an action is ethical. despite the limitations of these principles, they can provide useful insights into making nlp systems more ethical. through the four case studies discussed in this paper, we demonstrate how these principles can be used to evaluate the decisions made by nlp systems and to identify the missing aspects. for each of the case studies, we also present potential directions for nlp research to move forward and make the system more ethical.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  PROMPTRANK\n",
            "\u001b[35mSection_title: 2.1  Path Reranking with PROMPTRANK\n",
            "\u001b[35mSection_title: 2.2  Instruction Search and Ensembling\n",
            "\u001b[35mSection_title: 2.3  Demonstration Ensembling\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Retrieval Performance\n",
            "\u001b[35mSection_title: 3.2  Full QA Performance\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[35mSection_title: 4.1  Comparison to Single-hop Reranking\n",
            "\u001b[35mSection_title: 4.2  Role of Instruction\n",
            "\u001b[35mSection_title: 4.3  Choice of Language Model\n",
            "\u001b[35mSection_title: 4.4  Further Analysis and Comparisons\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[32mconclusion: in this work, we propose a simple method to improve task-agnostic distillation generalization by leveraging mtl. the teacher is first augmented by mtl and then distilled. empirical results show that our method outperforms several baselines on in-domain, out-domain and low-resource tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: compared with vanilla task-agnostic distillation, mitkd has an additional multi-task finetuning stage which may require additional computation resources.    \n",
            "\u001b[35mSection_title: 6  Limitation\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[32mconclusion: in this work, we propose a simple method to improve task-agnostic distillation generalization by leveraging mtl. the teacher is first augmented by mtl and then distilled. empirical results show that our method outperforms several baselines on in-domain, out-domain and low-resource tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: compared with vanilla task-agnostic distillation, mitkd has an additional multi-task finetuning stage which may require additional computation resources.    \n",
            "\u001b[35mSection_title: 6  Limitation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Context encoding\n",
            "\u001b[35mSection_title: 3.2  Reasoning over HDE graph\n",
            "\u001b[35mSection_title: 3.3  Score accumulation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Experimental settings\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Ablation studies\n",
            "\u001b[35mSection_title: 4.5  Result analysis\n",
            "\u001b[32mconclusion: we propose a new gnn-based method for multihop rc across multiple documents. we introduce the hde graph, a heterogeneous graph for multiple-hop reasoning over nodes representing different granularity levels of information. we use co-attention and self-attention to encode candidates, documents, entities of mentions of candidates and query subjects into query-aware representations, which are then employed to initialize graph node representations. evaluated on wiki-hop, our end-to-end trained single neural model achieves the state-of-the-art performance on the blind test set. in the future, we would like to investigate explainable gnn for this task, such as explicit reasoning path in (kundu et al., 2018), and work on other data sets such as hotpotqa.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  LM-based Reward for Fluency\n",
            "\u001b[35mSection_title: 3.2  Discriminator-Based Reward for Relevance\n",
            "\u001b[35mSection_title: 3.3  QA-Based Reward for Answerability\n",
            "\u001b[35mSection_title: 3.4  Model Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Automatic Evaluation\n",
            "\u001b[35mSection_title: 4.2  Comparison with Baselines\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[35mSection_title: 4.4  Consistency between Rewards and Human Judgement\n",
            "\u001b[35mSection_title: 4.5  Mesoscopic Analysis of Generated Questions\n",
            "\u001b[32mconclusion: in this paper, we optimize three question-specific rewards via reinforcement learning on a seq2seq based question generator, aiming to improve the fluency, relevance and answerability of the generated questions. through comprehensive analytic experiments, including automatic and human evaluation, consistency validation, and meso analysis, we show that the effectiveness of a reward is poorly reflected by automatic evaluation metrics such as bleu. instead, we find a reward that correlates well with the human judgement generally has better effects on improving the question quality. in future works, we believe these observations can help to guide the design of other qg-specific rewards that target on unexplored aspects of question generation, such as the informativeness and the utility of questions.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Method\n",
            "\u001b[35mSection_title: 2.1.  Unsupervised Question Decomposition\n",
            "\u001b[35mSection_title: 2.1.1.  CREATING PSEUDO-DECOMPOSITIONS\n",
            "\u001b[35mSection_title: 2.1.2.  LEARNING TO DECOMPOSE\n",
            "\u001b[35mSection_title: 2.2.  Answering Sub-Questions\n",
            "\u001b[35mSection_title: 2.3.  QA using Decompositions\n",
            "\u001b[35mSection_title: 3.  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1.  Question Answering Task\n",
            "\u001b[35mSection_title: 3.2.  Unsupervised Decomposition\n",
            "\u001b[35mSection_title: 3.2.1.  QUESTION DATA\n",
            "\u001b[35mSection_title: 3.2.2.  CREATING PSEUDO-DECOMPOSITIONS\n",
            "\u001b[35mSection_title: 3.2.3.  UNSUPERVISED DECOMPOSITION MODELS\n",
            "\u001b[35mSection_title: 3.3.  Single-hop Question Answering Model\n",
            "\u001b[35mSection_title: 4.  Results on Question Answering\n",
            "\u001b[35mSection_title: 4.1.  Question Type Breakdown\n",
            "\u001b[35mSection_title: 4.3.  How Do Decompositions Help?\n",
            "\u001b[35mSection_title: 4.4.  Example Decompositions\n",
            "\u001b[35mSection_title: 5.  Analysis\n",
            "\u001b[35mSection_title: 5.1.  Unsupervised Decomposition Model\n",
            "\u001b[35mSection_title: 5.2.  Single-hop Question Answering Model\n",
            "\u001b[35mSection_title: 5.3.  Multi-hop Question Answering Model\n",
            "\u001b[35mSection_title: 6.  Related Work\n",
            "\u001b[32mconclusion: we proposed an algorithm that decomposes questions without supervision, using 3 stages: (1) learning to decompose using pseudo-decompositions without supervision, (2) answering sub-questions with an off-the-shelf qa system, and (3) answering hard questions more accurately using sub-questions and their answers as additional input. when evaluated on hotpotqa, a standard benchmark for multihop qa, our approach significantly improved accuracy over an equivalent model that did not use decompositions. our approach relies only on the final answer as supervision but works as effectively as state-of-the-art methods that rely on strong supervision, such as supporting fact labels or example decompositions. qualitatively, we found that unsupervised decomposition resulted in fluent sub-questions whose answers often match the annotated supporting facts in hotpotqa. our unsupervised decompositions are largely extractive, which is effective for compositional, multi-hop questions but not all complex questions, showing room for future work. overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems.\n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset collection\n",
            "\u001b[35mSection_title: 2.1  HAMLET\n",
            "\u001b[35mSection_title: 2.2  Annotation details\n",
            "\u001b[35mSection_title: 2.3  Round 1\n",
            "\u001b[35mSection_title: 2.4  Round 2\n",
            "\u001b[35mSection_title: 2.5  Round 3\n",
            "\u001b[35mSection_title: 2.6  Comparing with other datasets\n",
            "\u001b[35mSection_title: 3  Dataset statistics\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  The effectiveness of adversarial training\n",
            "\u001b[35mSection_title: 4.2  Stress Test Results\n",
            "\u001b[35mSection_title: 4.3  Hypothesis-only results\n",
            "\u001b[35mSection_title: 5  Linguistic analysis\n",
            "\u001b[32mconclusion: in this work, we used a human-and-model-in-theloop training method to collect a new benchmark for natural language understanding. the benchmark is designed to be challenging to current stateof-the-art models. annotators were employed to act as adversaries, and encouraged to find vulnerabilities that fool the model into misclassifying, but that another person would correctly classify. we found that non-expert annotators, in this gamified setting and with appropriate incentives, are remarkably creative at finding and exploiting weaknesses. we collected three rounds, and as the rounds progressed, the models became more robust and the test sets for each round became more difficult. training on this new data yielded the state of the art on existing nli benchmarks.\n",
            "\u001b[35mSection_title: 7  Discussion & Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Knowledge Source\n",
            "\u001b[35mSection_title: 3  Tasks\n",
            "\u001b[35mSection_title: 3.1  Fact Checking\n",
            "\u001b[35mSection_title: 3.2  Entity Linking\n",
            "\u001b[35mSection_title: 3.3  Slot Filling\n",
            "\u001b[35mSection_title: 3.4  Open Domain Question Answering\n",
            "\u001b[35mSection_title: 3.5  Dialogue\n",
            "\u001b[35mSection_title: 4  Provenance Annotation Campaign\n",
            "\u001b[35mSection_title: 5  Evaluation Metrics\n",
            "\u001b[35mSection_title: 6  Baselines\n",
            "\u001b[35mSection_title: 7  Results\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\u001b[32mconclusion: we introduce kilt, a benchmark for assessing models that need to condition on specific knowledge in a defined snapshot of wikipedia to solve tasks spanning five domains. the goal is to catalyze and facilitate research towards general and explainable models equipped with task-agnostic representations of knowledge. our experiments show promising results for a general solution combining dense retrieval and seq2seq generations, although there is large room for improvements. in particular, we find that provenance of current models is generally low.\n",
            "\u001b[35mSection_title: 10  Conclusion\n",
            "\u001b[35mSection_title: 11  Acknowledgment\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Temporal QA algorithms\n",
            "\u001b[35mSection_title: 3.1  Temporal KG\n",
            "\u001b[35mSection_title: 3.2  Temporal Questions\n",
            "\u001b[35mSection_title: 3.2.1  Question Categorization\n",
            "\u001b[35mSection_title: 4  Temporal KG Embeddings\n",
            "\u001b[35mSection_title: 4.1  ComplEx\n",
            "\u001b[35mSection_title: 4.2  TComplEx, TNTComplEx\n",
            "\u001b[35mSection_title: 4.3  TimePlex\n",
            "\u001b[35mSection_title: 5  CRONKGQA: Our proposed method\n",
            "\u001b[35mSection_title: 6  Experiments and diagnostics\n",
            "\u001b[35mSection_title: 6.1  Other methods compared\n",
            "\u001b[35mSection_title: 6.2  Main results\n",
            "\u001b[35mSection_title: 6.3  Performance across question types\n",
            "\u001b[35mSection_title: 6.5  Temporal vs. non-temporal KG embeddings\n",
            "\u001b[32mconclusion: in this paper we introduce cronquestions, a new dataset for temporal knowledge graph question answering. while there exist some temporal kgqa datasets, they are all based on non-temporal kgs (e.g., freebase) and have relatively few questions. our dataset consists of both a temporal kg as well as a large set of temporal questions requiring various structures of reasoning. in order to develop such a large dataset, we used a synthetic   generation procedure, leading to a question distribution that is artificial from a semantic perspective. however, having a large dataset provides an opportunity to train models, rather than just evaluate them. we experimentally show that increasing the training dataset size steadily improves the performance of certain methods on the tkgqa task. we first apply large pre-trained lm based qa methods on our new dataset. then we inject kg embeddings, both temporal and non-temporal, into these lms and observe significant improvement in performance. we also propose a new method, cronkgqa, that is able to leverage temporal kg embeddings to perform tkgqa. in our experiments, cronkgqa outperforms all baselines. these results suggest that kg embeddings can be effectively used to perform temporal kgqa, although there remains significant scope for improvement when it comes to complex reasoning questions.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  OpenQA\n",
            "\u001b[35mSection_title: 3.1  Structured Data QA Solver\n",
            "\u001b[35mSection_title: 3.1.1  Semantic Analysis QA\n",
            "\u001b[35mSection_title: 3.1.2  Deep Learning Parsing QA\n",
            "\u001b[35mSection_title: 3.2  Unstructured Data QA Solver\n",
            "\u001b[35mSection_title: 3.2  Answer Selection\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[32mconclusion: we propose an intelligent question-answering system based on structured and unstructured data, that is, openqa, in which users can give query questions and the model can quickly give accurate answers back to users. we integrate the kbqa structured question answering based on semantic deep learning analysis and the two-stage unstructured question answering based on machine reading comprehension into the intelligent question answering system, and return the answer with the highest probability through the answer selection module based on transformer.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Model Architecture\n",
            "\u001b[35mSection_title: 3.2  Unsupervised Pre-training Data\n",
            "\u001b[35mSection_title: 3.3  Supervised Fine-tuning Data\n",
            "\u001b[35mSection_title: 3.4  Training Details\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Zero-shot Text Classification\n",
            "\u001b[35mSection_title: 4.2  Unsupervised Text Retrieval\n",
            "\u001b[35mSection_title: 4.3  Massive Text Embedding Benchmark\n",
            "\u001b[35mSection_title: 4.4  Code Search\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Impact of Scaling\n",
            "\u001b[35mSection_title: 5.2  Training Behavior\n",
            "\u001b[35mSection_title: 5.3  Influence of Different Training Stages\n",
            "\u001b[35mSection_title: 5.4  Training Data Mixture\n",
            "\u001b[35mSection_title: 5.5  Ablation of the Contrastive Objective\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: this paper presents a multi-stage contrastive learning approach to develop text embedding model that can be applied to various tasks. our model benefits from a diverse training data mixture, enabling it to achieve good generalization performance for single vector embedding. through extensive evaluation on multiple benchmarks, we demonstrate the effectiveness and versatility of our text embedding model. our future work will focus on scaling the model to support longer context, extending it to support multilingual and multi-modal applications, as well as exploring the benefits of prompts and instructions. academic paper the scientific articles usually have a higher quality due to its formal nature. for each paper, we use the title as query and its abstract as document for constructing text pairs. the articles are mined from different websites (such as arxiv, biorxiv, medrxiv, pubmed and semantic scholar) to cover a wide range of topics.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Teaching Broad-Coverage Reasoning Skills in a Robust Fashion\n",
            "\u001b[35mSection_title: 3  TEABREAC Dataset Construction\n",
            "\u001b[35mSection_title: 3.1  Instance Generator\n",
            "\u001b[35mSection_title: 3.2  Dataset Generator\n",
            "\u001b[35mSection_title: 3.3  Additional QA Instances for Primitives\n",
            "\u001b[35mSection_title: 3.4  Final Dataset\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1.1  Datasets\n",
            "\u001b[35mSection_title: 4.1.2  Models\n",
            "\u001b[35mSection_title: 4.2  Experimental Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: large language models demonstrate impressive reading comprehension abilities and a wide variety of reasoning skills. despite these abilities and the availability of large scale multihop qa datasets, large lm-based qa models do not reliably learn to use such reasoning skills for answering complex questions. in this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions. our transfer results on actual qa datasets also add to the line of work that shows synthetic datasets can be used to inject useful skills that transfer over to real natural language tasks. given the artifact issues in real datasets (specifically, in their contexts) and the difficulty in controlling for them via perturbations, leveraging existing multihop questions for their broad reasoning patterns but using synthetic contexts appears to be a viable alternative for carefully constructing teaching datasets, where models can learn the right way to reason. \n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  INSTRUCTOR\n",
            "\u001b[35mSection_title: 2.1  Embedding Architecture\n",
            "\u001b[35mSection_title: 2.2  Training Objective\n",
            "\u001b[35mSection_title: 2.3  MEDI: Multitask Embedding Data with Instructions\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Main Results\n",
            "\u001b[35mSection_title: 4  Analysis and Ablations\n",
            "\u001b[35mSection_title: 4.1  Instructions Enable Diverse Training\n",
            "\u001b[35mSection_title: 4.2  Instruction Robustness\n",
            "\u001b[35mSection_title: 4.3  Complexity of Instructions\n",
            "\u001b[35mSection_title: 4.4  Model Sizes and Instruction Finetuning\n",
            "\u001b[35mSection_title: 4.5  Instructions Mitigate Domain Shifts\n",
            "\u001b[35mSection_title: 4.6  Qualititive Analysis\n",
            "\u001b[32mconclusion: we introduced instructor, a single model that creates broadly-applicable text embeddings using natural language instructions. we constructed medi, a collection of diverse datasets, to finetune instructor with instructions. our extensive experiments showed that instructor achieves state-of-the-art performance on text embedding benchmarks, as well as prompt retrieval for fewshot in-context learning. we hope that researchers and practitioners will benefit from our embeddings or our datasets for tasks of their interest.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: although  table 5 in the appendix. at each step, we first randomly select a dataset and then construct a minibatch only using the examples from that dataset. in this way, we ensure that in-batch negatives are sampled from the same dataset, thereby preventing the model from using task differences to predict the negative label. we use the maximum batch size that fits the machine memory and run all our experiments on 40gb a100 gpus.training we initialize instructor with the gtr-large model (ni et al., 2021, 335m parameters) 4 and finetune it on medi using the adamw optimizer with learning rate 2 × 10 −5 and warmup ratio 0.1. we use a softmax temperature of 0.01 and finetune instructor for 20k steps.baselines we use the official mteb benchmark for comparisons, but here we highlight several strong baselines with the following two types. the first class of baselines is embedding models specializing in information retrieval: contriever-  , 2022). they are mainly trained on symmetric paraphrase datasets such as nli (williams et al., 2018) and the quora question pairs. 5 all of these baselines are based on pretrained language models, achieving strong performance on the mteb leaderboard. in particular, sent-t5-xxl and gtr-xxl (both with 4.8b parameters) achieve the first and second best average performances.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.  OBSERVATIONS AND DATA REDUCTION\n",
            "\u001b[35mSection_title: 2.2.  Previous events:\n",
            "\u001b[35mSection_title: 3.  MEASUREMENT METHODOLOGY FOR THE He II λ4686\n",
            "\u001b[35mSection_title: 4.  RESULTS\n",
            "\u001b[35mSection_title: 4.2.  Radial velocities\n",
            "\u001b[35mSection_title: 4.3.  Correlation between He II λ4686 and X-rays\n",
            "\u001b[35mSection_title: 4.4.  He II λ4686 far from periastron\n",
            "\u001b[35mSection_title: 5.  DISCUSSION\n",
            "\u001b[32mconclusion: in this paper, we present data from the last 4 low excitation events in η car in the light of the he ii λ4686 emission line. we summarize our results and conclusions below.\n",
            "\u001b[35mSection_title: 6.  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Next Document Prediction\n",
            "\u001b[35mSection_title: 3.2  Bridge Entity Re-Phrasing\n",
            "\u001b[35mSection_title: 3.3  Model Architecture\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Pre-Training Details\n",
            "\u001b[35mSection_title: 4.2  Fine-Tuning Details\n",
            "\u001b[35mSection_title: 4.3  Tasks\n",
            "\u001b[35mSection_title: 4.4  Multi-Hop Retrieval Strategy\n",
            "\u001b[35mSection_title: 4.5  Baselines\n",
            "\u001b[35mSection_title: 4.6  Metric\n",
            "\u001b[35mSection_title: 5  Results & Discussion\n",
            "\u001b[35mSection_title: 5.1  Supporting Document Prediction\n",
            "\u001b[35mSection_title: 5.2  End-to-End Multi-Hop QA\n",
            "\u001b[32mconclusion: answering complex questions includes reasoning across multiple documents. recent studies have found that reasoning requires learning sub-question detection and relevant document retrieval to predict n correct answer with supporting facts. however, building such datasets requires costly human annotation and has limited scalability. to address this issue, we proposed a weakly supervised pre-training method for multi-hop retriever, louvre. our pretraining method contains three elements: \"next document prediction\" task, \"bridge entity re-phrasing\", and a model. we demonstrated the efficacy of louvre and its robustness on few-shot settings with extensive experiments on supporting document retrieval task and end-to-end multi-hop qa task. we also showed that our method performs very well at a much lower inference cost.   calculate score of each paths/documents by jointly encode each document with the given question. as a result, reranking takes a huge portion of computation time of the end-to-end multi-hop qa pipeline. semanticretrievalmrs (nie et al., 2019) propose the document reranking model that takes output of sparse retrievers such as tf-idf. since the model outputs documents not a list of supporting documents, we use the same document rearranging method as tf-idf above. pathretriever (asai et al., 2020) and\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Continual Learning\n",
            "\u001b[35mSection_title: 2.2  Domain Adaptation\n",
            "\u001b[35mSection_title: 3  Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Problem Statement\n",
            "\u001b[35mSection_title: 3.2  Method overview\n",
            "\u001b[35mSection_title: 3.3  Initial Training\n",
            "\u001b[35mSection_title: 3.3.1  Prepare Backbone model Transformer Encoder Module:\n",
            "\u001b[35mSection_title: 3.3.2  Memory Initialization\n",
            "\u001b[35mSection_title: 3.4  Learning for New Domain\n",
            "\u001b[35mSection_title: 3.4.1  Memory-based Adversarial Training\n",
            "\u001b[35mSection_title: 3.4.2  Knowledge Distillation\n",
            "\u001b[35mSection_title: 3.4.3  Uncertainty-aware Memory Updating\n",
            "\u001b[35mSection_title: 3.4.4  Objective Function\n",
            "\u001b[35mSection_title: 4.1  Continual MRC Datasets\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Implementation Details\n",
            "\u001b[35mSection_title: 4.5  Results and Discussion\n",
            "\u001b[35mSection_title: 4.5.1  Main Results\n",
            "\u001b[35mSection_title: 4.5.2  Ablation Study\n",
            "\u001b[35mSection_title: 4.5.3  Effect of Memory\n",
            "\u001b[35mSection_title: 4.5.4  Effect of Domain Order\n",
            "\u001b[35mSection_title: 4.5.5  Efficiency Analysis\n",
            "\u001b[32mconclusion: in this paper, an incremental learning mrc model with uncertainty-aware fixed memory and adversarial domain adaptation, ma-mrc, is proposed for continual mrc and alleviating catastrophically forgetting. inspired by the human learning process, there are two main ideas of ma-mrc: a memory that stores a small number of samples in previous seen domain data and always focuses on what the model most needs to replay; and adversarial learning the domain adaptation in a two-player game to learn better transfer representations between previous and current domain data. experimental results show that the proposed ma-mrc can achieve a good continuous learning performance without catastrophically forgetting under cda-c and cda-q settings.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Creation\n",
            "\u001b[35mSection_title: 3.1  Crowdsourcing Instructions\n",
            "\u001b[35mSection_title: 3.2  Task Definitions\n",
            "\u001b[35mSection_title: 3.3  Validation\n",
            "\u001b[35mSection_title: 4  Tasks and Baseline Models\n",
            "\u001b[35mSection_title: 4.1  Retrieval\n",
            "\u001b[35mSection_title: 4.2  System State Tracking\n",
            "\u001b[35mSection_title: 4.3  Dialogue Generation\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Retrieval\n",
            "\u001b[35mSection_title: 5.2  System State Tracking\n",
            "\u001b[35mSection_title: 5.3  Dialogue Generation\n",
            "\u001b[35mSection_title: 5.4  Human Evaluation\n",
            "\u001b[32mconclusion: in this paper, we presented a novel dataset, hy-bridialogue, for information-seeking dialogue where knowledge is grounded in both tables and text. while previous work has combined table and text modality in the question-answering space, this has not been utilized in the dialogue setting. our results in the various tasks demonstrate that there is still significant room for improvement and illustrate the need to build models that can adapt well to this hybrid format. in addition to the baseline tasks, future research can utilize hybridialogue to explore automatic multihop question decomposition.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  LLM vs. Traditional Agents\n",
            "\u001b[35mSection_title: 3.  Components of AI Agents\n",
            "\u001b[35mSection_title: 3.1.  Overview\n",
            "\u001b[35mSection_title: 3.2.  Planning\n",
            "\u001b[35mSection_title: 3.2.1.  Task Decomposition\n",
            "\u001b[35mSection_title: 3.2.2.  Self-Reflection\n",
            "\u001b[35mSection_title: 3.3.  Memory\n",
            "\u001b[35mSection_title: 3.4.  Tool Use\n",
            "\u001b[35mSection_title: 4.  Application\n",
            "\u001b[35mSection_title: 4.1.  Chatbot\n",
            "\u001b[35mSection_title: 4.2.  Game\n",
            "\u001b[35mSection_title: 4.3.  Coding\n",
            "\u001b[35mSection_title: 4.4.  Design\n",
            "\u001b[35mSection_title: 4.5.  Research\n",
            "\u001b[35mSection_title: 4.6.  Collaboration\n",
            "\u001b[35mSection_title: 4.7.  General purpose\n",
            "\u001b[35mSection_title: 4.8.  Vision-Language model-based agent application\n",
            "\u001b[35mSection_title: 5.  Benchmarking\n",
            "bottom\n",
            "\u001b[32mconclusion: in this paper, we presented a comprehensive and systematic survey of the llm-based agents.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Unanswerable Questions\n",
            "\u001b[35mSection_title: 2.2  Robustness of MRC Models\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Models\n",
            "\u001b[35mSection_title: 4  Automatically Creating Unanswerable Questions\n",
            "\u001b[35mSection_title: 4.1  Criteria\n",
            "\u001b[35mSection_title: 4.2  AGent Pipeline\n",
            "\u001b[35mSection_title: 4.3  Human Reviewing\n",
            "\u001b[35mSection_title: 5  Experiments and Analysis\n",
            "\u001b[35mSection_title: 5.1  Training Sets\n",
            "\u001b[35mSection_title: 5.2  Testing Sets\n",
            "\u001b[35mSection_title: 5.3  Main Results\n",
            "\u001b[35mSection_title: 5.4  Analysis on Natural Questions\n",
            "\u001b[35mSection_title: 6  Robustness against Syntactic Variations\n",
            "\u001b[35mSection_title: 6.1  TextBugger\n",
            "\u001b[35mSection_title: 6.2  Robustness against TextBugger\n",
            "\u001b[32mconclusion: in this work, we propose agent, a novel pipeline designed to automatically generate two sets of unanswerable questions from a dataset of answerable questions. we systematically apply agent on squad and hotpotqa to generate unanswerable questions. through a two-stage process of human reviewing, we demonstrate that agent unanswerable questions exhibit a low error rate.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Works\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Setting\n",
            "\u001b[35mSection_title: 3.1  Data Usage\n",
            "\u001b[35mSection_title: 3.2  Training Procedure\n",
            "\u001b[35mSection_title: 3.3  Evaluation Protocol\n",
            "\u001b[35mSection_title: 4  Task-level MoE Transformers\n",
            "\u001b[35mSection_title: 5  Applying Task-level MoE Models to Multi-task Learning\n",
            "\u001b[35mSection_title: 5.1  Experiment Details\n",
            "\u001b[35mSection_title: 5.2  Investigation on Baselines\n",
            "\u001b[35mSection_title: 5.3  Investigation on Design Choices\n",
            "\u001b[35mSection_title: 5.4  Results and Findings\n",
            "\u001b[35mSection_title: 6  Generalizing to Unseen Tasks\n",
            "\u001b[35mSection_title: 6.1  Few-shot Adaptation\n",
            "\u001b[35mSection_title: 6.2  Zero-shot Generalization\n",
            "\u001b[35mSection_title: 7.1  Learning Dynamics of the Routes\n",
            "\u001b[35mSection_title: 7.2  Correlation with Task Features\n",
            "\u001b[35mSection_title: 7.3  Expert Disabling Experiments\n",
            "\u001b[32mconclusion: inspired by how humans accumulate skills from past experience and re-use them to solve new tasks, in this paper, we develop and conduct extensive experiments with transformer-based task-level mixture-of-expert (moe) models, in hope to provide new insights on multi-task learning and crosstask generalization in nlp. firstly, we empirically investigate importance design choices and quantify their influence on final model. secondly, in both few-shot and zero-shot settings, we demonstrate that task-level mixture-of-expert models are better at generalizing to new tasks. finally, by conducting a detailed analysis on the routing decisions, we find they have strong correlations with human-defined task characteristics, even when the decisions are learned spontaneously without no prior knowledge such as pre-computed task representations. we hope our work provide useful advice on training and interpreting multi-task models in nlp and we hope it will inspire future work in improving multitask learning and cross-task generalization in nlp.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Protein Interfaces, Surfaces and Assemblies\n",
            "\u001b[35mSection_title: 2.2.  Molecular Dynamics\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\u001b[35mSection_title: 3.1.  The Interaction Surfaces of the Rhodocytin-CLEC-2 Interaction\n",
            "\u001b[35mSection_title: 3.2.  Dynamic Flexibility of the Rhodocytin-CLEC-2 Complexes\n",
            "\u001b[32mconclusion: we have previously solved the crystal structures of both rhodocytin and clec-2, but there is no crystallographic structure of the rhodocytin-clec-2 complex. using three dimensional structures of both rhodocytin and clec-2, we have generated models of the likely modes of interaction of the venom protein and its receptor on platelets and have investigated and analysed the computational models of the interaction which we have generated [30,32,33]. using a set of analytical algorithms and approaches, we have assessed the properties of the interfacing surfaces and the contribution made to the interaction by specific intermolecular contacts, including salt bridges and hydrogen bonds. in addition to this, we have evaluated the potential flexibility of these model complexes. the model wherein two molecules of clec-2 associate with tetrameric rhodocytin provides a more plausible model in terms of the composite effects related to the number of interfacing residues, the nature of their interactions and the predicted solvation energy effects. further, it is of potential significance that the predicted dynamic motions of this complex are suggestive of a mechanism whereby this interaction might cluster the receptors on the platelet surface, which could have implications for signaling. overall, the work presented indicates that a plausible mode of binding is that of one non-disulfide linked (αβ) 2 tetramer of rhodocytin with two dimers of clec-2. this analysis will be of value in the development of further studies to characterise the interaction further with a view to developing therapeutic approaches to disrupt the rhodocytin-clec-2 interaction on the platelet surface.\n",
            "\u001b[35mSection_title: 4.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  LONG-RANGE ARENA (LRA)\n",
            "\u001b[35mSection_title: 2.1  DESIDERATA\n",
            "\u001b[35mSection_title: 3.  Challenging:\n",
            "\u001b[35mSection_title: 4.  Long inputs:\n",
            "\u001b[35mSection_title: 2.2  TASKS\n",
            "\u001b[35mSection_title: 2.2.1  LONG LISTOPS\n",
            "\u001b[35mSection_title: 2.2.2  BYTE-LEVEL TEXT CLASSIFICATION\n",
            "\u001b[35mSection_title: 2.2.3  BYTE-LEVEL DOCUMENT RETRIEVAL\n",
            "\u001b[35mSection_title: 2.2.4  IMAGE CLASSIFICATION ON SEQUENCES OF PIXELS\n",
            "\u001b[35mSection_title: 2.2.5  PATHFINDER (LONG-RANGE SPATIAL DEPENDENCY)\n",
            "\u001b[35mSection_title: 2.2.6  PATHFINDER-X (LONG-RANGE SPATIAL DEPENDENCIES WITH EXTREME LENGTHS)\n",
            "\u001b[35mSection_title: 2.3  REQUIRED ATTENTION SPAN OF LRA TASKS\n",
            "\u001b[35mSection_title: 3.1  MODELS\n",
            "\u001b[35mSection_title: 3.2  PHILOSOPHY BEHIND THE BENCHMARK\n",
            "\u001b[35mSection_title: 3.3  QUANTITATIVE RESULTS\n",
            "\u001b[35mSection_title: 3.4  EFFICIENCY BENCHMARKS\n",
            "\u001b[35mSection_title: 3.5  OVERALL RESULTS: NO ONE-SIZE-FITS-ALL\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[35mSection_title: 4.1  EFFICIENT TRANSFORMERS\n",
            "\u001b[35mSection_title: 4.2  EXISTING BENCHMARKS\n",
            "\u001b[32mconclusion: we proposed long range arena (lra), a new benchmark for evaluating progress on efficient transformer research. our new benchmark is challenging and probes at model capabilities in dealing with diverse data types and structures such as text, mathematics, and visual data. our benchmark comprises of tasks ranging from 1k to 16k tokens. for the first time, we conduct an extensive side-by-side comparison of ten recently proposed efficient transformer models. the experimental results show that these tasks are very challenging even for long-range transformer models. the overall results show that there is no one-size-fits-all solution and trade-offs have to be made in terms of model quality and speed/memory. we plan to open source our code and benchmarks to facilitate future benchmarking, research and model development.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  LONG-RANGE ARENA (LRA)\n",
            "\u001b[35mSection_title: 2.1  DESIDERATA\n",
            "\u001b[35mSection_title: 3.  Challenging:\n",
            "\u001b[35mSection_title: 4.  Long inputs:\n",
            "\u001b[35mSection_title: 2.2  TASKS\n",
            "\u001b[35mSection_title: 2.2.1  LONG LISTOPS\n",
            "\u001b[35mSection_title: 2.2.2  BYTE-LEVEL TEXT CLASSIFICATION\n",
            "\u001b[35mSection_title: 2.2.3  BYTE-LEVEL DOCUMENT RETRIEVAL\n",
            "\u001b[35mSection_title: 2.2.4  IMAGE CLASSIFICATION ON SEQUENCES OF PIXELS\n",
            "\u001b[35mSection_title: 2.2.5  PATHFINDER (LONG-RANGE SPATIAL DEPENDENCY)\n",
            "\u001b[35mSection_title: 2.2.6  PATHFINDER-X (LONG-RANGE SPATIAL DEPENDENCIES WITH EXTREME LENGTHS)\n",
            "\u001b[35mSection_title: 2.3  REQUIRED ATTENTION SPAN OF LRA TASKS\n",
            "\u001b[35mSection_title: 3.1  MODELS\n",
            "\u001b[35mSection_title: 3.2  PHILOSOPHY BEHIND THE BENCHMARK\n",
            "\u001b[35mSection_title: 3.3  QUANTITATIVE RESULTS\n",
            "\u001b[35mSection_title: 3.4  EFFICIENCY BENCHMARKS\n",
            "\u001b[35mSection_title: 3.5  OVERALL RESULTS: NO ONE-SIZE-FITS-ALL\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[35mSection_title: 4.1  EFFICIENT TRANSFORMERS\n",
            "\u001b[35mSection_title: 4.2  EXISTING BENCHMARKS\n",
            "\u001b[32mconclusion: we proposed long range arena (lra), a new benchmark for evaluating progress on efficient transformer research. our new benchmark is challenging and probes at model capabilities in dealing with diverse data types and structures such as text, mathematics, and visual data. our benchmark comprises of tasks ranging from 1k to 16k tokens. for the first time, we conduct an extensive side-by-side comparison of ten recently proposed efficient transformer models. the experimental results show that these tasks are very challenging even for long-range transformer models. the overall results show that there is no one-size-fits-all solution and trade-offs have to be made in terms of model quality and speed/memory. we plan to open source our code and benchmarks to facilitate future benchmarking, research and model development.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Background\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Reagents.\n",
            "\u001b[35mSection_title: 2.2.  Cell Culture.\n",
            "\u001b[35mSection_title: 2.3.  Immunofluorescence Microscopy.\n",
            "\u001b[35mSection_title: 2.4.  Electrophoretic Mobility Shift Assay (EMSA).\n",
            "\u001b[35mSection_title: 2.5.  Reverse Transcription-Real-Time Polymerase Chain\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Endoderm Lineage Differentiation of Wildtype and GATA Deficient Embryonic Stem Cells Induced by Aggregation and/or\n",
            "\u001b[35mSection_title: 3.2.  Requirement of GATA5 in Aggregation-Induced Expression and Formation of Basement Membrane in Embryoid Bodies.\n",
            "\u001b[35mSection_title: 3.3.  Binding of GATA Factors to the Dab2 Promoter.\n",
            "\u001b[35mSection_title: 3.4.  Comparison of Gene Expression Profiles in Endoderm Differentiation of Wildtype and GATA Deficient ES Cells in\n",
            "\u001b[35mSection_title: 3.5.  Verification of a Panel of Markers Identified from CDNA\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[32mconclusion: we found that the deletion of one gata factor, either gata4, gata5, or gata6, can drastically alter the gene expression profiles and lineage determination of es cells induced to differentiate by retinoic acid. es cells lacking a single gata factor, either gata4, gata5, or gata6, exhibit a unique pattern of gene expression profile when differentiated. deletion of gata6 terminates the differentiation of es cells to endoderm but leads to mesoderm lineage differentiation. normally, during in vitro differentiation, the majority of es cells differentiate into primitive endoderm cells [25]. thus, the deletion of gata6 allows the selection of lineage other than yolk sac endoderm. this study demonstrates a potential approach in redirecting the lineage determination of es cells in vitro by altering the expression of gata factors.\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Tasks\n",
            "\u001b[35mSection_title: 3.2  Instruction-following Models\n",
            "\u001b[35mSection_title: 4  Correctness w.r.t Information Need\n",
            "\u001b[35mSection_title: 4.1  Retrieval\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[35mSection_title: 4.4  Automatic Correctness Evaluation\n",
            "\u001b[35mSection_title: 5  Faithfulness w.r.t Provided Knowledge\n",
            "\u001b[35mSection_title: 5.1  Faithfulness Metrics\n",
            "\u001b[35mSection_title: 5.2  Faithfulness w.r.t Relevant Knowledge\n",
            "\u001b[35mSection_title: 5.3  Faithfulness w.r.t Irrelevant Knowledge\n",
            "\u001b[32mlimitation: below, we highlight several key findings of this paper and discuss some of its limitations.which evaluation metrics are best? our analysis on correctness ( §4) and faithfulness ( §5) demonstrates that widely-used metrics are not suitable for evaluating the correctness (due to errors such as elaborate answers, open-ended questions, and list of named-entities) and faithfulness (due to partially grounded responses). correlating the metrics with human judgements (table 2 and table 5) reveals that recall and gpt4-eval are the best lexical and model-based metrics for correctness and k-precision and llmcritic (gpt-4) are the best lexical and model-based metrics for faithfulness, respectively. however, these model-based metrics, especially the ones based on llms, are usually slow to run, expensive, difficult to reproduce, and may exhibit systematic biases. while we propose that recall and k-precision are the most widely-accessible and human-aligned metrics for correctness and faithfulness, respectively, we emphasize that these simple lexical-based metrics are easy to hack. one model can copy all the retrieved knowledge as the output, leading to high recall and k-precision metrics. however, such a model will be penalized heavily when evaluated for faithfulness w.r.t. irrelevant knowledge.instruction-following models according to the most human aligned and easy to use metrics (i.e., recall and k-precision), we conclude that gpt-3.5 outperforms other models on majority of the datasets in correctness w.r.t information need. however, when analyzing the faithfulness w.r.t relevant knowledge, flan-t5 is shown to be the best model in all three datasets. moreover, our further analysis on the models' faithfulness w.r.t irrelevant knowledge demonstrates that models struggle to correctly identify whether the provided knowledge is relevant or not.limitations it is worth mentioning that the experiments for evaluating the faithfulness of the models are conducted in a modified setting, where a relevant or irrelevant passage is provided in the prompt on purpose. this is different from the real-world scenario, where the retrieved passages can contain a mix of relevant and irrelevant knowledge.finally, it should also be noted that beyond qualitative investigation, we did not explore a wide range of prompts for the tasks studied in this work. recent work has shown that the performance of instruction-following models can vary greatly depending upon the provided prompt (zhao et al., 2021;liu et al., 2023b). we leave it to future works to investigate better prompts for instructionfollowing models in a retrieval-augmented setting.\n",
            "\u001b[35mSection_title: 6  Discussion and Limitations\n",
            "\u001b[32mconclusion: we extensively study the capability of instructionfollowing models to correctly and faithfully respond to questions in three qa settings (natural, multi-hop, and conversational). first, we uncover various issues with using traditional metrics, like f1 score, to evaluate the correctness of models. through correlation with human judgement, we find that llm-based metrics (e.g. gpt-4) and token-level recall are promising metrics for evaluating the correctness w.r.t information need. moreover, our further faithfulness analysis shows that llm-based metrics like llmcritic (gpt-4) and lexical-based k-precision are more aligned with human judgements in evaluating the faithfulness of the models given the relevant knowledge.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 3  FACET 2: SOCIAL BIAS AND FAIRNESS\n",
            "\u001b[35mSection_title: 3.1  THE CASE OF GENDER BIAS: WINOBIAS\n",
            "\u001b[35mSection_title: 4.1  EVALUATION SETUP\n",
            "\u001b[35mSection_title: 4.2  RESULTS\n",
            "\u001b[35mSection_title: 5  FACET 4: FACTUALITY VIA KNOWLEDGE UPDATING\n",
            "\u001b[35mSection_title: 5.1  MEMORIZATION VS UPDATING\n",
            "\u001b[35mSection_title: 5.2  RETRIEVAL-AUGMENTED OPEN-DOMAIN QA\n",
            "\u001b[35mSection_title: 5.3  REASONING-AUGMENTED MULTI-HOP QA\n",
            "\u001b[32mconclusion: our work systematically studies the reliability of gpt-3 from four key facets: generalizability, fairness, calibration, and factuality. we develop effective prompting strategies to make gpt-3 outperform supervised models by large margins on these facets. our work reveals new insights of llms and provides practical recommendations for users of gpt-3. we hope our work can inspire more future work to: (1) examine more facets of reliability, such as avoiding harmful generations; (2) apply the prompting methods in this paper to more real-world applications, such as incorporating human feedback for collaborative multi-step planning; (3) further explore more effective prompting strategies to improve reliability, such as post-hoc calibration on language model probabilities.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  PMR: A Pre-trained Machine Reader\n",
            "\u001b[35mSection_title: 2.1  Pre-training of PMR\n",
            "\u001b[35mSection_title: 2.2  Fine-tuning PMR on Extraction Tasks\n",
            "\u001b[35mSection_title: 3.1  Wikipedia Data Pre-processing\n",
            "\u001b[35mSection_title: 3.2  Downstream Extraction Tasks\n",
            "\u001b[35mSection_title: 3.3  Baselines and Implementation Details\n",
            "\u001b[35mSection_title: 4  Main Results\n",
            "\u001b[35mSection_title: 5  Further Discussions\n",
            "\u001b[35mSection_title: 6  Explainable Sequence Classification\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: this paper presents a novel mrc-sytle pre-training model, namely pmr. pmr can fully resolve the learning objective and data format gaps that frequently appear in fine-tuning existing plms. ex-perimental results over multiple dimensions, including effectiveness in solving few-shot tasks, outof-domain generalization, demonstrate its strong benefits in bridge the pretrain-finetune gaps. pmr also shows encouraging potential in explaining the sequence classification process in an extractiondriven mrc framework.  (trischler et al., 2017), searchqa (dunn et al., 2017, hot-potqa (yang et al., 2018), bioasq (tsatsaronis et al., 2015), drop (dua et al., 2019), duorc (saha et al., 2018), race (lai et al., 2017), re-lationextraction (levy et al., 2017), textbookqa (kembhavi et al., 2017. eqa has always been treated as an mrc problem, where the question serves as the mrc query, and the passage containing the answers serves as the mrc context. for ner, we follow li et al. (2020) to formulate ner into the mrc paradigm, where the entity label together with its description serves as the mrc query, and the input text serves as the mrc context. the goal is to extract the corresponding entities as answers. we use the eq. 4 as the learning objective, where y ext i,j indicates that the input span x i:j is an answer/entity.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Notation and problem statement\n",
            "\u001b[35mSection_title: 3  The CRUSH4SQL Approach\n",
            "\u001b[35mSection_title: 3.1  LLM-based Query Transformation\n",
            "\u001b[35mSection_title: 3.2  Collective Retrieval\n",
            "\u001b[35mSection_title: 3.2.2  Retrieval objective\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Methods Compared\n",
            "\u001b[35mSection_title: 5.3  Overall Comparison\n",
            "\u001b[35mSection_title: 5.4  Robustness Across Prompt Variations\n",
            "\u001b[35mSection_title: 5.5  Ablation on Collective Retriever\n",
            "\u001b[32mconclusion: while llms incorporate vast world knowledge and corpus statistics, they may be unfamiliar with (possibly private) client db schemas, which can be very large, rendering impractical or expensive any attempt to upload the full schema in-context along with questions for text-to-sql applications.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.3  Attacks Transfer Out-of-Domain\n",
            "\u001b[35mSection_title: 4.4  Attacks on Multi-Vector Retrievers\n",
            "\u001b[35mSection_title: 4.5  Targeted Attack with Misinformation\n",
            "\u001b[35mSection_title: 5  Additional Analysis and Discussion\n",
            "\u001b[32mconclusion: we proposed a new attack for dense retrievers, in which adversarial passages are inserted into the corpus to mislead their retrieval outputs.we show that even a small number of adversarial passages can successfully attack state-of-the-art dense retrievers and generalize to queries from unseen domains.these findings have important implications for the future deployment of robust retrieval systems in real-world applications.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Data Collection and Statistics\n",
            "\u001b[35mSection_title: 3.2  Reasoning Types of the Dataset\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[35mSection_title: 6.1  Ablation Tests\n",
            "\u001b[35mSection_title: 6.2  Transfer Learning\n",
            "\u001b[35mSection_title: 6.3  Performance Across Different Lengths\n",
            "\u001b[35mSection_title: 6.4  Lexical Overlap\n",
            "\u001b[35mSection_title: 6.5  Reasoning Types\n",
            "\u001b[32mconclusion: we have presented logiqa, a large-scale logical reasoning reading comprehension dataset. in addition to testing reasoning capacities of machine reading, our dataset can also serve as a benchmark for re-examining the long pursued research of logical ai in the deep learning nlp era. results show that the state-of-the-art machine readers still fall far behind human performance, making our dataset one of the most challenging test for reading comprehension.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Multi-domain ReQA (MultiReQA)\n",
            "\u001b[35mSection_title: 4  Baseline Models\n",
            "\u001b[35mSection_title: 4.1  BERT\n",
            "\u001b[35mSection_title: 4.2  Universal Sentence Encoder QA\n",
            "\u001b[35mSection_title: 4.3  BM25\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Fine-tuning and Configurations\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Transfer Learning across Domains\n",
            "\u001b[35mSection_title: 6.2  Error Analysis\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: in this paper, we convert eight existing qa tasks from the mrqa shared task (fisch et al., 2019b) into sentence-level retrieval tasks, by treating the sentence containing the ground-truth span as the target sentence-level answer. in additional to a new evaluation suite for sentence level retrieval, we provide strong baselines using unsupervised term-based information retrieval methods (bm25), and three neural models, off-the-self use-qa, finetuned use-qa, and bert dual encoders.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Multi-bias Debiasing Framework\n",
            "\u001b[35mSection_title: 3.1  Multi-domain Knowledge Distillation\n",
            "\u001b[35mSection_title: 3.2  Bias Models\n",
            "\u001b[35mSection_title: 3.3  Multi-Bias Weighting Function\n",
            "\u001b[35mSection_title: 3.4  Bias-Aware Loss Function\n",
            "\u001b[35mSection_title: 4.2  Examined Biases and Bias Models\n",
            "\u001b[35mSection_title: 4.3  Data\n",
            "\u001b[35mSection_title: 4.4  Evaluation Settings\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Strength of biases on different datasets\n",
            "\u001b[35mSection_title: 5.2  Impact of debiasing on SD training\n",
            "\u001b[35mSection_title: 5.3  Impact of debiasing on MD training\n",
            "\u001b[35mSection_title: 6  Discussion and Analysis\n",
            "\u001b[35mSection_title: 6.1  Why our debiasing improves in-domain and out-of-domain performances?\n",
            "\u001b[35mSection_title: 6.2  Impact of the Framework Components\n",
            "\u001b[35mSection_title: 6.3  Is debiasing always beneficial?\n",
            "\u001b[32mconclusion: in this paper we (1) investigate the impact of debiasing methods on qa model generalization for both single and multi-domain training scenarios, and (2) propose a new framework for improving the in-domain and out-of-domain performances by concurrent modeling of multiple biases. our framework weights each training example according to multiple biases and based on the strength of each bias in the training data. it uses the resulting bias weights in the training objective to prevent the model from mainly focusing on learning biases. we evaluate our framework using two different training objectives, i.e., multi-bias confidence regularization and multi-bias loss re-weighting, and show its effectiveness in both single and multidomain training scenarios. we further compare our framework with two state-of-the-art debiasing methods of utama et al. (2020) and mahabadi et al. (2020). we show that knowledge distillation, modeling multiple biases at once, and weighting the impact of each bias based on its strength in the training data are all important factors in improving the in-domain and out-of-domain performances. while recent literature on debiasing in nlp focuses on improving the performance on adversarial evaluation sets, this work opens new research directions on wider uses of debiasing methods. the main advantage of using our debiasing methods is that they improve the performance and generalization without requiring additional training data or larger models. future work could build upon our framework by applying it to a wide range of tasks beyond qa using task-specific bias models. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Voice-Controlled Document Narration\n",
            "\u001b[35mSection_title: 2.2  Interactions with Productivity Software\n",
            "\u001b[35mSection_title: 2.3  Question Answering\n",
            "\u001b[35mSection_title: 3  STEP 1 -SURVEY\n",
            "\u001b[35mSection_title: 3.1  Survey Overview\n",
            "\u001b[35mSection_title: 3.2  Participants\n",
            "\u001b[35mSection_title: 3.3  Document Selection\n",
            "\u001b[35mSection_title: 3.4  Survey Results\n",
            "\u001b[35mSection_title: 3.4.1  Part 1 -Survey Questions.\n",
            "\u001b[35mSection_title: 3.4.2  Part 2 -Collecting Questions.\n",
            "\u001b[35mSection_title: 3.5  Classifying Question Types\n",
            "\u001b[35mSection_title: 3.6  Answering RQ1 and RQ2\n",
            "\u001b[35mSection_title: 4  STEP 2 -DATA COLLECTION\n",
            "\u001b[35mSection_title: 4.1  Question Collection\n",
            "\u001b[35mSection_title: 4.2  Answer Collection\n",
            "\u001b[35mSection_title: 4.3  Dataset Statistics\n",
            "\u001b[35mSection_title: 5  STEP 3 -BASELINE MODELING\n",
            "\u001b[35mSection_title: 5.1  Data Preprocessing\n",
            "\u001b[35mSection_title: 5.2  Passage Ranking\n",
            "\u001b[35mSection_title: 5.2.1  Passage Construction.\n",
            "\u001b[35mSection_title: 5.2.2  Baseline\n",
            "\u001b[35mSection_title: 5.2.3  Baseline Passage Ranking 2 -First passage.\n",
            "\u001b[35mSection_title: 5.2.4  Baseline Passage Ranking 3 -BM25.\n",
            "\u001b[35mSection_title: 5.3  Results for Passage Ranking\n",
            "\u001b[35mSection_title: 5.4  Answer Selection\n",
            "\u001b[35mSection_title: 5.4.1  Passage Construction\n",
            "\u001b[35mSection_title: 5.4.2  Baseline Answer\n",
            "\u001b[35mSection_title: 5.4.5  Baseline\n",
            "\u001b[35mSection_title: 5.4.6  Baseline\n",
            "\u001b[35mSection_title: 5.4.7  Baseline\n",
            "\u001b[35mSection_title: 5.5  Results for Answer Selection\n",
            "\u001b[35mSection_title: 5.6  Answering RQ3\n",
            "\u001b[32mconclusion: in this paper, we explored the novel domain of document-centered digital assistance. we focused on a consumption scenario, in which individuals are a (co-)owner of a document. through a survey, we identified a set of primary capabilities people expect from a digital assistant in a document-centered scenario, as well as a large set of questions that gave us insight into the types of queries that people might pose about a document when they have an approximate or good idea what the document is about. our explorations shed light on the hierarchy of questions that might be posed, and demonstrate that the types of questions people ask in a document-centered scenario are different from the factoid questions in conventional qa datasets. we show that state-of-the-art qa models can be finetuned to perform with reasonable accuracy on the new dqa data. yet, it has proven to be an unsolved task, which makes this a fertile area for future work. this research opens a new direction for digital assistance. avenues for future work include deeper explorations of query rewriting to better tailor document-centered questions to conventional qa systems, and also exploring ways to scale up the data to a much larger and broader range of documents.  \n",
            "\u001b[35mSection_title: 6  CONCLUSIONS AND FUTURE WORK\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Voice-Controlled Document Narration\n",
            "\u001b[35mSection_title: 2.2  Interactions with Productivity Software\n",
            "\u001b[35mSection_title: 2.3  Question Answering\n",
            "\u001b[35mSection_title: 3  STEP 1 -SURVEY\n",
            "\u001b[35mSection_title: 3.1  Survey Overview\n",
            "\u001b[35mSection_title: 3.2  Participants\n",
            "\u001b[35mSection_title: 3.3  Document Selection\n",
            "\u001b[35mSection_title: 3.4  Survey Results\n",
            "\u001b[35mSection_title: 3.4.1  Part 1 -Survey Questions.\n",
            "\u001b[35mSection_title: 3.4.2  Part 2 -Collecting Questions.\n",
            "\u001b[35mSection_title: 3.5  Classifying Question Types\n",
            "\u001b[35mSection_title: 3.6  Answering RQ1 and RQ2\n",
            "\u001b[35mSection_title: 4  STEP 2 -DATA COLLECTION\n",
            "\u001b[35mSection_title: 4.1  Question Collection\n",
            "\u001b[35mSection_title: 4.2  Answer Collection\n",
            "\u001b[35mSection_title: 4.3  Dataset Statistics\n",
            "\u001b[35mSection_title: 5  STEP 3 -BASELINE MODELING\n",
            "\u001b[35mSection_title: 5.1  Data Preprocessing\n",
            "\u001b[35mSection_title: 5.2  Passage Ranking\n",
            "\u001b[35mSection_title: 5.2.1  Passage Construction.\n",
            "\u001b[35mSection_title: 5.2.2  Baseline\n",
            "\u001b[35mSection_title: 5.2.3  Baseline Passage Ranking 2 -First passage.\n",
            "\u001b[35mSection_title: 5.2.4  Baseline Passage Ranking 3 -BM25.\n",
            "\u001b[35mSection_title: 5.3  Results for Passage Ranking\n",
            "\u001b[35mSection_title: 5.4  Answer Selection\n",
            "\u001b[35mSection_title: 5.4.1  Passage Construction\n",
            "\u001b[35mSection_title: 5.4.2  Baseline Answer\n",
            "\u001b[35mSection_title: 5.4.5  Baseline\n",
            "\u001b[35mSection_title: 5.4.6  Baseline\n",
            "\u001b[35mSection_title: 5.4.7  Baseline\n",
            "\u001b[35mSection_title: 5.5  Results for Answer Selection\n",
            "\u001b[35mSection_title: 5.6  Answering RQ3\n",
            "\u001b[32mconclusion: in this paper, we explored the novel domain of document-centered digital assistance. we focused on a consumption scenario, in which individuals are a (co-)owner of a document. through a survey, we identified a set of primary capabilities people expect from a digital assistant in a document-centered scenario, as well as a large set of questions that gave us insight into the types of queries that people might pose about a document when they have an approximate or good idea what the document is about. our explorations shed light on the hierarchy of questions that might be posed, and demonstrate that the types of questions people ask in a document-centered scenario are different from the factoid questions in conventional qa datasets. we show that state-of-the-art qa models can be finetuned to perform with reasonable accuracy on the new dqa data. yet, it has proven to be an unsolved task, which makes this a fertile area for future work. this research opens a new direction for digital assistance. avenues for future work include deeper explorations of query rewriting to better tailor document-centered questions to conventional qa systems, and also exploring ways to scale up the data to a much larger and broader range of documents.  \n",
            "\u001b[35mSection_title: 6  CONCLUSIONS AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  SIMULTQA Framework\n",
            "\u001b[35mSection_title: 3.1  Reasoning Path Construction\n",
            "\u001b[35mSection_title: 3.2  Reranking and Answer Generation\n",
            "\u001b[35mSection_title: 3.3  Training and Inference\n",
            "\u001b[35mSection_title: 4  Knowledge Transfer Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  RQ1: Quantitative measurement\n",
            "\u001b[35mSection_title: 4.3  RQ2: What has been transferred?\n",
            "\u001b[35mSection_title: 4.4  RQ3: Answering complex questions by both knowledge sources\n",
            "\u001b[35mSection_title: 1.  Green Hornet\n",
            "\u001b[35mSection_title: 5  Discussion for future directions\n",
            "\u001b[32mconclusion: in this paper, we study cqa over structured and unstructured knowledge sources (i.e., kb and text particularly), and focus on studying the knowledge transfer between different knowledge sources. to facilitate the transfer, we first propose a unified cqa framework, simultqa to bridge kbqa and textqa systems. empirical results show that knowledge transfer enables substantial improvements on low-resource domains. more importantly, we conduct fine-grained analyses to shed more light on how knowledge is transferred to inspire future research on knowledge transfer between sources, and we conclude the paper with insights for future cqa datasets and systems.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary\n",
            "\u001b[35mSection_title: 2.1  Recommender System\n",
            "\u001b[35mSection_title: 2.2  Dialog System\n",
            "\u001b[35mSection_title: 2.3  End-to-End System\n",
            "\u001b[35mSection_title: 3  Proposed Model\n",
            "\u001b[35mSection_title: 3.1  Dialog-Aware Recommendation with Knowledge\n",
            "\u001b[35mSection_title: 3.2  Recommendation-Aware Dialog\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Setting\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Baselines\n",
            "\u001b[35mSection_title: 4.5  Results\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Does dialog help recommendation?\n",
            "\u001b[35mSection_title: 5.2  Does recommendation help dialog?\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we propose a novel end-to-end framework, kbrd, which bridges the gap between the recommender system and the dialog system via knowledge propagation. through a series of experiments, we show that kbrd can reach better performances in both recommendation and dialog generation in comparison with the baselines. we also discuss how the two systems benefit each other. dialog information is effective for the recommender system especially in the setting of cold start, and the introduction of knowledge can strengthen the recommendation performance significantly. information from the recommender system that contains the user preference and the relevant knowledge can enhance the consistency and diversity of the generated dialogs.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK 2.1 Large Language Models\n",
            "\u001b[35mSection_title: 2.2  Data Augmentation for Dense Retrieval\n",
            "\u001b[35mSection_title: 2.3  Document Summarization\n",
            "\u001b[35mSection_title: 2.4  Alignment Techniques\n",
            "\u001b[35mSection_title: 3  METHOD\n",
            "\u001b[35mSection_title: 3.1  Contrastive Data Construction\n",
            "\u001b[35mSection_title: 3.2  RLCF Optimization\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP 4.1 Tasks and Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Evaluation\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 5.1  Data Augmentation for Dense Retrieval\n",
            "\u001b[35mSection_title: 5.2  Document Summarization\n",
            "\u001b[35mSection_title: 5.3  Case Study\n",
            "\u001b[32mconclusion: in this work, we propose a novel framework that leverages contrastive feedback to optimize large language models through reinforcement learning, namely rlcf. the capabilities of llm could be aligned with the context of information retrieval through the proposed rlcf. specifically, we first construct a group of similar documents by a dense retrieval model. subsequently, documents in the same group are fed into a llm to be optimized. the responses are obtained by the llm for these similar documents. the contrastive feedback is obtained from these responses generated by llm with respect to corresponding documents. the contrastive feedback is calculated by dense retrieval model. formally, we employ a novel reward function, batched-mrr, as the contrastive feedback, which is a variant of mrr. then, the contrastive feedback could be utilized to optimized llm through ppo algorithm, which is a widely used reinforcement learning method. we conduct experiments on two tasks of information retrieval, demonstrating the effectiveness of our proposed rlcf. the rlcf-optimized llm could generates specific queries for data augmentation, achieving promising performance on few-shot dense retrieval. besides, we introduce a brand-new setting of document summarization, which is under the context of information retrieval. to be specific, the summarizes should be specific to each document among similar documents, which is desired for users to filter out target document. to evaluate the effectiveness of summarization in the proposed setting, we introduce rouge-diff, a variant of rouge score, which is calculated in the group level. in future work, we suggest exploring more domains which could use the rlcf for optimization, such as style transfer, harmless alignment, helpfulness alignment and etc.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary\n",
            "\u001b[35mSection_title: 2.1  Natural Questions Dataset\n",
            "\u001b[35mSection_title: 2.2  Data Preprocessing\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Input & Output Definition\n",
            "\u001b[35mSection_title: 3.2  Multi-granularity Document 1 Modeling\n",
            "\u001b[35mSection_title: 3.3  Graph Encoder\n",
            "\u001b[35mSection_title: 3.3.1  Graph Attention Networks\n",
            "\u001b[35mSection_title: 3.3.2  Self-Attention Layer\n",
            "\u001b[35mSection_title: 3.3.3  Graph Integration Layer\n",
            "\u001b[35mSection_title: 3.3.4  Feed-Forward Layer\n",
            "\u001b[35mSection_title: 3.3.5  Relational Embedding\n",
            "\u001b[35mSection_title: 3.3.6  Graph Initialization\n",
            "\u001b[35mSection_title: 3.4  Output Layer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Data Preprocessing Details\n",
            "\u001b[35mSection_title: 4.2  Experimental Settings\n",
            "\u001b[35mSection_title: 4.3  Comparison\n",
            "\u001b[35mSection_title: 4.4  Ablation Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we present a novel multi-grained mrc framework based on graph attention networks and bert. we model documents at different levels of granularity to learn the hierarchical nature of the document. on the natural questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers. the experiments show that our proposed methods are effective and outperform the previously existing methods by a large margin. improving our graph structure of representing the document as well as the document-level pretraining tasks is our future research goals. besides, the currently existing methods actually cannot process a long document without truncating or slicing it into fragments. how to model long documents is still a problem that needs to be solved.  question: what 's the dog 's name on tom and jerry long answer: tom ( named \" jasper \" in his debut appearance ) is a grey and white domestic shorthair cat . \" tom \" is a generic name for a male cat . he is usually but not always , portrayed as living a comfortable , or even pampered life , while jerry ...\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Preliminaries of RetroMAE\n",
            "\u001b[35mSection_title: 3.2  Extension to DupMAE\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Main Results\n",
            "\u001b[35mSection_title: 4.2  Ablation Studies\n",
            "\u001b[32mconclusion: this paper presents dupmae, a new retrievaloriented pre-training method where the semantic representation capacities can be jointly enhanced for all contextualized embeddings of the language models. it leverages retromae's decoding task for [cls]'s embedding and introduces a bow-based decoding task for ot embeddings. the training losses from the two tasks are added up for a unified encoder. the two types of embeddings, after dimension reduction, are aggregated for the joint semantic representation. the effectiveness of our proposed method is empirically verified, as remarkable retrieval performances are achieved on ms marco and beir throughout different situations.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Preliminaries of RetroMAE\n",
            "\u001b[35mSection_title: 3.2  Extension to DupMAE\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Main Results\n",
            "\u001b[35mSection_title: 4.2  Ablation Studies\n",
            "\u001b[32mconclusion: this paper presents dupmae, a new retrievaloriented pre-training method where the semantic representation capacities can be jointly enhanced for all contextualized embeddings of the language models. it leverages retromae's decoding task for [cls]'s embedding and introduces a bow-based decoding task for ot embeddings. the training losses from the two tasks are added up for a unified encoder. the two types of embeddings, after dimension reduction, are aggregated for the joint semantic representation. the effectiveness of our proposed method is empirically verified, as remarkable retrieval performances are achieved on ms marco and beir throughout different situations.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The FORECASTQA Task\n",
            "\u001b[35mSection_title: 4  Dataset Construction and Analysis\n",
            "\u001b[35mSection_title: 4.1  Construction Details\n",
            "\u001b[35mSection_title: 4.2  Dataset Analysis\n",
            "\u001b[35mSection_title: 5  Methods\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Experimental Setup\n",
            "\u001b[35mSection_title: 6.2  Human Performance\n",
            "\u001b[35mSection_title: 6.3  Results and Performance Analysis\n",
            "\u001b[32mconclusion: forecasting is a difficult task that requires every possible advantage to do well. it would be wise to harness this pool of unstructured data for training automatic event forecasting agents. to utilize this form of data for forecasting, we proposed a question-answering task that requires forecasting skills to solve forecastqa, and provided the accompanying dataset. various baseline methods did not perform well, but this is not surprising given the inherent difficulty of forecasting. our benchmark dataset can benefit future research beyond natural language understanding and hope forecasting performance will be significantly improved.  \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Background\n",
            "\u001b[35mSection_title: 3.1  Architecture and setup\n",
            "\u001b[35mSection_title: 3.2  Objective functions\n",
            "\u001b[35mSection_title: 4.1  Span Selection\n",
            "\u001b[35mSection_title: 4.2  Extended Pre-training\n",
            "\u001b[35mSection_title: 5  MRC Tasks\n",
            "\u001b[35mSection_title: 5.1  SQuAD\n",
            "\u001b[35mSection_title: 5.2  Natural Questions\n",
            "\u001b[35mSection_title: 5.3  HotpotQA\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  SQuAD\n",
            "\u001b[35mSection_title: 6.2  Natural Questions\n",
            "\u001b[35mSection_title: 6.3  HotpotQA\n",
            "\u001b[35mSection_title: 6.4  Exploration of SSPT Instance Types\n",
            "\u001b[35mSection_title: 6.5  Comparison to Previous Work\n",
            "\u001b[32mconclusion: span selection pre-training is effective in improving reading comprehension across four diverse datasets, including both generated and natural questions, and with provided contexts of passages, documents and even passage sets. this style of pretraining focuses the model on finding semantic connections between two sequences, and supports a style of cloze that can train deep semantic understanding without demanding memorization of specific knowledge in the model. the span selection task is suitable for pre-training on any domain, since it makes no assumptions about document structure or availability of summary/article pairs. this allows pre-training of language understanding models in a very generalizable way.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work 2.1 Evidence for Transfer\n",
            "\u001b[35mSection_title: 2.2  Studies of Why Transfer Happens\n",
            "\u001b[35mSection_title: 2.3  Extreme Cross-Domain Transfer\n",
            "\u001b[35mSection_title: 3  Experimental Paradigm\n",
            "\u001b[35mSection_title: 3.1  Evaluation Pipeline\n",
            "\u001b[35mSection_title: 3.2  Scrambling with Similar Frequency\n",
            "\u001b[35mSection_title: 3.3  Random Scrambling\n",
            "\u001b[35mSection_title: 4  Models\n",
            "\u001b[35mSection_title: 5  Tasks\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Sequence Classification\n",
            "\u001b[35mSection_title: 6.2  Sequence Labeling\n",
            "\u001b[35mSection_title: 7  Analysis\n",
            "\u001b[35mSection_title: 7.1  Frequency Effects\n",
            "\u001b[35mSection_title: 7.2  Does Scrambling Preserve Meaning?\n",
            "\u001b[35mSection_title: 7.3  Transfer or Simple Retraining?\n",
            "\u001b[35mSection_title: 7.4  Assessing Transfer with Frozen BERT Parameters\n",
            "\u001b[32mconclusion: in this paper, we propose an evaluation pipeline for pretrained models by testing their transferability without word identity information. specifically, we take an english pretrained bert off-the-shelf and fine-tune it with a scrambled english dataset. we conduct analyses across six tasks covering both classification and sequence labeling. by evaluating performance against multiple baselines, we aim to assess where bert can transfer knowledge even without word identities. we find considerable transfer for bert as compared to even powerful baselines, by only for classification tasks.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  READTWICE\n",
            "\u001b[35mSection_title: 2.2  Pre-training\n",
            "\u001b[35mSection_title: 3  Related Work\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Pre-training setup\n",
            "\u001b[35mSection_title: 4.2  Evaluation setup\n",
            "\u001b[35mSection_title: 4.3  Main Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Analysis & Discussion\n",
            "\u001b[32mconclusion: readtwice performs well on several qa tasks, particularly narrativeqa where long-range dependencies among entities appear to be very important. the proposed method is conceptually simple, easy to implement and is capable of reading entire books. for future work, we plan to explore new memory types, hierarchies and aggregation functions. we also aim to apply the model to other tasks, particularly long text summarization, likely to benefit from a memory-forming mechanism.\n",
            "\u001b[35mSection_title: 5  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Dataset Creation\n",
            "\u001b[35mSection_title: 3.1.  Evaluation of Generated Questions\n",
            "\u001b[35mSection_title: 3.2.  Annotation Guidelines\n",
            "\u001b[35mSection_title: 3.3.  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.  Methods\n",
            "\u001b[35mSection_title: 5.  Experiments, Results and Discussions\n",
            "\u001b[35mSection_title: 5.1.  Error Analysis\n",
            "\u001b[35mSection_title: 5.2.  Observation and main message\n",
            "\u001b[35mSection_title: 6.  Robustness of QA model\n",
            "\u001b[32mconclusion: in this paper, we have presented covidread, a new dataset for benchmark evaluation of methods for qa/mrc on covid-19. the dataset is created in a semi-automatic way. the dataset consists of context-question-answer triples over 100k, out of which 40k are human-annotated. researchers and editors/associate editors would be benefited out of this kind of models and the dataset. we have proposed three approaches that are based on viz. (i.) vanilla bert (ii.) bio-medical bert (i.e., biobert) and (iii.) clinical bert. our proposed models are competitive compared to the existing state-of-the-art models. our future works would include:\n",
            "\u001b[35mSection_title: 7.  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Preliminaries\n",
            "\u001b[35mSection_title: 4  Counterfactual Multihop QA\n",
            "\u001b[35mSection_title: 4.1  Disentanglement of causal effect\n",
            "\u001b[35mSection_title: 4.2  Training and Inference\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Quantitative Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Study\n",
            "\u001b[32mconclusion: in this work, we proposed a novel counterfactual reasoning approach to reduce disconnected reasoning in multi-hop qa. we used the causal graph to explain the existing multi-hop qa approaches' behaviors, which consist of the shortcut impacts and reasoning impacts. the shortcut impacts capture the disconnected reasoning and they are formulated as the natural direct causal effects. then we constructed the counterfactual examples during the training phase to estimate the both natural direct effects of question and context on answer prediction as well as supporting facts identification. the reasoning impact represents the multi-hop reasoning and is estimated by introducing learnable parameters.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Preliminaries\n",
            "\u001b[35mSection_title: 4  Counterfactual Multihop QA\n",
            "\u001b[35mSection_title: 4.1  Disentanglement of causal effect\n",
            "\u001b[35mSection_title: 4.2  Training and Inference\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Quantitative Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Study\n",
            "\u001b[32mconclusion: in this work, we proposed a novel counterfactual reasoning approach to reduce disconnected reasoning in multi-hop qa. we used the causal graph to explain the existing multi-hop qa approaches' behaviors, which consist of the shortcut impacts and reasoning impacts. the shortcut impacts capture the disconnected reasoning and they are formulated as the natural direct causal effects. then we constructed the counterfactual examples during the training phase to estimate the both natural direct effects of question and context on answer prediction as well as supporting facts identification. the reasoning impact represents the multi-hop reasoning and is estimated by introducing learnable parameters.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  BAMBOO Benchmark\n",
            "\u001b[35mSection_title: 3.1  Design Principles\n",
            "\u001b[35mSection_title: 3.2  Data Collection\n",
            "\u001b[35mSection_title: 3.3  Task Construction\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Metrics\n",
            "\u001b[35mSection_title: 4.3  Overall Results\n",
            "\u001b[35mSection_title: 4.4  Key Questions of Long Text Modeling\n",
            "\u001b[35mSection_title: 4.5  Qualitative Analysis and Discussions\n",
            "\u001b[32mconclusion: we propose bamboo, a benchmark for comprehensively evaluating the long text modeling capabilities of llms.bamboo consists of five tasks with two length levels, enabling the evaluation of llms' main capacities across various dimensions and domains.based on the evaluation of several long context models on bamboo, we give an overall analysis of the performances of different models and tasks.we have discussed key questions of long text models, provide a qualitative analysis of long text modeling tasks, and suggest directions for improving long context modeling abilities.we believe bamboo can be employed to analyze the extensive capacities and advance the long text modeling proficiency of llms in the future.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background: Memory Bottleneck in Training BERT\n",
            "\u001b[35mSection_title: 2.1  Memory Profiling\n",
            "\u001b[35mSection_title: 2.2  A Regression Analysis on Activation Memory\n",
            "\u001b[35mSection_title: 2.3  Techniques for Reducing Traing Memory\n",
            "\u001b[35mSection_title: 3  Model: BlockBERT\n",
            "\u001b[35mSection_title: 3.1  Blockwise Multi-Head Attention\n",
            "\u001b[35mSection_title: 3.2  Analysis of Memory Usage Reduction\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Pre-training\n",
            "\u001b[35mSection_title: 4.2  Fine-tuning Tasks\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 5.1  Low-memory neural networks training\n",
            "\u001b[35mSection_title: 5.2  Efficient Transformer\n",
            "\u001b[32mconclusion: in this work, we study the lightweight bert model with the goal of achieving both efficiency and effectiveness. we profile and analyze the memory bottlenecks of bert and focus on optimize dotproduct self-attention, which consumes quadratic memory with respect to the sequence length. to reduce both time and memory consumption, we present blockbert, which sparsifies the attention matrices to be sparse block matrices. the proposed model achieves time and memory saving without significant loss of performance.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Unimodal Text and Table QA Datasets\n",
            "\u001b[35mSection_title: 3  The Task: ManyModal QA Reasoning\n",
            "\u001b[35mSection_title: 4  Data Collection\n",
            "\u001b[35mSection_title: 4  Data Collection\n",
            "\u001b[35mSection_title: 5  Data Analysis\n",
            "\u001b[35mSection_title: 5  Data Analysis\n",
            "\u001b[35mSection_title: 5.1  Question Properties\n",
            "\u001b[35mSection_title: 5.2  Answer Properties\n",
            "\u001b[35mSection_title: 6  Baseline Models and Results\n",
            "\u001b[35mSection_title: 6.1  Modality Selection Network\n",
            "\u001b[35mSection_title: 6.2  Modality Selection Network Results\n",
            "\u001b[35mSection_title: 6.3  Modality Selection and Question-Answering Network\n",
            "\u001b[35mSection_title: 6.4  Modality Selection and Question Answering Network Results\n",
            "\u001b[35mSection_title: 6.5  Recent Results with RoBERTa and LXMERT\n",
            "\u001b[32mconclusion: we presented a new challenge to the community, hoping to promote research in manymodal qa. we structured our challenge in a way that encourages research in other, more general areas, such as transfer learning and end-to-end modality disambiguation + multimodal qa. we hope that this challenge will serve as a test bed for further work and that our model will inspire directions of subsequent research. we plan to continue our work by collecting data that exhibits a stronger form of multimodality, where the question can only be answered after combining multiple modalities, and by adding new modalities, such as video and audio.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Overview\n",
            "\u001b[35mSection_title: 2.1  Task Formalization and Metrics\n",
            "\u001b[35mSection_title: 2.2  Question Types\n",
            "\u001b[35mSection_title: 3.1  Wikipedia and Wikidata\n",
            "\u001b[35mSection_title: 3.2  Dataset Generation Process\n",
            "\u001b[35mSection_title: 4  Data Analysis\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Evaluate the Dataset Quality\n",
            "\u001b[35mSection_title: 5.2  Baseline Results\n",
            "\u001b[35mSection_title: 5.3  Human Performance\n",
            "\u001b[35mSection_title: 5.4  Analysis of Mismatched Examples between Wikipedia and Wikidata\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this study, we presented 2wikimultihopqa-a large and high quality multi-hop dataset that provides comprehensive explanations for predictions. we utilized logical rules in the kb to create more natural questions that still require multi-hop reasoning. through experiments, we demonstrated that our dataset ensures multi-hop reasoning while being challenging for the multi-hop models. we also demonstrated that bootstrapping the multi-hop mrc dataset is beneficial by utilizing large-scale available data on wikipedia and wikidata.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 2.2  Continual Learning\n",
            "\u001b[35mSection_title: 2.2.1  Network regularization.\n",
            "\u001b[35mSection_title: 2.2.2  Dynamic\n",
            "\u001b[35mSection_title: 2.3  MRC Datasets\n",
            "\u001b[35mSection_title: 2.3.1  Basic\n",
            "\u001b[35mSection_title: 2.3.2  Derived Datasets.\n",
            "\u001b[35mSection_title: 3  CONTINUAL DOMAIN ADAPTATION FOR MRC\n",
            "\u001b[35mSection_title: 3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.2  Benchmark Construction\n",
            "\u001b[35mSection_title: 3.2.1  CDA Dataset for Context Drift.\n",
            "\u001b[35mSection_title: 3.2.2  CDA Dataset for Question Drift.\n",
            "\u001b[35mSection_title: 4  ANALYSIS OF BERTQA MODEL ON CDA\n",
            "\u001b[35mSection_title: 5  METHODS\n",
            "\u001b[35mSection_title: 5.1  BERTQA\n",
            "\u001b[35mSection_title: 5.1.2  Feed Forward Network. The outputs of SA is passed to twolayer feed forward network (FFN). FFN is shown as\n",
            "\u001b[35mSection_title: 5.2  RegBERTQA\n",
            "\u001b[35mSection_title: 5.3  ProgBERTQA\n",
            "\u001b[35mSection_title: 5.3.1  Overview.\n",
            "\u001b[35mSection_title: 5.3.2  The Adapter Insertion.\n",
            "\u001b[35mSection_title: 5.3.3  The Adapter Structure.\n",
            "\u001b[35mSection_title: 5.4  Model Training and Testing\n",
            "\u001b[35mSection_title: 6  EXPERIMENTS\n",
            "\u001b[35mSection_title: 6.1  Experimental Settings\n",
            "\u001b[35mSection_title: 6.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 6.3  Models\n",
            "\u001b[35mSection_title: 6.4  Major Results\n",
            "\u001b[35mSection_title: 6.5  Breakdown Analysis\n",
            "\u001b[32mconclusion: we introduce the continual domain adaptation task for mrc. so far as we know, this is the first study on the continual learning perspective of mrc. we build two datasets cda-q and cda-c for the cda task, by re-organizing existing mrc collections into different domains with respect to the question type and passage type. we conduct preliminary experiments showing the existence of catastrophic forgetting (cf) phenomenon of existing mrc models under the cda setting. further, we propose regularization-based regbertqa and dynamic-architecture progbertqa to tackle the cda for mrc. we conduct extensive experiments to analysis the effectiveness of both methods and validate that the proposed dynamic-architecture based model achieves the best performance.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 209  210\n",
            "\u001b[35mSection_title: 223  224\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  MUMUQA Task\n",
            "\u001b[35mSection_title: 3  Benchmark Construction\n",
            "\u001b[35mSection_title: 3.1  News Data Acquisition\n",
            "\u001b[35mSection_title: 3.2  Evaluation Set Construction\n",
            "\u001b[35mSection_title: 3.3  Dataset Analysis\n",
            "\u001b[35mSection_title: 4  Silver Training Set Generation\n",
            "\u001b[35mSection_title: 4.1  Training Data Generation Overview\n",
            "\u001b[35mSection_title: 4.2  Multimedia Entity Grounding\n",
            "\u001b[35mSection_title: 4.3  Visual Attribute Extraction\n",
            "\u001b[35mSection_title: 4.4  Conditioned Question Generation\n",
            "\u001b[35mSection_title: 4.5  Question Editing and Filtering\n",
            "\u001b[35mSection_title: 5  Question Answering Models\n",
            "\u001b[35mSection_title: 5.1  Multi-hop Text-only QA\n",
            "\u001b[35mSection_title: 5.2  End-to-End Multimedia QA\n",
            "\u001b[35mSection_title: 5.3  Pipeline-based Multimedia QA\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Settings\n",
            "\u001b[35mSection_title: 6.2  Results and Analysis\n",
            "\u001b[35mSection_title: 6.3  Training Set Analysis\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we present a new qa task, mumuqa, along with an evaluation benchmark for multimedia news understanding. the task is challenging in the requirement of cross-media grounding over images, captions, and news body text. we demonstrate the benefit of using multimedia knowledge extraction, both for generating silver-standard training data and for a pipeline-based multimedia qa system. the multimedia baselines are still considerably behind human performance, suggesting ample room for improvement. future work will incorporate other forms of media in news, such as video and audio, to facilitate information seeking from more comprehensive data sources. another direction is to infuse the endto-end multimedia qa system with additional input from the grounding and visual attribute extraction systems.  \n",
            "\u001b[35mSection_title: 8  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Test-Time Reading Comprehension\n",
            "\u001b[35mSection_title: 3  Self-Supervised QA Generation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 5  Results and Discussion\n",
            "\u001b[35mSection_title: 5.1  Unsupervised Question Answering\n",
            "\u001b[35mSection_title: 5.2  Few-Shot Question Answering\n",
            "\u001b[35mSection_title: 5.3  Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose test-time learning (ttl) as a new framework for unsupervised extractive question answering (eqa). we present four variants of ttl with a simple but effective context expansion method. we utilize four question-answer pair generation methods for eqa and propose using qa-srl as an additional source of qa pairs, to supplement prior methods. we show ttl enables \"understanding\" of contexts at test-time, without human-authored annotations, and significantly improves eqa, including low parameter models. we envision ttl as a framework that can direct work in reading comprehension to be viewed as a problem of ever-evolving datasets instead of a static corpus. natural language itself undergoes continuous evolution (gentner and france, 1988;traugott and dasher, 2001;hamilton et al., 2016) via changes in preference for syntactical structures; creation of new words and phrases; and changing usage frequencies and semantics for existing words. ttl can potentially be applied to such scenarios with semantic drift or domain shift. further improvements w.r.t. selection of similar contexts for k-neighbor ttl could be explored by leveraging hard sample selection, hard negative mining, bootstrapping, and contrastive learning, along with improved currculum strategies.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND\n",
            "\u001b[35mSection_title: 3  TRAINING WITH HINDSIGHT\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL EVALUATION\n",
            "\u001b[35mSection_title: 4.1  MODELS\n",
            "\u001b[35mSection_title: 4.2  TASKS\n",
            "\u001b[35mSection_title: 4.3  RELEVANCE EVALUATION\n",
            "\u001b[35mSection_title: 4.4  GROUNDEDNESS EVALUATION\n",
            "\u001b[35mSection_title: 4.5  END-TO-END EVALUATION\n",
            "\u001b[35mSection_title: 5  DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Document Coherence Measurement\n",
            "\u001b[35mSection_title: 2.2  Document Coherence Datasets\n",
            "\u001b[35mSection_title: 2.3  Dataset Artefacts\n",
            "\u001b[35mSection_title: 2.4  Linguistic Probes\n",
            "\u001b[35mSection_title: 3  Dataset Construction\n",
            "\u001b[35mSection_title: 3.1  Dataset Desiderata\n",
            "\u001b[35mSection_title: 3.2  Data Sources\n",
            "\u001b[35mSection_title: 3.3  Generating Candidate Positive Samples\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.1  Statistics of the Dataset\n",
            "\u001b[35mSection_title: 4.2  Types of Incoherence\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Testing for Dataset Artefacts\n",
            "\u001b[35mSection_title: 4.5  Human Verification\n",
            "\u001b[35mSection_title: 5  Models\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Preliminary Results\n",
            "\u001b[35mSection_title: 6.2  Results over the Existing Dataset\n",
            "\u001b[35mSection_title: 6.3  Performance on Documents of Different Difficulty Levels\n",
            "\u001b[35mSection_title: 6.4  Analysis over Documents with High Human Agreement\n",
            "\u001b[35mSection_title: 6.5  Question Revisited\n",
            "\u001b[35mSection_title: 7.2  Experimental Results\n",
            "\u001b[32mconclusion: we propose the new task of detecting whether there is an intruder sentence in a document, generated by replacing an original sentence with a similar sentence from a second document. to benchmark model performance over this task, we construct a large-scale dataset consisting of documents from english wikipedia and cnn news articles. experimental results show that pretrained lms which incorporate larger document contexts in pretraining perform remarkably well in-domain, but experience a substantial drop cross-domain. in follow-up analysis based on human annotations, substantial divergences from human intuitions were observed, pointing to limitations in their ability to model document coherence. further results over a linguistic probe dataset show that pretrained models fail to identify some linguistic characteristics that affect document coherence, suggesting room to improve for them to truly capture document coherence, and motivating the construction of a dataset with intruder text at the intra-sentential level.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1976  U2\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 2.1  Dataset\n",
            "\u001b[35mSection_title: 2.2  Question Decompositions\n",
            "\u001b[35mSection_title: 2.3  Models\n",
            "\u001b[35mSection_title: 2.4  Probing with Sub-Questions\n",
            "\u001b[35mSection_title: 2.5  Simulatability\n",
            "\u001b[35mSection_title: 3  Sub-question answering can distinguish incorrect and correct model predictions\n",
            "\u001b[35mSection_title: 4  Sub-question explanations allow humans to predict model behavior\n",
            "\u001b[35mSection_title: 5  Quality of question decompositions matters\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we have demonstrated the utility of question decomposition as an effective means to probe pretrained multi-hop question-answering models for supporting evidence. through simulatability experiments, we show the effectiveness of this explanation form at allowing humans to predict model behavior, a sign that it helps humans to form an accurate mental model of the machine learning system (jacovi et al., 2022). this ability to predict system performance occurs at the instance level instead of a sense of trust of the overall system, which can be important if the accuracy of the system is variable based on the question.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\u001b[32mlimitation: our simulability study results (section 4) are conducted on silver labels. as section 5 reveals, there is a need for higher-quality question decompositions. while we have demonstrated the potential for decomposition probes to help users build mental models of system behavior, these results are not fully realizable in real applications until decompo-sition systems improve.the probing strategy explored in this paper is particular to the qa setting and datasets that don't have predefined categories of answers. other probing strategies may exist that are not explored in this paper.it is noted that multi-hop questions do not always require multi-hop reasoning to solve. indeed we intentionally use a non-multi-hop questionanswering model to answer the original question to disadvantage the system so that explanations are required. multi-hop questions afford the use of a decompositional probing strategy. our study did not look at non-multi-hop questions, which may require other probing strategies yet to be invented.\n",
            "\u001b[35mSection_title: 8  Limitations\n",
            "\u001b[35mSection_title: 9  Ethics & Broader Impacts\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Mathematical model for translation\n",
            "\u001b[35mSection_title: 1.1  Delay two-state model for translating ribosome per RNA\n",
            "\u001b[35mSection_title: 1.2  Delay two-state model fitting\n",
            "\u001b[35mSection_title: 1.3  Profile likelihood analysis and confidence interval\n",
            "\u001b[35mSection_title: 2  Mathematical models for RNA degradation\n",
            "\u001b[35mSection_title: 2.1  Ribosome-flux dependent degradation\n",
            "\u001b[35mSection_title: 2.1.1  Fixed probability to degrade after each initiation event\n",
            "\u001b[35mSection_title: 2.1.2  Increasing probability to degrade after each initiation event\n",
            "\u001b[35mSection_title: 2.1.3  Degradation rate dependent on the ribosome flux\n",
            "\u001b[35mSection_title: 2.2  Ribosome-number dependent degradation\n",
            "\u001b[35mSection_title: 2.3  Simulation of the RNA degradation models\n",
            "\u001b[35mSection_title: 2.4  Fit of the degradation models\n",
            "\u001b[35mSection_title: 2.5  Model selection\n",
            "\u001b[35mSection_title: 2.6  Confidence interval analysis\n",
            "\u001b[35mSection_title: 3  Mathematical model for nonsense-mediated degradation\n",
            "\u001b[35mSection_title: 4  Mathematical models for miRNA-mediated degradation\n",
            "\u001b[35mSection_title: 5  Computing life expectancy of a mRNA molecule\n",
            "\u001b[35mSection_title: 6  Best fit parameters and model predictions standard error\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A Latent-Skill Multitask Model\n",
            "\u001b[35mSection_title: 2.1  Soft Partitions\n",
            "\u001b[35mSection_title: 2.2  Skill-specific Parameters\n",
            "\u001b[35mSection_title: 2.3  Inductive Biases\n",
            "\u001b[35mSection_title: 2.4  Parameter Efficiency\n",
            "\u001b[35mSection_title: 2.5  Baselines\n",
            "\u001b[35mSection_title: 3  Reinforcement Learning Experiments\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Experimental Setup\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 4  Supervised Learning Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we argued that a modular design is crucial to ensure that neural networks can learn from a few examples and generalise robustly across tasks by recombining autonomous facets of knowledge. to this end, we proposed a model where a subset of latent, discrete skills from a fixed inventory is allocated to each task in an end-to-end fashion. the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters. we evaluate the sample efficiency of our model on multitask instruction following through reinforcement learning and its few-shot adaptability on multitask text-to-text generation through supervised learning. in both experiments, we surpass competitive baselines where parameters are fully shared, task-specific, combined according to expert knowledge, or generated conditionally on the task. finally, we show that our model facilitates interpretability by learning an explicit hierarchy of tasks based on the skills they require. \n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Measuring Disconnected Reasoning\n",
            "\u001b[35mSection_title: 3.1  Disconnected Reasoning\n",
            "\u001b[35mSection_title: 3.2  Probing Disconnected Reasoning\n",
            "\u001b[35mSection_title: 3.3  Use Cases of DiRe Probe\n",
            "\u001b[35mSection_title: 4  Reducing Disconnected Reasoning\n",
            "\u001b[35mSection_title: 4.1  Contrastive Support Sufficiency\n",
            "\u001b[35mSection_title: 4.2  Transforming Existing Datasets\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[32mconclusion: progress in multi-hop qa under the reading comprehension setting relies on understanding and quantifying the types of undesirable reasoning current models may perform. this work introduced a formalization of disconnected reasoning, a form of bad reasoning prevalent in multi-hop models. it showed that a large portion of current progress in multifact reasoning can be attributed to disconnected reasoning. using a notion of contrastive sufficiency, it showed how to automatically transform existing support-annotated multi-hop datasets to create a more difficult and less cheatable dataset that results in reduced disconnected reasoning.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data\n",
            "\u001b[35mSection_title: 3.1  FinQA Dataset\n",
            "\u001b[35mSection_title: 3.2  Distribution Gap\n",
            "\u001b[35mSection_title: 4  Method\n",
            "\u001b[35mSection_title: 4.1  Retriever\n",
            "\u001b[35mSection_title: 4.2  Generator\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Implementation Details\n",
            "\u001b[35mSection_title: 5.2  Results of Retriever\n",
            "\u001b[35mSection_title: 5.3  Results of Generator\n",
            "\u001b[35mSection_title: 5.4  Ablation Studies\n",
            "\u001b[32mconclusion: in this paper, we propose a method that developing models that have different specialized capabilities and fuse their strengths to tackle the task objective of finqa. we first analyze the data distribution of finqa by adversarial validation, and build a new validation dataset that has a similar distribution to test set by re-splitting the training and validation set. besides, in order to reduce the influence of distribution gap, we develop and fuse several small models rather than train a large model which is easily over-fitting to training set. furthermore, we obtain a high performance by fusing the strengths of different small model. overall, we achieve the 1st place in finqa challenge with 71.93% execution accuracy and 67.03% program accuracy.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 2.1  Passage Retrieval\n",
            "\u001b[35mSection_title: 2.2  Unsupervised pretraining\n",
            "\u001b[35mSection_title: 2.3  Passage Re-ranking\n",
            "\u001b[35mSection_title: 2.4  BEIR benchmark\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Translation of the datasets\n",
            "\u001b[35mSection_title: 3.2  Baseline models\n",
            "\u001b[35mSection_title: 3.2.1  Unsupervised dense bi-encoder\n",
            "\u001b[35mSection_title: 3.2.2  HerBERT based reranker models\n",
            "\u001b[35mSection_title: 3.2.3  Polish T5 based re-ranker models\n",
            "\u001b[35mSection_title: 3.3  Late interaction HerBERT based re-ranker model\n",
            "\u001b[35mSection_title: 3.3.1  Pre-existing multilingual models\n",
            "\u001b[35mSection_title: 3.4  Experimental setup\n",
            "\u001b[35mSection_title: 3.5  Evaluation metrics\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[32mconclusion: information retrieval in the polish language is still developing and is an intensive research area. therefore, there is a great need for resources to enable further training and more accurate evaluation of existing and new deep neural ir models. in this work, we introduced the translated beir-pl benchmark and showed the results of a broad family of ir baseline models. we would like to encourage other researchers to participate in further development of polish and multilingual ir models using our new resource.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Context and structured information\n",
            "\u001b[35mSection_title: 3  Preliminary evidence retrieval\n",
            "\u001b[35mSection_title: 4  Next hop prediction\n",
            "\u001b[35mSection_title: 5  Label classification\n",
            "\u001b[32mconclusion: team papelo's system for feverous achieves .281 feverous score on the development set, with .658 label accuracy and .348 evidence recall. the largest increase in performance over the baseline comes from the label classifier, which uses a different model architecture and is trained on extracted evidence chains including irrelevant evidence. we also achieve better evidence recall through our table cell ranking module, which was trained with a multiple choice cross entropy loss similar to dpr. additional gains are achieved by our multi-hop evidence retrieval. these modules can only be effective when given good representations of the context of sentences, list items and table cells, which we have carefully constructed.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Architectures spécifiques au raisonnement numérique\n",
            "\u001b[35mSection_title: 2.2  Modèles de langues pré-entraînés augmentés de compétences numériques\n",
            "\u001b[35mSection_title: 2.3  Modèles de langues pour la génération de programmes de raisonnement\n",
            "\u001b[35mSection_title: 3  Discussion et perspectives\n",
            "\u001b[35mSection_title: 3.1  Capacité des modèles de langues à générer des chaînes de raisonnement\n",
            "\u001b[32mconclusion: cet article s'intéresse à l'amélioration des modèles de langue en raisonnement numérique.bien que le pré-entraînement sur des données numériques soit maintenant une approche courante, les résultats ne sont pas satisfaisants.notre synthèse de l'état de l'art des approches d'intégration du raisonnement numérique dans les modèles de langue penche clairement en faveur des architectures hybrides mêlant génération de texte et calculateurs externes.nous sommes convaincus du potentiel de ces modèles, à la fois en raisonnement automatique à partir d'énoncés textuels et dans le cadre des applications de data-to-text.nous travaillons au développement d'architectures capables de traduire ces problématiques en code python, d'exécuter ces programmes puis d'incorporer les résultats retournés dans un texte pertinent.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHOD\n",
            "\u001b[35mSection_title: 3.1  PROBLEM DEFINITION\n",
            "\u001b[35mSection_title: 3.2  COMPOSITIONAL SEMANTIC GRAPH OF DIALOGUE CONTEXT\n",
            "\u001b[35mSection_title: 3.3  LEARNING TO GENERATE REASONING PATHS\n",
            "\u001b[35mSection_title: 3.4  MULTIMODAL REASONING FROM REASONING PATHS\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[32mconclusion: we proposed pdc, a novel approach to learning a reasoning path over dialogue turns for videogrounded dialogues. our approach exploits the compositional semantics in each dialogue turn to construct a semantic graph, which is then used to derive an optimal path for feature propagation. our experiments demonstrate that our model can learn to retrieve paths that are most relevant to the current question. we hope our approach can motivate further study to investigate reasoning over multiple turns, especially in complex settings with interconnected dialogue flows (sun et al., 2019).\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Characteristics\n",
            "\u001b[35mSection_title: 3.3  Evaluation Settings\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 4.1  Fine-tuning BioBERT\n",
            "\u001b[35mSection_title: 4.2  Long Answer as Additional Supervision\n",
            "\u001b[35mSection_title: 4.3  Multi-phase Fine-tuning Schedule\n",
            "\u001b[35mSection_title: 4.4  Compared Models\n",
            "\u001b[35mSection_title: 4.5  Compared Training Schedules\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Human Performance\n",
            "\u001b[35mSection_title: 5.2  Main Results\n",
            "\u001b[35mSection_title: 5.3  Intermediate Results\n",
            "\u001b[32mconclusion: we present pubmedqa, a novel dataset aimed at biomedical research question answering using yes/no/maybe, where complex quantitative reasoning is required to solve the task. pubmedqa has substantial automatically collected instances as well as the largest size of expert annotated yes/no/maybe questions in biomedical domain. we provide a strong baseline using multi-phase fine-tuning of biobert with long answer as additional supervision, but it's still much worse than just single human performance. there are several interesting future directions to explore on pubmedqa, e.g.: (1) about 21% of pubmedqa contexts contain no natural language descriptions of numbers, so how to properly handle these numbers is worth studying; (2) we use binary bow statistics prediction as a simple demonstration for additional supervision of long answers. learning a harder but more informative auxiliary task of long answer generation might lead to further improvements.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Background\n",
            "\u001b[35mSection_title: 2.  Results\n",
            "\u001b[35mSection_title: 2.1  Quality control of SELDI analysis\n",
            "\u001b[35mSection_title: 2.2  Protein peak detection and data preprocessing\n",
            "\u001b[35mSection_title: 2.3  Validation of the serum proteomic profiles in testing set\n",
            "\u001b[35mSection_title: 2.4  Biomarker purification and identification\n",
            "\u001b[35mSection_title: 2.5  Immunohistochemistry of NAP-2\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[32mconclusion: in summary, we have identified a set of protein peaks that could discriminate hcc from healthy controls. from the protein peaks specific to hcc disease, we identified and characterized neutrophil-activating peptide-2 as a potential proteomic biomarker of hcc. further studies with larger sample sizes will be needed to verify this specific protein marker and to address its efficacy, especially with regard to discriminating histologic types of hcc and disease stages. nevertheless, our study demonstrates a rational approach for identifying hcc biomarkers that could be used for detection and monitoring hcc by proteomic techniques.\n",
            "\u001b[35mSection_title: 4.  Conclusion\n",
            "\u001b[35mSection_title: 5.  Materials and methods\n",
            "\u001b[35mSection_title: 5.1  Patients and samples\n",
            "\u001b[35mSection_title: 5.2  SELDI-TOF-MS analysis of serum protein profiles\n",
            "\u001b[35mSection_title: 5.3  Bioinformatics and biostatistics\n",
            "\u001b[35mSection_title: 5.4  Biomarker purification and identification\n",
            "\u001b[35mSection_title: 5.5  Immunohistochemistry (IHC)\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Task Definition\n",
            "\u001b[35mSection_title: 2.2  Investigated Model\n",
            "\u001b[35mSection_title: 2.3  Pilot Experiment\n",
            "\u001b[35mSection_title: 3  Our Method: Random Padding\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset Preparation\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 5  Main Results\n",
            "\u001b[35mSection_title: 5.1  Train Short, Test Long\n",
            "\u001b[35mSection_title: 5.2  Train/Test with Similar Context Lengths\n",
            "\u001b[35mSection_title: 6  Analysis and Discussions\n",
            "\u001b[35mSection_title: 6.1  Analysis on Answer Positions\n",
            "\u001b[35mSection_title: 6.2  How Random Padding Improves QA\n",
            "\u001b[35mSection_title: 7  Results on More Benchmark Datasets\n",
            "\u001b[32mconclusion: in this work, we propose a simple strategy, random padding, to improve the performance of extractive qa models, especially when they are trained on short contexts but evaluated on longer contexts. our method only re-organizes the input token sequences when fine-tuning, without any modifications to the architectures of plms. experiments reveal that our simple method can effectively enhance qa models when predicting answers at the rear positions, where the position embeddings may not be sufficiently updated without random padding. we also show that our simple strategy can improve the performance of plm components in more benchmarks and tasks where accurate local context representations over longer context are necessary.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  LLM-BASED AGENTS\n",
            "\u001b[35mSection_title: 2.2  TOOL AUGMENTED LLMS\n",
            "\u001b[35mSection_title: 2.3  CONSTRAINED DECODING\n",
            "\u001b[35mSection_title: 3  METHOD\n",
            "\u001b[35mSection_title: 3.1  LINEAR TEMPORAL LOGIC\n",
            "\u001b[35mSection_title: 3.2  SPECIFYING AGENT BEHAVIOR\n",
            "\u001b[35mSection_title: 3.3  CONSTRAINING AGENT BEHAVIOR\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  SETUP\n",
            "\u001b[35mSection_title: 5  RESULTS\n",
            "\u001b[32mconclusion: in this work, we introduced a framework for defining agents using ltl in a task-agnostic, domain general fashion.the ltl formulation of an agent can be used to construct a constrained decoder, which controls the high-level behavior of an agent at runtime.we demonstrated the utility of our framework with experiments on three datasets, where we found that the hard constraints imposed on generation by our approach can lead to an increase in agent performance.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Explainable Fake News Detection\n",
            "\u001b[35mSection_title: 2.2  Fact Verification with Language Models\n",
            "\u001b[35mSection_title: 3  Our HiSS Prompting Method\n",
            "\u001b[35mSection_title: 3.1  Research Questions\n",
            "\u001b[35mSection_title: 3.2  Claim Decomposition\n",
            "\u001b[35mSection_title: 3.3  Subclaim-level Step-by-Step Verification\n",
            "\u001b[35mSection_title: 3.4  Final Prediction\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results of Claim Veracity Classification\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  Human Evaluation\n",
            "\u001b[32mconclusion: in this paper, we study different prompting methods for using llms in news claim verification.we introduce a hierarchical step-by-step (hiss) method that prompts llm to perform the verification in fine-grained steps, aiming to mitigate the omission of thoughts and fact hallucination.validated on two public datasets, hiss prompting improves the performance of llms on the task over fullysupervised sota models and its strong few-shot icl-based counterparts.hiss prompted explanations show superior explainability in their coverage readability.in the future, we will build a conversational factchecking model based on llms which can be userfriendly and incorporate human fact-checkers in the loop.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\u001b[32mlimitation: despite the promising performance of llms based on few-shot icl, fact verification is a challenging research problem given the fact that performance scores are still quite low in general.there are a few limitations.firstly, in this work, we highlight that all the baselines and our proposed method solely rely on textual information.we focus on an unimodal approach utilizing language models and do not consider the potential assistance from other modalities, such as images and videos, for this task.although the exploration of multimodal approaches has gradually drawn some research attention (wang et al., 2018;silva et al., 2021;bu et al., 2023), it falls outside the scope of our current work.meanwhile, the scope of this study is limited to the verification of news claims, which represents only a subset of the broader issue of misinformation.misinformation encompasses a wide range of false or misleading information, including rumors, fake news articles, and spams (wu et al., 2019).while our focus was specifically on news claims, future research could explore the detection and mitigation of misinformation in other formats.further, our proposed prompting method heavily relies on the capabilities of backbone llms, which can come with substantial computational costs.our method leverages the advancements in multi-step reasoning exhibited by these llms, necessitating high-performance expectations.however, it is worth noting that most state-of-the-art llms are currently not open-source and only available as services.for instance, gpt-3.5 can only be accessed via api.the reliance on such llms makes deep model control infeasible, and the need for api access poses challenges in terms of cost.finally, while our approach leverages search engines to mitigate the fact hallucination issue in llms, it operates under the assumption that pertinent information is readily accessible through web search.however, not all information is indexed or available in search engines.for instance, if someone claims to have witnessed a rare meteorological phenomenon in a small town, such event might not be reported on major news websites or databases.such firsthand, non-digitized accounts might be retrieved or fact-checked.this underscores the limitation in relying solely on search engines as a primary source of external knowledge for factchecking with llms.another limitation of our method lies in the claims that are beyond established world knowledge when necessary relevant knowledge is not complete or even not available.this necessitates the model's ability to infer novel knowledge by formulating and subsequently validating appropriate hypotheses, a task that remains beyond the capabilities of existing technologies.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3.2  Math II (Counting Happy Numbers): CoT Also Less Effective\n",
            "\u001b[35mSection_title: 3.3  Answering Sensitive Questions: Safer but Less Rationale\n",
            "\u001b[35mSection_title: 3.4  OpinionQA Survey: Lower Response Rate\n",
            "\u001b[35mSection_title: 3.5  Code Generation: Less Adherence to Formatting Instructions\n",
            "\u001b[35mSection_title: 3.6  LangChain HotpotQA Agent: Poor Prompt Stability\n",
            "\u001b[35mSection_title: 3.7  USMLE Medical Exam: Small Decrease in GPT-4 Performance\n",
            "\u001b[35mSection_title: 3.8  Visual Reasoning: Small Improvements in Both Models\n",
            "\u001b[32mconclusion: our findings demonstrate that the behavior of gpt-3.5 and gpt-4 has varied significantly over a relatively short amount of time.this highlights the need to continuously evaluate and assess the behavior of llm drifts in applications, especially as it is not transparent how llms such as chatgpt are updated over time.our study also underscores the challenge of uniformly improving llms' multifaceted abilities.improving the model's performance on some tasks, for example with fine-tuning on additional data, can have unexpected side effects on its behavior in other tasks.consistent with this, both gpt-3.5 and gpt-4 got worse on some tasks but saw improvements in other dimensions.moreover, the trends for gpt-3.5 and gpt-4 are often divergent.beyond the final performances, it's interesting to observe shifts in chain-of-thought behaviors and verbosity of the models.\n",
            "\u001b[35mSection_title: 5  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.1.3  Key Concepts.\n",
            "\u001b[35mSection_title: 2.2  Categories of Inference\n",
            "\u001b[35mSection_title: 2.2.2  Deductive Inference and Defeasible Inference.\n",
            "\u001b[35mSection_title: 2.3  Potentials, Challenges, and Requirements of NLR\n",
            "\u001b[35mSection_title: 3  WHY PLMS FOR NATURAL LANGUAGE REASONING 3.1 Introduction to PLMs\n",
            "\u001b[35mSection_title: 3.2  Empirical Development\n",
            "\u001b[35mSection_title: 4  METHODOLOGIES OF NLR\n",
            "\u001b[35mSection_title: 4.1  End-to-End Reasoning\n",
            "\u001b[35mSection_title: 4.1.1  Training specialized models.\n",
            "\u001b[35mSection_title: 4.1.2  Finetuning vanilla medium-size PLMs.\n",
            "\u001b[35mSection_title: 4.1.3  Few-shot decoder-only LLMs.\n",
            "\u001b[35mSection_title: 4.1.4  Specialized pretraining.\n",
            "\u001b[35mSection_title: 4.2  Forward Reasoning\n",
            "\u001b[35mSection_title: 4.2.1  Trustworthiness.\n",
            "\u001b[35mSection_title: 4.2.2  Performance improvement.\n",
            "\u001b[35mSection_title: 4.3  Backward Reasoning\n",
            "\u001b[35mSection_title: 4.3.1  Backward Chaining.\n",
            "\u001b[35mSection_title: 4.3.2  Question Decomposition.\n",
            "\u001b[35mSection_title: 4.4  Summary\n",
            "\u001b[35mSection_title: 5  NLR BENCHMARKS\n",
            "\u001b[35mSection_title: 5.1  Classical Logical Reasoning\n",
            "\u001b[35mSection_title: 5.1.1  Deductive reasoning.\n",
            "\u001b[35mSection_title: 5.1.2  Defeasible reasoning.\n",
            "\u001b[35mSection_title: 5.2  Natural Language Inference\n",
            "\u001b[35mSection_title: 5.3  Multi-Hop Question Answering\n",
            "\u001b[35mSection_title: 5.4  Commonsense Reasoning\n",
            "\u001b[35mSection_title: 5.4.1  Datasets & Benchmarks.\n",
            "\u001b[35mSection_title: 5.4.2  Reasoning.\n",
            "\u001b[35mSection_title: 5.5  Complex Reasoning\n",
            "\u001b[35mSection_title: 5.6  Others\n",
            "\u001b[35mSection_title: 6  DISCUSSION\n",
            "\u001b[35mSection_title: 6.1  Open questions\n",
            "\u001b[32mlimitation: we introduce both limitations of the current research and intrinsic in plms. firstly, there are gaps in defeasible reasoning and reasoning path evaluation.• research gap on defeasible reasoning. while defeasible reasoning is widely used in our daily life, this topic is still under-explored in nlp. [4] found that it is more challenging for chatgpt to perform abductive reasoning and inductive reasoning than deduction, among which induction is the much more difficult one. • lack of effective ways to evaluate reasoning paths. it is still challenging to automatically evaluate generated reasoning paths without ground truth. evaluating reasoning paths might become increasingly important to build explainable and reliable ai systems, especially when more people contact and use chatgpt-like products nowadays.secondly, there are also limitations intrinsic to plms.• soft deduction can produce invalid conclusions. transformers can only predict conclusions with probability, irrespective of whether the conclusion of deductive reasoning is necessarily true in nature, which might prevent it from precise reasoning. this characteristic can result in a sub-optimal solution to deductive problems (including arithmetic reasoning and symbolic reasoning). for example, while chatgpt is impressive on reasoning tasks, it still fails to achieve perfect performance on the simplest one-step deductive inference task [4].• biases on content. plms make their prediction based on context. while llms have made huge progress in reasoning, [32] found that llms are biased by content like humans when performing deduction. for example, they perform worse in abstract or counterfactual situations than the realistic ones. such biases will hinder them from actual reasoning and lead to wrong answers, degrading downstream performance. more severely, it might cause harmful societal influences due to some social biases such as gender, which also exist in gpt4 [16].\n",
            "\u001b[35mSection_title: 6.2  Limitations\n",
            "\u001b[35mSection_title: 6.3  Future\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Title Generation\n",
            "\u001b[35mSection_title: 3.2  Passage Selection\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Metrics\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Boosting with Correction\n",
            "\u001b[32mconclusion: in this work, we propose an information retrieval (ir) pipeline built on the advances of query decomposition and event linking. specifically, we associate the input query with generated events relevant to the query through the decomposition step and then link these events to real-world facts in a knowledge base with an event-linking model. we show that with simple passage selection through bm-25 and cross-domain qa supervision, our pipeline outperforms existing state-of-the-art unsupervised or cross-domain ir models on five datasets by an average of 6%. compared with existing methods, our pipeline does not involve heavy pre-training, parameter tuning, or domain-specific supervision.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Title Generation\n",
            "\u001b[35mSection_title: 3.2  Passage Selection\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Metrics\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Boosting with Correction\n",
            "\u001b[32mconclusion: in this work, we propose an information retrieval (ir) pipeline built on the advances of query decomposition and event linking. specifically, we associate the input query with generated events relevant to the query through the decomposition step and then link these events to real-world facts in a knowledge base with an event-linking model. we show that with simple passage selection through bm-25 and cross-domain qa supervision, our pipeline outperforms existing state-of-the-art unsupervised or cross-domain ir models on five datasets by an average of 6%. compared with existing methods, our pipeline does not involve heavy pre-training, parameter tuning, or domain-specific supervision.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3.1  QAR use cases\n",
            "\u001b[35mSection_title: 4  Research plan 4.1 Dataset\n",
            "\u001b[35mSection_title: 4.1.1  In the text questions\n",
            "\u001b[35mSection_title: 4.1.2  In my head questions\n",
            "\u001b[35mSection_title: 5  Summary\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Matreials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Patients and Serum\n",
            "\u001b[35mSection_title: 2.2.  SELDI-TOF-MS Protein\n",
            "\u001b[35mSection_title: 2.3.  Bioinformatics and Biostatistics.\n",
            "\u001b[35mSection_title: 2.4.  Statistical Analysis.\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Serum SELDI Profiles of CRC with Regional Lymph\n",
            "\u001b[35mSection_title: 3.2.  Quality Control and Reproducibility.\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Formulation for ALBERT\n",
            "\u001b[35mSection_title: 3.3  Recursive Dynamic Gating\n",
            "\u001b[35mSection_title: 3.4  Training Objective\n",
            "\u001b[35mSection_title: 4.1  Experimental Setups\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 5  Explainability Evaluation\n",
            "\u001b[35mSection_title: 5.1  Human Evaluation\n",
            "\u001b[35mSection_title: 5.2  Case Analysis\n",
            "\u001b[35mSection_title: 5.3  Negative Sample Analysis\n",
            "\u001b[32mconclusion: in this paper, we aim to improve the explainability for the machine reading comprehension task, which is different from most of the previous works that were only striving for better objective evaluation scores. to achieve this goal, we propose a novel mechanism called recursive dynamic gating (rdg) to gradually refine the amount of the input information in each layer of the pre-trained language model. also, we propose an attention smoothing technique that will increase the accuracy of the rdg mechanism. experimental results on three multiple-choice machine reading comprehension datasets show that the proposed rdg mechanism could not only improve the objective evaluation scores, but also show an advantage over the traditional attention mechanism in explainability.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Data\n",
            "\u001b[35mSection_title: 3  System Overview\n",
            "\u001b[35mSection_title: 3.1  XLNet\n",
            "\u001b[35mSection_title: 3.2  Domain Sampling\n",
            "\u001b[35mSection_title: 3.3  Negative Sampling\n",
            "\u001b[35mSection_title: 3.4  Paraphrasing by Back-Translation\n",
            "\u001b[35mSection_title: 3.4.1  Augmentation Strategy\n",
            "\u001b[35mSection_title: 3.4.2  Active Learning\n",
            "\u001b[35mSection_title: 4  Experiments and Discussion\n",
            "\u001b[32mconclusion: this paper describes experiments on various competitive pre-trained models (bert, xlnet), domain sampling strategies, negative sampling, data augmentation via back-translation, and active learning. we determine which of these strategies help and hurt multi-domain generalization, finding ultimately that some of the simplest techniques offer surprising improvements. the most significant benefits came from sampling no answer segments, which proved to be particularly important for training extractive models on long sequences. in combination these findings culminated in the second ranked submission on the mrqa-19 shared task.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Definition\n",
            "\u001b[35mSection_title: 2.1  Metric: Calibrator performance\n",
            "\u001b[35mSection_title: 2.2  Metric: End task performance\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Calibrator Architecture\n",
            "\u001b[35mSection_title: 3.2  Input Example Embedding Feature From\n",
            "\u001b[35mSection_title: 3.2.1  Base QA Model\n",
            "\u001b[35mSection_title: 3.3  Data Augmentation Via Paraphrasing\n",
            "\u001b[35mSection_title: 4  Experimental Settings\n",
            "\u001b[35mSection_title: 4.1  Data\n",
            "\u001b[35mSection_title: 4.2  Comparison Systems\n",
            "\u001b[35mSection_title: 4.2.1  Reading Comprehension\n",
            "\u001b[35mSection_title: 4.2.2  Open Retrieval QA\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6.2  Quality of Back Translation\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we introduce a richer feature space for question answering calibrators with question and context embeddings and paraphrase-augmented inputs. our work suggests deciding the correctness of a qa system depends on both the semantics of the questioncontext and the confidence of the model. we thoroughly test our calibrator in domain shift, adversarial, and open domain qa settings. the experiments show noticeable gains in performance across all settings. we further demonstrate our calibrator's general applicability by using it as a reranker in extractive open domain qa. to summarize, our calibrator is simple, effective and general, with potential to be incorporated into existing models or extended for other nlp tasks.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS\n",
            "\u001b[35mSection_title: 3  MODEL\n",
            "\u001b[35mSection_title: 3.1  BACKGROUND\n",
            "\u001b[35mSection_title: 3.2  QUERY EMBEDDINGS\n",
            "\u001b[35mSection_title: 3.3  NEURAL REPRESENTATIONS OF PARAGRAPHS\n",
            "\u001b[35mSection_title: 3.4  ITERATIVE ATTENTION\n",
            "\u001b[35mSection_title: 3.5  RUNTIME EFFICIENCY\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  EXTRACTIVE QA: HYBRIDQA AND QASPER\n",
            "\u001b[35mSection_title: 4.2  HOTPOTQA-LONG\n",
            "\u001b[32mconclusion: we consider on the problem of answering complex questions over long structured documents. like multi-hop open qa tasks, this problem requires not only conventional \"machine reading\" abilities, but the ability to retrieve relevant information and refine queries based on retrieved information. additionally, it requires the ability to navigate through a document, by understanding the relationship between sections of the document and parts of the question. in our framework, navigation is modeling similarly to retrieval in multi-hop models: the model attends to a document section, and uses a compact neural encoding of the section to update the query. unlike most prior multihop qa models, however, queries are updated in embedding space, rather than by appending to a discrete representation of question text. this approach is end-to-end differentiable and very fast. experiments also demonstrate that this use of hierarchical attention can significantly improve the performance on qa tasks: in fact, the dochopper model achieves the start-of-the-art results on four challenging qa datasets, outperforming the baseline models by 3-5%, while also being 3-10 times faster.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\u001b[35mSection_title: 6  ETHICS STATEMENT\n",
            "\u001b[35mSection_title: 7  REPRODUCIBILITY STATEMENT\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  Evidence Path Mining\n",
            "\u001b[35mSection_title: 3.3  Adapting Retrievers for Path Ranking\n",
            "\u001b[35mSection_title: 3.3.1  Dense passage retriever (DPR)\n",
            "\u001b[35mSection_title: 3.3.2  Adapting DPR in Cross-document RE\n",
            "\u001b[35mSection_title: 3.3.3  Contextual DPR\n",
            "\u001b[35mSection_title: 3.4  Input Preparation\n",
            "\u001b[35mSection_title: 3.5  Downstream RE Methods\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  Case Study\n",
            "\u001b[32mconclusion: we study efficient and effective ways to extract multi-hop evidence for cross-document re and propose mr.cod. mr.cod extracts evidence paths from an open set of documents and ranks them with adapted dense retrievers as scorers. to overcome the gap between retrieval in odqa and evidence retrieval for re, we develop a contextual dpr that augments sparse queries with passage context. extensive experiments show high-quality evidence retrieved by mr.cod boosts end-to-end cross-document re performance. future works include extending mr.cod to more retrieval methods, such as generative dense retrievers.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Bottlenecked Masked Autoencoders\n",
            "\u001b[35mSection_title: 3.2  Token Importance Aware Masking\n",
            "\u001b[35mSection_title: 3.3  Pre-training\n",
            "\u001b[35mSection_title: 3.4  Fine-tuning\n",
            "\u001b[35mSection_title: 3.5  Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablations and Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Motivation\n",
            "\u001b[35mSection_title: 3  Behavior on Counterfactuals\n",
            "\u001b[35mSection_title: 4  Explanation Techniques\n",
            "\u001b[35mSection_title: 4.1  Token Attribution-Based\n",
            "\u001b[35mSection_title: 4.2  Feature Interaction-Based\n",
            "\u001b[35mSection_title: 4.3  Layer-wise Attention Attribution\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Hotpot Yes-No Questions\n",
            "\u001b[35mSection_title: 5.2  Hotpot Bridge Questions\n",
            "\u001b[35mSection_title: 5.3  SQuAD Adversarial\n",
            "\u001b[32mlimitation: we show that feature attributions can reveal known dataset biases and reasoning shortcuts in hotpotqa without having to perform a detailed manual analysis. this confirms the suitability of our attribution methods for at least this use case: model designers can look at them in a semi-automated way and determine how robust the model is going to be when faced with counterfactuals.our analysis also highlights the limitations of current explanation techniques. we experimented with other counterfactuals by permuting the order of the paragraphs in the context, which often gave rise to different predictions. we believe the model prediction was in these cases impacted by biases in positional embeddings (e.g., the answer tends to occur in the first retrieved paragraph), which cannot be indicated by current attribution methods. we believe this is a useful avenue for future investigation. by first thinking about what kind of counterfactuals and what kind of behaviours we want to explain, we can motivate the development of new explanation techniques to serve these needs.\n",
            "\u001b[35mSection_title: 5.4  Discussion and Limitations\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we have presented a new methodology using explanations to understand model behavior on realistic counterfactuals. we show explanations can indeed be connected to model behavior, and therefore we can compare explanations to understand which ones truly give us actionable insights about what our models are doing.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Task Formulation\n",
            "\u001b[35mSection_title: 4.  WEBQA\n",
            "\u001b[35mSection_title: 4.1.  Answers from Images\n",
            "\u001b[35mSection_title: 4.2.  Answers From Text\n",
            "\u001b[35mSection_title: 4.3.  Quality Control\n",
            "\u001b[35mSection_title: 4.4.  Dataset Statistics\n",
            "\u001b[35mSection_title: 5.  Metrics\n",
            "\u001b[35mSection_title: 5.1.  Question Answering Metrics\n",
            "\u001b[35mSection_title: 6.  Baseline Models\n",
            "\u001b[35mSection_title: 6.1.  Fine-tuning Approach\n",
            "\u001b[35mSection_title: 6.2.  Zero-shot Full-scale Retrieval Approach\n",
            "\u001b[35mSection_title: 6.3.  Few-shot Question Answering Approach\n",
            "\u001b[35mSection_title: 7.  Results & Analysis\n",
            "\u001b[35mSection_title: 7.1.  Source Retrieval\n",
            "\u001b[35mSection_title: 7.2.  Question Answering\n",
            "\u001b[35mSection_title: 7.3.  Qualitative Analysis\n",
            "\u001b[32mconclusion: webqa is a new multi-hop, multi-modal question answering challenge for our community. designed to simulate the heterogeneous information landscape one might expect during a web search, webqa covers a series of opendomain general visual queries while also forcing models to still reason about text. our task requires a system to determine relevant sources, perform aggregation and reasoning. we also propose a novel general recipe for evaluation on webqa which measures both fluency and accuracy.\n",
            "\u001b[35mSection_title: 8.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND\n",
            "\u001b[35mSection_title: 2.1  Multi-hop Reading Comprehension\n",
            "\u001b[35mSection_title: 2.2  Paragraph Retrieval\n",
            "\u001b[35mSection_title: 2.3  Sentence Embedding\n",
            "\u001b[35mSection_title: 3  OUR METHOD\n",
            "\u001b[35mSection_title: 3.1  Notations\n",
            "\u001b[35mSection_title: 3.2  Evidence Paragraph Retrieval\n",
            "\u001b[35mSection_title: 3.3  Evidence Sentence Extraction\n",
            "\u001b[35mSection_title: 3.4  Answer Span Extraction\n",
            "\u001b[35mSection_title: 3.5  Multi-task Prediction\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Experimental Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Study\n",
            "\u001b[35mSection_title: 5  DISCUSSION\n",
            "\u001b[35mSection_title: 5.1  The efficacy of our paragraph retrieval method\n",
            "\u001b[35mSection_title: 5.2  The effect of sentence embedding\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[32mconclusion: this paper proposes a novel \"select-to-guide\" model (s2g) for multi-hop reading comprehension in more effective and convenient way. as an alternative of the existing graph modeling, the proposed graph-free s2g model consists of an evidence paragraph retrieval module which selects evidence paragraphs in a step-by-step multihop manner, and a multi-task module that simultaneously extracts evidence sentences and answer spans.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Problem Setup\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Source Prompt Pre-training\n",
            "\u001b[35mSection_title: 3.2  Target Prompt Training\n",
            "\u001b[35mSection_title: 3.2.1  Input-prompt Attentions\n",
            "\u001b[35mSection_title: 3.2.2  Prompt Interpolation\n",
            "\u001b[35mSection_title: 3.3  Multi-task Training and Inference\n",
            "\u001b[35mSection_title: 3.4  Parameter Efficiency of ATTEMPT\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Source and Target Tasks\n",
            "\u001b[35mSection_title: 4.2  Baselines and Implementation Details\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Few-shot Domain Adaptations\n",
            "\u001b[35mSection_title: 5.3  Analyses\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we present a new parameter-effluent tuning method attempt, which learns to produce instance-wise prompts by interpolating multiple reusable soft prompts trained on source tasks and a new taskspecific prompt, while keeping the original lm frozen. our large-scale experiments demonstrate that attempt achieves a great trade-off between task performance and efficiency, introducing an interpretable and modular task transfer.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Compressive Graph Selector Network\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  Overview of the Model\n",
            "\u001b[35mSection_title: 3.3  Local Graph Network\n",
            "\u001b[35mSection_title: 3.4  Global Graph Network\n",
            "\u001b[35mSection_title: 3.5  Evidence Memory Network\n",
            "\u001b[35mSection_title: 3.6  Select-then-read Methods versus End-to-end Methods\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Study\n",
            "\u001b[35mSection_title: 5.3  Effects of Paragraph Number and Global Node Number\n",
            "\u001b[35mSection_title: 5.4  Effects of Graph Hop Number\n",
            "\u001b[35mSection_title: 5.5  Effects of the Global Graph Network\n",
            "\u001b[35mSection_title: 5.6  Effects of Evidence Memory\n",
            "\u001b[32mconclusion: to solve the problem of lacking global structure in ldqa methods, we propose compress graph selector network to capture the global structure over the long document when selecting evidence pieces. extensive experiments demonstrate the strong per-formance of the model.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Preliminaries and Problem Setting\n",
            "\u001b[35mSection_title: 3.1  QA Systems for Unstructured Text\n",
            "\u001b[35mSection_title: 3.2  Precision/Recall Tradeoff\n",
            "\u001b[35mSection_title: 3.3  Question Filtering Opportunity Space\n",
            "\u001b[35mSection_title: 4  Modelling QA System Scores using Input Questions\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Models\n",
            "\u001b[35mSection_title: 5.3  Baselines\n",
            "\u001b[35mSection_title: 5.4  Approximating Precision/Recall of M\n",
            "\u001b[35mSection_title: 5.5  Selecting Threshold τ 2 for F\n",
            "\u001b[35mSection_title: 5.6  Qualitative Analysis\n",
            "\u001b[32mconclusion: in this paper, we have presented a novel paradigm of training a question filter to capture the semantics of a qa system's answering capability by distilling the knowledge of the answer scores from it. our experiments on three academic and one industrial qa benchmark show that the trained question models can estimate the pr/re curves of the qa system well, and can be used to effectively filter questions while only incurring a small drop in recall. an interesting future work direction is to analyze the impact/behavior of the question filters in a cross-domain setting, where the training and testing corpora are from different domains. this would allow examining the transferability of the semantics learned by the question filters. a complementary future work direction could be knowledge distillation from a sophisticated answer verification module like (rodriguez et al., 2019;kamath et al., 2020;zhang et al., 2021).\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Collecting a Dataset for Literary Evidence Retrieval\n",
            "\u001b[35mSection_title: 2.1  Collecting and Preprocessing RELiC\n",
            "\u001b[35mSection_title: 2.2  Comparison to other retrieval datasets\n",
            "\u001b[35mSection_title: 2.3  Analyzing different types of quotation\n",
            "\u001b[35mSection_title: 3  Literary Evidence Retrieval\n",
            "\u001b[35mSection_title: 3.1  Task formulation\n",
            "\u001b[35mSection_title: 3.2  Models\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 4  Human performance and analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we introduce the task of literary evidence retrieval and an accompanying dataset, relic. we find that direct quotation of primary sources in literary analysis is most commonly used as evidence for literary claims or arguments. we train a dense retriever model for our task; while it significantly outperforms baselines, human performance indicates a large room for improvement. important future directions include (1) building better models of primary sources that integrate narrative and discourse structure into the candidate representations instead of computing them out-of-context, and (2) integrating relic models into real tools that can benefit humanities researchers.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.1  Pre-trained Language Models\n",
            "\u001b[35mSection_title: 2.2  Pre-training for IR\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Hyperlink Relationships\n",
            "\u001b[35mSection_title: 3.2  Pre-training Strategy\n",
            "\u001b[35mSection_title: 3.2.3  Mask Strategy for Anchor Text.\n",
            "\u001b[35mSection_title: 3.3  Discussion\n",
            "\u001b[35mSection_title: 3.3.1  Utilizing\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS 4.1 Datasets and Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.1.2  Downstream Datasets.\n",
            "\u001b[35mSection_title: 4.1.3  Evaluation Metrics.\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.3.3  Fine-tuning Settings.\n",
            "\u001b[35mSection_title: 4.4  Experimental Results\n",
            "\u001b[35mSection_title: 4.5  Further Analysis\n",
            "\u001b[35mSection_title: 4.5.1  Ablation Study.\n",
            "\u001b[35mSection_title: 4.5.2  Extension to Other Tasks.\n",
            "\u001b[35mSection_title: 4.5.3  Low-resource and\n",
            "\u001b[32mconclusion: in this work, we proposed a progressive pre-training strategy for adhoc retrieval, which consists of three hyperlink-based pre-training tasks, namely hyperlink prediction, symmetric hyperlink prediction, and most relevant document selection. these pre-training tasks can simulate different stages of a retrieval process, and gradually enhance the model's performance on relevance modeling. compared with existing methods, our strategy can make better use of utilize the hyperlink, which reveal the great potential of hyperlink in learning relevance modeling. the experimental results demonstrated the effectiveness of our model. in the future, we plan to integrate our method with other pre-training objectives (such as rop), which may bring further improvement.\n",
            "\u001b[35mSection_title: 5  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 2.1.  Multihop Question Answering over Knowledge Base.\n",
            "\u001b[35mSection_title: 2.2.  Multihop Question\n",
            "\u001b[35mSection_title: 2  Complexity\n",
            "\u001b[35mSection_title: 3.  Task Formulation\n",
            "\u001b[35mSection_title: 4.  Solution Approach\n",
            "\u001b[35mSection_title: 4.1.  Process Overview.\n",
            "\u001b[35mSection_title: 4.2.  Input Module. An input question\n",
            "\u001b[35mSection_title: 4.3.  Contextual Module.\n",
            "\u001b[35mSection_title: 4.4.  Reasoning Module.\n",
            "\u001b[35mSection_title: 4.5.  Prediction Module.\n",
            "\u001b[35mSection_title: 5.  Experiments\n",
            "\u001b[35mSection_title: 5.1.  Datasets.\n",
            "\u001b[35mSection_title: 5.2.  Model Comparison.\n",
            "\u001b[35mSection_title: 5.2.1.  Baseline.\n",
            "\u001b[35mSection_title: 5.2.2.  NMN.\n",
            "\u001b[35mSection_title: 5.2.3.  KGNN.\n",
            "\u001b[35mSection_title: 5.2.4.  DFGN.\n",
            "\u001b[35mSection_title: 5.2.5.  BERT.\n",
            "\u001b[35mSection_title: 5.2.6.  Coarse-Grained Decomposition Strategy.\n",
            "\u001b[35mSection_title: 5.2.7.  Fine-Grained Interaction\n",
            "\u001b[35mSection_title: 5.3.  Implementation Details.\n",
            "\u001b[35mSection_title: 5.3.1.  Baseline Model for HotpotQA Dataset.\n",
            "\u001b[35mSection_title: 5.3.2.  Dynamically Fused Graph Network.\n",
            "\u001b[35mSection_title: 5.4.  Main Results.\n",
            "\u001b[35mSection_title: 5.5.  Ablation Studies.\n",
            "\u001b[35mSection_title: 5.6.  Analysis and Visualization.\n",
            "\u001b[32mconclusion: in this paper, we propose a dual-channel reasoning architecture for complex question answering. e dual-channel reasoning architecture is applied to the feature interaction framework and graph-based models to verify its general applicability. in the experiments, we show that our models   complexity significantly and consistently outperform the baseline model, especially in supporting fact prediction tasks. after more detailed experimental analysis, it is proved that the dual-channel reasoning structure has stronger step-by-step reasoning ability than the single-channel reasoning structure. in the future, we believe that the following issue will be worth studying. for the dual-channel reasoning architecture, the interaction strategy between the two channels, such as the soft parameter sharing of the homogeneous neural network components of the two channels, is worthy of further study.\n",
            "\u001b[35mSection_title: 6.  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1.  Biomedical QA Datasets\n",
            "\u001b[35mSection_title: 2.2.  Yes/No QA\n",
            "\u001b[35mSection_title: 2.3.  Sentiment-Enhanced QA\n",
            "\u001b[35mSection_title: 3.  MATERIALS AND METHODS\n",
            "\u001b[35mSection_title: 3.1.  Dataset\n",
            "\u001b[35mSection_title: 3.2.  Learning Task\n",
            "\u001b[35mSection_title: 3.3.  An Overview of the Learning Framework\n",
            "\u001b[35mSection_title: 3.4.  BioBERT for Word Embedding\n",
            "\u001b[35mSection_title: 3.5.  Fine-Tuning T5\n",
            "\u001b[35mSection_title: 3.6.  Fine-Tuning RoBERTa\n",
            "\u001b[35mSection_title: 3.7.  Training an XGBoost Classifier\n",
            "\u001b[35mSection_title: 3.8.  Prediction\n",
            "\u001b[35mSection_title: 3.9.  Performance Metric\n",
            "\u001b[35mSection_title: 4.  EXPERIMENTS AND RESULTS\n",
            "\u001b[35mSection_title: 4.1.  Implementation Details\n",
            "\u001b[35mSection_title: 4.2.  Hyperparameters\n",
            "\u001b[35mSection_title: 4.3.  Benchmarks\n",
            "\u001b[35mSection_title: 4.4.  Model Fine-Tuning\n",
            "\u001b[35mSection_title: 4.5.  Ablation Study\n",
            "\u001b[35mSection_title: 4.6.  Comparison With the SOTA\n",
            "\u001b[35mSection_title: 4.7.  Error Analysis\n",
            "\u001b[35mSection_title: 5.  DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Completion of the graph structure\n",
            "\u001b[35mSection_title: 3.2  Graph Attention with Hierarchies\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Experimental Results\n",
            "\u001b[35mSection_title: 4.3  Error Analysis\n",
            "\u001b[32mconclusion: in this paper, we proposed two extensions to hierarchical graph network (hgn) for the multihop question answering task on hotpotqa. first, we completed the hierarchical graph structure by adding new edges between the query and context sentence nodes. second, we introduced gath as the mechanism for neural node updates, a novel extension to gat that can update node representations sequentially, based on hierarchical levels. to the best of our knowledge, this is the first time the hierarchical graph structure is directly exploited in the update mechanism for information propagation.\n",
            "\u001b[35mSection_title: 5  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Bucketing-based Calibration and ECE\n",
            "\u001b[35mSection_title: 2  Temperature Scaling\n",
            "\u001b[35mSection_title: 3  Calibration in Open-Domain Question Answering\n",
            "\u001b[35mSection_title: 3.1  The ODQA Model\n",
            "\u001b[35mSection_title: 3.2  Temperature Scaling For ODQA\n",
            "\u001b[35mSection_title: 3.3  Experiments\n",
            "\u001b[35mSection_title: 4  Flaws in ECE and Better Alternatives\n",
            "\u001b[35mSection_title: 4.1  What's Wrong With ECE?\n",
            "\u001b[35mSection_title: 4.2  The Alternative View of Calibration\n",
            "\u001b[35mSection_title: 4.3  New Metric: MacroCE\n",
            "\u001b[35mSection_title: 4.4  Comparing Different Metrics Through Controlled Experiments\n",
            "\u001b[35mSection_title: 5  Re-Evaluating Other Calibration Methods\n",
            "\u001b[35mSection_title: 5.1  Feature Based Classifier\n",
            "\u001b[35mSection_title: 5.2  Neural Reranker\n",
            "\u001b[35mSection_title: 5.3  Label Smoothing\n",
            "\u001b[35mSection_title: 6  New Calibration Method: Consistency Calibration\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A Model for Temporal Relation Discovery\n",
            "\u001b[35mSection_title: 2.1  Temporal Concept Frame\n",
            "\u001b[35mSection_title: 2.2  Temporal Relations\n",
            "\u001b[35mSection_title: 3  Rule Construction\n",
            "\u001b[35mSection_title: 3.1  General Rules for Temporal\n",
            "\u001b[35mSection_title: 3.3  Rules for Resolving Con icts (R1)\n",
            "\u001b[35mSection_title: 5  Experiment Setup and Error Analysis\n",
            "\u001b[32mconclusion: the issues of mapping linguistic patterns to temporal relations are addressed in the paper. these mapping is preconditioned on the temporal indicators and achieved on a set of pre-de ned rules. the mapping mechanism was validated. on 7429 sentences describing temporal relevance, we achieved 92.77% accuracy in average. these relations will be useful to for information extraction, information retrieval and questionanswering application. once the corresponding frames have been instantiated and their slots lled after temporal natural language processing. the related temporal concepts will be linked together according to their chronological orders, to be applied as the knowledge to ful ll users' queries.\n",
            "\u001b[35mSection_title: 6  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Data\n",
            "\u001b[35mSection_title: 2.1  Symbolic Reasoning\n",
            "\u001b[35mSection_title: 2.2  Memorization\n",
            "\u001b[35mSection_title: 3  BERT Model\n",
            "\u001b[35mSection_title: 4.1.2  Logical Rules\n",
            "\u001b[35mSection_title: 4.2  Pre-trained BERT\n",
            "\u001b[35mSection_title: 4.3  Memorization\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[32mconclusion: this work is a first study towards understanding bert's ability to capture knowledge seen during pre-training by investigating it's reasoning and memorization capabilities. we identified factors influencing what knowledge is stored and what is forgotten and what is learnable beyond knowledge explicitly seen during training. we saw that theoretically bert is able to infer facts not explicitly seen during training via symbolic rules. future work should investigate how to enable bert during pre-training to use this capability. we see the need to incentivize plms to capture symbolic rules and factual knowledge as this could potentially improve plm's performance also on downstream tasks where reasoning capabilities or implicit knowledge leverage is needed.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Biref introduction to SCOOP\n",
            "\u001b[35mSection_title: 3.  The alias calculus\n",
            "\u001b[35mSection_title: 3.1.  Extension to unbounded executions\n",
            "\u001b[35mSection_title: 3.2.  A sound over-approximation\n",
            "\u001b[35mSection_title: 4.  A K-machinery for collecting aliases\n",
            "\u001b[35mSection_title: 4.1.  The K-machinery by example\n",
            "\u001b[35mSection_title: 5.  Aliasing in SCOOP\n",
            "\u001b[35mSection_title: 6.  Deadlocking in SCOOP\n",
            "\u001b[35mSection_title: 6.1.  Formalizing deadlocks in SCOOP\n",
            "\u001b[35mSection_title: 6.2.  Testing deadlocks\n",
            "\u001b[35mSection_title: 6.3.  Model-checking deadlocks\n",
            "\u001b[35mSection_title: 7.  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Generation-Augmented Retrieval\n",
            "\u001b[35mSection_title: 3.3  Retrieval-Augmented Generation\n",
            "\u001b[35mSection_title: 3.4  Generation-Augmented Retrieval Adaptation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Settings\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Implementation Details\n",
            "\u001b[35mSection_title: 4.5  Main Results\n",
            "\u001b[35mSection_title: 4.6  Ablation Study\n",
            "\u001b[35mSection_title: 4.6.1  Generation Augments Retrieval\n",
            "\u001b[35mSection_title: 4.6.2  ITER-RETGEN Leverages Parametric and Non-Parametric Knowledge Better\n",
            "\u001b[35mSection_title: 4.7  Error Analysis\n",
            "\u001b[35mSection_title: 5  Case Study\n",
            "\u001b[32mconclusion: we demonstrate the effectiveness of iter-retgen in answering questions with complex information needs.despite simple, iter-retgen outperforms retrieval-augmented methods that have a more complex workflow, which we believe could serve as a strong baseline for future research on retrieval-augmented generation.we also show that generation-augmented retrieval adaptation can further improve the performance of iter-retgen while also reducing overheads.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Task Definition\n",
            "\u001b[35mSection_title: 2.2  Semantic Role Labeling\n",
            "\u001b[35mSection_title: 2.3  Contextual Encoding\n",
            "\u001b[35mSection_title: 2.4  Inferential Network\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Data and Task Description\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Overall Results\n",
            "\u001b[35mSection_title: 3.4  Ablation Study\n",
            "\u001b[35mSection_title: 3.5  Influence of Semantic Information\n",
            "\u001b[35mSection_title: 3.6  Influence of Inferential Mechanism\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: this work focuses on two typical nlu tasks, machine reading comprehension and natural language inference by refining the use of semantic clues and inferential model. the proposed semantics-aware inferential network (sain) is capable of taking multiple semantic structures as input of an inferential network by closely integrating semantics and reasoning steps in a creative way. experiment results on 11 benchmarks, including 4 nli tasks and 7 mrc tasks, show that our model outperforms all previous strong baselines, which consistently indicate the general effectiveness of our model 6 .\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Works\n",
            "\u001b[35mSection_title: 2.1.  Image Feature Extraction\n",
            "\u001b[35mSection_title: 2.2.  Image Caption Generation\n",
            "\u001b[35mSection_title: 3.  Methods\n",
            "\u001b[35mSection_title: 3.1.  Overview\n",
            "\u001b[35mSection_title: 3.2.  Visual Relationship Extraction\n",
            "\u001b[35mSection_title: 3.2.1  Semantic Visual Relationship\n",
            "\u001b[35mSection_title: 3.2.2  Spatial Visual Relationship\n",
            "\u001b[35mSection_title: 3.3.  Hierarchical Graph Construction\n",
            "\u001b[35mSection_title: 3.4.  Attention in Transformer\n",
            "\u001b[35mSection_title: 3.5.  Training and Inference\n",
            "\u001b[35mSection_title: 4.  Experiments\n",
            "\u001b[35mSection_title: 4.1.  Datasets and Experimental Settings\n",
            "\u001b[35mSection_title: 4.2.  Performance Comparison and Analysis\n",
            "\u001b[35mSection_title: 4.2.1  Quantitative Analysis\n",
            "\u001b[35mSection_title: 4.2.2  Qualitative Analysis\n",
            "\u001b[35mSection_title: 4.2.3  Performance on COCO Online Testing Server\n",
            "\u001b[35mSection_title: 4.2.4  Human Perception Evaluation\n",
            "\u001b[35mSection_title: 5.  Conclutions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Encoder-conditioned Decoders\n",
            "\u001b[35mSection_title: 2.1  Adapter Layers\n",
            "\u001b[35mSection_title: 2.2  Adapter-generating Hypernetworks\n",
            "\u001b[35mSection_title: 2.3  Encoder-conditioned Decoders\n",
            "\u001b[35mSection_title: 2.4  Multi-tasking\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.1.1  GLUE\n",
            "\u001b[35mSection_title: 3.1.3  Summarisation and NLI\n",
            "\u001b[35mSection_title: 3.2  Experimental Details\n",
            "\u001b[35mSection_title: 3.3  Baselines\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  GLUE\n",
            "\u001b[35mSection_title: 4.2  MRQA\n",
            "\u001b[35mSection_title: 4.3  Summarisation & NLI\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Hypernetwork Embeddings\n",
            "\u001b[35mSection_title: 5.2  Ablations\n",
            "\u001b[35mSection_title: 6.1  Multi-task Models\n",
            "\u001b[35mSection_title: 6.2  Parameter-efficient Tuning\n",
            "\u001b[35mSection_title: 6.3  Hypernetwork-based Adaption Methods\n",
            "\u001b[32mconclusion: we propose a novel method for generating adapters conditioned on a model's input and show that this improves performance in multi-task settings across a variety of tasks. we explore the effectiveness of our approach for sequence classification, qa, and summarisation tasks, and find that it often outperforms strong parameter-efficient baselines. future work could examine applying our approach to other architectures (e.g. decoder-only models) or explore the tradeoffs between shared and generated parameters across different layers. an analysis of our approach suggests the primary benefits come from improved control of the encoder over the decoder, enhancing the effects of positive transfer from the shared encoder. this allows our approach to efficiently adapt a pretrained language model to multiple tasks unseen during pretraining while still benefiting strongly from positive transfer.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction: Vagueness of Gradable Adjectives\n",
            "\u001b[35mSection_title: 1.1  Vague Descriptions\n",
            "\u001b[35mSection_title: 1.2  Vagueness in NLG\n",
            "\u001b[35mSection_title: 1.3  Plan of This Article\n",
            "\u001b[35mSection_title: 2.  The Meaning of Vague Descriptions\n",
            "\u001b[35mSection_title: 2.1  Linguistic Motivation\n",
            "\u001b[35mSection_title: 2.2  Caveat: Full NP Anaphora\n",
            "\u001b[35mSection_title: 2.3  Caveat: Evaluative Adjectives\n",
            "\u001b[35mSection_title: 3.  Generation of Crisp Descriptions\n",
            "\u001b[35mSection_title: 3.1  The Incremental Algorithm\n",
            "\u001b[35mSection_title: 3.2  The Existing Treatment of Gradables\n",
            "\u001b[35mSection_title: 4.  The Generation of Vague Descriptions\n",
            "\u001b[35mSection_title: 4.1  Expressing One\n",
            "\u001b[35mSection_title: 4.1.2  Exploiting Numerical Properties, Singular.\n",
            "\u001b[35mSection_title: 4.1.3  Exploiting Numerical Properties, Plural.\n",
            "\u001b[35mSection_title: 4.1.4  Ordering of Properties.\n",
            "\u001b[35mSection_title: 4.1.5  Beyond Content Determination (CD).\n",
            "\u001b[35mSection_title: 4.2  Expressing Several Vague Properties\n",
            "\u001b[35mSection_title: 4.2.1  Descriptions Using (In)equalities.\n",
            "\u001b[35mSection_title: 4.2.2  Adjectives in Superlative and Base Form. To generate descriptions like the ones in examples\n",
            "\u001b[35mSection_title: 4.3  Computational Complexity\n",
            "\u001b[35mSection_title: 5.  Pragmatic Constraints\n",
            "\u001b[35mSection_title: 6.  Linguistic Realization\n",
            "\u001b[35mSection_title: 7.  Empirical Grounding\n",
            "\u001b[35mSection_title: 7.1  Human Speakers' Use of Vague Descriptions\n",
            "\u001b[35mSection_title: 7.2  Testing the Correctness of the Generated Expressions\n",
            "\u001b[35mSection_title: 7.3  Testing the Felicity of the Generated Expressions\n",
            "\u001b[35mSection_title: 8.  Incrementality: Help or Hindrance?\n",
            "\u001b[35mSection_title: 6.  Perform linguistic realization.\n",
            "\u001b[35mSection_title: 8.1  Problems with Incrementality\n",
            "\u001b[35mSection_title: 8.2  Low Preference for Gradable Properties?\n",
            "\u001b[35mSection_title: 9.  Extensions of the Approach\n",
            "\u001b[35mSection_title: 9.1  Relational Descriptions\n",
            "\u001b[35mSection_title: 9.2  Boolean Combinations\n",
            "\u001b[35mSection_title: 9.3.2  Multidimensional Adjectives (and Color).\n",
            "\u001b[35mSection_title: 9.4  Salience as a Gradable Property\n",
            "\u001b[35mSection_title: 9.4.1  A New Perspective on Salience.\n",
            "\u001b[35mSection_title: 9.4.3  Salience as a Multidimensional Property.\n",
            "\u001b[35mSection_title: 9.5.2  Pointing.\n",
            "\u001b[32mconclusion: if the usefulness of nlg resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. in principle, this might be done by providing the generator with vague input-in which case no special algorithms are needed-but suitably contextualized vague input is often not available (mellish 2000). the only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. it is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to gre. far from being a peculiarity of a few adjectives, vagueness is widespread. we believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (section 9.3); r nouns that allow different degrees of strictness (section 9.5); r degrees of salience (section 9.4); and r imprecise pointing (section 9.5).\n",
            "\u001b[35mSection_title: 10.  Conclusion\n",
            "\u001b[35mSection_title: 1.  Introduction: Vagueness of Gradable Adjectives\n",
            "\u001b[35mSection_title: 1.1  Vague Descriptions\n",
            "\u001b[35mSection_title: 1.2  Vagueness in NLG\n",
            "\u001b[35mSection_title: 1.3  Plan of This Article\n",
            "\u001b[35mSection_title: 2.  The Meaning of Vague Descriptions\n",
            "\u001b[35mSection_title: 2.1  Linguistic Motivation\n",
            "\u001b[35mSection_title: 2.2  Caveat: Full NP Anaphora\n",
            "\u001b[35mSection_title: 2.3  Caveat: Evaluative Adjectives\n",
            "\u001b[35mSection_title: 3.  Generation of Crisp Descriptions\n",
            "\u001b[35mSection_title: 3.1  The Incremental Algorithm\n",
            "\u001b[35mSection_title: 3.2  The Existing Treatment of Gradables\n",
            "\u001b[35mSection_title: 4.  The Generation of Vague Descriptions\n",
            "\u001b[35mSection_title: 4.1  Expressing One\n",
            "\u001b[35mSection_title: 4.1.2  Exploiting Numerical Properties, Singular.\n",
            "\u001b[35mSection_title: 4.1.3  Exploiting Numerical Properties, Plural.\n",
            "\u001b[35mSection_title: 4.1.4  Ordering of Properties.\n",
            "\u001b[35mSection_title: 4.1.5  Beyond Content Determination (CD).\n",
            "\u001b[35mSection_title: 4.2  Expressing Several Vague Properties\n",
            "\u001b[35mSection_title: 4.2.1  Descriptions Using (In)equalities.\n",
            "\u001b[35mSection_title: 4.2.2  Adjectives in Superlative and Base Form. To generate descriptions like the ones in examples\n",
            "\u001b[35mSection_title: 4.3  Computational Complexity\n",
            "\u001b[35mSection_title: 5.  Pragmatic Constraints\n",
            "\u001b[35mSection_title: 6.  Linguistic Realization\n",
            "\u001b[35mSection_title: 7.  Empirical Grounding\n",
            "\u001b[35mSection_title: 7.1  Human Speakers' Use of Vague Descriptions\n",
            "\u001b[35mSection_title: 7.2  Testing the Correctness of the Generated Expressions\n",
            "\u001b[35mSection_title: 7.3  Testing the Felicity of the Generated Expressions\n",
            "\u001b[35mSection_title: 8.  Incrementality: Help or Hindrance?\n",
            "\u001b[35mSection_title: 6.  Perform linguistic realization.\n",
            "\u001b[35mSection_title: 8.1  Problems with Incrementality\n",
            "\u001b[35mSection_title: 8.2  Low Preference for Gradable Properties?\n",
            "\u001b[35mSection_title: 9.  Extensions of the Approach\n",
            "\u001b[35mSection_title: 9.1  Relational Descriptions\n",
            "\u001b[35mSection_title: 9.2  Boolean Combinations\n",
            "\u001b[35mSection_title: 9.3.2  Multidimensional Adjectives (and Color).\n",
            "\u001b[35mSection_title: 9.4  Salience as a Gradable Property\n",
            "\u001b[35mSection_title: 9.4.1  A New Perspective on Salience.\n",
            "\u001b[35mSection_title: 9.4.3  Salience as a Multidimensional Property.\n",
            "\u001b[35mSection_title: 9.5.2  Pointing.\n",
            "\u001b[32mconclusion: if the usefulness of nlg resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. in principle, this might be done by providing the generator with vague input-in which case no special algorithms are needed-but suitably contextualized vague input is often not available (mellish 2000). the only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. it is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to gre. far from being a peculiarity of a few adjectives, vagueness is widespread. we believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (section 9.3); r nouns that allow different degrees of strictness (section 9.5); r degrees of salience (section 9.4); and r imprecise pointing (section 9.5).\n",
            "\u001b[35mSection_title: 10.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Task Formulation\n",
            "\u001b[35mSection_title: 2.2  RL-based Sentence Selection\n",
            "\u001b[35mSection_title: 2.3  Deduction Generation\n",
            "\u001b[35mSection_title: 2.4  Overall Model Training\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 3.3  Implementation Details\n",
            "\u001b[35mSection_title: 3.4  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4  Results & Analysis\n",
            "\u001b[35mSection_title: 4.1  Main Results\n",
            "\u001b[35mSection_title: 4.2  Ablation Study\n",
            "\u001b[35mSection_title: 4.3  In-hoc Reasoning\n",
            "\u001b[35mSection_title: 4.4  Data Efficiency\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we presented rlet, a rl-based entailment tree generation framework, which contains sentences selection and deduction generation modules and can be trained with cumulative signals across the entire reasoning tree. experiments show that rlet outperforms existing baselines on structure correctness and is applicable in practical scenarios. future directions include applying rl framework on other stepwise methods with more stable and sophisticated rl algorithms.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Framework\n",
            "\u001b[35mSection_title: 3.1  Document Selection Module\n",
            "\u001b[35mSection_title: 3.2  Question Answering Module\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Experimental Results\n",
            "\u001b[35mSection_title: 4.4  Ablation\n",
            "\u001b[32mconclusion: we propose fe2h, a simple yet effective framework for multi-hop qa that divides both the document selection and question answering into two stages following an easy-to-hard manner. experimental results demonstrate that since we cannot feed all of the candidate documents to the plms at a time due to the input length limitation, taking the multi-hop reasoning nature into consideration at the document selection phase significantly improves the overall performance. as for the subsequent qa phase, thanks to the great natural language understanding ability of the plms, the performance of our simple two-stage reader is better than the stateof-the-art approaches without any graph structure and explicit reasoning chains. we hope this work could facilitate more simple yet powerful multihop qa approaches with the help of the advanced plms.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Conditional Self-Attention\n",
            "\u001b[35mSection_title: 4  Query-based Summarization Model\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Abstractive Query-based Summarization\n",
            "\u001b[35mSection_title: 5.2  Extractive Query-based Summarization\n",
            "\u001b[32mconclusion: this paper introduces conditional self-attention (csa) and its variants as versatile and plugin mod-ules for conditional contextual dependency modeling. we develop an attention-only neural network built from csa and transformer for querybased summarization. it consistently outperforms vanilla transformer and other baselines for abstractive and extractive qsumm tasks on debatepedia and hotpotqa datasets.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 1.  Human generates question q and\n",
            "\u001b[35mSection_title: 3  Annotation Methodology\n",
            "\u001b[35mSection_title: 3.1  Annotation Protocol\n",
            "\u001b[35mSection_title: 3.2  Annotation Details\n",
            "\u001b[35mSection_title: 3.3  Quality Control\n",
            "\u001b[35mSection_title: 3.4  Dataset Statistics\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Consistency of the Model in the Loop\n",
            "\u001b[35mSection_title: 4.2  Adversarial Generalisation\n",
            "\u001b[35mSection_title: 4.3  Generalisation to Non-Adversarial Data\n",
            "\u001b[35mSection_title: 4.4  Generalisation to DROP and NaturalQuestions\n",
            "\u001b[35mSection_title: 5  Qualitative Analysis\n",
            "\u001b[35mSection_title: 5.1  Comprehension Requirements\n",
            "\u001b[35mSection_title: 5.2  Observations\n",
            "\u001b[32mconclusion: we have in this work investigated an rc annotation paradigm which includes a model in the loop that has to be \"beaten\" by the annotator. applying this approach with a series of progressively stronger rc models in the annotation loop, we arrived at three separate rc datasets, graduated by the difficulty of the model adversary. based on this dataset series we investigated several questions surrounding the annotation paradigm, in particular whether such datasets grow outdated as stronger models emerge, and about their generalisation to standard (non-adversarially collected) questions. we found that stronger rc models can still learn from data collected with a weak adversary in the loop, and their generalisation improves even on datasets collected with a very strong adversary. models trained on data collected with a model in the loop furthermore generalise well towards nonadversarially collected data, both on squad and on naturalquestions, yet we observe a slow deterioration with progressively stronger adversaries.\n",
            "\u001b[35mSection_title: 6  Discussion and Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Data Analysis\n",
            "\u001b[35mSection_title: 3.1  Question Types\n",
            "\u001b[35mSection_title: 3.2  Answer Types\n",
            "\u001b[35mSection_title: 3.3  Inference Types\n",
            "\u001b[35mSection_title: 4  Model\n",
            "\u001b[35mSection_title: 4.1  Table-Only Model\n",
            "\u001b[35mSection_title: 4.2  Passage-Only Model\n",
            "\u001b[35mSection_title: 4.3  HYBRIDER\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Setting\n",
            "\u001b[35mSection_title: 5.2  Evaluation\n",
            "\u001b[35mSection_title: 5.3  Experimental Results\n",
            "\u001b[35mSection_title: 5.4  Error Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we present hybridqa, which is collected as the first hybrid question answering dataset over both tabular and textual data. we release the data to facilitate the current research on using heterogeneous information to answer real-world questions. we design hybrider as a strong baseline and offer interesting insights about the model. we believe hybridqa is an interesting yet challenging nextproblem for the community to solve.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  UKP-SQuARE\n",
            "\u001b[35mSection_title: 3  Learning Question Answering with UKP-SQuARE\n",
            "\u001b[35mSection_title: 3.1  Learning Basic QA Components\n",
            "\u001b[35mSection_title: 3.1.1  Contrasting Different QA Formats\n",
            "\u001b[35mSection_title: 3.1.2  Learning Information Retrieval\n",
            "\u001b[35mSection_title: 3.2  Learning Trustworthy QA Systems\n",
            "\u001b[35mSection_title: 3.2.1  Explainability Methods\n",
            "\u001b[35mSection_title: 3.2.2  Behavioral Tests in QA models\n",
            "\u001b[35mSection_title: 3.2.3  Adversarial Attacks\n",
            "\u001b[35mSection_title: 3.2.4  Graph-based QA Models\n",
            "\u001b[35mSection_title: 3.3  Learning Multi-Agent Systems\n",
            "\u001b[35mSection_title: 3.4  Assignments with UKP-SQuARE\n",
            "\u001b[35mSection_title: 3.5  User Study\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present a novel method to teach question-answering to postgraduate nlp students following the learner-centered method of flipped classrooms. we propose to provide reading materials to the students before the class and use the ukp-square platform as a driving tool to conduct the class. this platform integrates the most popular qa pipelines and an ecosystem of tools to analyze the available models. these tools include explainability methods, behavioral tests, adversarial attacks, and graph visualizations. we provide a series of use cases for teaching based on the provided models and methods by ukp-square, showing that classes can become much more interactive by using ukp-square than in conventional lectures.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Preliminary\n",
            "\u001b[35mSection_title: 2.2  Relevance-Aware Contrastive Learning\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  Main Results\n",
            "\u001b[35mSection_title: 3.2.1  BEIR\n",
            "\u001b[35mSection_title: 3.2.2  Open-Domain QA Retrieval\n",
            "\u001b[35mSection_title: 3.3  Practical Use Cases\n",
            "\u001b[35mSection_title: 3.4  Ablation Study\n",
            "\u001b[32mconclusion: in this work, we propose recontriever to further explore the potential of contrastive pre-training to reduce the demand of human-annotated data for dense retrievers. benefiting from multiple positives from the same document as well as relevance-aware contrastive loss, our model achieves remarkable performance under zero-shot cases. additional results on low data resources further verify its value under various practical scenarios.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Setting\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Generating and Updating Graphs\n",
            "\u001b[35mSection_title: 3.1.2  Relative Position\n",
            "\u001b[35mSection_title: 3.1.3  Semantic Role Labeling (Parser-based)\n",
            "\u001b[35mSection_title: 3.2  Encoding Graph Representations\n",
            "\u001b[35mSection_title: 3.3  Aggregating Multiple Modalities\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 6  Broader Impact\n",
            "\u001b[32mconclusion: we explore to leverage graph representations in the challenging imrc tasks. we investigate different categories of graph structures that can capture text information at various levels. we describe methods that dynamically generate the graphs during information gathering. experiment results show that graph representations provide consistent improvement across settings. this evinces our hypothesis that graph representations are proper inductive biases in imrc. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Study Design\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 5  Qualitative Analysis\n",
            "\u001b[32mconclusion: in this paper, we demonstrated that across a variety of models and datasets, training on adversarial data leads to better performance on evaluation sets created in a similar fashion, but tends to yield worse performance on out-of-domain evaluation sets not created adversarially. additionally, our results suggest that the adc process (regardless of the outcome) might matter more than successfully fooling a model. we also identify key qualitative differences between data generated via adc and sdc, particularly the kinds of questions created.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Challenge Task Definition\n",
            "\u001b[35mSection_title: 4  Dataset: COMMAQA Benchmark\n",
            "\u001b[35mSection_title: 4.1  Agent Definition\n",
            "\u001b[35mSection_title: 4.2  Complex Task Definition\n",
            "\u001b[35mSection_title: 4.3  Auxiliary Information\n",
            "\u001b[35mSection_title: 4.4  COMMAQA Dataset\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Models\n",
            "\u001b[35mSection_title: 5.1.1  COMMAQA Baseline Model\n",
            "\u001b[35mSection_title: 5.1.2  Auxiliary Supervision Models\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Compositional Generalization\n",
            "\u001b[35mSection_title: 6  Closing Remarks\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  The MVP Model\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Model Architecture\n",
            "\u001b[35mSection_title: 3.3  Training Details\n",
            "\u001b[35mSection_title: 4  Experiment Results\n",
            "\u001b[35mSection_title: 4.1  Full Tuning Performance\n",
            "\u001b[35mSection_title: 4.2  Zero-shot Performance\n",
            "\u001b[35mSection_title: 4.3  Generality to Unseen Tasks\n",
            "\u001b[35mSection_title: 4.4  Parameter-Efficient Tuning Performance\n",
            "\u001b[35mSection_title: 4.5  Human Evaluation\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[32mconclusion: in this paper, we present multi-task supervised pre-training (mvp) for natural language generation. firstly, we collect a large-scale nlg corpus, mvpcorpus, from 77 datasets over 11 diverse nlg tasks. after converting various nlg tasks into a unified text-to-text format, we propose multi-task supervised pre-training to learn an effective and general model mvp with task-specific prompts for nlg tasks. extensive experiments have demonstrated that: (1) supervised pre-training is beneficial for nlg tasks as an effective solution.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Problem Statement\n",
            "\u001b[35mSection_title: 1.2  Prior Research\n",
            "\u001b[35mSection_title: 1.2.1  Model Development\n",
            "\u001b[35mSection_title: 1.2.2  Actual Data Generation\n",
            "\u001b[35mSection_title: 1.2.3  Synthetic Data Generation\n",
            "\u001b[35mSection_title: 1.3  Objective and Contributions\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Model\n",
            "\u001b[35mSection_title: 2.2  Data\n",
            "\u001b[35mSection_title: 2.3  Metrics\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[32mconclusion: this paper studies the impact of augmenting human-made data with synthetic data on the task of extractive question answering by using bert (devlin et al., 2018) as the language model and squad 2.0 (rajpurkar et al., 2018) as the baseline dataset. two sets of synthetic data are used for augmenting the baseline data: a set of answerable and another set of unanswerable questionsanswers. conducted experiments show that using both these synthetic datasets can tangibly improve the performance of the selected language model for eqa, while the unans data, generated by the authors, has a more pronounced impact on improving the performance. adding the unans dataset to the original data yields a gain of 5% in both f1 and em scores, whereas the ans dataset yields around a quarter of this gain. enhancing the original data with a combination of the two synthetic datasets improves the f1 score of bert on the test-set by 7% and the em score by 5% which are sizable improvements compared to the performance of the baseline models and similar efforts in the literature. the obtained results indicate the great potential of using synthetic data to complement the costly human-generated datasets: this augmentation can help provide the massive data required for training the modern language models at a very low cost.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\u001b[32mlimitation: the presented approach has limitations similar to (lewis et al., 2019b): although we tried to avoid using any human-labeled data for generating the synthetic question-answers, the questiongenerating models rely on manually-labeled data from ontonotes 5 (for ner system) and penn treebank (for extracting subclauses). further, the question-generation pipeline of this work uses english language-specific heuristics. hence, the applicability of this approach is limited to languages and domains that already have a certain amount of human-labeled data for question generation, and porting this model to another language would require extra preparatory efforts. an extensive amount of training examples are required to achieve tangible performance gains, and this results in substantial training times and compute costs for both generating synthetic data and training the bert model. these high training times and resource costs prevented us from performing the experiments on the full squad 2.0 dataset. nonetheless, given the homogeneity of the original dataset, we expect the synthetic training examples to bring similar performance improvements if added to the full dataset with similar proportions.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\u001b[35mSection_title: 7  Future Work\n",
            "\u001b[35mSection_title: 8  Acknowledgments\n",
            "\u001b[35mSection_title: 9  Authorship Statements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCCIÓN\n",
            "\u001b[35mSection_title: 2.  MÉTODO Y MATERIALES\n",
            "\u001b[35mSection_title: 3.  RESULTADOS\n",
            "\u001b[35mSection_title: 4.  DISCUSIONES\n",
            "\u001b[32mconclusion: primero, se identificaron que los atributos del perfil docente establecidos por la institución enfocado por competencias son planificador, mediador, agente cultural y promotor los cuales se miden a través de los atributos del desempeño docente que son mediador, planificador, evaluador, promotor y agente cultural.\n",
            "\u001b[35mSection_title: 5.  CONCLUSIONES\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Unsupervised training on unaligned documents\n",
            "\u001b[35mSection_title: 3.1.1  Contrastive learning\n",
            "\u001b[35mSection_title: 3.1.2  Building positive pairs from a single document\n",
            "\u001b[35mSection_title: 3.1.3  Building large set of negative pairs\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Multilingual retrieval\n",
            "\u001b[35mSection_title: 5.1  Multilingual pre-training\n",
            "\u001b[35mSection_title: 5.2  Fine-tuning\n",
            "\u001b[35mSection_title: 5.3  Evaluation\n",
            "\u001b[35mSection_title: 5.4  Baselines\n",
            "\u001b[35mSection_title: 5.5  Results\n",
            "\u001b[35mSection_title: 6  Ablation studies\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  DECOMPOSED PROMPTING\n",
            "\u001b[35mSection_title: 3.1  DECOMPOSED PROMPTS\n",
            "\u001b[35mSection_title: 3.2  PROMPT EXECUTION AND INFERENCE\n",
            "\u001b[35mSection_title: 3.3  DECOMP CAPABILITIES\n",
            "\u001b[35mSection_title: 4.2  LIST REVERSAL (RECURSIVE DECOMPOSITION)\n",
            "\u001b[35mSection_title: 4.3  LONG-CONTEXT QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 4.5  ADDITIONAL RESULTS\n",
            "\u001b[32mconclusion: we proposed a new approach, decomposed prompting, to solve complex tasks using few-shot prompts, by decomposing them into a prompting program built out of simpler sub-tasks. drawing inspiration from software libraries, our decomposer and shared sub-tasks are designed in a modular fashion: they use their own few-shot prompts, allowing one to independently optimize each prompt, decompose a sub-task further if necessary, or even seamlessly replace it with a symbolic system. we show that decomposed prompting outperforms prior work on four different tasks and generalization settings, establishing it as an effective few-shot paradigm for solving complex tasks. we treat the number of paragraphs to retrieve (k) in nodecomp-ctxt and decomp-ctxt models as a hyperparameter. we select it based on a grid search on a set of values to maximize performance on a held out set of 100 questions for each dataset. for nodecomp-ctxt, we search k ∈ {6, 8, 10} for gpt3 models and k ∈ 2, 4, 6, 8 for flan-t5-* models. for decomp-ctxt, we search k ∈ {2, 4, 6} for gpt3 and flan-t5-* models. note that the ranges are different between gpt3 and flan-t5-* as gpt3 can fit in more number of tokens. the ranges are different for nodecomp-ctxt and decomp-ctxt as k refers to number of paragraphs retrieved in each round of retrieval, and nodecomp-ctxt has only one step of retrieval whereas decomp-ctxt usually has multiple retrieval steps.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Predicting Beneficial Transfer Sources\n",
            "\u001b[35mSection_title: 3  Adapter-Based Sequential Transfer\n",
            "\u001b[35mSection_title: 3.1  Tasks\n",
            "\u001b[35mSection_title: 3.2  Experimental Setup\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 4  Methods for the Efficient Selection of Intermediate Tasks\n",
            "\u001b[35mSection_title: 4.1  Metadata: Educated Guess\n",
            "\u001b[35mSection_title: 4.2  Intermediate Task Data\n",
            "\u001b[35mSection_title: 4.4  Intermediate Model and Task Data\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Hyperparameters\n",
            "\u001b[35mSection_title: 5.2  Metrics\n",
            "\u001b[32mconclusion: in this work we have established that intermediate pre-training can yield gains in adapter-based setups, however, around 44% of all transfer combinations result in decreased performances. we have consolidated several existing and new methods for efficiently identifying beneficial intermediate tasks.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Pre-Finetuning Through Massive Multitask Learning\n",
            "\u001b[35mSection_title: 3.1  Tasks and Losses\n",
            "\u001b[35mSection_title: 3.2  Optimization\n",
            "\u001b[35mSection_title: 3.3  Loss Scaling\n",
            "\u001b[35mSection_title: 3.4  Sampling\n",
            "\u001b[35mSection_title: 3.5  Experimental Setup\n",
            "\u001b[35mSection_title: 4  Empirical Results\n",
            "\u001b[35mSection_title: 4.1  Finetuning Outside of Pre-Finetuning Domain\n",
            "\u001b[35mSection_title: 5  Understanding Multi-Task at Scale\n",
            "\u001b[35mSection_title: 5.1  Importance of Scale\n",
            "\u001b[35mSection_title: 5.2  Importance of Heterogenous Batches\n",
            "\u001b[35mSection_title: 5.3  Low Resource Experiments\n",
            "\u001b[32mconclusion: in this work, we propose pre-finetuning, a stage after pre-training to further refine representations before end-task finetuning. we show that we can effectively learn more robust representations through multi-task learning (mtl) at scale. our mtl models outperform their vanilla pre-trained counterparts across several tasks. our analysis shows that properly scaling mtl with heterogeneous batches and loss scaling is critical to leveraging better representations. we also show a critical point regarding the number of tasks when doing multi-task learning, where fewer tasks degrade representations compared to the pre-trained model, but more tasks than this point improve representations.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Encoding Module\n",
            "\u001b[35mSection_title: 3.2  Evidence Detector\n",
            "\u001b[35mSection_title: 3.3  Evidence-Emphasized Reasoning Graph\n",
            "\u001b[35mSection_title: 3.4  Prediction Module\n",
            "\u001b[35mSection_title: 4  Training with Distant Supervision\n",
            "\u001b[35mSection_title: 5  Experiment\n",
            "\u001b[35mSection_title: 5.1  Dataset and Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.2  Experiment Settings\n",
            "\u001b[35mSection_title: 5.3  Baselines\n",
            "\u001b[35mSection_title: 5.4  Main Results\n",
            "\u001b[35mSection_title: 5.5  Ablation Study\n",
            "\u001b[35mSection_title: 5.6  Case Study\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary\n",
            "\u001b[35mSection_title: 2.1  The Transformer\n",
            "\u001b[35mSection_title: 2.2  Self-Attention Head Importance\n",
            "\u001b[35mSection_title: 2.3  The Lottery Ticket Hypothesis\n",
            "\u001b[35mSection_title: 2.4  Reading Comprehension Task and Domain Variance\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Identifying the Lottery Network\n",
            "\u001b[35mSection_title: 3.2  Adapting the Lottery Subnetwork\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 5  Results and Analyses\n",
            "\u001b[35mSection_title: 5.1  Domain Adaptation Results\n",
            "\u001b[35mSection_title: 5.2  Analyses\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose alter, a simple and effective domain adaptation paradigm for few-shot reading comprehension. we exploit a small fraction of parameters of the over-parameterized source domain model to adapt to the target domain by first identifying and then fine-tuning the lottery subnetwork. we introduce self-attention attribution, an interpreting method for transformer, to identify better subnetworks and improve the target domain performance. further exploration on using several heuristic methods to reveal subnetwork structures find that subnetwork structures are critical to the effectiveness besides using fewer parameters. \n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Continuous problem\n",
            "\u001b[35mSection_title: 3  Notations and preliminary theoretical results\n",
            "\u001b[35mSection_title: 4  Stabilized finite element method\n",
            "\u001b[35mSection_title: 4.1  Discrete spaces and interpolation analysis\n",
            "\u001b[35mSection_title: 4.2  Discrete forms\n",
            "\u001b[35mSection_title: 4.3  Discrete scheme\n",
            "\u001b[35mSection_title: 5.1  Stability analysis\n",
            "\u001b[35mSection_title: 5.2  Error analysis\n",
            "\u001b[35mSection_title: 6  Numerical Experiments\n",
            "\u001b[35mSection_title: 6.1  Fluid and magneto convective dominant regime\n",
            "\u001b[35mSection_title: 6.2  Magneto convective dominant regime\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Pre-training\n",
            "\u001b[35mSection_title: 2.2  Fine-tuning\n",
            "\u001b[35mSection_title: 2.3  Aligning the fine-tuning with pre-training\n",
            "\u001b[35mSection_title: 3  Modeling details 3.1 Architecture\n",
            "\u001b[35mSection_title: 3.2  Input-output design & fine-tuning objective\n",
            "\u001b[35mSection_title: 3.3  Generation strategy\n",
            "\u001b[35mSection_title: 3.4  Answer extraction\n",
            "\u001b[35mSection_title: 3.5  Multilingual extension\n",
            "\u001b[35mSection_title: 3.6  Hyperparameters\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Comparing the standard vs aligned fine-tuning framework\n",
            "\u001b[35mSection_title: 4.3  Few-shot results\n",
            "\u001b[35mSection_title: 4.4  Choice of input-outputs and fine-tuning objectives\n",
            "\u001b[35mSection_title: 4.5  Multilingual results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we present an effective few-shot question answering (qa) system that combines the use of pre-trained text-to-text models and a fine-tuning framework aligned with their pre-training counterpart. through experimental studies on various qa benchmarks and few-shot configurations, we show that this system can produce significant gains including in scenarios where the training data is extremely scarce (an absolute gain of 34 f1 points on average in comparison to the current standard of the fine-tuning framework). we also present extensions to multilingual and larger model settings and show that the gains translate well to these settings (eg:-up to an absolute 40 f1 point gain in comparison to xlm-roberta + a span-selection objective). through ablation studies, we study the impact of model size, fine-tuning objectives, inputoutput design and illustrate the factors leading to such strong gains. for future, as our framework doesn't explicitly enforce the answer to be a span in the input text, it'd be interesting to consider its applications to generative qa tasks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Iterative Justification Retrieval\n",
            "\u001b[35mSection_title: 3.2  Answer Classification\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Evidence Selection Results\n",
            "\u001b[35mSection_title: 4.3  Question Answering Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Semantic Drift Analysis\n",
            "\u001b[35mSection_title: 5.2  Robustness to Hyper Parameters\n",
            "\u001b[35mSection_title: 5.3  Saturation of Supervised Learning\n",
            "\u001b[32mconclusion: we introduced a simple, unsupervised approach for evidence retrieval for question answering. our approach combines three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using glove embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, and (c) a simple stopping condition that concludes the iterative process when all terms in the given question and candidate answers are covered by the retrieved justifications. overall, despite its simplicity, unsupervised nature, and its sole reliance on glove embeddings, our approach outperforms all previous methods (including supervised ones) on the evidence selection task on two datasets: multirc and qasc. when these evidence sentences are fed into a roberta answer classification component, we achieve the best qa performance on these two datasets. further, we show that considerable improvements can be obtained by aggregating knowledge from parallel evidence chains retrieved by our method. in addition of improving qa, we hypothesize that these simple unsupervised components of air will benefit future work on supervised neural iterative retrieval approaches by improving their query reformulation algorithms and termination criteria.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Motivation\n",
            "\u001b[35mSection_title: 4  Our Method\n",
            "\u001b[35mSection_title: 4.1  Unit Segmentation and Annotation\n",
            "\u001b[35mSection_title: 4.2  Training for Generalization\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Setups\n",
            "\u001b[35mSection_title: 5.2  Retrieval Performance\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.4  Model Analysis\n",
            "\u001b[32mconclusion: in this paper, we propose an effective method called berm to improve the generalization ability of dense retrieval without target domain data and additional modules. the basic idea of berm is learning the domain-invariant feature, that is, matching signal. to achieve it, we introduce a novel concept of dense retrieval to represent the matching information between two texts, the matching representation. further, we propose two requirements for matching and text representations as the constraint in the training of dense retrieval to enhance the ability to extract essential matching information from the passage according to different queries under the premise of balanced expression of the text. the two requirements unlock the ability of dense retrieval to capture matching signal without additional interaction. experimental results show that berm is a flexible method that can be combined with different dense retrieval training methods without inference overhead to improve the out-of-domain generalization ability. in domain adaptation setting, our method is also effective and performs better than baselines.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  Intrinsic Prompt Tuning\n",
            "\u001b[35mSection_title: 4  Experiment and Analysis\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Analyses and Properties\n",
            "\u001b[32mconclusion: we study the hypothesis that plm adaptations to various tasks can be reparameterized as optimizations within a unified low-dimensional intrinsic task subspace. we develop an analysis tool ipt. it first finds a subspace by jointly decomposing the adaptive parameters of multiple tasks and then tunes parameters within the subspace for unseen data and tasks. experiments show the found subspaces contain good solutions for plm adaptations, which is strong evidence for our hypothesis.\n",
            "\u001b[35mSection_title: 5  Conclusion and Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Question Decomposition Formalism\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.1  Quantitative Analysis\n",
            "\u001b[35mSection_title: 4.2  Quality Analysis\n",
            "\u001b[35mSection_title: 4.3  Annotation Consistency\n",
            "\u001b[35mSection_title: 5  QDMR for Open-domain QA\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 6  QDMR for Semantic Parsing\n",
            "\u001b[35mSection_title: 7  QDMR Parsing\n",
            "\u001b[35mSection_title: 7.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 7.2  QDMR Parsing Models\n",
            "\u001b[35mSection_title: 7.3  Results\n",
            "\u001b[35mSection_title: 8  Related Work\n",
            "\u001b[32mconclusion: in this paper, we presented a formalism for question understanding. we have shown it is possible to train crowd-workers to produce such representations with high quality at scale, and created break, a benchmark for question decomposition with over 83k decompositions of questions from 10 datasets and 3 modalities (db, images, text). we presented the utility of qdmr for both open-domain question answering and semantic parsing, and constructed a qdmr parser with reasonable performance. qdmr proposes a promising direction for modeling question understanding, which we believe will be useful for multiple tasks in which reasoning is probed through questions.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Data Preparation\n",
            "\u001b[35mSection_title: 2.2  Question Generation\n",
            "\u001b[35mSection_title: 2.3  Question Answering\n",
            "\u001b[35mSection_title: 2.4  Query Generation\n",
            "\u001b[35mSection_title: 2.5  Query Verification\n",
            "\u001b[35mSection_title: 2.6  Extend to Fact Verification\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  Result\n",
            "\u001b[35mSection_title: 3.3  Analysis\n",
            "\u001b[32mconclusion: we propose a few-shot data synthesis framework to training smaller models for efficient open domain multi-hop question answering with less than 10 human annotations. our framework consists of generation functions parameterized by llms and prompts, which requires less hand-crafted features than prior work while still achieving better performance. we show that our approach is general by extending to fact verification tasks. in experiments, we benchmark our approach on three multihop question answering and one fact verification benchmarks. the results show that our approach leads to significantly better models that rival the performance of previous methods employing models nearly three times larger in terms of parameter counts. the analysis shows the importance of the filtering steps and our approach benefits models of various sizes.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: we highlight three limitations on our work: (1) our approach depends on synthesizing large amounts of data, which are expensive even if we used llama 65b which are much smaller than palm 540b and gpt-3.5;(2) our approach finetunes language models and thus is not applicable to the closedsource language models, e.g., gpt-3 and palm; and (3) our approach depends on the availability of powerful llms for synthesizing finetuning data. \n",
            "\u001b[35mSection_title: 6  Limitation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work:\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Dataset Pruning:\n",
            "\u001b[35mSection_title: 3.2  Dataset creation:\n",
            "\u001b[35mSection_title: 4  Experimental Protocol\n",
            "\u001b[35mSection_title: 4.1  Exploratory Analysis:\n",
            "\u001b[35mSection_title: 4.2  Proposed Experiments:\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related work\n",
            "\u001b[35mSection_title: 3.  Dataset collection & analysis\n",
            "\u001b[35mSection_title: 3.1.  Annotation process\n",
            "\u001b[35mSection_title: 3.2.  Statistics\n",
            "\u001b[35mSection_title: 3.3.  Challenges raised by adversarial questions\n",
            "\u001b[35mSection_title: 4.  Evaluation metrics\n",
            "\u001b[35mSection_title: 5.  French monolingual experiments\n",
            "\u001b[35mSection_title: 5.1.  Baselines\n",
            "\u001b[35mSection_title: 5.2.  Comparison with FQuAD1.1 scores\n",
            "\u001b[35mSection_title: 5.3.  Learning curves\n",
            "\u001b[35mSection_title: 5.4.  Baseline performances by question category\n",
            "\u001b[35mSection_title: 6.  Multilingual experiments\n",
            "\u001b[32mconclusion: in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions.\n",
            "\u001b[35mSection_title: 7.  Conclusion & future work\n",
            "\u001b[35mSection_title: 8.  Acknowledgments\n",
            "\u001b[35mSection_title: 9.  Bibliographical References\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related work\n",
            "\u001b[35mSection_title: 3.  Dataset collection & analysis\n",
            "\u001b[35mSection_title: 3.1.  Annotation process\n",
            "\u001b[35mSection_title: 3.2.  Statistics\n",
            "\u001b[35mSection_title: 3.3.  Challenges raised by adversarial questions\n",
            "\u001b[35mSection_title: 4.  Evaluation metrics\n",
            "\u001b[35mSection_title: 5.  French monolingual experiments\n",
            "\u001b[35mSection_title: 5.1.  Baselines\n",
            "\u001b[35mSection_title: 5.2.  Comparison with FQuAD1.1 scores\n",
            "\u001b[35mSection_title: 5.3.  Learning curves\n",
            "\u001b[35mSection_title: 5.4.  Baseline performances by question category\n",
            "\u001b[35mSection_title: 6.  Multilingual experiments\n",
            "\u001b[32mconclusion: in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions.\n",
            "\u001b[35mSection_title: 7.  Conclusion & future work\n",
            "\u001b[35mSection_title: 8.  Acknowledgments\n",
            "\u001b[35mSection_title: 9.  Bibliographical References\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Autoregressive Document Retrieval\n",
            "\u001b[35mSection_title: 3  Data Efficient Document Retrieval\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Evaluation\n",
            "\u001b[35mSection_title: 4.2  Implementation\n",
            "\u001b[35mSection_title: 5  Results and Discussion\n",
            "\u001b[35mSection_title: 5.1  Pre-training Intrinsic Evaluation\n",
            "\u001b[35mSection_title: 5.2  Downstream Extrinsic Evaluation\n",
            "\u001b[32mconclusion: we show that distant supervision and pre-training enables high precision autoregressive document retrieval with fewer annotated training data. while previous work has studied the utility of pre-training   (2020)).\n",
            "\u001b[35mSection_title: 6  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The Reading Comprehension Task\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Multi-Dataset Knowledge Distillation\n",
            "\u001b[35mSection_title: 3.2  Augmenting KD with Synthetic Questions\n",
            "\u001b[35mSection_title: 3.3  Domain-Invariant Learning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: this paper puts forward the view with empirical evidence for qa that contrary to popular belief, multi-source domain generalization (dg) is better modeled as a problem of addressing model underfitting than overfitting. our experimental results show that by simply learning the training domains well, even when the number of such domains is relatively small, strong out-of-domain generalization can be achieved without the need for cross-domain regularization. we rely on knowledge distillation in our experiments for improved source domain learning over erm. in light of these findings, we believe that focusing our efforts on adequately fitting the source domain patterns might be a more reasonable path forward for dg. that said, further research is needed on the topic before a definitive conclusion can be reached; we hope that our work will inspire future explorations of this problem.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 6  Ethics\n",
            "\u001b[32mlimitation: we explore the problem of multi-source domain generalization (dg) in qa with new and existing methods. we believe that our findings will generalize to more baselines and datasets, but here we only show proof of concept for a select set of existing baselines and a single dg benchmark (which consists of multiple datasets from various domains).\n",
            "\u001b[35mSection_title: 6.1  Limitations\n",
            "\u001b[35mSection_title: 6.2  Risks\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Datasets and Models\n",
            "\u001b[35mSection_title: 4  ChatGPT's Failures\n",
            "\u001b[35mSection_title: 5  Abilities Behind Errors\n",
            "\u001b[35mSection_title: 6  Methods\n",
            "\u001b[35mSection_title: 6.1  Knowledge Memorization\n",
            "\u001b[35mSection_title: 6.2  Knowledge Association\n",
            "\u001b[35mSection_title: 6.3  Knowledge Reasoning\n",
            "\u001b[35mSection_title: 7.2  Providing relevant background aids in associating key knowledge\n",
            "\u001b[35mSection_title: 7.3  Problem decomposition benefits reasoning, especially with explicit decomposition\n",
            "\u001b[35mSection_title: 8  Towards Faithfulness in Question Answering\n",
            "\u001b[32mconclusion: our paper investigates the common failures of chat-gpt in complex open-domain question answering. we identify four types of errors: comprehension, factualness, specificity, and inference. we also examine the key abilities knowledge memorization, knowledge association, and knowledge reasoning, which are critical to these failures. additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on association, and the effect of decomposition on reasoning. finally, we suggest several techniques to help users more effectively use chat-gpt as a question-answering tool and enable system builders to develop better qa systems. our research contributes to the understanding of what influencing the faithfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more efficient and reliable language models.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Intermediate Annotations\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  Cost vs Benefit\n",
            "\u001b[35mSection_title: 3.3  Bias Evaluation\n",
            "\u001b[35mSection_title: 3.4  Qualitative Result\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. however, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. we proposed a simple semi-supervision technique to expose the model to these annotations. we believe that in future they can be used more directly to yield better performance gains. we have also released these annotations for the research community at https: //github.com/ddua/intermediate_annotations. motteux was also without heirs and bequeathed sandringham, together with another norfolk estate and a property in surrey, to the third son of his close friend, emily lamb, the wife of lord palmerston. at the time of his inheritance in 1843, charles spencer cowper was a bachelor diplomat, resident in paris. on succeeding to motteux's estates, he sold the other properties and based himself at sandringham. he undertook extensions to the hall, employing samuel sanders teulon to add an elaborate porch and conservatory. cowper's style of living was extravagant he and his wife spent much of their time on the continent and within 10 years the estate was mortgaged for £89,000. the death of their only child, mary harriette, from cholera in 1854 led the couple to spend even more time abroad, mainly in paris, and by the early 1860s cowper was keen to sell the estate. figure 9: predicted relevant spans for question answered correctly with annotation (prediction:\"charles spencer cowper\") and incorrectly without annotations (prediction:\"lord palmerston\") by xlnet on quoref\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  THE VIQUAE DATASET 3.1 Automatic annotation\n",
            "\u001b[35mSection_title: 3.2  Application on TriviaQA\n",
            "\u001b[35mSection_title: 3.3  Manual refinement\n",
            "\u001b[35mSection_title: 4  DATA ANALYSIS\n",
            "\u001b[35mSection_title: 5  THE VIQUAE KNOWLEDGE BASE\n",
            "\u001b[35mSection_title: 6  EXPERIMENTAL SETTINGS\n",
            "\u001b[35mSection_title: 7  INFORMATION RETRIEVAL\n",
            "\u001b[35mSection_title: 7.1  Methods\n",
            "\u001b[35mSection_title: 7.1.3  Multimodal fusion.\n",
            "\u001b[35mSection_title: 7.2  Results\n",
            "\u001b[35mSection_title: 7.2.2  Mono-vs. Multi-modal.\n",
            "\u001b[32mconclusion: despite the improvement brought by dpr and the multimodal fusion, there is still a lot of room for improvement, which highlights the difficulty of the task, especially for questions about non-human entities. this can be explained by the specialized image representation of arcface, whereas imagenet-resnet and clip are more general. it also highlights the need to study visual representation of non-human entities, as exemplified in section 1.\n",
            "\u001b[35mSection_title: 7.2.3  Conclusion.\n",
            "\u001b[35mSection_title: 8  READING COMPREHENSION\n",
            "\u001b[35mSection_title: 8.1  Methods\n",
            "\u001b[35mSection_title: 8.2  Results\n",
            "\u001b[32mconclusion: we introduce a new dataset, viquae, designed as a benchmark to track the progress of kvqae systems. the dataset has been annotated with a semi-automatic pipeline that we also provide. questions in the dataset may be answered using a freely available kb of 1.5m wikipedia articles paired with images. we propose a baseline along with the benchmark that addresses kvqae as a two-stage problem: ir and rc, with both zero-and few-shot learning methods for the two stages. first, ir is carried out with well-established technologies: term-based text retrieval, cnn-based image retrieval, and face recognition, as well as recent bert-based retrieval techniques. then, rc also takes advantage of the ubiquitous bert model. while both stages could be improved, the experiments highlight the need for a better ir. indeed, our late fusion scheme neglects interaction between the modalities. future work should focus on a better multimodal representation, ideally embedding text and image in the same space, on both the query and kb sides. special attention should be paid to the representation of non-human entities. as exemplified in section 1 and demonstrated in section 7.2, humans can be clearly represented with their face, while other entities have more heterogeneous depictions. we believe that multimodal representations will also benefit the rc stage, as our experiments show that using a text-only reader is insufficient if the ir stage is noisy.\n",
            "\u001b[35mSection_title: 9  CONCLUSION AND PERSPECTIVES\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Setup\n",
            "\u001b[35mSection_title: 2.1  Zero-shot Dense Retrieval\n",
            "\u001b[35mSection_title: 2.2  Experimental Setup\n",
            "\u001b[35mSection_title: 2.3  Initial Analysis on MSMARCO and NQ\n",
            "\u001b[35mSection_title: 3  Experimental Analysis and Findings\n",
            "\u001b[35mSection_title: 3.1  The Effect of Source Query Set\n",
            "\u001b[35mSection_title: 3.1.1  Experimental Results\n",
            "\u001b[35mSection_title: 3.1.2  Query Vocabulary Overlap\n",
            "\u001b[35mSection_title: 3.1.3  Query Type Distribution\n",
            "\u001b[35mSection_title: 3.2  The Effect of Source Document Set\n",
            "\u001b[35mSection_title: 3.3  The Effect of Data Scale\n",
            "\u001b[35mSection_title: 3.3.1  Query Scale\n",
            "\u001b[35mSection_title: 3.3.2  Document Scale\n",
            "\u001b[35mSection_title: 3.4  The Bias from Target Datasets\n",
            "\u001b[35mSection_title: 3.5  Summarizing the Main Findings\n",
            "\u001b[35mSection_title: 4  Model Analysis\n",
            "\u001b[35mSection_title: 4.1  Reviewing Existing Solutions\n",
            "\u001b[35mSection_title: 4.2  Comparison of Zero-shot Methods\n",
            "\u001b[32mconclusion: in this paper, we thoroughly examine the zeroshot capability of dr models. we conduct empirical analysis by extensively studying the effect of various factors on the retrieval performance. in particular, we find that the factors of vocabulary overlap, query type distribution, and data scale are likely to affect the zero-shot performance of dense retriever. besides, the performance between bm25 and dr models varies significantly on different target datasets, where the dataset bias (e.g., a dataset is created based on exact match) is likely to make such comparison unfair. overall, we find that the zero-shot performance of dense retrieval models still has room to improve and deserves further study.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Question Answering\n",
            "\u001b[35mSection_title: 2.2  Question Generation and QA Data\n",
            "\u001b[35mSection_title: 2.3  Self-enhanced bootstrapping system\n",
            "\u001b[35mSection_title: 3  QASnowball\n",
            "\u001b[35mSection_title: 3.1  The Overall Framework\n",
            "\u001b[35mSection_title: 3.2  Answer Extractor\n",
            "\u001b[35mSection_title: 3.3  Question Generator\n",
            "\u001b[35mSection_title: 3.4  Data Filter\n",
            "\u001b[35mSection_title: 3.5  Iterative Bootstrapping Process\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Experimental Settings\n",
            "\u001b[35mSection_title: 4.3  The Quality of SGQ\n",
            "\u001b[35mSection_title: 4.4  The Effect of Self-Enhanced Mechanism\n",
            "\u001b[35mSection_title: 4.5  The Effect of Data Size\n",
            "\u001b[35mSection_title: 4.6  Human Evaluation\n",
            "\u001b[35mSection_title: 4.7  Case Study\n",
            "\u001b[32mconclusion: in this paper, we propose qasnowball, a novel iterative bootstrapping framework that can continually generate high-quality and large-scale qa data.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Retrieval models\n",
            "\u001b[35mSection_title: 2.2  Query generation systems\n",
            "\u001b[35mSection_title: 3  Dataset Description and Generation\n",
            "\u001b[35mSection_title: 4  Experimental setting\n",
            "\u001b[35mSection_title: 4.1  Models: IR model\n",
            "\u001b[35mSection_title: 4.2  BEIR Benchmark\n",
            "\u001b[35mSection_title: 4.3  Metrics\n",
            "\u001b[35mSection_title: 5  Experiments and Results\n",
            "\u001b[35mSection_title: 6  Analysis of query generation\n",
            "\u001b[35mSection_title: 6.1  Questions lost during generation\n",
            "\u001b[35mSection_title: 6.2  Top 10 starting words\n",
            "\u001b[35mSection_title: 6.3  Question generation time\n",
            "\u001b[32mconclusion: in this paper, we have explored the use of large language models (llms) for synthetic query generation and compared them to the rule-based independent cropping method for unsupervised domain adaptation.our results show that llm-based methods outperform independent cropping in all scenarios by a significant margin.although llms require more time, the benefits they offer in terms of performance make them a viable alternative for query generation.\n",
            "\u001b[35mSection_title: 7  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Models and Training Strategies\n",
            "\u001b[35mSection_title: 2.2  Graph Neural Network in Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 3  Coreference-aware Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 3.1  Coreference Resolution\n",
            "\u001b[35mSection_title: 3.2  Graph Neural Network\n",
            "\u001b[35mSection_title: 3.3  Coreference-enhanced Attention\n",
            "\u001b[35mSection_title: 3.4  Integration\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Model Settings\n",
            "\u001b[35mSection_title: 4.2  Setup\n",
            "\u001b[35mSection_title: 4.3  Tasks and Datasets\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Model Efficiency\n",
            "\u001b[35mSection_title: 5.2  Case Studies\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[32mconclusion: in this paper, we present intuitive methods to solve coreference-intensive machine reading comprehension tasks by following the reading process of human in which people connect the anaphoric expressions with explicit instructions. we demonstrate that all our three fine-tuning methods, including coref gnn , coref addatt and coref multiatt , are superior to the pre-trained language models that incorporate the coreference information in the pretraining stage, such as corefroberta large . as the fine-tuning methods rely on the coreference resolution models supplied by other researchers, their performance is also constrained by the accuracy of those coreference resolution models. in addition, the questions that require multistep reasoning, span multiple entities or contain multiple answer items also pose the challenges to our models. in the future, with more in-depth study on human reasoning in reading comprehension and more progress in graph neural networks, the gnnbased coreference graph can be enriched with more edge types and diverse structures to leverage more linguistic knowledge and gain better performance.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  A FINER-GRAINED LOOK AT DOCUMENTS\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  Relevance signals appear at different positions in documents\n",
            "\u001b[35mSection_title: 3.3  Fuzzy matching may help select better blocks on some collections\n",
            "\u001b[35mSection_title: 4  SELECTING BLOCKS WITH STANDARD IR FUNCTIONS: TF-IDF AND BM25\n",
            "\u001b[35mSection_title: 4.1  KeyB(vBERT)\n",
            "\u001b[35mSection_title: 4.2  KeyB(PARADE )\n",
            "\u001b[35mSection_title: 4.3  Model training\n",
            "\u001b[35mSection_title: 5  LEARNING TO SELECT BLOCKS\n",
            "\u001b[35mSection_title: 5.1  Improving Vanilla BERT\n",
            "\u001b[35mSection_title: 5.2  Improving PARADE\n",
            "\u001b[35mSection_title: 6  EXPERIMENTS ON STANDARD IR COLLECTIONS\n",
            "\u001b[35mSection_title: 6.1  Experimental design\n",
            "\u001b[35mSection_title: 6.2  Experimental results\n",
            "\u001b[35mSection_title: 6.3  Memory usage\n",
            "\u001b[35mSection_title: 6.4  Ranking speed\n",
            "\u001b[35mSection_title: 6.5  Analysis of the position of selected blocks\n",
            "\u001b[35mSection_title: 7  EXPERIMENT ON TREC 2019 DL AND COMPARISON WITH SPARSE ATTENTION BASED MODELS AND IDCM\n",
            "\u001b[32mconclusion: benefiting from pre-trained bert models, the field of information retrieval has seen remarkable progress in neural ir models, as exemplified by the success of vanilla bert which has become a strong, yet simple, baseline for neural ir models. to overcome the limitations of bert-based models regarding long documents, we have proposed to divide documents into blocks and to select only the most important key blocks. this is reminiscent of the way humans assess the relevance of a document for a given query: one first identifies blocks relevant to the query, blocks which are then aggregated to obtain the overall assessment of the document. in order to select blocks, we have investigated two approaches: the first one is straightforward and makes use of standard retrieval functions as tf-idf or bm25; the second one learns a single bert model used for both ranking blocks and documents. both approaches have been shown to improve over standard baselines and previous bert-based models. we have followed the same approach on another highly competitive neural ir model, namely parade, here again with improved results. all in all, selecting blocks is advantageous for the two models studied here, vanilla bert and parade. we conjecture that this selection is a way to remove passages in documents which are not relevant to the query and which are likely to bring noise when matching queries and documents.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset and Problem Definition\n",
            "\u001b[35mSection_title: 2.1  Fact Annotation\n",
            "\u001b[35mSection_title: 2.2  Synthesizing Question-Answer Pairs\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 3.1  BigBird Extractive Model\n",
            "\u001b[35mSection_title: 3.2  FiD Generative Model\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Main Results\n",
            "\u001b[35mSection_title: 4.4  Human-Paraphrased Results\n",
            "\u001b[35mSection_title: 4.5  Model Analysis\n",
            "\u001b[35mSection_title: 4.6  Error Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: though time-sensitive facts are pervasive in our daily text corpus, there has been little prior work exploring this direction. in this paper, we build the first dataset to investigate whether existing models can understand time-sensitive facts. our experiments show that the sota models are still lagged behind humans in temporal reasoning. in order to empower the future nlp models to understand temporal information, different temporal-aware models need to be proposed. finally, this paper opens up new research directions for better modeling temporal information in text representations.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The Point Process Model\n",
            "\u001b[35mSection_title: 2.1  Renewal Processes with Varying Variance (RPVV)\n",
            "\u001b[35mSection_title: 2.2  Examples of RPVVs\n",
            "\u001b[35mSection_title: 2.3  The Full Model\n",
            "\u001b[35mSection_title: 3  The Multiple Filter Test (MFT)\n",
            "\u001b[35mSection_title: 3.1  Derivation of the MFT\n",
            "\u001b[35mSection_title: 3.2  Variance Estimation\n",
            "\u001b[35mSection_title: 4  Multiple Filter Algorithm (MFA) for Change Point Detection\n",
            "\u001b[35mSection_title: 4.1  Single Filter Algorithm (SFA)\n",
            "\u001b[35mSection_title: 4.2  Multiple Filter Algorithm (MFA)\n",
            "\u001b[35mSection_title: 4.3  Application to a Simulated Point Process\n",
            "\u001b[35mSection_title: 4.4  Choosing the Window Set H\n",
            "\u001b[35mSection_title: 5.1.1  Empirical Significance Level in Simulations\n",
            "\u001b[35mSection_title: 5.1.2  Comparison of the MFT to a Bootstrap Test\n",
            "\u001b[35mSection_title: 5.1.3  True change points do not increase the frequency of falsely detected change points\n",
            "\u001b[35mSection_title: 5.2  Multiple Filters Increase the Detection Probability\n",
            "\u001b[35mSection_title: 6.1  Data Analysis\n",
            "\u001b[35mSection_title: 6.2  Practical Issues and R-Code\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Edge Probing & QA: The Setup\n",
            "\u001b[35mSection_title: 4  RQ1: EP Tests & Fine-Tuned Encoders\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  RQ2: EP Test for Coreference\n",
            "\u001b[35mSection_title: 5.1  Finding Coreference Questions\n",
            "\u001b[35mSection_title: 5.1.1  Model-Agnostic Filter\n",
            "\u001b[35mSection_title: 5.1.2  Model-Dependent Filter\n",
            "\u001b[35mSection_title: 5.1.3  Experiments & Results\n",
            "\u001b[35mSection_title: 5.2  Target Task vs Encoder EP Test\n",
            "\u001b[35mSection_title: 6.1  Analysis\n",
            "\u001b[35mSection_title: 6.2  EP Test Heuristics\n",
            "\u001b[35mSection_title: 6.2.1  Results\n",
            "\u001b[35mSection_title: 6.3  EP Tests: Hard & Easy Instances\n",
            "\u001b[35mSection_title: 6.3.1  Results\n",
            "\u001b[32mconclusion: edge probing tests are the predominant method to probe for linguistic information in large language models. we use them to evaluate how the process of fine-tuning an lm for qa might change the grammatical knowledge in an encoder, and observe no significant differences between pre-trained and fine-tuned lms. more importantly, we find this phenomenon in carefully designed target tasks where the models must use the said grammatical knowledge. from similar ep test results, previous works have concluded that fine-tuning does not change the encoding of grammatical knowledge. however, our analysis provides a 'dataset bias' explanation for the consistency of the results and provides some clues as to why any representation tends to achieve very similar results for ep tests. this is different from the previous task-design criticisms of the ep tests.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Dynamic Adversarial Data Collection (DADC)\n",
            "\u001b[35mSection_title: 2.2  Generative Model Annotation Support\n",
            "\u001b[35mSection_title: 2.3  Active Learning and Weak Supervision\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Standard versus Adversarial Data Collection\n",
            "\u001b[35mSection_title: 4.2  Improving Standard Data Collection\n",
            "\u001b[35mSection_title: 4.3  Improving Adversarial Data Collection\n",
            "\u001b[35mSection_title: 4.4  Investigating Answer Prompting\n",
            "\u001b[35mSection_title: 5  Annotator Interaction with GAAs\n",
            "\u001b[32mconclusion: in this work, we introduce generative annotation assistants (gaas) and investigate their potential to aid crowdworkers with creating more effective training data more efficiently. we perform a thorough analysis of how gaas can be used for improving qa dataset annotation in different settings, including different generative model training data, sampling strategies, and whether to also provide annotators with answer suggestions. we find that gaas are beneficial in both the standard and adversarial data collection settings. in the standard data collection setting, and under the assumption of no access to adversarially-collected data, gaas with prompts sampled based on likelihood provide annotation speed-ups, while prompts sampled by adversarial performance or uncertainty metrics provide benefits to both the model error rates on the collected data as well as subsequent downstream qa performance. we find that while gaas are effective for improving standard data collection, we still do not approach the performance obtained when using adversarial data collection.\n",
            "\u001b[35mSection_title: 6  Discussion and Conclusion\n",
            "\u001b[35mSection_title: 7  Ethical Considerations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Analysis Methods\n",
            "\u001b[35mSection_title: 2.1  Dataset\n",
            "\u001b[35mSection_title: 2.2  Analytical Method for the Ability of Multi-Hop Reasoning\n",
            "\u001b[35mSection_title: 3  Baseline Models\n",
            "\u001b[35mSection_title: 3.1  Baseline Models\n",
            "\u001b[35mSection_title: 4  Multi-hop Reasoning Approaches\n",
            "\u001b[35mSection_title: 4.1  Query-Reformulation Approach\n",
            "\u001b[35mSection_title: 4.2  Co-Matching Approach\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Settings\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: in this paper, we analyze qa models' capability in multi-hop reasoning by assessing if the reasoning chain could help existing multi-hop readers. we observed the general weakness of stat-or-the-art models in multi-hop reasoning and proposed a comatching based method to mitigate. despite the fact that co-matching is designed to encode only three input sequences to achieve limited multi-hop reasoning, we consider this as the most promising one that demonstrates the concrete reasoning capability and has the potential for real multi-hop reasoning.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Piecewise Renewal Processes\n",
            "\u001b[35mSection_title: 3  The Filtered Derivative Process in Case of a Change Point\n",
            "\u001b[35mSection_title: 3.1  The Shark Function\n",
            "\u001b[35mSection_title: 3.1.1  Detection Probability in Change Point Estimation\n",
            "\u001b[35mSection_title: 4  The Distortion -Estimation of Process Parameters\n",
            "\u001b[35mSection_title: 5  Summary\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Logic Pre-training\n",
            "\u001b[35mSection_title: 3  The Adversarial Training Framework\n",
            "\u001b[35mSection_title: 3.1  Training of Verifier\n",
            "\u001b[35mSection_title: 3.2  Training of Generator\n",
            "\u001b[35mSection_title: 4.3  Models\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Study and Analysis\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[35mSection_title: 7  Related Works\n",
            "\u001b[32mconclusion: in this work, we hypothesize that (i) logic ability plays a key role in a wide scope of tasks requiring general reasoning; and (ii) plms' logic ability can be further improved beyond their original linguistic ability. we correspondingly propose logigan, an unsupervised adversarial pre-training framework for logical reasoning enhancement. logigan circumvents the non-differentiable challenge of sequential gan via a novel generator-verifier scoring consensus mechanism, and enables largescale pre-training with longer target length. extensive experiments and ablation studies reveal the effectiveness and functional components of logigan, providing evidence to our major hypothesis.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Datasets from standard tests\n",
            "\u001b[35mSection_title: 2.2  Datasets with explanations\n",
            "\u001b[35mSection_title: 3  Dataset Overview\n",
            "\u001b[35mSection_title: 3.1  Error reasons of a distractor\n",
            "\u001b[35mSection_title: 3.2  Required reasoning skills\n",
            "\u001b[35mSection_title: 3.2.1  Level 1: Basic information capturing\n",
            "\u001b[35mSection_title: 3.2.2  Level 2: Local information integration\n",
            "\u001b[35mSection_title: 3.2.3  Level 3: Global information integration\n",
            "\u001b[35mSection_title: 4  Construction and Annotation of GCRC\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  QA accuracy and GCRC difficulty level\n",
            "\u001b[35mSection_title: 5.2  Performance of locating sentencelevel SFs using BERT\n",
            "\u001b[35mSection_title: 5.4  Performance of different reasoning skills required by questions using BERT\n",
            "\u001b[32mconclusion: in this paper, we present a new challenging machine reading comprehension dataset (gcrc), collected from gaokao chinese, consisting of 8,719 high-level comprehensive multiple-choice questions. to the best of our knowledge, this is currently the most comprehensive, challenging, and high-quality dataset in mrc domain. in addition, we spend considerable effort to label three types of information, including sentence-level sfs, ers of a distractor, and reasoning skills required for qa, aiming to comprehensively evaluate systems in an explainable way. through experiments, we observe gcrc is very challenging data set for existing models, and we hope it can inspire innovative machine learning and reasoning approach to tackle the challenging problem and make mrc as an enabling technology for many real-world applications.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  AQUAMUSE\n",
            "\u001b[35mSection_title: 2.1  Approach\n",
            "\u001b[35mSection_title: 2.2  Implementation details\n",
            "\u001b[35mSection_title: 2.3  Dataset Statistics\n",
            "\u001b[35mSection_title: 3  Quality Assessment\n",
            "\u001b[35mSection_title: 3.1  Correctness\n",
            "\u001b[35mSection_title: 3.2  Fluency\n",
            "\u001b[35mSection_title: 3.3  Overall quality\n",
            "\u001b[35mSection_title: 3.4  Examples\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Abstractive Summarization\n",
            "\u001b[35mSection_title: 4.2  Extractive Summarization\n",
            "\u001b[35mSection_title: 4.3  Incorporating Query in SDS/MDS\n",
            "\u001b[35mSection_title: 4.4  Human Evaluation\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we have presented aquamuse, a scalable methodology for constructing new qmds datasets, along with in-depth analyses and baseline experiments to demonstrate properties of one such dataset instance. many parts of the approach are configurable providing researchers a rich sandbox for evaluating summarization models under different task conditions. our methodology greatly reduces the cost of data collection by converting a predominantly generative human annotation task (e.g., reading documents and writing succinct summaries) to a discriminative human annotation task (e.g., deciding on sentence-document relevance). while our present work do not propose new methods for query-based summarization, we ran baseline experiments on one specific instance of the aquamuse dataset using a few popular neural approaches re-adapted with query conditioning. our experiments demonstrates that there is still much headroom for existing state-of-the-art models and we hope aquamuse will spur further advancements query focused multi-document summarization algorithms.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  BACKGROUND\n",
            "\u001b[35mSection_title: 3.2  TASK FORMULATION\n",
            "\u001b[35mSection_title: 3.3  EXPLANATION STRUCTURE\n",
            "\u001b[35mSection_title: 4  DATASET\n",
            "\u001b[35mSection_title: 4.1  DATA COLLECTION\n",
            "\u001b[35mSection_title: 4.2  DATASET DESCRIPTION\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  EXPERIMENTAL SETTINGS AND MODELS\n",
            "\u001b[35mSection_title: 5.2  AUTOMATIC EVALUATION METRICS\n",
            "\u001b[35mSection_title: 5.3  HUMAN EVALUATION\n",
            "\u001b[35mSection_title: 5.4  RESULTS\n",
            "\u001b[32mconclusion: with this paper, we release wikiwhy, a question-answering dataset enabling the analysis and improvement of llms' reasoning capability. we propose explanation between grounded cause-effect pairs to distinguish memorization of the relation from a genuine understanding of the underlying mechanics. compared to related works on explainable qa, our explanation format finds a natural middle ground that balances complexity and depth, allowing our crowdsourcing methods to produce thought-provoking examples while being highly scalable. we exploit this scalability to cover topics previously overlooked by other explanation datasets and demonstrate our proposed task to be difficult with strong baselines (our experiments feature models failing to produce satisfying explanations even under ideal conditions). finally, we motivate the development of new automatic metrics that are better able to handle the complexities of generated reasoning.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Datasets\n",
            "\u001b[35mSection_title: 1.2  Related Work\n",
            "\u001b[35mSection_title: 2  Methods\n",
            "\u001b[35mSection_title: 2.1  Model Architecture\n",
            "\u001b[35mSection_title: 2.2  Multi-task Learning\n",
            "\u001b[35mSection_title: 3  Experiment Settings\n",
            "\u001b[35mSection_title: 4  Results\n",
            "bottom\n",
            "\u001b[35mSection_title: 5  Acknowledgement\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Description\n",
            "\u001b[35mSection_title: 3  Type of the tutorial\n",
            "\u001b[35mSection_title: 4  Outline\n",
            "\u001b[35mSection_title: 1.  Introduction (15 minutes long): This section\n",
            "\u001b[35mSection_title: 5  Breadth\n",
            "\u001b[35mSection_title: 6  Prerequisites\n",
            "\u001b[35mSection_title: 7  Reading List\n",
            "\u001b[35mSection_title: 9  Estimated Attendance\n",
            "\u001b[35mSection_title: 10  Venue\n",
            "bottom\n",
            "\u001b[35mSection_title: 11  Open Access\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Overview\n",
            "\u001b[35mSection_title: 3  QDMR Logical Forms\n",
            "\u001b[35mSection_title: 3.1  Definitions\n",
            "\u001b[35mSection_title: 3.2  LF-based Evaluation (LF-EM)\n",
            "\u001b[35mSection_title: 4  From LFs to Dependency Graphs\n",
            "\u001b[35mSection_title: 4.1  Token Alignment\n",
            "\u001b[35mSection_title: 4.2  Spans Dependencies Extraction\n",
            "\u001b[35mSection_title: 4.3  SDG→DG\n",
            "\u001b[35mSection_title: 5  Models\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Experimental Setup\n",
            "\u001b[35mSection_title: 6.2  Results\n",
            "\u001b[35mSection_title: 6.3  Analysis\n",
            "\u001b[32mconclusion: in this work, we propose to represent qdmr structures with a dependency graph over the input tokens, and propose a graph parser and a seq2seq model that uses graph supervision as an auxiliary loss. we show that a graph parser is 16x faster than a seq2seq model, and that it exhibits better sample coplexity. moreover, using graphs as auxiliary supervision improves out-of-domain generalization and leads to better performance on questions that represent a long sequence of computational steps. last, we propose a new evaluation metric for qdmr parsing and show it better corresponds to human intuitions.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary\n",
            "\u001b[35mSection_title: 2.1  In-Context Learning\n",
            "\u001b[35mSection_title: 2.2  Sequential ICL\n",
            "\u001b[35mSection_title: 2.3  Parallel ICL\n",
            "\u001b[35mSection_title: 3.1  Experiment Setup\n",
            "\u001b[35mSection_title: 3.2  Result Analysis\n",
            "\u001b[32mconclusion: we raise concerns about the use of parallelintegrated methods to address context length restriction: (1) pcw is functionally equal with a simple weighted sum ensemble on label distribution among context windows; (2) pcw degrades the multi-step reasoning capabilities of llms in complex tasks requiring knowledge understanding. de-spite the fact that parallel-integrated methods sometimes show better classification performance when the label space is large, they merely brute-force ensemble each window's context, consequently weakening logical reasoning and knowledge comprehension.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Table-Question Alignment\n",
            "\u001b[35mSection_title: 2.3  Hybrid QA\n",
            "\u001b[35mSection_title: 2.4  Table Cell Retrieval\n",
            "\u001b[35mSection_title: 3  Framework\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Data Construction\n",
            "\u001b[35mSection_title: 3.3  Passage Filtering\n",
            "\u001b[35mSection_title: 3.4  Table Alignment & Cell Selection\n",
            "\u001b[35mSection_title: 3.4.1  Table-Question Alignment\n",
            "\u001b[35mSection_title: 3.4.2  Table-Cell Selection\n",
            "\u001b[35mSection_title: 3.5  Passage Question-Answering\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Quantitative Analysis\n",
            "\u001b[35mSection_title: 4.4  Qualitative Analysis\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[35mSection_title: 4.6  Case Study\n",
            "\u001b[35mSection_title: 4.7  Error Analysis\n",
            "\u001b[32mconclusion: this paper presents tacr, a \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: in this paper, we focus on the hybrid qa task, where the answers to most questions can be extracted from cell values in tables and linked passages using a reading comprehension model. although tacr performs well in cell selection, one of its limitations is that it lacks numerical reasoning ability across different cells, such as counting and comparing. to enable tacr to answer numerical questions, we will further develop its numerical reasoning capabilities in future work. another limitation of tacr is that it shows a strong ability in column selection while performing relatively worse in row selection. for future work, we plan to try to improve its row-selection accuracy. \n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Challenge Sets and Adversarial Settings\n",
            "\u001b[35mSection_title: 2.2  Adversarial Training and Testing\n",
            "\u001b[35mSection_title: 2.3  Other Related Work\n",
            "\u001b[35mSection_title: 3  Dynabench\n",
            "\u001b[35mSection_title: 3.1  Features and Implementation Details\n",
            "\u001b[35mSection_title: 3.2  Initial Tasks\n",
            "\u001b[35mSection_title: 3.3  Dynabenchmarking NLP\n",
            "\u001b[35mSection_title: 4  Caveats and Objections\n",
            "\u001b[32mconclusion: we introduced dynabench, a research platform for dynamic benchmarking. dynabench opens up exciting new research directions, such as investigating the effects of ensembles in the loop, distributional shift characterisation, exploring annotator efficiency, investigating the effects of annotator expertise, and improving model robustness to targeted adversarial attacks in an interactive setting. it also facilitates further study in dynamic data collection, and more general cross-task analyses of humanand-machine interaction. the current iteration of the platform is only just the beginning of a longer journey. in the immediate future, we aim to achieve the following goals: anyone can run a task. having created a tool that allows for human-in-the-loop model evaluation and data collection, we aim to make it possible for anyone to run their own task. to get started, only three things are needed: a target model, a (set of) context(s), and a pool of annotators.\n",
            "\u001b[35mSection_title: 5  Conclusion and Outlook\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 4.  Downsampling benchmarks to as few as 10K\n",
            "\u001b[35mSection_title: 2  Measuring Concurrence\n",
            "\u001b[35mSection_title: 3  Do Modeling Improvements Hold\n",
            "\u001b[35mSection_title: 3.1  Results\n",
            "\u001b[35mSection_title: 4  Exploring the Limits of Concurrence\n",
            "\u001b[35mSection_title: 4.1  Downsampling Benchmarks\n",
            "\u001b[35mSection_title: 4.2  Cloze Benchmarks\n",
            "\u001b[35mSection_title: 4.3  High Concurrence Is Not Universal: Improvements Do Not Hold On bAbI\n",
            "\u001b[35mSection_title: 4.4  What is Sufficient for Concurrence on Non-Pretrained Modeling Approaches?\n",
            "\u001b[35mSection_title: 4.5  What is Sufficient for Concurrence on Pre-Trained and Non-Pretrained Modeling Approaches?\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: this work studies whether qa modeling improvements hold across the diverse landscape of qa benchmarks. we develop the notion of concurrence, which quantifies the similarity between benchmarks' rankings of modeling approaches. experiments with 32 qa benchmarks and 20 diverse modeling approaches indicate that humanconstructed benchmarks largely have high concurrence amongst themselves, even when their passage and question distributions or linguistic phenomena of focus are very different. to better understand how different benchmark attributes affect concurrence, we explore downsampled benchmarks and various programmatically-generated benchmarks, the latter having high concurrence only when they target phenomena that are also useful for better performance on human-constructed benchmarks (e.g., identifying paraphrase and lexical overlap). our results indicate that the modeling improvements studied hold broadly, despite years of intense community focus on a small number of benchmarks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Encoder\n",
            "\u001b[35mSection_title: 3.3  Decoder\n",
            "\u001b[35mSection_title: 3.4  Reinforcement Learning for Fine-Tuning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Models for Comparison\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Results and Analysis\n",
            "\u001b[35mSection_title: 4.4.1  Comparisons with Baseline Models\n",
            "\u001b[35mSection_title: 4.4.2  Ablation Study\n",
            "\u001b[35mSection_title: 4.4.3  Case Study\n",
            "\u001b[32mconclusion: deep question generation aims to generate complex questions that require reasoning over multiple pieces of information. in this paper, we propose an answer-driven end-to-end deep question generation model (addqg) based on reinforcement learning. an answer-aware initialization module with a gated connection layer and a semantic-rich fusion attention mechanism are designed to incorporate document and answer information into the generation process. reinforcement learning is further applied to integrate both syntactic and semantic metrics as the reward to enhance the training of addqg. experiments show that addqg outperforms the state-of-the-art systems on the challenging dqg dataset. ablation studies have demonstrated the effectiveness of our designs, and human evaluations show that our model can produce more coherent and answer-focused questions.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Methods\n",
            "\u001b[35mSection_title: 2.1.  CoT prompting\n",
            "\u001b[35mSection_title: 2.2.  Factored decomposition\n",
            "\u001b[35mSection_title: 2.3.  CoT decomposition\n",
            "\u001b[35mSection_title: 2.4.  Implementation\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Question-Answering Performance\n",
            "\u001b[35mSection_title: 3.2.  Faithfulness Measured via Reasoning Perturbation\n",
            "\u001b[35mSection_title: 3.2.1.  EARLY ANSWERING\n",
            "\u001b[35mSection_title: 3.2.2.  ADDING MISTAKES\n",
            "\u001b[32mconclusion: overall, our results from the reasoning perturbation experiments suggest that question decomposition leads to more faithful model-generated reasoning. factored decomposition generates the most faithful reasoning, whereas cot decomposition generates less faithful reasoning than factored decomposition but more faithful reasoning than cot prompting. this is shown by the early answering experiments, which find comparable faithfulness between cot decomposition and cot prompting, and the adding mistakes experiments, which find cot decomposition has intermediate faithfulness.\n",
            "\u001b[35mSection_title: 3.2.3.  CONCLUSIONS\n",
            "\u001b[35mSection_title: 3.3.  Faithfulness Measured via Biasing Contexts\n",
            "\u001b[35mSection_title: 3.3.1.  BIASED REASONING FROM ANSWER ALWAYS A\n",
            "\u001b[35mSection_title: 3.3.2.  BIASED REASONING FROM SUGGESTED ANSWER\n",
            "\u001b[32mconclusion: our findings studying the faithfulness of model-generated reasoning via biased contexts suggests that factored decomposition leads to more faithful reasoning than cot or cot decomposition. cot decomposition reasoning looks less faithful than cot reasoning via these metrics, but our measurements from the reasoning perturbation experiments suggest otherwise. we do not make any claims about any ordering of the methods in terms of their importance to overall faithfulness, so by simple averaging (after normalizing to a 0-1 scale), we assess cot decomposition reasoning as more faithful than cot reasoning.\n",
            "\u001b[35mSection_title: 3.3.3.  CONCLUSIONS\n",
            "\u001b[35mSection_title: 3.4.  Qualitative Findings\n",
            "\u001b[32mlimitation: our findings indicate that using question decomposition over cot prompting provides faithfulness gains at the cost of question-answering performance. factored decomposition generates the most faithful reasoning but leads to the worst question-answering performance. cot decomposition provides intermediately faithful reasoning and performance. we are uncertain how this observed trade-off might be affected by other improvements such as further training, especially training geared towards improving a model's ability to answer questions via decomposition. such training or other techniques may lead to pareto-dominating methods for highly faithful and performant model-generated reasoning, which we believe to be an exciting goal for future work.our work leans heavily on the methods we use to assess the faithfulness of model-generated reasoning. these methods are limited by our inability to access the ground truth for the model's reasoning. our claim that question decomposition improves reasoning faithfulness is one based on multiple, fairly independent, lines of evidence, but we are open to future tools for assessing reasoning faithfulness, perhaps those based on a mechanistic understanding of the internal computations of our models (olah, 2023), changing our conclusions. additionally, we evaluate our methods on only four question-answering tasks and on only one model (an rlhf-finetuned llm); pretrained llms may be more or less prone to generating ignored or biased reasoning, which may increase or reduce the faithfulness benefit obtained via decomposition. expanding the diversity of the tasks and models evaluated could lead to more robust conclusions about the relative performance and reasoning faithfulness of cot prompting and question decomposition approaches.\n",
            "\u001b[35mSection_title: 3.5.  Discussion and Limitations\n",
            "\u001b[35mSection_title: 4.  Related Work\n",
            "\u001b[32mconclusion: we explore three prompting strategies for improving the question-answering performance while eliciting faithful reasoning from llms: chain-of-thought (cot) prompting, cot decomposition, and factored decomposition. our work shows it is possible to greatly improve the faithfulness of model-generated reasoning by prompting models to perform question decomposition while maintaining similar levels of question-answering accuracy, suggesting that there is even more headroom for progress using other techniques.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data\n",
            "\u001b[35mSection_title: 3.1  Collection Methodology and Task Definitions\n",
            "\u001b[35mSection_title: 3.2  Data Statistics\n",
            "\u001b[35mSection_title: 3.3  Categories of Prior Knowledge\n",
            "\u001b[35mSection_title: 4.1  Distance-Based Sliding Window\n",
            "\u001b[35mSection_title: 4.2  Co-Matching\n",
            "\u001b[35mSection_title: 4.3  Fine-Tuning Pre-Trained Language Models\n",
            "\u001b[35mSection_title: 5  Experiment\n",
            "\u001b[35mSection_title: 5.1  Experimental Settings\n",
            "\u001b[35mSection_title: 5.2  Baseline Results\n",
            "\u001b[35mSection_title: 5.3  Discussions on Distractor Plausibility\n",
            "\u001b[35mSection_title: 5.4  Discussions on Data Augmentation\n",
            "\u001b[32mconclusion: we present the first free-form multiple-choice chinese machine reading comprehension dataset (c 3 ), collected from real-world language exams, requiring linguistic, domain-specific, or general world knowledge to answer questions based on the given written or orally oriented texts. we study the prior knowledge needed in this challenging machine reading comprehension dataset and carefully investigate the impacts of distractor plausibility and data augmentation (based on similar resources for english) on the performance of state-of-the-art neural models. experimental results demonstrate the there is still a significant performance gap between the best-performing model (68.5%) and human readers (96.0%) and a need for better ways for exploiting rich resources in other languages.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Proposed Metric\n",
            "\u001b[35mSection_title: 3.1.  Prompt Design\n",
            "\u001b[35mSection_title: 3.2.  Metric Computation\n",
            "\u001b[35mSection_title: 4.  Reliability of the Metric\n",
            "\u001b[35mSection_title: 4.1.  Manually Created Test Samples\n",
            "\u001b[35mSection_title: 4.2.  Model Generated Test Samples\n",
            "\u001b[35mSection_title: 5.  Application of the Metric\n",
            "\u001b[35mSection_title: 5.1.  MQG Task\n",
            "\u001b[35mSection_title: 5.2.  Dataset\n",
            "\u001b[35mSection_title: 5.3.  Models\n",
            "\u001b[35mSection_title: 5.4.  Metrics\n",
            "\u001b[35mSection_title: 5.5.  Results\n",
            "\u001b[32mconclusion: this paper highlights an urgent issue in the field of question generation: the absence of an effective automatic evaluation metric to assess whether the generated questions are answerable.to address this issue, we propose a prompting-based metric on answerability (pman), leveraging chatgpt to assess answerability through chain-of-thought (cot) prompting.experiments with both manually created and model-generated samples demonstrate its reliability and strong alignment with human evaluations.applying our metric to evaluate qg models further indicates its potential to complement conventional metrics and guide future research in qg toward the generation of more answerable questions.however, pman currently fails to assess \"yes/no\" type questions with high accuracy, which remains as a direction for future research.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Test-Time Adaptation\n",
            "\u001b[35mSection_title: 3.1  TTA with Tent and PL\n",
            "\u001b[35mSection_title: 3.2  Online Imitation Learning\n",
            "\u001b[35mSection_title: 3.2.1  Instantiation of TTA with OIL\n",
            "\u001b[35mSection_title: 3.2.2  Enhancing OIL with Causal Inference\n",
            "\u001b[35mSection_title: 3.3  Implementation of TTA for the QA Task\n",
            "\u001b[35mSection_title: 4  Robustness Tuning\n",
            "\u001b[35mSection_title: 5  COLDQA\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Setup\n",
            "\u001b[35mSection_title: 6.2  Main Results\n",
            "\u001b[35mSection_title: 6.3  Further Analysis\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Test-Time Adaptation\n",
            "\u001b[35mSection_title: 3.1  TTA with Tent and PL\n",
            "\u001b[35mSection_title: 3.2  Online Imitation Learning\n",
            "\u001b[35mSection_title: 3.2.1  Instantiation of TTA with OIL\n",
            "\u001b[35mSection_title: 3.2.2  Enhancing OIL with Causal Inference\n",
            "\u001b[35mSection_title: 3.3  Implementation of TTA for the QA Task\n",
            "\u001b[35mSection_title: 4  Robustness Tuning\n",
            "\u001b[35mSection_title: 5  COLDQA\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Setup\n",
            "\u001b[35mSection_title: 6.2  Main Results\n",
            "\u001b[35mSection_title: 6.3  Further Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary: Federated Learning\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Data Augmentation\n",
            "\u001b[35mSection_title: 3.3  Dynamic Aggregation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  Overall Performance\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  Varying the Number of Clients\n",
            "\u001b[35mSection_title: 4.5  Training Cost\n",
            "\u001b[35mSection_title: 4.6  Analysis on the Impact of Dynamic Aggregation\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we have presented fewfedweight, enabling few-shot learning across massive nlp tasks with federated learning. in this new framework, the global model synthesizes pseudo samples for each client model, which are weighted by an energy-based algorithm. aggregation weights of client models are estimated according to their performance during training. experiments on 118 different tasks demonstrate the effectiveness of the proposed fewfedweight.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Dense Retriever\n",
            "\u001b[35mSection_title: 2.2  Query Generation\n",
            "\u001b[35mSection_title: 2.3  Exploiting External Knowledge\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  Query Interpretation as Triplet Form\n",
            "\u001b[35mSection_title: 3.3  Dataset Construction for Query Generator\n",
            "\u001b[35mSection_title: 3.4  Applying Template for General Document\n",
            "\u001b[35mSection_title: 4  Experimental setups\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Metrics\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 5  Result & Discussion\n",
            "\u001b[35mSection_title: 5.1  Overall Result\n",
            "\u001b[35mSection_title: 5.2  Analysis of Synthetic Queries\n",
            "\u001b[32mconclusion: we presented a novel query generation method, qgek, that generates synthetic queries in a form more similar to human labeled queries by using external knowledge. in order to use unprocessed external knowledge, we convert a query into a tripletbased template, which can include information of subjects and answers. remarkably, when dense retrieval models are trained with the queries generated from qgek, the performance has improved much compared to using the queries without external knowledge. also, we have shown that including external knowledge give rises to the distribution of the unique words similar to that of the human labeled queries. we believe that qgek can also be applied to the other generation methods by orthogonally adding some external knowledge processing modules. for future work, we plan to generate queries both close to human labeled ones and optimized for ir tasks and to allow the template to accept more general logical forms for diverse highquality queries. the code and data will be made available for public access.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Creation\n",
            "\u001b[35mSection_title: 2.1  Online Medical Consultation Records\n",
            "\u001b[35mSection_title: 2.2  Online Medical Encyclopedia\n",
            "\u001b[35mSection_title: 2.3  Online Medical Knowledge Bases\n",
            "\u001b[35mSection_title: 3  Data Statistics and Analysis\n",
            "\u001b[35mSection_title: 4  Benchmarks\n",
            "\u001b[35mSection_title: 4.1  Retrieval based benchmark\n",
            "\u001b[35mSection_title: 4.1.1  Baselines and Experimental Settings\n",
            "\u001b[35mSection_title: 4.1.2  Results\n",
            "\u001b[35mSection_title: 4.2  Generation Based Benchmark\n",
            "\u001b[35mSection_title: 4.2.1  Baselines and Experimental Settings\n",
            "\u001b[35mSection_title: 4.2.2  Results\n",
            "\u001b[35mSection_title: 5  Applications\n",
            "\u001b[35mSection_title: 5.1  Transfer for Other QA Dataset\n",
            "\u001b[35mSection_title: 5.2  As an External Knowledge\n",
            "\u001b[35mSection_title: 5.3  As a Pre-trained Corpus\n",
            "\u001b[35mSection_title: 5.3.1  Experimental Settings\n",
            "\u001b[35mSection_title: 5.3.2  Experimental Data\n",
            "\u001b[35mSection_title: 5.3.3  Results\n",
            "\u001b[32mconclusion: in this paper, we propose the largest chinese medical qa dataset to date, consisting of 26 million medical qa pairs, expanding the size of existing datasets by more than 2 orders of magnitude. at the same time, we benchmark many existing works based on the data set and found that these methods still have a lot of room for improvement in medical qa scenarios. we also demonstrate the possible uses of the dataset in practice. the experimental results show that the dataset contains rich medical knowledge that can be very helpful to existing datasets and tasks. we hope that the huatuo-26m dataset can not only help promote the research of medical qa, but also practically help doctors and patients.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and methods\n",
            "\u001b[35mSection_title: 2.1.  Animals\n",
            "\u001b[35mSection_title: 2.2.  DNA extraction\n",
            "\u001b[35mSection_title: 2.3.  Polymerase Chain Reaction (PCR)\n",
            "\u001b[35mSection_title: 2.4.  Restriction Fragment Length Polymorphism (RFLP)\n",
            "\u001b[35mSection_title: 2.5.  DNA sequencing\n",
            "\u001b[35mSection_title: 2.6.  Statistical analysis\n",
            "\u001b[35mSection_title: 3.  Results and discussion\n",
            "\u001b[32mconclusion: in the end, this study is considered to be a step advancing for further studies that may add to give additional information about the genetic polymorphism of meat and growth characters of egyptian goat breeds and the improvement of these economically important traits. \n",
            "\u001b[35mSection_title: 4.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.2  Young diagrams and Schur polynomials\n",
            "\u001b[35mSection_title: 1.3  Robinson-Schensted-Knuth correspondence for generalized permutations\n",
            "\u001b[35mSection_title: 1.5.2  Solitaire game\n",
            "\u001b[35mSection_title: 1.5.3  Anticipating large n asymptotics\n",
            "\u001b[35mSection_title: 1.5.4  A transition probability and Plancherel measure\n",
            "\u001b[35mSection_title: 1.5.5  Random walks\n",
            "\u001b[35mSection_title: 1.6  Probability measure on words\n",
            "\u001b[35mSection_title: 1.7  Generalized Permutations, Percolation and growth models\n",
            "\u001b[35mSection_title: 1.7.2  Percolation model with geometrically distributed entries\n",
            "\u001b[35mSection_title: 1.7.4  Queuing problem\n",
            "\u001b[35mSection_title: 1.7.5  Discrete polynuclear growth models\n",
            "\u001b[35mSection_title: 2  Probability on partitions, Toeplitz and Fredholm determinants\n",
            "\u001b[35mSection_title: 2.1  Probability on partitions expressed as Toeplitz determinants\n",
            "\u001b[35mSection_title: 2.2  The calculus of infinite wedge spaces\n",
            "\u001b[35mSection_title: 2.3  Probability on partitions expressed as Fredholm determinants\n",
            "\u001b[35mSection_title: 2.4  Probability on partitions expressed as U(n) integrals\n",
            "\u001b[35mSection_title: 3  Examples\n",
            "\u001b[35mSection_title: 3.1  Plancherel measure and Gessel's theorem\n",
            "\u001b[35mSection_title: 3.2  Probability on random words\n",
            "\u001b[35mSection_title: 3.3  Percolation\n",
            "\u001b[35mSection_title: 4.2  Limit theorem for longest increasing sequences\n",
            "\u001b[35mSection_title: 4.4  Limit theorem for the geometrically distributed percolation model, when both sides of the matrix tend to ∞\n",
            "\u001b[35mSection_title: 4.5  Limit theorem for the exponentially distributed percolation model, when both sides of the matrix tend to ∞\n",
            "\u001b[35mSection_title: 5.1  Orthogonal polynomials\n",
            "\u001b[35mSection_title: 5.2  Time dependent Orthogonal polynomials and the KP equation\n",
            "\u001b[35mSection_title: 6.2  Examples\n",
            "\u001b[35mSection_title: 7  Random matrices\n",
            "\u001b[35mSection_title: 7.2  Random Hermitian ensemble\n",
            "\u001b[35mSection_title: 7.3  Reproducing kernels\n",
            "\u001b[35mSection_title: 7.4  Correlations and Fredholm determinants\n",
            "\u001b[35mSection_title: 8.1.2  Estimating covariances of complex Gaussian populations and the Laguerre Hermitian ensemble\n",
            "\u001b[35mSection_title: 8.1.3  Estimating the canonical correlations between two Gaussian populations and the Jacobi Hermitian ensemble\n",
            "\u001b[35mSection_title: 8.2.2  The Laguerre ensemble\n",
            "\u001b[35mSection_title: 8.2.3  The Jacobi ensemble\n",
            "\u001b[35mSection_title: 8.3  Chazy and Painlevé equations\n",
            "\u001b[35mSection_title: 9.2  Soft edge scaling limit for GUE and the Tracy-Widom distribution\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.  Practicing Healthy Habits\n",
            "\u001b[35mSection_title: 2.  Taking Precautions When You're Sick\n",
            "\u001b[35mSection_title: 2  Model\n",
            "\u001b[35mSection_title: 2.1  General Architectures\n",
            "\u001b[35mSection_title: 2.2  Retriever\n",
            "\u001b[35mSection_title: 2.3  Generator\n",
            "\u001b[35mSection_title: 2.4  Subtopics Selection\n",
            "\u001b[35mSection_title: 2.5  Training\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.3  Overall Performance with and without Subtopic Planning\n",
            "\u001b[35mSection_title: 3.4  Effects of Subtopic Selection\n",
            "\u001b[35mSection_title: 3.5  Effects of Question Categories\n",
            "\u001b[35mSection_title: 3.6  Effects of Other Factors\n",
            "\u001b[35mSection_title: 4  Related Works\n",
            "\u001b[32mconclusion: we proposed a novel subtopic planning based architecture for answering how-to questions. our architecture is able to generate answers with better structure, higher diversity and more consistent quality. moreover, our subtopic selection method effectively singles out high quality subtopics with relevance and independence. both automatic and human evaluation proved the effectiveness of our methods. we consider the two directions for future research: 1) improving the answer's quality by applying end-to-end retrieval-generation models, e.g. (lewis et al., 2020b). 2) developing precise metrics to evaluate long-form and non-factoid answers. it suggests that incorporating commonsense knowledge may improve the performance of a subtopic decomposition model.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  FIRS Dataset Creation\n",
            "\u001b[35mSection_title: 3.1  Analysis of FIRS\n",
            "\u001b[35mSection_title: 4  Generating Fact-Infused Questions\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 5.  ASs2s\n",
            "\u001b[35mSection_title: 5  Experiments and Results\n",
            "\u001b[35mSection_title: 5.1  Results and Observations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 2.2  Logical Reasoning\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.2  Graph Construction\n",
            "\u001b[35mSection_title: 3.2.1  Logical Graph.\n",
            "\u001b[35mSection_title: 3.3  Graph Transformer\n",
            "\u001b[35mSection_title: 3.4  Decoder\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  Datasets and Baselines\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Comparison Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[35mSection_title: 4.5  Supplementary Analysis\n",
            "\u001b[35mSection_title: 5  CASE STUDY\n",
            "\u001b[32mconclusion: we propose a two-branch graph transformer network for logical reasoning of text, which is named as logiformer. firstly, we introduce two different strategies to construct the logical graph and syntax graph respectively. especially for the logical graph, we are the first to model both causal relations and negations in the logical reasoning task. secondly, we feed the extracted node sequences to the fully connected graph transformer for each graph. the topology of the graph is utilized to form the attention bias for the self-attention layers. thirdly, a dynamic gate mechanism is applied to make a fusion of the features from two branches. to improve the awareness of different question types, the question feature is updated based on the self-attention module. finally, the concatenated text sequence is passed through the feed forward layer and obtains the answer prediction. the whole reasoning process provides the interpretability, reflected by logical units with explicit relations and the visualization of the attention maps.\n",
            "\u001b[35mSection_title: 6  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.2  Gas disk models\n",
            "\u001b[35mSection_title: 2.3  Numerical method\n",
            "\u001b[35mSection_title: 3  RESULTS\n",
            "\u001b[35mSection_title: 3.1  Rapid Fueling\n",
            "\u001b[35mSection_title: 4.4  Eects of the rapid fueling on the stellar bar\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  BACKGROUND\n",
            "\u001b[35mSection_title: 3  HYBRID THRESHOLDING (HT)\n",
            "\u001b[35mSection_title: 3.1  Trainable and approximate thresholding\n",
            "\u001b[35mSection_title: 3.2  Threshold and token weight updating\n",
            "\u001b[35mSection_title: 4  EVALUATION\n",
            "\u001b[35mSection_title: 5  CONCLUDING REMARKS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Related Work\n",
            "\u001b[35mSection_title: 3  Problem Formulation\n",
            "\u001b[35mSection_title: 3.1  QDMR\n",
            "\u001b[35mSection_title: 3.2  Multistep Question Answering\n",
            "\u001b[35mSection_title: 4  Proposed Methods\n",
            "\u001b[35mSection_title: 4.1  Chain-of-Questions Framework\n",
            "\u001b[35mSection_title: 4.2  Learning with Latent Answers\n",
            "\u001b[35mSection_title: 4.3  Chain-of-Questions Training Algorithm\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Details\n",
            "\u001b[35mSection_title: 5.2  Task-Specific Modifications\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 5.4  Qualitative Analysis on QDMR\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Data\n",
            "\u001b[35mSection_title: 2.2  Models\n",
            "\u001b[35mSection_title: 2.3  Decomposition Strategies\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: our work explored when decompositions are helpful for machine reading. we showed that decompositions are helpful when there is limited data available, or when parameters cannot be tuned. however, when enough data exists (empirically around a few hundred instances) and parameters can be fine-tuned, it is best to let the model learn the decompositions implicitly through end-to-end training. we hope that our work will help to inform readers as they create new datasets and select methods to use for complex question answering.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Relative Consistency\n",
            "\u001b[35mSection_title: 2.1  Background\n",
            "\u001b[35mSection_title: 2.2  Achievable Consistency Scores\n",
            "\u001b[35mSection_title: 2.3  Distribution of Achievable Consistencies\n",
            "\u001b[35mSection_title: 3  Analysis with Simulated Contrastive Set\n",
            "\u001b[35mSection_title: 4  Meta-Analysis of Prior Work\n",
            "\u001b[35mSection_title: 4.1  Gardner et al. (2020)\n",
            "\u001b[35mSection_title: 4.2  Dua et al. (2021)\n",
            "\u001b[35mSection_title: 4.3  Ravichander et al. (2022)\n",
            "\u001b[32mconclusion: we introduce relative consistency, which complements standard contrast consistency by allowing an accuracy and consistency score pair to be examined to determine whether a higher consistency was possible with that accuracy.this facilitates the comparison of consistencies achieved by models that achieved different levels of accuracy.we show that relative consistency enriches conclusions we make about whether a model is more consistent than another, and occasionally even leads us to different takeaways.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: this mathematical model is based on a simplified version of contrastive datasets.contrastive datasets may have more than two edits for each original instance, which will result in a different distribution.although we provide formulas for distributions of arbitrary bundle size in appendix b, these distributions are less intuitive, more expensive to compute, and additionally have the drawback that, if a model achieves high pairwise rc on two of the elements of the bundle, it is likely to achieve high bundle rc, even if the other elements of the test set do not achieve high pairwise rc.in general, we recommend formulating questions of consistency in terms of bundles with one instance exhibiting a feature and the other instance lacking that feature.moreover, contrastive datasets may include extra data that is not contrastive; e.g., condaqa has a small number of bundles with a single instance because other instances in the bundle were filtered because they did not pass quality checks.in §2.3, we state the drawbacks of the distribution (5).namely, we do not consider that the distribution might be skewed due to the varying example difficulty and other inherent properties of datasets and models.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Problem Definition\n",
            "\u001b[35mSection_title: 2.2  Dual Encoder Retriever\n",
            "\u001b[35mSection_title: 2.3  Zero-Shot Cross-Attention Scorer\n",
            "\u001b[35mSection_title: 2.4  Training Algorithm\n",
            "\u001b[35mSection_title: 2.5  ART as an Autoencoder\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Datasets and Evaluation\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Baselines\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  Zero-Shot Passage Retrieval\n",
            "\u001b[35mSection_title: 4.2  Sample Efficiency\n",
            "\u001b[35mSection_title: 4.3  Zero-Shot Out-of-Distribution Transfer\n",
            "\u001b[35mSection_title: 4.4  Scaling Model Size\n",
            "\u001b[35mSection_title: 4.5  Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we introduced art, a novel approach to train a dense passage retriever using only questions. art does not require question-passage pairs or hardnegative examples for training and yet achieves state-of-the-art results. the key to making art work is to optimize the retriever to select relevant passages such that conditioning on them, the question generation likelihood computed using a large pre-trained language model iteratively improves. despite requiring much less supervision, art substantially outperforms dpr when evaluated on multiple qa datasets and also generalizes better on out-of-distribution questions.\n",
            "\u001b[35mSection_title: 6  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background & Related Work\n",
            "\u001b[35mSection_title: 2.1  Token-Decomposed Scoring in Neural IR\n",
            "\u001b[35mSection_title: 2.2  Vector Compression for Neural IR\n",
            "\u001b[35mSection_title: 2.3  Improving the Quality of Single-Vector Representations\n",
            "\u001b[35mSection_title: 2.4  Out-of-Domain Evaluation in IR\n",
            "\u001b[35mSection_title: 3  ColBERTv2\n",
            "\u001b[35mSection_title: 3.1  Modeling\n",
            "\u001b[35mSection_title: 3.2  Supervision\n",
            "\u001b[35mSection_title: 3.3  Representation\n",
            "\u001b[35mSection_title: 3.4  Indexing\n",
            "\u001b[35mSection_title: 3.5  Retrieval\n",
            "\u001b[35mSection_title: 4  LoTTE: Long-Tail, Cross-Domain Retrieval Evaluation\n",
            "\u001b[35mSection_title: 5  Evaluation\n",
            "\u001b[35mSection_title: 5.1  In-Domain Retrieval Quality\n",
            "\u001b[35mSection_title: 5.2  Out-of-Domain Retrieval Quality\n",
            "\u001b[35mSection_title: 5.3  Efficiency\n",
            "\u001b[32mconclusion: we introduced colbertv2, a retriever that advances the quality and space efficiency of multivector representations. we hypothesized that cluster centroids capture context-aware semantics of the token-level representations and proposed a residual representation that leverages these patterns to dramatically reduce the footprint of multi-vector systems off-the-shelf. we then explored improved supervision for multi-vector retrieval and found that their quality improves considerably upon distillation from a cross-encoder system. the proposed colbertv2 considerably outperforms existing retrievers in within-domain and out-of-domain evaluations, which we conducted extensively across 28 datasets, establishing state-of-the-art quality while exhibiting competitive space footprint.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background & Related Work\n",
            "\u001b[35mSection_title: 2.1  Token-Decomposed Scoring in Neural IR\n",
            "\u001b[35mSection_title: 2.2  Vector Compression for Neural IR\n",
            "\u001b[35mSection_title: 2.3  Improving the Quality of Single-Vector Representations\n",
            "\u001b[35mSection_title: 2.4  Out-of-Domain Evaluation in IR\n",
            "\u001b[35mSection_title: 3  ColBERTv2\n",
            "\u001b[35mSection_title: 3.1  Modeling\n",
            "\u001b[35mSection_title: 3.2  Supervision\n",
            "\u001b[35mSection_title: 3.3  Representation\n",
            "\u001b[35mSection_title: 3.4  Indexing\n",
            "\u001b[35mSection_title: 3.5  Retrieval\n",
            "\u001b[35mSection_title: 4  LoTTE: Long-Tail, Cross-Domain Retrieval Evaluation\n",
            "\u001b[35mSection_title: 5  Evaluation\n",
            "\u001b[35mSection_title: 5.1  In-Domain Retrieval Quality\n",
            "\u001b[35mSection_title: 5.2  Out-of-Domain Retrieval Quality\n",
            "\u001b[35mSection_title: 5.3  Efficiency\n",
            "\u001b[32mconclusion: we introduced colbertv2, a retriever that advances the quality and space efficiency of multivector representations. we hypothesized that cluster centroids capture context-aware semantics of the token-level representations and proposed a residual representation that leverages these patterns to dramatically reduce the footprint of multi-vector systems off-the-shelf. we then explored improved supervision for multi-vector retrieval and found that their quality improves considerably upon distillation from a cross-encoder system. the proposed colbertv2 considerably outperforms existing retrievers in within-domain and out-of-domain evaluations, which we conducted extensively across 28 datasets, establishing state-of-the-art quality while exhibiting competitive space footprint.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology for temporal Poisson processes\n",
            "\u001b[35mSection_title: 2.1  The mixture modeling approach\n",
            "\u001b[35mSection_title: 2.2  Prior specification\n",
            "\u001b[35mSection_title: 2.3  Posterior simulation\n",
            "\u001b[35mSection_title: 2.4  Model extensions to incorporate marks\n",
            "\u001b[35mSection_title: 3  Data examples\n",
            "\u001b[35mSection_title: 3.1  Decreasing intensity synthetic point pattern\n",
            "\u001b[35mSection_title: 3.2  Increasing intensity synthetic point pattern\n",
            "\u001b[35mSection_title: 3.3  Bimodal intensity synthetic point pattern\n",
            "\u001b[35mSection_title: 3.4  Coal-mining disasters data\n",
            "\u001b[35mSection_title: 4  Modeling for spatial Poisson process intensities\n",
            "\u001b[35mSection_title: 4.1  The Erlang mixture model for spatial NHPPs\n",
            "\u001b[35mSection_title: 4.2  Synthetic data example\n",
            "\u001b[35mSection_title: 4.3  Real data illustration\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 1.2  Computational details for the MCMC algorithm\n",
            "\u001b[35mSection_title: 2  Comparison with Gaussian process based models\n",
            "\u001b[35mSection_title: 2.1  Temporal Poisson process models\n",
            "\u001b[35mSection_title: 2.2  Spatial Poisson process models\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  CUSTOMIZED PRECISION HARDWARE\n",
            "\u001b[35mSection_title: 2.1  DESIGN SPACE\n",
            "\u001b[35mSection_title: 2.2  CUSTOMIZED PRECISION TYPES\n",
            "\u001b[35mSection_title: 2.3  HARDWARE IMPLICATIONS\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  ACCURACY\n",
            "\u001b[35mSection_title: 3.2  EFFICIENCY\n",
            "\u001b[35mSection_title: 3.3  EFFICIENT CUSTOMIZED PRECISION SEARCH\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.2  ACCURACY VERSUS EFFICIENCY TRADE-OFFS\n",
            "\u001b[35mSection_title: 4.3  SOURCES OF ACCUMULATION ERROR\n",
            "\u001b[35mSection_title: 4.4  CUSTOMIZED PRECISION SEARCH\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: in this work, we introduced the importance of carefully considering customized precision when realizing neural networks. we show that using the ieee 754 single precision floating point representation in hardware results in surrendering substantial performance. on the other hand, picking a configuration that has lower precision than optimal will result in severe accuracy loss. by reconsidering the representation from the ground up in designing custom precision hardware and using our search technique, we find an average speedup across deployable dnns, including googlenet and vgg, of 7.6× with less than 1% degradation in inference accuracy.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Long Document Classification Techniques\n",
            "\u001b[35mSection_title: 2.1  Transformer-based\n",
            "\u001b[35mSection_title: 2.2  Hybrid Methods\n",
            "\u001b[35mSection_title: 3  Comparison of the proposed methods\n",
            "\u001b[35mSection_title: 4  Datasets and Reported Accuracy\n",
            "\u001b[32mconclusion: it is beyond doubt that transformer architecture changed the way linguistic analysis is performed, and in a very short time bert has been widely accepted as the golden standard of semantic understanding. however, the greatest value of this concept may be tied to its flexibility, as it allows for extensive customization and specialization with only minimal modifications of the training procedure. while there have been numerous adaptations of successful transformer models in the past, it's highly likely that the number and quality of derivative work will increase in the near future. figuring out ways to improve an already impressive model is not easy, but growing presence of this topic in the online forums and greater availability of research papers dealing with some of the outstanding challenges could power the next wave of research in this direction. this process is already underway, and a breakthrough achieved with transformers is being actively exploited by research teams from around the world.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Directions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Background\n",
            "\u001b[35mSection_title: 2.  Methods\n",
            "\u001b[35mSection_title: 2.1.  Plant Materials and Treatment\n",
            "\u001b[35mSection_title: 2.2.  Total RNA Extraction and RNA Library Construction for Sequencing\n",
            "\u001b[35mSection_title: 2.3.  Identification and Differential Expression Analysis of miRNAs\n",
            "\u001b[35mSection_title: 2.4.  Analysis of Differential Expression Genes Based on Transcriptome Sequencing\n",
            "\u001b[35mSection_title: 2.5.  GO and KEGG Pathway Enrichment Analysis\n",
            "\u001b[35mSection_title: 2.6.  Quantitative Real-Time PCR Analysis\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Identification of miRNAs in Populus simonii × P. nigra\n",
            "\u001b[35mSection_title: 3.2.  Differential Analysis of Candidate miRNAs Involved In Cold Tolerance in Populus simonii × P. nigra\n",
            "\u001b[35mSection_title: 3.3.  Identification of Candidate Cold-Responsive Genes by Transcriptome Analysis\n",
            "\u001b[35mSection_title: 3.4.  Regulation Analysis of miRNAs via Prediction of Target mRNA\n",
            "\u001b[35mSection_title: 3.5.  Expression Analysis of miRNAs by qRT-PCR\n",
            "\u001b[35mSection_title: 3.6.  qRT-PCR Validation of the Transcription Level of Target Genes\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[35mSection_title: 4.1.  Cold-Responsive miRNAs in Populus simonii × P. nigra\n",
            "\u001b[35mSection_title: 4.2.  Differentially Expressed Transcripts under Cold Treatment\n",
            "\u001b[35mSection_title: 4.3.  miRNAs May Be Involved in Cold Tolerance by Negatively Regulating Target mRNAs\n",
            "\u001b[32mconclusion: cold tolerance mirnas and candidate target genes were identified through integrated srna and transcriptome analysis in cold treatment of populus simonii × p. nigra. mir genes such as mir319, mir159, mir167, mir172, mir395, mir393, mir390, and novel_63 and transcriptional factors including myb, sbp, bzip, arf, lhw, and atl showed differential expression and they might be the main contributors related to lrr receptor kinase, the arf pathway, the spl pathway, and dnaj-related photosystem ii involved in the cold tolerance of populus simonii × p. nigra. these results not only increase our knowledge of srnas involved in the post-transcriptional regulation of cold tolerance, but also provide candidate genes for future functional analysis of the cold tolerance-related signaling pathways in populus simonii × p. nigra.\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Tests for Multifact Reasoning\n",
            "\u001b[35mSection_title: 3.1  Contrastive Support Sufficiency Test\n",
            "\u001b[35mSection_title: 3.2  Contrastive Support Sufficiency Transform\n",
            "\u001b[35mSection_title: 4  Characterizing Non-Multifact Reasoning\n",
            "\u001b[35mSection_title: 4.1  Non-Multifact Answer Prediction\n",
            "\u001b[35mSection_title: 4.2  Non-Multifact Support Identification\n",
            "\u001b[35mSection_title: 4.3  Non-Multifact Support Sufficiency\n",
            "\u001b[35mSection_title: 5  Measuring Non-Multifact Reasoning\n",
            "\u001b[35mSection_title: 5.1  Probing Answer Prediction Test\n",
            "\u001b[35mSection_title: 5.2  Probing Support Identification Test\n",
            "\u001b[35mSection_title: 5.3  Probing Support Sufficiency Test\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Experimental Setup\n",
            "\u001b[35mSection_title: 6.2  Better Metric for Multi-fact Reasoning\n",
            "\u001b[35mSection_title: 6.3  Harder Test for NMF models\n",
            "\u001b[35mSection_title: 7  Closing Remarks\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Types of Human Explanations in NLP\n",
            "\u001b[35mSection_title: 3  Proxy Questions Used to Collect Human Explanations\n",
            "\u001b[35mSection_title: 4  Can Humans Provide Explanations?\n",
            "\u001b[32mconclusion: explanations represent a fascinating phenomenon and are actively studied in psychology, cognitive science, and other social sciences. while the growing interest in explanations from the nlp community is exciting, we encourage the community to view this as an opportunity to understand how humans approach explanations and contribute to understanding and exploring the explanation processes. this will in turn inform how to collect and use human explanations in nlp. a modest proposal is that it is useful to examine and characterize human explanations before assuming that all explanations are equal and chasing a leaderboard.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Likelihood-based analysis of the point process\n",
            "\u001b[35mSection_title: 2.1  The model\n",
            "\u001b[35mSection_title: 2.2  Likelihood-based estimation\n",
            "\u001b[35mSection_title: 2.3  Asymptotic theory\n",
            "\u001b[35mSection_title: 3  The bootstrap\n",
            "\u001b[35mSection_title: 3.1  Random time change\n",
            "\u001b[35mSection_title: 3.2  Fixed intensity bootstrap\n",
            "\u001b[35mSection_title: 3.3  Recursive intensity bootstrap\n",
            "\u001b[35mSection_title: 4  Validity of bootstrap inference\n",
            "\u001b[35mSection_title: 4.1  Preliminaries\n",
            "\u001b[35mSection_title: 4.2  Validity of the FIB\n",
            "\u001b[35mSection_title: 4.3  Validity of the RIB\n",
            "\u001b[35mSection_title: 5  Non-parametric FIB and RIB\n",
            "\u001b[35mSection_title: 6  Relation with the bootstrap for ACD models\n",
            "\u001b[35mSection_title: 7  Monte Carlo Simulations\n",
            "\u001b[35mSection_title: 7.1  Model and implementation\n",
            "\u001b[35mSection_title: 7.2  Results\n",
            "\u001b[35mSection_title: 8  Empirical illustrations\n",
            "\u001b[35mSection_title: 8.1  Dow Jones Index\n",
            "\u001b[35mSection_title: 8.2  COVID-19 Tweets\n",
            "\u001b[32mconclusion: in this paper we have discussed the theoretical foundations and practical implementations of bootstrap inference for self-exciting point process models. applications of the bootstrap in order to improve upon the poor quality of asymptotic approximations are scarce in the literature. classic 'recursive intensity bootstrap' (rib) schemes have been proposed in the recent literature, although without proof of their first-order validity. rib schemes can also be quite involved to implement in practice, as they generally require numerical integration for the recursive computation of the intensity for each bootstrap repetition. to improve, we have introduced a new bootstrap scheme, the 'fixed intensity bootstrap' (fib), where the conditional intensity is kept fixed across bootstrap repetitions. by doing so, conditionally on the original data the bootstrap data generating process follows a simple inhomogeneous point process with known intensity; therefore, it is very simple to implement and to use in practice. for both bootstrap schemes, we have provided a new bootstrap (asymptotic) theory, which allows to assess bootstrap validity for both bootstraps. monte carlo evidence supports the idea that the bootstrap is a valid inference method when applied to point process models.\n",
            "\u001b[35mSection_title: 9  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Contributions\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 4.  Follow-up question Generator:\n",
            "\u001b[35mSection_title: 3.1  Input Embeddings and BERT\n",
            "\u001b[35mSection_title: 3.2  Marker Embeddings\n",
            "\u001b[35mSection_title: 3.2.1  Dialog History Embedding:\n",
            "\u001b[35mSection_title: 3.2.2  Scenario Embedding:\n",
            "\u001b[35mSection_title: 3.3  Answering and Follow-up question Generation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Augmented ShARC dataset\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Evaluation\n",
            "\u001b[35mSection_title: 4.4.1  Ablation Study Model\n",
            "\u001b[35mSection_title: 4.4.2  Error Analysis\n",
            "\u001b[35mSection_title: 4.4.3  Probing Study\n",
            "\u001b[35mSection_title: 4.4.4  Leader-Board Submission\n",
            "\u001b[32mconclusion: in this paper we show how the existing neural models exploit spurious patterns that exist in the data for the sharc task -a conversation qa that requires reasoning over rules expressed in natural language. we demonstrate how existing models can exploit spurious patterns in such conversational qa datasets and introduce an augmented version of the sharc dataset that discourages a model from exploiting such spurious clues. we also present a simple yet effective model, ur-canet, that learns embedding representation from the dialog history, dialog turns, and the history of past follow-up question and answer pairs. the network generate intermediate representations which is input to a copy decoder to generate a follow-up question. urcanet outperforms existing systems on both the original sharc corpus and the augmented sharc corpus.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methods\n",
            "\u001b[35mSection_title: 2.1  Answer-focused Document Encoding\n",
            "\u001b[35mSection_title: 2.2  Multi-hop Answer-focused Reasoning\n",
            "\u001b[35mSection_title: 2.3  Aggregation Layer\n",
            "\u001b[35mSection_title: 2.4  Decoder\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Experiment Setting\n",
            "\u001b[35mSection_title: 3.2  Results and Analysis\n",
            "\u001b[35mSection_title: 3.3  Human Evaluation\n",
            "\u001b[35mSection_title: 3.4  Implementation Details\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we proposed a new task that asks complex questions given a collection of documents and the corresponding answer by discovering and modeling the multiple entities and their semantic relations across the documents. to solve the problem, we propose answer-focused multi-hop reasoning by leveraging different granularity levels of semantic information in the answer-centric entity graph built from natural language text. extensive experiment results demonstrate the superiority of our proposed model in terms of automatically computed metrics and human evaluation. our work provides a baseline for the new task and sheds light on future work in the multi-hop question generation scenario.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  Methodology\n",
            "\u001b[35mSection_title: 4.1  Document Retrieval\n",
            "\u001b[35mSection_title: 4.1.1  Query Paraphrasing\n",
            "\u001b[35mSection_title: 4.1.2  Search Engine\n",
            "\u001b[35mSection_title: 4.2  Relevant Snippet Selector\n",
            "\u001b[35mSection_title: 4.2.1  Question Answering\n",
            "\u001b[35mSection_title: 4.2.2  Highlights Generation and Answer Re-ranking\n",
            "\u001b[35mSection_title: 4.3  Summarization\n",
            "\u001b[32mconclusion: we have described our system, caire-covid, comprising of three major modules, information retrieval, question answering, and summarization, which uses the cord-19 dataset consisting of published scientific articles concerning covid-19. our system can answer user queries related to covid-19 by retrieving relevant paragraphs from articles available in the dataset, using our qa models to answer the question, and also generate two versions of a concise summary of the top paragraphs via the two summarization models. we believe that getting factual information regarding covid-19 and showing them in a comprehensible way, we can prioritise scientific facts about the virus, and help the community in the fight against the ongoing global pandemic.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 0.1  Algorithms and the future of physics\n",
            "\u001b[35mSection_title: 0.2  Preface\n",
            "\u001b[35mSection_title: 0.2.1  Constructivism and its role in quantum theory\n",
            "\u001b[35mSection_title: 0.2.2  My vision of the history of question\n",
            "\u001b[35mSection_title: 0.2.3  Brief review of contents\n",
            "\u001b[35mSection_title: 1.1  What does the simulation of processes mean\n",
            "\u001b[35mSection_title: 1.2  Visualization and the role of user\n",
            "\u001b[35mSection_title: 4.  Photons.\n",
            "\u001b[35mSection_title: 1.3  Evolutionary principle in the simulation of dynamics\n",
            "\u001b[35mSection_title: 1.3.1  Cauchy problem\n",
            "\u001b[35mSection_title: 1.3.2  When the evolutionary simulation is required\n",
            "\u001b[35mSection_title: 1.4  Summary of the simulation of dynamical scenarios\n",
            "\u001b[35mSection_title: 2.0.1  Review of mathematical constructivism\n",
            "\u001b[35mSection_title: 2.1  Algorithms and computable functions\n",
            "\u001b[35mSection_title: 2.2  Church-Turing-Markov thesis\n",
            "\u001b[35mSection_title: 2.2.1  Computations with oracle\n",
            "\u001b[35mSection_title: 2.3  Constructive mathematical logic and quantum theory\n",
            "\u001b[35mSection_title: 2.3.1  Standard mathematical logic\n",
            "\u001b[35mSection_title: 2.3.2  The problems of the consistency of logical theories\n",
            "\u001b[35mSection_title: 2.3.3  Constructive mathematical logic\n",
            "\u001b[35mSection_title: 2.3.4  Idea of pluralism and its importance for physics\n",
            "\u001b[35mSection_title: 2.4  Constructive mathematical analysis\n",
            "\u001b[35mSection_title: 2.4.1  Constructive real numbers\n",
            "\u001b[35mSection_title: 2.4.2  Constructive functions of constructive real variable\n",
            "\u001b[35mSection_title: 2.5  Constructive algebra for quantum mechanics\n",
            "\u001b[35mSection_title: 2.5.1  Algebraic apparatus of quantum theory\n",
            "\u001b[35mSection_title: 2.5.2  Classical algebraic systems\n",
            "\u001b[35mSection_title: 2.5.3  Constructive algebraic systems\n",
            "\u001b[35mSection_title: 2.6  Summary of mathematical constructivism\n",
            "\u001b[35mSection_title: 3.1  Particles and elementary interaction\n",
            "\u001b[35mSection_title: 3.2  Differential equations\n",
            "\u001b[35mSection_title: 3.3  About the scalability of classical models\n",
            "\u001b[35mSection_title: 4.1  Main concepts of one particle quantum mechanics\n",
            "\u001b[35mSection_title: 4.1.1  Qubit formalism\n",
            "\u001b[35mSection_title: 4.2  Feynman path integral\n",
            "\u001b[35mSection_title: 4.3  Formalism of the many body quantum theory\n",
            "\u001b[35mSection_title: 4.4  Unitary dynamics and measurements\n",
            "\u001b[35mSection_title: 4.5  Many world interpretation of quantum theory\n",
            "\u001b[35mSection_title: 4.6  Quantum computer\n",
            "\u001b[35mSection_title: 4.6.1  Idea of quantum computer\n",
            "\u001b[35mSection_title: 4.6.2  Abstract model of quantum computer\n",
            "\u001b[35mSection_title: 4.7  Role of entanglement\n",
            "\u001b[35mSection_title: 4.8  Formalism of quantum electrodynamics in qubit form\n",
            "\u001b[35mSection_title: 4.8.1  Review of standard formalism for QED\n",
            "\u001b[35mSection_title: 4.9  Simulation of quantum systems\n",
            "\u001b[35mSection_title: 4.10  Ensembles of identical fermions and bosons\n",
            "\u001b[35mSection_title: 4.11  Spin and spatial coordinates\n",
            "\u001b[35mSection_title: 4.12  Problem of decoherence or why to reconsider the basement of quantum theory\n",
            "\u001b[35mSection_title: 4.13  Resume of standard quantum mechanics\n",
            "\u001b[35mSection_title: 5.1  About physical sense of algebraic operations\n",
            "\u001b[35mSection_title: 5.2  Amplitude quanta and Born rule\n",
            "\u001b[35mSection_title: 5.3  Absolute model of decoherence\n",
            "\u001b[35mSection_title: 5.4  Method of collective behavior\n",
            "\u001b[35mSection_title: 5.4.1  What is constructivism in practice\n",
            "\u001b[35mSection_title: 5.4.2  Statement of problem\n",
            "\u001b[35mSection_title: 5.4.3  Diffusion Monte Carlo method\n",
            "\u001b[35mSection_title: 5.4.4  Known ways of approximation of Shredinger equation by quasi-classical ensembles\n",
            "\u001b[35mSection_title: 5.4.5  Dynamical diffusion swarm\n",
            "\u001b[35mSection_title: 5.  ALGORITHMIC MODIFICATION OF QUANTUM THEORY\n",
            "\u001b[35mSection_title: 5.4.6  Differential equations for the dynamical diffusion swarm\n",
            "\u001b[35mSection_title: 5.4.7  About the diffusion swarm with the non uniform intensity\n",
            "\u001b[35mSection_title: 5.4.8  Equivalence of quantum and diffusion swarms\n",
            "\u001b[35mSection_title: 5.4.9  Restoration of wave function from the dynamical diffusion swarm\n",
            "\u001b[35mSection_title: 5.4.10  The method of collective behavior for many particles\n",
            "\u001b[35mSection_title: 5.4.11  Problems of simulation of the dynamical diffusion\n",
            "\u001b[35mSection_title: 5.5  Simulation of quantum dynamics via quantum state selection\n",
            "\u001b[35mSection_title: 5.5.1  Scattering of a proton on a hydrogen atom\n",
            "\u001b[35mSection_title: 5.5.2  Effective selection algorithm for states of n particles\n",
            "\u001b[35mSection_title: 5.  ALGORITHMIC MODIFICATION OF QUANTUM THEORY\n",
            "\u001b[35mSection_title: 5.6  Identity of electrons from the viewpoint of collective behavior\n",
            "\u001b[35mSection_title: 5.7  Method of collective behavior for quantum electrodynamics\n",
            "\u001b[35mSection_title: 5.7.1  Swarm representation of charged particles in electromagnetic field\n",
            "\u001b[35mSection_title: 5.7.2  Swarm description of fundamental processes of QED\n",
            "\u001b[35mSection_title: 5.7.3  Procedure of quantum state selection with photons\n",
            "\u001b[35mSection_title: 5.7.4  Description of chemical association with the implicit account of photons\n",
            "\u001b[35mSection_title: 5.7.5  About scalability of QED\n",
            "\u001b[35mSection_title: 5.8  Bounds between samples\n",
            "\u001b[35mSection_title: 5.8.1  Bonds for one real particle\n",
            "\u001b[35mSection_title: 5.8.2  Bonds for many particles. Quantum \"worlds\" ordering\n",
            "\u001b[35mSection_title: 5.9  Heuristic of collective behavior\n",
            "\u001b[35mSection_title: 5.10  Evolution of net\n",
            "\u001b[35mSection_title: 5.10.1  Pointers\n",
            "\u001b[35mSection_title: 5.10.2  Separation of spatial and spin variables\n",
            "\u001b[35mSection_title: 5.10.3  Constructive treatment of uncertainty relations\n",
            "\u001b[35mSection_title: 5.11  Features of the description of QED by nets\n",
            "\u001b[35mSection_title: 5.11.1  Photons and entanglement\n",
            "\u001b[35mSection_title: 5.11.2  Photon threads\n",
            "\u001b[35mSection_title: 5.12  What do we hope to reach with the heuristic of collective behavior\n",
            "\u001b[35mSection_title: 5.12.  WHAT DO WE HOPE TO REACH WITH THE HEURISTIC OF COLLECTIVE BEHAVIOR21\n",
            "\u001b[35mSection_title: 5.13  Back door in quantum informatics\n",
            "\u001b[35mSection_title: 5.13.1  Constructive treatment of search problems\n",
            "\u001b[35mSection_title: 5.13.2  About the application of limited quantum processors in supercomputers\n",
            "\u001b[35mSection_title: 5.13.3  Influence of physical constructivism to quantum cryptography\n",
            "\u001b[35mSection_title: 5.14  Review of algorithmic modification of quantum theory\n",
            "\u001b[35mSection_title: 5.14.  REVIEW OF ALGORITHMIC MODIFICATION OF QUANTUM THEORY229\n",
            "\u001b[35mSection_title: 6.1  Actuality of PCN\n",
            "\u001b[35mSection_title: 6.2  What is the data compression and why it does not satisfy us\n",
            "\u001b[35mSection_title: 6.3  About the language of PCN\n",
            "\u001b[35mSection_title: 6.4  Why PCN is needed\n",
            "\u001b[35mSection_title: 6.5  Account of random factor in PCN\n",
            "\u001b[35mSection_title: 6.6  About the individuality of elementary particles\n",
            "\u001b[35mSection_title: 6.6.1  Pierson criterion of agreement\n",
            "\u001b[35mSection_title: 6.6.2  Individuality of samples of particles\n",
            "\u001b[32mconclusion: we familiarized ourselves with ideas joint by the common name of constructive physics. these ideas do not form the separate discipline, as the constructive mathematics is not some separate science. constructivism is the direction, which aroused in mathematics, namely, in its foundations, and in its development absorbs physics. this process is unavoidable and wholesome. i will be glad if this book helps a reader to form more definite attitude to the constructivism, even more if it excites the desire to take up the development of this direction immediately. i permit myself to enumerate some problems which solution seems to me accessible in the framework of constructivism right now.\n",
            "\u001b[35mSection_title: 6.7  Conclusion\n",
            "\u001b[35mSection_title: 3.3  GSA\n",
            "\u001b[35mSection_title: 5.1  Factoring of integers\n",
            "\u001b[35mSection_title: 7.3  Formalism of occupation numbers\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Multimodal Output\n",
            "\u001b[35mSection_title: 3  Multimodal Output QA\n",
            "\u001b[35mSection_title: 4  Dataset & Pretraining\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[32mconclusion: we presented one of the first exploration, to the best of our knowledge, of multimodal output question answering from multimodal inputs and proposed usage of publicly available textual datasets for it. we proposed strong baselines by utilizing the existing frameworks for extract textual answers and independently match them with an appropriate image. we demonstrate the value of a joint-multimodal understanding for multimodal outputs in our problem setup by developing a multimodal framework mexbert which outperformed the baselines significantly on several metrics. we also developed a proxy supervision technique in absence of labelled outputs and showed its effectiveness for improved multimodal question answering. we used some existing metrics to compare the different models and justified the usage of these metrics based on a human experiment.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Definition and Categorization of Logical Reasoning\n",
            "\u001b[35mSection_title: 3  Advantages of LRNL\n",
            "\u001b[35mSection_title: 3.1  Advantages over Formal Language\n",
            "\u001b[35mSection_title: 3.2  Advantages over Existing NeSy Systems\n",
            "\u001b[35mSection_title: 3.3  Advantages over E2E Neural Methods\n",
            "\u001b[35mSection_title: 4  Deductive Reasoning over Natural Language\n",
            "\u001b[35mSection_title: 4.1  Existing Task Formulations\n",
            "\u001b[35mSection_title: 4.2  Methods\n",
            "\u001b[35mSection_title: 4.2.1  Hypothesis Classification\n",
            "\u001b[35mSection_title: 4.2.2  Proof Generation\n",
            "\u001b[35mSection_title: 4.2.3  Proof with Incomplete Information\n",
            "\u001b[35mSection_title: 4.2.4  Implication Enumeration\n",
            "\u001b[35mSection_title: 4.3  Robustness of PLM as Reasoner\n",
            "\u001b[35mSection_title: 5.1  Existing Task Formulations\n",
            "\u001b[35mSection_title: 5.2  Methods\n",
            "\u001b[35mSection_title: 5.2.1  Rule Classification\n",
            "\u001b[35mSection_title: 5.2.2  Rule Generation\n",
            "\u001b[35mSection_title: 6.1  Existing Task Formulations\n",
            "\u001b[35mSection_title: 6.2  Methods\n",
            "\u001b[35mSection_title: 6.2.1  Explanation Classification\n",
            "\u001b[35mSection_title: 6.2.2  Explanation Generation without Theory\n",
            "\u001b[35mSection_title: 6.2.3  Explanation Generation with Theory\n",
            "\u001b[35mSection_title: 7  Challenges of LRNL\n",
            "\u001b[35mSection_title: 8  Desirable Tasks and Methods in the Future\n",
            "\u001b[32mconclusion: in this paper, we propose a new concept, logical reasoning over natural language as knowledge representation (lrnl), and provide a detailed and up-to-date review of lrnl. moreover, we have introduced the philosophical foundations, advantages of lrnl, benchmarks and methods, challenges, desirable tasks & methods, and the relation of lrnl to related nlp fields ( §a.1).\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Hyperlink-induced Pre-training (HLP)\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  Hyperlink-induced Q-P Pairs\n",
            "\u001b[35mSection_title: 3.3  Bi-encoder Training\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Pre-training Corpus\n",
            "\u001b[35mSection_title: 4.2  Downstream Datasets\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.4  Baselines\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Few-shot Learning\n",
            "\u001b[35mSection_title: 5.4  Multi-hop Retrieval\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 6.1  Ablation Study\n",
            "\u001b[35mSection_title: 6.2  Analysis on Q-P Overlap\n",
            "\u001b[35mSection_title: 6.3  Human Evaluation on Q-P pairs\n",
            "\u001b[32mconclusion: this paper proposes hyperlink-induced pretraining (hlp), a pre-training method for openqa passage retrieval by leveraging the online textual relevance induced by hyperlink-based topology. our experiments show that hlp gains significant improvements across multiple qa datasets under different scenarios, consistently outperforming other pre-training methods. our method provides insights into openqa passage retrieval by analyzing the underlying bi-text relevance. future work involves addressing tasks like ms marco where the granularity of the information-seeking target is at the passage level. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 3.2.  Definições do estudo fluxo pressão\n",
            "\u001b[35mSection_title: 3.2.1.  Função detrusora, durante a fase miccional, em pessoas que podem iniciar micção voluntária.\n",
            "\u001b[35mSection_title: 3.2.1.4.  Esvaziamento vesical balanceado:\n",
            "\u001b[35mSection_title: 3.2.2.1.  Esvaziamento vesical iniciado por reflexo:\n",
            "\u001b[35mSection_title: 4.  DISFUNÇÃO NEUROGÊNICA DO TRATO URI NÁRIO INFERIOR -DIAGNÓSTICO CLÍNICO\n",
            "\u001b[35mSection_title: 4.2.  Lesão medular suprapontina (LMSP\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Motivation\n",
            "\u001b[35mSection_title: 3  Background\n",
            "\u001b[35mSection_title: 3.1  Question answering software\n",
            "\u001b[35mSection_title: 3.1.1  QA Software based on information in knowledge base\n",
            "\u001b[35mSection_title: 3.1.2  QA Software based on information in textual materials\n",
            "\u001b[35mSection_title: 3.2  Metamorphic testing\n",
            "\u001b[35mSection_title: 4  Methodology\n",
            "\u001b[35mSection_title: 4.1  A recursive metamorphic testing method for QA software\n",
            "\u001b[35mSection_title: 4.2  Proposed metamorphic relations\n",
            "\u001b[35mSection_title: 4.3  Declarative sentence synthesis\n",
            "\u001b[35mSection_title: 4.3.1  Declarative sentence synthesis based on general question and its answer\n",
            "\u001b[35mSection_title: 4.3.2  Declarative sentence synthesis based on alternative question and its answer\n",
            "\u001b[35mSection_title: 4.3.3  Declarative sentence synthesis based on Wh-question and its answer\n",
            "\u001b[35mSection_title: 4.3.4  Declarative sentence synthesis based on Wh-question and extra information about its answer\n",
            "\u001b[35mSection_title: 4.4  Follow-up question sentence generation\n",
            "\u001b[35mSection_title: 4.4.1  General question sentence generation\n",
            "\u001b[35mSection_title: 4.4.2  Wh-question sentence generation\n",
            "\u001b[35mSection_title: 4.5  Violation measurement\n",
            "\u001b[35mSection_title: 4.5.1  Existence measurement\n",
            "\u001b[35mSection_title: 4.5.2  Affirmation measurement\n",
            "\u001b[35mSection_title: 5  Experimental setup\n",
            "\u001b[35mSection_title: 5.1  Research questions\n",
            "\u001b[35mSection_title: 5.2  Data preparation\n",
            "\u001b[35mSection_title: 5.3  Test objects\n",
            "\u001b[35mSection_title: 6  Results and analysis\n",
            "\u001b[35mSection_title: 6.1  RQ1: The overall effectiveness of QAAskeR +\n",
            "\u001b[35mSection_title: 6.2  RQ2: Effectiveness comparison with Non-recursive metamorphic relations\n",
            "\u001b[35mSection_title: 6.3  RQ3: Validity of the revealed violations\n",
            "\u001b[35mSection_title: 6.4  RQ4: Analysis on the revealed true violations\n",
            "\u001b[35mSection_title: 6.5  RQ5: Helpfulness to fix the answering issues revealed by MRs\n",
            "\u001b[35mSection_title: 7  Discussion on real-life usage\n",
            "\u001b[35mSection_title: 8  Threats to validity\n",
            "\u001b[35mSection_title: 9  Related works\n",
            "\u001b[35mSection_title: 9.1  Benchmark datasets for QA software\n",
            "\u001b[35mSection_title: 9.2  Metamorphic testing for deep learning software\n",
            "\u001b[32mconclusion: question answering (qa) software has been widely used in our daily life. in this paper, we propose a novel recursive metamorphic testing method qaasker + with five novel recursive metamorphic relations. qaasker + tests qa software by checking its behaviors on multiple recursively asked questions that are relevant to the same or some further enriched knowledge. it cuts off the reliance on the preannotated labels of test cases, thus enables both the flexible just-in-time test during usage and the extensible test with massive unlabeled data for qa software, which cannot be supported by the current reference-based test paradigm. we evaluate the effectiveness of qaasker + by using it to test four representative state-of-the-art qa software that covers two mainstream types of qa software, as well as a popular reallife qa application, the google search service. comprehensive results demonstrate that qaasker + can reveal quantities of valid violations that depict diverse answering issues for various kinds of mainstream qa software. besides, we also found that our recursive mrs have a better fault detection effectiveness than two representative non-recursive mrs and can even help to fix the revealed issues.\n",
            "\u001b[35mSection_title: 10  Conclusion and future work\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Motivation\n",
            "\u001b[35mSection_title: 3  Background\n",
            "\u001b[35mSection_title: 3.1  Question answering software\n",
            "\u001b[35mSection_title: 3.1.1  QA Software based on information in knowledge base\n",
            "\u001b[35mSection_title: 3.1.2  QA Software based on information in textual materials\n",
            "\u001b[35mSection_title: 3.2  Metamorphic testing\n",
            "\u001b[35mSection_title: 4  Methodology\n",
            "\u001b[35mSection_title: 4.1  A recursive metamorphic testing method for QA software\n",
            "\u001b[35mSection_title: 4.2  Proposed metamorphic relations\n",
            "\u001b[35mSection_title: 4.3  Declarative sentence synthesis\n",
            "\u001b[35mSection_title: 4.3.1  Declarative sentence synthesis based on general question and its answer\n",
            "\u001b[35mSection_title: 4.3.2  Declarative sentence synthesis based on alternative question and its answer\n",
            "\u001b[35mSection_title: 4.3.3  Declarative sentence synthesis based on Wh-question and its answer\n",
            "\u001b[35mSection_title: 4.3.4  Declarative sentence synthesis based on Wh-question and extra information about its answer\n",
            "\u001b[35mSection_title: 4.4  Follow-up question sentence generation\n",
            "\u001b[35mSection_title: 4.4.1  General question sentence generation\n",
            "\u001b[35mSection_title: 4.4.2  Wh-question sentence generation\n",
            "\u001b[35mSection_title: 4.5  Violation measurement\n",
            "\u001b[35mSection_title: 4.5.1  Existence measurement\n",
            "\u001b[35mSection_title: 4.5.2  Affirmation measurement\n",
            "\u001b[35mSection_title: 5  Experimental setup\n",
            "\u001b[35mSection_title: 5.1  Research questions\n",
            "\u001b[35mSection_title: 5.2  Data preparation\n",
            "\u001b[35mSection_title: 5.3  Test objects\n",
            "\u001b[35mSection_title: 6  Results and analysis\n",
            "\u001b[35mSection_title: 6.1  RQ1: The overall effectiveness of QAAskeR +\n",
            "\u001b[35mSection_title: 6.2  RQ2: Effectiveness comparison with Non-recursive metamorphic relations\n",
            "\u001b[35mSection_title: 6.3  RQ3: Validity of the revealed violations\n",
            "\u001b[35mSection_title: 6.4  RQ4: Analysis on the revealed true violations\n",
            "\u001b[35mSection_title: 6.5  RQ5: Helpfulness to fix the answering issues revealed by MRs\n",
            "\u001b[35mSection_title: 7  Discussion on real-life usage\n",
            "\u001b[35mSection_title: 8  Threats to validity\n",
            "\u001b[35mSection_title: 9  Related works\n",
            "\u001b[35mSection_title: 9.1  Benchmark datasets for QA software\n",
            "\u001b[35mSection_title: 9.2  Metamorphic testing for deep learning software\n",
            "\u001b[32mconclusion: question answering (qa) software has been widely used in our daily life. in this paper, we propose a novel recursive metamorphic testing method qaasker + with five novel recursive metamorphic relations. qaasker + tests qa software by checking its behaviors on multiple recursively asked questions that are relevant to the same or some further enriched knowledge. it cuts off the reliance on the preannotated labels of test cases, thus enables both the flexible just-in-time test during usage and the extensible test with massive unlabeled data for qa software, which cannot be supported by the current reference-based test paradigm. we evaluate the effectiveness of qaasker + by using it to test four representative state-of-the-art qa software that covers two mainstream types of qa software, as well as a popular reallife qa application, the google search service. comprehensive results demonstrate that qaasker + can reveal quantities of valid violations that depict diverse answering issues for various kinds of mainstream qa software. besides, we also found that our recursive mrs have a better fault detection effectiveness than two representative non-recursive mrs and can even help to fix the revealed issues.\n",
            "\u001b[35mSection_title: 10  Conclusion and future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Knowledge Sources\n",
            "\u001b[35mSection_title: 2.1  Encyclopedic Knowledge\n",
            "\u001b[35mSection_title: 2.2  Commonsense Knowledge\n",
            "\u001b[35mSection_title: 2.3  Characteristics of Current Knowledge Sources\n",
            "\u001b[35mSection_title: 3  Knowledge-Intensive NLP Tasks\n",
            "\u001b[35mSection_title: 3.1  Overview of Knowledge-Intensive NLP Tasks\n",
            "\u001b[35mSection_title: 3.2  Characteristics of Knowledge-Intensive Tasks\n",
            "\u001b[35mSection_title: 4  Knowledge Fusion Methods\n",
            "\u001b[35mSection_title: 4.1  Pre-Fusion Methods\n",
            "\u001b[35mSection_title: 4.2  Post-Fusion Methods\n",
            "\u001b[35mSection_title: 4.3  Hybrid-Fusion Methods\n",
            "\u001b[35mSection_title: 4.4  Representative Models for Specific Tasks\n",
            "\u001b[35mSection_title: 5  Challenges and Future Directions\n",
            "\u001b[35mSection_title: 5.1  Unified PLMKEs Across Tasks and Domains\n",
            "\u001b[35mSection_title: 5.2  Reliabilty of Knowledge Sources\n",
            "\u001b[35mSection_title: 5.3  Reasoning Module Design\n",
            "\u001b[32mconclusion: we comprehensively survey existing works about knowledgeintensive nlp with pre-trained language models and summarize the current progress in terms of the three critical components in plmkes: knowledge sources, knowledge-intensive nlp tasks, and knowledge fusion methods. based on the discussion about the three components, we further pose several challenges that would be influential in the practical usage and propose the related future directions in response to the challenges. we hope that this paper could provide nlp practitioners with a clear picture on the topic and boost the development of the current knowledge-intensive nlp technologies.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Definition\n",
            "\u001b[35mSection_title: 2.1  Semantics\n",
            "\u001b[35mSection_title: 2.2  Formulations\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Parameterization\n",
            "\u001b[35mSection_title: 3.3  Learning the Model\n",
            "\u001b[35mSection_title: 3.4  Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Metrics\n",
            "\u001b[35mSection_title: 4.2  Fully Supervised Learning\n",
            "\u001b[35mSection_title: 4.3  Few-shot Learning\n",
            "\u001b[35mSection_title: 4.4  Zero-shot Evaluation\n",
            "\u001b[35mSection_title: 4.5  Generalization Ability\n",
            "\u001b[35mSection_title: 4.6  Ablation Studies\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose probr, a novel probabilistic graph reasoning framework for joint question answering and proof generation. probr defines a joint distribution over all possible answers and proofs, which can directly characterize the interaction between answers and proofs. experiments prove the effectiveness of proposed probr.        1. in all ablation experiments, probr achieved the best qa performance, demonstrating that probr can capture critical information for question answering in a variety of settings. however, since some of the dataset are artificially synthesized, it is difficult to guarantee that probr will work in the real dataset as well. we leave it as future work.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Data Augmentation for Few-shot NLP\n",
            "\u001b[35mSection_title: 2.2  Overview of KnowDA\n",
            "\u001b[35mSection_title: 2.3  Knowledge Mixture Training\n",
            "\u001b[35mSection_title: 2.4  Generating Synthetic Data Using KnowDA\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Data Augmentation On FewGLUE\n",
            "\u001b[35mSection_title: 3.4  Few-shot Task Solver\n",
            "\u001b[35mSection_title: 3.5  Discussion\n",
            "\u001b[35mSection_title: 3.5.1  Task Knowledge Transfer\n",
            "\u001b[35mSection_title: 3.5.2  Synthetic Data Quality Analysis\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: this paper explores multi-task learning paradigms at a massive scale for data augmentation in fewshot language learning for the first time. we demonstrate that the proposed knowledge mixture training enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences). experiments verified the effectiveness of our knowda, and knowda outperforms state-of-the-art data augmentation approaches on well-established benchmarks superglue, conll'03, and wikiann in the few-shot setting. we also perform ablation studies indicating the importance of including demonstrations and the impact of different keys. moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration. [30] prashanth vijayaraghavan, ivan sysoev, soroush vosoughi, and deb roy. deepstance at semeval-2016 task 6: detecting stance in tweets using character and word-level cnns. arxiv preprint arxiv:1606.05694, 2016.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Data Augmentation for Few-shot NLP\n",
            "\u001b[35mSection_title: 2.2  Overview of KnowDA\n",
            "\u001b[35mSection_title: 2.3  Knowledge Mixture Training\n",
            "\u001b[35mSection_title: 2.4  Generating Synthetic Data Using KnowDA\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Data Augmentation On FewGLUE\n",
            "\u001b[35mSection_title: 3.4  Few-shot Task Solver\n",
            "\u001b[35mSection_title: 3.5  Discussion\n",
            "\u001b[35mSection_title: 3.5.1  Task Knowledge Transfer\n",
            "\u001b[35mSection_title: 3.5.2  Synthetic Data Quality Analysis\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: this paper explores multi-task learning paradigms at a massive scale for data augmentation in fewshot language learning for the first time. we demonstrate that the proposed knowledge mixture training enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences). experiments verified the effectiveness of our knowda, and knowda outperforms state-of-the-art data augmentation approaches on well-established benchmarks superglue, conll'03, and wikiann in the few-shot setting. we also perform ablation studies indicating the importance of including demonstrations and the impact of different keys. moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration. [30] prashanth vijayaraghavan, ivan sysoev, soroush vosoughi, and deb roy. deepstance at semeval-2016 task 6: detecting stance in tweets using character and word-level cnns. arxiv preprint arxiv:1606.05694, 2016.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 1.1.  Overview of the Research\n",
            "\u001b[35mSection_title: 1.2.  Organization of the Paper\n",
            "\u001b[35mSection_title: 1.3.  Related Works\n",
            "\u001b[35mSection_title: 1.3.1.  Neural Language Models\n",
            "\u001b[35mSection_title: 1.3.2.  Information Retrieval\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Hardware and Software\n",
            "\u001b[35mSection_title: 2.2.  CORD-19 Dataset\n",
            "\u001b[35mSection_title: 2.3.  The Language Model\n",
            "\u001b[35mSection_title: 2.3.1.  Bibliography Latent Information\n",
            "\u001b[35mSection_title: 2.3.2.  Training Set Creation\n",
            "\u001b[35mSection_title: 2.3.3.  Loss Function\n",
            "\u001b[35mSection_title: 2.4.  The IR System\n",
            "\u001b[35mSection_title: 2.4.1.  The Indexer\n",
            "\u001b[35mSection_title: 2.4.2.  The Retriever\n",
            "\u001b[35mSection_title: 2.4.3.  The Ranker\n",
            "\u001b[35mSection_title: 2.4.4.  IR System Configuration\n",
            "\u001b[35mSection_title: 2.5.  Language Model Fine-Tuning with Teacher\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  TREC-COVID Test Set\n",
            "\u001b[35mSection_title: 3.2.  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.2.1.  Precision\n",
            "\u001b[35mSection_title: 3.2.2.  nDCG\n",
            "\u001b[35mSection_title: 3.2.3.  MAP\n",
            "\u001b[35mSection_title: 3.2.4.  Bpref\n",
            "\u001b[35mSection_title: 3.3.  IR Results\n",
            "\u001b[35mSection_title: 3.4.  System Size Comparison\n",
            "\u001b[35mSection_title: 3.5.  Bibliography Embeddings Evaluation\n",
            "\u001b[35mSection_title: 3.6.  SUBLIMER: Web Information Retrieval Application\n",
            "\u001b[32mconclusion: we proposed a new self-supervised method to create a latent semantic space from unlabelled corpora of papers, where the spatial proximity among them represents their semantic similarity. however in unsupervised corpora of papers, such as the cord-19 that contains a large collection of the covid literature, is unknown which papers are positively and negatively related each other.\n",
            "\u001b[35mSection_title: 4.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  ROBUT Benchmark\n",
            "\u001b[35mSection_title: 3.2  Table Content Perturbation\n",
            "\u001b[35mSection_title: 1.  Human generates question q and\n",
            "\u001b[35mSection_title: 2.  (p, q) sent to the model.\n",
            "\u001b[35mSection_title: 3  Annotation Methodology\n",
            "\u001b[35mSection_title: 3.1  Annotation Protocol\n",
            "\u001b[35mSection_title: 3.2  Annotation Details\n",
            "\u001b[35mSection_title: 2.  (T, Q perturb ) is input to the model.\n",
            "\u001b[35mSection_title: 1.  Given\n",
            "\u001b[35mSection_title: 3.3  NLQ Perturbation\n",
            "\u001b[35mSection_title: 3.4  Mix Perturbation\n",
            "\u001b[35mSection_title: 4  Diagnostic Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Diagnostic Results\n",
            "\u001b[35mSection_title: 5  LETA Framework\n",
            "\u001b[35mSection_title: 5.1  Table Header Augmentation\n",
            "\u001b[35mSection_title: 5.2  Table Content Augmentation\n",
            "\u001b[35mSection_title: 5.3  NLQ Augmentation\n",
            "\u001b[35mSection_title: 6  Adversarial Training Experiments\n",
            "\u001b[35mSection_title: 6.1  Experiment Setup\n",
            "\u001b[35mSection_title: 6.2  Results\n",
            "\u001b[35mSection_title: 6.3  Analysis\n",
            "\u001b[32mconclusion: this work proposes robut, the first benchmark for \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 3  Sicherheitsanalyse DICOM\n",
            "\u001b[35mSection_title: 4  Sicherheitsanalyse HL7\n",
            "\u001b[35mSection_title: 5  Softwareplattform zum Testen medizinischer Software und Systeme\n",
            "\u001b[35mSection_title: 5.1  MedFUZZ -Fuzzer für medizinische Software\n",
            "\u001b[35mSection_title: 5.2  MedVAS -Scanner für medizinische Systeme\n",
            "\u001b[35mSection_title: 6  Fazit\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  3\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 70  B A\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  OVERVIEW 2.1 Problem Definition\n",
            "\u001b[35mSection_title: 2.2  System Overview\n",
            "\u001b[35mSection_title: 3  IDRQA SYSTEM 3.1 Graph-based Reranking Model\n",
            "\u001b[35mSection_title: 3.2  Question Updater\n",
            "\u001b[35mSection_title: 3.3  Reader Module\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS 4.1 Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Datasets\n",
            "\u001b[35mSection_title: 4.3  Data Preprocessing\n",
            "\u001b[35mSection_title: 4.4  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.5  Overall Results\n",
            "\u001b[35mSection_title: 4.6  Detailed Analysis\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: we present a qa framework that can answer any-hop open-domain questions, which iteratively retrieves, reranks and filters documents with a graph-based reranking model, and adaptively decides how many steps of retrieval and reranking are needed for a multi-hop question. our method consistently achieves promising performance on both single-and multi-hop open-domain qa datasets.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Our approach to distant protein relation discovery\n",
            "\u001b[35mSection_title: 3.1  Data structures\n",
            "\u001b[35mSection_title: 3.2  Alignment algorithm\n",
            "\u001b[35mSection_title: 3.3  Translation-dependent scoring function\n",
            "\u001b[35mSection_title: 5  Experimental results\n",
            "\u001b[35mSection_title: 5.1  Tests on known overlapping and frameshifted genes\n",
            "\u001b[35mSection_title: 5.2  New divergence scenarios for orthologous proteins\n",
            "\u001b[32mconclusion: in this paper, we addressed the problem of finding distant protein homologies, in particular affected by frameshift events, from a codon evolution perspective. we search for protein common origins by implicitly aligning all their putative coding dna sequences, stored in efficient data structures called back-translation graphs. our approach relies on a dynamic programming alignment algorithm for these graphs, which involves a non-monotonic gap penalty that handles differently frameshifts and full codon indels. we designed a powerful translation-dependent scoring function for nucleotide pairs, based on codon substitution models, whose purpose is to reflect the expected dynamics of coding dna sequences.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related work\n",
            "\u001b[35mSection_title: 3.  Multiple-choice comprehension\n",
            "\u001b[35mSection_title: 3.1.  Task\n",
            "\u001b[35mSection_title: 3.2.  Machine comprehension\n",
            "\u001b[35mSection_title: 3.3.  World knowledge\n",
            "\u001b[35mSection_title: 3.4.  Partial context\n",
            "\u001b[35mSection_title: 4.  Experiments\n",
            "\u001b[35mSection_title: 4.1.  Data\n",
            "\u001b[35mSection_title: 4.2.  Training details and hyperparameters\n",
            "\u001b[35mSection_title: 4.3.  Assessment\n",
            "\u001b[35mSection_title: 5.  Results\n",
            "\u001b[35mSection_title: 5.1.  World knowledge\n",
            "\u001b[35mSection_title: 5.2.  Partial information access\n",
            "\u001b[32mconclusion: this work highlights the trade-off between contextual comprehension and world knowledge in multiple-choice reading and listening comprehension tests. we found that automated reading comprehension systems perform significantly better than random, even with limited access to the context passage. these findings provide content creators with an approach to capture the balance between comprehension and world knowledge in their questions. we further investigated to what extent a context needs to be read before the correct answer can be deduced, finding that it is possible to answer some questions across several reading/listening comprehension datasets with only access to a fraction of the context. overall, our findings guide content creators in constructing more valid and reliable assessments, ensuring accurate evaluation of language proficiency.\n",
            "\u001b[35mSection_title: 6.  Conclusions\n",
            "\u001b[32mlimitation: a limitation for the ibm-debater dataset is that the contexts have been truncated to 512 tokens prior to any experiments despite the average length being approximately 1000 tokens to use the standard pretrained language model finetuned on race++.this research is funded by the epsrc (the engineering and physical sciences research council) doctoral training partnership (dtp) phd studentship and supported by cambridge assessment, university of cambridge and alta.\n",
            "\u001b[35mSection_title: 7.  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  METHODS\n",
            "\u001b[35mSection_title: 2.1  InPars\n",
            "\u001b[35mSection_title: 2.2  Promptagator\n",
            "\u001b[35mSection_title: 3  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.1  Commands\n",
            "\u001b[35mSection_title: 4  RESULTS\n",
            "\u001b[32mconclusion: we have introduced the inpars toolkit, a codebase designed to generate synthetic data using llms in a reproducible manner for neural ir tasks. the toolkit comprises an end-to-end pipeline that encompasses data generation, training, reranking, and evaluating the trained models. additionally, the codebase is integrated with two major libraries for commonly used datasets from the beir benchmark, and it supports both gpu and tpu training and inference. our goal is to make research on these methods more accessible and to pave the way for this emerging research trend in the ir community. our experiments have demonstrated that training reranker models using synthetic data and evaluating them on gpu infrastructure yielded results comparable to those obtained when training on the tpu setup. additionally, we have also made available all synthetic data generated for all beir datasets and the models finetuned on this data.\n",
            "\u001b[35mSection_title: 5  CONCLUSIONS\n",
            "\u001b[35mSection_title: 6  FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Knowledge-Augmented Language Models\n",
            "\u001b[35mSection_title: 3.2  KALMV: Learning to Verify Knowledge-Augmented Language Models\n",
            "\u001b[35mSection_title: 3.3  Strategies for Rectifying Errors of\n",
            "\u001b[35mSection_title: 4  Experimental Setups\n",
            "\u001b[35mSection_title: 4.1  Tasks and Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines and Our Model\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Implementation Details\n",
            "\u001b[32mconclusion: in this work, we proposed knowledge-augmented language model verification (kalmv), which identifies not only the relevance of the retrieved knowledge to the input query but also the faithfulness of the reflection of knowledge in the generated answers, in order to prevent incorrect answer generations with knowledge-augmented lms.to this end, we developed a verifier that can detect errors in both the knowledge retrieval and answer generation stages by instruction-finetuning lms.further, during inference, we proposed to rectify errors by re-retrieving knowledge and re-generating answers if our kalmv detects errors, and also perform an ensemble over multiple verification outputs from different instructions, to improve the efficacy of the verifier.we validated kalmv on two question answering tasks and showed its effectiveness in significantly reducing hallucinations.we believe that kalmv will bring substantial practical impact in improving the reliability of lm-based systems, especially since it is a plug-and-play module.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Literature review and research motivations\n",
            "\u001b[35mSection_title: 3  Problem statement based on state-space-time network representation\n",
            "\u001b[35mSection_title: 3.1  Description of the PDPTW in state-space-time networks\n",
            "\u001b[35mSection_title: 3.2  Representing the state of system and calculating the number of states\n",
            "\u001b[35mSection_title: 3.3  State transition associated with pickup and delivery links\n",
            "\u001b[35mSection_title: 4  Time-discretized multi-commodity network flow programming model\n",
            "\u001b[35mSection_title: 5.1  Time-dependent forward dynamic programming and computational complexity\n",
            "\u001b[35mSection_title: 5.2  Lagrangian relaxation-based solution procedure\n",
            "\u001b[35mSection_title: 5.3  Search region reduction\n",
            "\u001b[35mSection_title: 6  Computational results and discussions\n",
            "\u001b[35mSection_title: 6.1  Illustrative cases\n",
            "\u001b[35mSection_title: 6.2  Results from medium-scale and large-scale transportation networks\n",
            "\u001b[35mSection_title: 6.3  Handling randomly generated test instances\n",
            "\u001b[35mSection_title: 6.4  Challenges of multi-vehicle assignment problems and usefulness of single-vehicle routing algorithm\n",
            "\u001b[32mconclusion: a new generation of transportation network companies uses mobile-phone-based platforms to seamlessly connect drivers to passengers from different origins to different destinations with specific, preferred departure or arrival times.many relevant practical aspects need to be carefully formulated for real-world planning/dispatching system deployment, such as time-dependent link travel times on large-scale regional transportation networks, and tight vehicle capacity and passenger service time window constraints.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Motivation\n",
            "\u001b[35mSection_title: 3  Aggretriever\n",
            "\u001b[35mSection_title: 3.1  Text Aggregation Pooling\n",
            "\u001b[35mSection_title: 3.2  Text Aggregation Pruning\n",
            "\u001b[35mSection_title: 3.3  Fine-Tuning and Retrieval\n",
            "\u001b[35mSection_title: 4.2  Models\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  In-Domain Evaluations\n",
            "\u001b[35mSection_title: 5.2  Zero-Shot Evaluations\n",
            "\u001b[35mSection_title: 5.3  Fine-Tuning with Noisy Hard Negatives\n",
            "\u001b[35mSection_title: 5.4  Ablation Study\n",
            "\u001b[35mSection_title: 5.5  Query Encoding Latency\n",
            "\u001b[35mSection_title: 5.6  Comparison with Sparse Retrievers\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present aggretriever, a singlevector dense retrieval model that exploits all contextualized token embeddings from the input to bert. we introduce a simple approach to aggregate the contextualized token embeddings into a dense vector, agg . experiments show that agg combined with the standard [cls] vector achieves better retrieval effectiveness than using the [cls] vector alone for both in-domain and zero-shot evaluations. our work demonstrates that mlm pre-trained transformers can be fine-tuned into effective dense retrievers without further pre-training or expensive fine-tuning strategies.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Motivation\n",
            "\u001b[35mSection_title: 3  Aggretriever\n",
            "\u001b[35mSection_title: 3.1  Text Aggregation Pooling\n",
            "\u001b[35mSection_title: 3.2  Text Aggregation Pruning\n",
            "\u001b[35mSection_title: 3.3  Fine-Tuning and Retrieval\n",
            "\u001b[35mSection_title: 4.2  Models\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  In-Domain Evaluations\n",
            "\u001b[35mSection_title: 5.2  Zero-Shot Evaluations\n",
            "\u001b[35mSection_title: 5.3  Fine-Tuning with Noisy Hard Negatives\n",
            "\u001b[35mSection_title: 5.4  Ablation Study\n",
            "\u001b[35mSection_title: 5.5  Query Encoding Latency\n",
            "\u001b[35mSection_title: 5.6  Comparison with Sparse Retrievers\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present aggretriever, a singlevector dense retrieval model that exploits all contextualized token embeddings from the input to bert. we introduce a simple approach to aggregate the contextualized token embeddings into a dense vector, agg . experiments show that agg combined with the standard [cls] vector achieves better retrieval effectiveness than using the [cls] vector alone for both in-domain and zero-shot evaluations. our work demonstrates that mlm pre-trained transformers can be fine-tuned into effective dense retrievers without further pre-training or expensive fine-tuning strategies.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Method\n",
            "\u001b[35mSection_title: 2.2.  Question Generation:q ∼ p(q|â, c)\n",
            "\u001b[35mSection_title: 2.3.  Roundtrip Filtration\n",
            "\u001b[35mSection_title: 3.  Experiment Setup\n",
            "\u001b[35mSection_title: 4.  Results\n",
            "\u001b[35mSection_title: 5.  Model Scale\n",
            "\u001b[35mSection_title: 5.1.  Scaling Question Generation\n",
            "\u001b[35mSection_title: 5.2.  Scaling Answer Generation\n",
            "\u001b[35mSection_title: 5.3.  Scaling Question Filtration\n",
            "\u001b[35mSection_title: 6.  Modeling Choices\n",
            "\u001b[35mSection_title: 6.1.  Question Generation\n",
            "\u001b[35mSection_title: 6.2.  Answer Generation\n",
            "\u001b[35mSection_title: 6.3.  Question Filtration\n",
            "\u001b[35mSection_title: 7.  Related Work\n",
            "\u001b[32mconclusion: we build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation. with our best models, we generate large question answering datasets from unlabeled wikipedia documents and finetune a 345 million parameter bert-style model achieving 88.4 em score. finetuning the resulting model on real squad1.1 data further boosts the em score to 89.4. this amounts to a 1.7 point improvement over our fully supervised baseline. finally, we generate synthetic text from a wikipedia-finetuned gpt-2 model, generate answer candidates and synthetic questions based on those answers, and then train a bert-large model to achieve similar question answering accuracy without directly using any real data at all. doing so required us to scale model size for our answer generators, question generators, and filtration models. we hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.\n",
            "\u001b[35mSection_title: 8.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Literature review and hypotheses development\n",
            "\u001b[35mSection_title: 2.1  IT capability and firm growth\n",
            "\u001b[35mSection_title: 2.2  IT capability and open technological innovation\n",
            "\u001b[35mSection_title: 2.3  Mediating effect of open technological innovation\n",
            "\u001b[35mSection_title: 3.  Data and methods\n",
            "\u001b[35mSection_title: 3.1  Sample and data collection\n",
            "\u001b[35mSection_title: 3.2  Variables and measures\n",
            "\u001b[35mSection_title: 4.  \"\n",
            "\u001b[35mSection_title: 3.3  Common method bias\n",
            "\u001b[35mSection_title: 3.4  Data analysis methods\n",
            "\u001b[35mSection_title: 4.  Empirical results\n",
            "\u001b[35mSection_title: 4.1  Reliability and validity\n",
            "\u001b[35mSection_title: 4.2  Hypothesis testing\n",
            "\u001b[32mconclusion: conclusionsthis study contributes to the existing literature on the relationship between it capability and firm growth by examining the mediating effects of open technological innovation.our findings are consistent with previous research that suggests it capability positively influences firm growth [60,61].however, our study provides a more nuanced understanding of this relationship by showing that open technological innovation plays a partial mediating role in this relationship.specifically, our results suggest that new ventures that are able to leverage open technological innovation can better translate their it capabilities, such as it flexibility and it integrality, into firm growth.\n",
            "\u001b[35mSection_title: 5.  Conclusions and discussion\n",
            "\u001b[32mconclusion: this study contributes to the existing literature on the relationship between it capability and firm growth by examining the mediating effects of open technological innovation.our findings are consistent with previous research that suggests it capability positively influences firm growth [60,61].however, our study provides a more nuanced understanding of this relationship by showing that open technological innovation plays a partial mediating role in this relationship.specifically, our results suggest that new ventures that are able to leverage open technological innovation can better translate their it capabilities, such as it flexibility and it integrality, into firm growth.\n",
            "\u001b[35mSection_title: 5.1  Conclusions\n",
            "\u001b[35mSection_title: 5.2  Theoretical contribution\n",
            "\u001b[35mSection_title: 5.3  Managerial implications\n",
            "\u001b[32mlimitation: while this study provides important contributions to the literature on the relationship between it capability, open technological innovation, and firm growth, there are several limitations that need to be addressed in future research.firstly, the cross-sectional design adopted in this study limits the ability to establish causality between the constructs over time.future research should use a longitudinal study to overcome this limitation and consolidate the results.secondly, the smaller sample size limits the generalizability of our findings.the data used in this study may not be representative of all new ventures and may be limited by the interviewees' cognition.to address this limitation, future research should focus on a specific industry in china and explore the effects of it capability on open technological innovation and firm growth under the regulatory impact of the sharing economy environment to better understand the relationship among them.additionally, there may be other variables, such as organizational learning, that mediate the relationship between it capability and firm growth.therefore, future research should investigate the role of other mediators to provide a more comprehensive understanding of the mechanisms through which it capability influences firm growth.by addressing these limitations, future research can further enhance our understanding of the complex relationship between it capability, open technological innovation, and firm growth in the context of new ventures in china.h2. it capability positively affects open technological innovation.h2a.it flexibility positively affects open technological innovation.h2b.it integration positively affects open technological innovation.\n",
            "\u001b[35mSection_title: 5.4  Limitations and future research\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Text Alignment Model\n",
            "\u001b[35mSection_title: 3.1  Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Natural Language Understanding Tasks\n",
            "\u001b[35mSection_title: 4.1.1  Experiment Setup\n",
            "\u001b[35mSection_title: 4.1.2  Results\n",
            "\u001b[35mSection_title: 4.2  Factual Consistency Evaluation for Language Generation\n",
            "\u001b[35mSection_title: 4.2.1  Experiment Setup\n",
            "\u001b[35mSection_title: 4.2.2  Results\n",
            "\u001b[35mSection_title: 4.3  Question Answering with Unanswerable Question\n",
            "\u001b[35mSection_title: 4.3.1  Experiment Setup\n",
            "\u001b[35mSection_title: 4.3.2  Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Study\n",
            "\u001b[32mconclusion: we propose to unify diverse language tasks into a text pair alignment problem.this framework yields an alignment model (align) that, despite being less versatile than llms, solves a wide range of language problems efficiently with superior performance.we show that align outperforms task-specific models finetuned on several nlu tasks while having performance comparable to llms that are orders of magnitude larger.additionally, align excels in factual consistency evaluation, and can be used as an add-on to augment llms in qa tasks by identifying unanswerable questions.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  PROBLEM FORMULATION: RECOMP\n",
            "\u001b[35mSection_title: 3  LEARNING THE COMPRESSORS\n",
            "\u001b[35mSection_title: 3.1  EXTRACTIVE COMPRESSION\n",
            "\u001b[35mSection_title: 3.2  ABSTRACTIVE COMPRESSION\n",
            "\u001b[35mSection_title: 3.2.1  CREATING TRAINING DATASET FOR DISTILLATION\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETTINGS\n",
            "\u001b[35mSection_title: 4.1  LANGUAGE MODELING\n",
            "\u001b[35mSection_title: 4.2  OPEN-DOMAIN QA\n",
            "\u001b[35mSection_title: 4.3  BASELINES AND ORACLES\n",
            "\u001b[35mSection_title: 5  RESULTS\n",
            "\u001b[35mSection_title: 6  ANALYSIS AND DISCUSSIONS\n",
            "\u001b[35mSection_title: 7  RELATED WORK\n",
            "\u001b[32mconclusion: we introduce recomp, a method which compresses retrieved documents into textual summaries before prepending them to improve in-context retrieval augmented language models.we present two compression models -an extractive compressor and an abstractive compressor.we design a training scheme which leverages end task signals from a blackbox lm to generate useful summaries and allowing the compression models to perform selective augmentation.our experiments show that our compressors can improve the efficiency of retrieval augmented lms significantly with minimal drop in performances.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 3  HIERARCHICAL GRAPH NETWORK\n",
            "\u001b[35mSection_title: 3.2  NODE\n",
            "\u001b[35mSection_title: 3.3  GRAPH REASONING\n",
            "\u001b[35mSection_title: 3.4  NODE CLASSIFY\n",
            "\u001b[35mSection_title: 4  LANGUAGE MODEL FINE-TUNING\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 5.3  ABLATION STUDIES\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.2  Decomposition Process\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Experimental Results\n",
            "\u001b[35mSection_title: 5.2  Analysis\n",
            "\u001b[32mconclusion: the recent trend of building large lms may not be sustainable to solve evolving benchmarks. we believe that modifying data samples can significantly help the model improve performance. we study the effect of question decomposition (qd) on a diverse set of tasks. we decompose questions manually and significantly improve model performance (24% for gpt3 and 29% for roberta-squad along with a symbolic calculator). our findings indicate that human-in-the-loop question decomposition (hqd) can potentially provide an alternate path to building large lms. our approach provides a viable option to involve people in nlp research. we hope our work will encourage the community to develop human-centric solutions that actively involve humans while leveraging nlp resources.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3.1  Preliminary: Fully-Supervised ODQA\n",
            "\u001b[35mSection_title: 3.2  Learning DISTDR: Hard-EM\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Compared Methods\n",
            "\u001b[35mSection_title: 4.4  Implementation details\n",
            "\u001b[35mSection_title: 4.5  Main Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Analysis on Model Training\n",
            "\u001b[35mSection_title: 5.2  Analysis on Retrieved Evidence\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we present distdr, a distantly-supervised odqa system that improves over a weak retriever by iter-atively finding evidence from a corpus, and using the evidence as distant supervision for model training.without using any evidence labels, distdr matches the fully-supervised sota approaches on both multi-hop and single-hop qa benchmarks.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Statement\n",
            "\u001b[35mSection_title: 4  Method\n",
            "\u001b[35mSection_title: 4.1  Data Generation\n",
            "\u001b[35mSection_title: 3.  Query generation:\n",
            "\u001b[35mSection_title: 4.  Output creation:\n",
            "\u001b[35mSection_title: 4.2  Training Procedure\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  True/False Queries\n",
            "\u001b[35mSection_title: 5.2  Beyond True/False Queries\n",
            "\u001b[32mconclusion: in this paper, we proposed an architecture for the integration of a reasoning model in task-oriented dialogue systems. we formulated the problem as a sequence prediction problem given a user query and context, and presented an approach for generating data and fine-tuning generative models to reason over a set of facts in the dialogue context. we demonstrated our approach for a shopping assistant and reported experimental results for different formulations of the problem. we showed that these models can learn to do logical reasoning to 1) answer questions from the dialogue context when all the information is available, 2) extract constraints when partial information is available, and 3) delegate to the dialogue policy when no reasoning is required.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Study area\n",
            "\u001b[35mSection_title: 2.2  Sampling and analysis\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Data preprocessing\n",
            "\u001b[35mSection_title: 3.2  Exploratory framework\n",
            "\u001b[35mSection_title: 3.3  Dimension reduction\n",
            "\u001b[35mSection_title: 3.3.1  Principal component analysis\n",
            "\u001b[35mSection_title: 3.3.2  Isometric feature mapping\n",
            "\u001b[35mSection_title: 3.3.3  Interpretation of components\n",
            "\u001b[35mSection_title: 3.4  Time series analysis\n",
            "\u001b[35mSection_title: 3.4.1  Lomb-Scargle method\n",
            "\u001b[35mSection_title: 3.4.2  Theil-Sen estimator and Mann-Kendall test\n",
            "\u001b[35mSection_title: 3.4.3  Locally weighted regression (LOESS)\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Multivariate components\n",
            "\u001b[35mSection_title: 4.2  Multiple sites\n",
            "\u001b[35mSection_title: 4.3  Long-term patterns\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Multivariate components\n",
            "\u001b[35mSection_title: 5.2  Multiple sites\n",
            "\u001b[35mSection_title: 5.3  Long-term patterns\n",
            "\u001b[35mSection_title: 5.4  Effects of the irregular sampling\n",
            "\u001b[35mSection_title: 5.5  Exploratory framework\n",
            "\u001b[32mconclusion: we suggested and tested an exploratory approach for the detection of dominant changes in multivariate water quality data sets with irregular sampling in space and time.the combination of the selected methods aimed to provide a broadly applicable exploratory framework for typical existing monitoring data sets, e.g. from environmental agencies, which are often characterized by relatively low sampling frequency and irregularities of the sampling in space and/or time.in the approach, we applied a dimension-reduction method to derive multivariate water quality components and analysed their spatiotemporal features with respect to changes that concerned more than single sites, short-term fluctuations or single events.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  COMPARISON\n",
            "\u001b[35mSection_title: 2.1  Differences Between the DGDS and the other DS\n",
            "\u001b[35mSection_title: 2.2  Differences Between the CRC and the DBD\n",
            "\u001b[35mSection_title: 2.3  Summarization\n",
            "\u001b[35mSection_title: 3  ARCHITECTURE\n",
            "\u001b[35mSection_title: 3.1  Joint Modeling\n",
            "\u001b[35mSection_title: 3.2  Knowledge Selection\n",
            "\u001b[35mSection_title: 3.3  Response Generation\n",
            "\u001b[35mSection_title: 3.4  Evaluation\n",
            "\u001b[35mSection_title: 3.5  Summarization\n",
            "\u001b[35mSection_title: 4  DATASETS\n",
            "\u001b[35mSection_title: 4.1  CRC Datasets\n",
            "\u001b[35mSection_title: 4.2  DBD Datasets\n",
            "\u001b[35mSection_title: 4.3  Summarization\n",
            "\u001b[35mSection_title: 5  CRC MODELS\n",
            "\u001b[35mSection_title: 5.1  Joint Modeling in the CRC\n",
            "\u001b[35mSection_title: 5.1.3  Summarization.\n",
            "\u001b[35mSection_title: 5.2  Knowledge Selection in the CRC\n",
            "\u001b[35mSection_title: 5.2.2  Document(s)-based. In this section, we introduce the current CRC model of the KS for the document(s) from\n",
            "\u001b[35mSection_title: 5.3  Response Generation in the CRC\n",
            "\u001b[35mSection_title: 5.3.1  Indirect Generation.\n",
            "\u001b[35mSection_title: 5.3.2  Direct Generation.\n",
            "\u001b[35mSection_title: 5.3.3  Hybrid Generation. Zhong and Zettlemoyer\n",
            "\u001b[35mSection_title: 5.4  Evaluation Method in the CRC\n",
            "\u001b[35mSection_title: 6  DBD MODELS\n",
            "\u001b[35mSection_title: 6.1  Joint Modeling in the DBD\n",
            "\u001b[35mSection_title: 6.1.2  Incremental\n",
            "\u001b[35mSection_title: 6.2  Knowledge Selection in the DBD\n",
            "\u001b[35mSection_title: 6.2.2  Reasoning.\n",
            "\u001b[35mSection_title: 6.3  Response Generation in the DBD\n",
            "\u001b[35mSection_title: 6.4  Evaluation Method in the DBD\n",
            "\u001b[35mSection_title: 6.4.3  Generative Auto Evaluation.\n",
            "\u001b[35mSection_title: 6.5  Summarization\n",
            "\u001b[35mSection_title: 7  FUTURE WORK\n",
            "\u001b[35mSection_title: 7.1  Fundamental problems\n",
            "\u001b[35mSection_title: 7.2  Technical Problems\n",
            "\u001b[35mSection_title: 7.2.2  Reasoning Ability.\n",
            "\u001b[35mSection_title: 7.2.4  Model Generalization.\n",
            "\u001b[32mconclusion: the document grounded dialogue system (dgds) can mine document(s) information and discuss specific document(s) in a real human conversation. we believe that extracting unstructured document(s) information in dialogue is the future trend of the ds because a large amount of human knowledge is contained in these document(s). the research of the dgds not only possesses a broad application prospect but also facilitates the ds to better understand human knowledge and natural language. this article introduces the dgds, defines the related concepts, analyzes the current datasets and models, and provides views on future research trends in this field, hoping to be helpful for the community.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  COMPARISON\n",
            "\u001b[35mSection_title: 2.1  Differences Between the DGDS and the other DS\n",
            "\u001b[35mSection_title: 2.2  Differences Between the CRC and the DBD\n",
            "\u001b[35mSection_title: 2.3  Summarization\n",
            "\u001b[35mSection_title: 3  ARCHITECTURE\n",
            "\u001b[35mSection_title: 3.1  Joint Modeling\n",
            "\u001b[35mSection_title: 3.2  Knowledge Selection\n",
            "\u001b[35mSection_title: 3.3  Response Generation\n",
            "\u001b[35mSection_title: 3.4  Evaluation\n",
            "\u001b[35mSection_title: 3.5  Summarization\n",
            "\u001b[35mSection_title: 4  DATASETS\n",
            "\u001b[35mSection_title: 4.1  CRC Datasets\n",
            "\u001b[35mSection_title: 4.2  DBD Datasets\n",
            "\u001b[35mSection_title: 4.3  Summarization\n",
            "\u001b[35mSection_title: 5  CRC MODELS\n",
            "\u001b[35mSection_title: 5.1  Joint Modeling in the CRC\n",
            "\u001b[35mSection_title: 5.1.3  Summarization.\n",
            "\u001b[35mSection_title: 5.2  Knowledge Selection in the CRC\n",
            "\u001b[35mSection_title: 5.2.2  Document(s)-based. In this section, we introduce the current CRC model of the KS for the document(s) from\n",
            "\u001b[35mSection_title: 5.3  Response Generation in the CRC\n",
            "\u001b[35mSection_title: 5.3.1  Indirect Generation.\n",
            "\u001b[35mSection_title: 5.3.2  Direct Generation.\n",
            "\u001b[35mSection_title: 5.3.3  Hybrid Generation. Zhong and Zettlemoyer\n",
            "\u001b[35mSection_title: 5.4  Evaluation Method in the CRC\n",
            "\u001b[35mSection_title: 6  DBD MODELS\n",
            "\u001b[35mSection_title: 6.1  Joint Modeling in the DBD\n",
            "\u001b[35mSection_title: 6.1.2  Incremental\n",
            "\u001b[35mSection_title: 6.2  Knowledge Selection in the DBD\n",
            "\u001b[35mSection_title: 6.2.2  Reasoning.\n",
            "\u001b[35mSection_title: 6.3  Response Generation in the DBD\n",
            "\u001b[35mSection_title: 6.4  Evaluation Method in the DBD\n",
            "\u001b[35mSection_title: 6.4.3  Generative Auto Evaluation.\n",
            "\u001b[35mSection_title: 6.5  Summarization\n",
            "\u001b[35mSection_title: 7  FUTURE WORK\n",
            "\u001b[35mSection_title: 7.1  Fundamental problems\n",
            "\u001b[35mSection_title: 7.2  Technical Problems\n",
            "\u001b[35mSection_title: 7.2.2  Reasoning Ability.\n",
            "\u001b[35mSection_title: 7.2.4  Model Generalization.\n",
            "\u001b[32mconclusion: the document grounded dialogue system (dgds) can mine document(s) information and discuss specific document(s) in a real human conversation. we believe that extracting unstructured document(s) information in dialogue is the future trend of the ds because a large amount of human knowledge is contained in these document(s). the research of the dgds not only possesses a broad application prospect but also facilitates the ds to better understand human knowledge and natural language. this article introduces the dgds, defines the related concepts, analyzes the current datasets and models, and provides views on future research trends in this field, hoping to be helpful for the community.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Construction and Analysis\n",
            "\u001b[35mSection_title: 2.1  Data Collection and Preprocessing\n",
            "\u001b[35mSection_title: 2.2  Dataset Annotation\n",
            "\u001b[35mSection_title: 2.3  Quality Control\n",
            "\u001b[35mSection_title: 2.4  Dataset Analysis\n",
            "\u001b[35mSection_title: 3  TAGOP Model\n",
            "\u001b[35mSection_title: 3.1  Sequence Tagging\n",
            "\u001b[35mSection_title: 3.2  Aggregation Operator\n",
            "\u001b[35mSection_title: 3.3  Scale Prediction\n",
            "\u001b[35mSection_title: 3.4  Training\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Results and Analysis\n",
            "\u001b[32mconclusion: we propose a new challenging qa dataset tat-qa, comprising real-word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain. to answer questions in tat-qa, the close relation between table and paragraphs and numerical reasoning are required. we also propose a baseline model tagop based on tat-qa, aggregating information from hybrid context and performing numerical reasoning over it with pre-defined operators to compute the final answer. experiments show tat-qa dataset is very challenging and more effort is demanded for tackling qa tasks over hybrid data. we expect our tat-qa dataset and tagop model would serve as a benchmark and baseline respectively to help build more advanced qa models, facilitating the development of qa technologies to address more complex and realistic hybrid data, especially those requiring numerical reasoning.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.1  MULTI-VECTOR RETRIEVAL\n",
            "\u001b[35mSection_title: 2.2  SPARSE ALIGNMENT FORMULATION\n",
            "\u001b[35mSection_title: 3  ALIGNER\n",
            "\u001b[35mSection_title: 3.1  ADAPTING PAIRWISE ALIGNMENT\n",
            "\u001b[35mSection_title: 3.2  LEARNING UNARY SALIENCE\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.2  RETRIEVAL ACCURACY\n",
            "\u001b[35mSection_title: 4.3  RETRIEVAL EFFICIENCY\n",
            "\u001b[35mSection_title: 4.4  INTERPRETABILITY\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: in this paper, we introduce aligner, a novel sparse alignment method for multi-vector document retrieval. we first formulate different retrieval models with token-level sparse alignments and propose aligner to tackle the limitations of existing models. specifically, aligner uses pairwise alignments and unary saliences that allow us to adapt to different tasks and prune unimportant tokens, respectively. as a result, we achieve strong performance on both zero-shot and few-shot document retrieval tasks while drastically improving the run-time and storage complexity of multi-vector retrieval. with its interpretable alignments and better performance with large language models, we envision that our multi-vector retrieval model can serve as a strong standalone retriever in the future.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\u001b[35mSection_title: 7  AUTHOR CONTRIBUTIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 3.1.  Knowledge Graph\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Approach\n",
            "\u001b[35mSection_title: 3.1  Task Description\n",
            "\u001b[35mSection_title: 3.2  Generating Examples for Training\n",
            "\u001b[35mSection_title: 3.3  Learning Answerability & Evidentiality\n",
            "\u001b[35mSection_title: 3.4  Passage Selection at Inference Time\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  RQ1: QA Effectiveness\n",
            "\u001b[35mSection_title: 4.2  RQ2: Evaluation of Pseudo-Evidentiality Annotation\n",
            "\u001b[35mSection_title: 4.3  RQ3: Generalization\n",
            "\u001b[32mconclusion: in this paper, we propose a new approach to train multi-hop qa models, not to take reasoning shortcuts of guessing right answers without sufficient evidences. we do not require annotations and generate pseudo-evidentiality instead, by regularizing qa model from being overconfident when evidences are insufficient. our experimental results show that our method outperforms baselines on hotpotqa and has the effectiveness to distinguish between evidence-positive and negative set.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Overview of Existing Datasets\n",
            "\u001b[35mSection_title: 4  Link Between EXNLP Data, Modeling, and Evaluation Assumptions\n",
            "\u001b[35mSection_title: 5  Rise of Structured Explanations\n",
            "\u001b[35mSection_title: 6  Increasing Explanation Quality\n",
            "\u001b[35mSection_title: 6.1  A Two-Stage Collect-And-Edit Approach\n",
            "\u001b[35mSection_title: 6.2  Teach and Test the Underlying Task\n",
            "\u001b[35mSection_title: 6.3  Addressing Ambiguity\n",
            "\u001b[35mSection_title: 7  Increasing Explanation Diversity\n",
            "\u001b[35mSection_title: 7.1  Use a Large Set of Annotators\n",
            "\u001b[35mSection_title: 7.2  Multiple Annotations Per Instance\n",
            "\u001b[35mSection_title: 7.3  Get Ahead: Add Contrastive and Negative Explanations\n",
            "\u001b[32mconclusion: we have presented a review of existing datasets for exnlp research, highlighted discrepancies in data collection that can have downstream modeling effects, and synthesized the literature both inside and outside exnlp into a set of recommendations for future data collection.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Overview of Existing Datasets\n",
            "\u001b[35mSection_title: 4  Link Between EXNLP Data, Modeling, and Evaluation Assumptions\n",
            "\u001b[35mSection_title: 5  Rise of Structured Explanations\n",
            "\u001b[35mSection_title: 6  Increasing Explanation Quality\n",
            "\u001b[35mSection_title: 6.1  A Two-Stage Collect-And-Edit Approach\n",
            "\u001b[35mSection_title: 6.2  Teach and Test the Underlying Task\n",
            "\u001b[35mSection_title: 6.3  Addressing Ambiguity\n",
            "\u001b[35mSection_title: 7  Increasing Explanation Diversity\n",
            "\u001b[35mSection_title: 7.1  Use a Large Set of Annotators\n",
            "\u001b[35mSection_title: 7.2  Multiple Annotations Per Instance\n",
            "\u001b[35mSection_title: 7.3  Get Ahead: Add Contrastive and Negative Explanations\n",
            "\u001b[32mconclusion: we have presented a review of existing datasets for exnlp research, highlighted discrepancies in data collection that can have downstream modeling effects, and synthesized the literature both inside and outside exnlp into a set of recommendations for future data collection.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND AND RELATED WORK\n",
            "\u001b[35mSection_title: 3  DESIGN CONSIDERATIONS\n",
            "\u001b[35mSection_title: 4  TWO-LEVEL GUIDED TRAVERSAL 4.1 Two-level guidance for MaxScore\n",
            "\u001b[35mSection_title: 4.2  Relevance properties of 2GTI\n",
            "\u001b[35mSection_title: 4.3  Alignment of tokens and weights\n",
            "\u001b[35mSection_title: 5  EVALUATIONS\n",
            "\u001b[35mSection_title: 6  CONCLUDING REMARKS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  DEMONSTRATE-SEARCH-PREDICT\n",
            "\u001b[35mSection_title: 2.1.  Pretrained Modules: LM and RM\n",
            "\u001b[35mSection_title: 2.2.  Datatypes and Control Flow\n",
            "\u001b[35mSection_title: 2.3.  DEMONSTRATE\n",
            "\u001b[35mSection_title: 2.4.  SEARCH\n",
            "\u001b[35mSection_title: 2.5.  PREDICT\n",
            "\u001b[35mSection_title: 3.  Evaluation\n",
            "\u001b[35mSection_title: 3.1.  Evaluation Methodology\n",
            "\u001b[35mSection_title: 3.2.  Pretrained Modules\n",
            "\u001b[35mSection_title: 3.3.  Baselines\n",
            "\u001b[35mSection_title: 3.4.  Proposed DSP Programs\n",
            "\u001b[35mSection_title: 3.5.  Development Datasets & Results\n",
            "\u001b[32mconclusion: for a long time, the dominant paradigm for building models in ai has centered around multiplication of tensor representations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration. however, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to combining different pretrained components into larger systems.\n",
            "\u001b[35mSection_title: 4.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Related Work\n",
            "\u001b[35mSection_title: 3  Parameter-Efficient Retrieval with Adapters\n",
            "\u001b[35mSection_title: 3.1  Self-Attention Transformer Layers\n",
            "\u001b[35mSection_title: 3.2  Adapters\n",
            "\u001b[35mSection_title: 3.3  Neural Sparse First Stage Retrievers\n",
            "\u001b[35mSection_title: 3.4  Cross-Encoding Rerankers\n",
            "\u001b[35mSection_title: 4  Experimental Setting and Results\n",
            "\u001b[35mSection_title: 4.1  RQ1: Adapters-SPLADE\n",
            "\u001b[35mSection_title: 4.2  RQ2: Adapter Layer Ablation\n",
            "\u001b[35mSection_title: 4.3  RQ3: Out-of-Domain Dataset Adaptation\n",
            "\u001b[35mSection_title: 4.4  RQ4: Knowledge Sharing between Rerankers and First stage Rankers\n",
            "\u001b[32mconclusion: retrieval models, based on plm, require finetuning millions of parameters which makes them memory inefficient and non-scalable for out-of-domain adaptation. this motivates the need for efficient methods to adapt them to information retrieval tasks. in this paper, we examine adapters for sparse retrieval models. we show that with approximately 2% of training parameters, adapters can be successfully employed for splade models with comparable or even better effectiveness on benchmark ir datasets such as ms marco and trec. we further analyze adapter layer ablation and see a further reduction in training parameters to 1.8% retains effectiveness of full finetuning. for domain adaptation, adapters are more stable and outperform finetuning, which is prone to overfitting, on tripclick dataset, adapters outperform on precision metrics torso and tail queries and performs comparably on head queries. we explore knowledge transfer between first stage rankers and rerankers as a final study. adapters underperform full finetuning when trying to reuse sparse model to rerankers. dense first stage rankers perform similarly for adapters and finetuning while sparse first stage rankers is less effective compared to finetuning. we leave this as future work. as memory-efficient adapters are effective for splade, we leave for future studying larger sparse models and their generalizability. finally, an interesting scenario could also be to tackle unsupervised domain adaptation with adapters.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Problem Statement\n",
            "\u001b[35mSection_title: 2.2  A Typical Vector-based Approach\n",
            "\u001b[35mSection_title: 2.3  Matrix-based Conditional approach\n",
            "\u001b[35mSection_title: 2.4  Sampling-based Training Strategy\n",
            "\u001b[35mSection_title: 2.5  Training\n",
            "\u001b[35mSection_title: 2.6  Ensemble for Inference\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 3.3  Experimental Settings\n",
            "\u001b[35mSection_title: 3.4  Main Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we first investigate different approaches of span extraction in mrc. to improve the current vector-based conditional approach, we propose a matrix-based conditional approach. more careful consideration of the dependencies between the start and end positions of the answer span can predict their values better. we also propose a sampling-based training strategy to address the training process of the matrix-based conditional approach. the final experimental results on a wide of datasets demonstrate the effectiveness of our approach and training strategy.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Measuring Concurrence Between Benchmarks\n",
            "\u001b[35mSection_title: 3  How Well Do Existing Benchmarks\n",
            "\u001b[35mSection_title: 3.2  Many Existing Cloze Benchmarks Have High Concurrence With SQuAD\n",
            "\u001b[35mSection_title: 3.3  Existing Synthetic Benchmarks Have Low Concurrence With SQuAD\n",
            "\u001b[35mSection_title: 4  Constructing Synthetic Benchmarks With High SQuAD Concurrence\n",
            "\u001b[35mSection_title: 4.1  FuzzySyntheticQA Has High\n",
            "\u001b[35mSection_title: 4.2  WikidataSyntheticQA Has High Concurrence With SQuAD\n",
            "\u001b[35mSection_title: 5  Benchmark Size Minimally Affects Concurrence With SQuAD\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[35mSection_title: 6.1  Qualifications of This Study\n",
            "\u001b[35mSection_title: 6.2  Direct vs. Indirect Improvements\n",
            "\u001b[35mSection_title: 6.3  Can Synthetic Benchmarks Drive\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[35mSection_title: 7.1  Transferability and Out-of-Domain Generalization\n",
            "\u001b[35mSection_title: 7.2  Synthetic Data in NLP\n",
            "\u001b[32mconclusion: although large natural datasets are crucial ingredients for training accurate, deployable nlp systems, we find that naturalness and size are not necessary qualities of benchmarks that recapitulate progress on squad. benchmarks with varying naturalness and size can offer challenges with relevance to natural language.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Proposed model\n",
            "\u001b[35mSection_title: 3.1  Intermediate Task Training\n",
            "\u001b[35mSection_title: 3.1.1  Question Generation\n",
            "\u001b[35mSection_title: 3.1.2  Question Answering\n",
            "\u001b[35mSection_title: 3.1.3  Answer related words generation\n",
            "\u001b[35mSection_title: 3.1.4  Cycle Consistency\n",
            "\u001b[35mSection_title: 3.2  Prompt-based fine-tuning\n",
            "\u001b[35mSection_title: 3.3  Model Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Multi-hop QG Results and Analysis\n",
            "\u001b[35mSection_title: 4.3.1  Automatic Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3.2  Human Evaluation\n",
            "\u001b[32mconclusion: in this work, we propose type-dependent prompt cycleqag with cycle consistency. since multihop qg needs to know more diverse information because it needs to gather more scattered pieces of information for generating a question, we introduce the nce for the first time in the qg task. also, we demonstrate that the intermediate task is effective in the qg task. furthermore, we show a significant performance improvement by using prompt-style fine-tuning to make the most of the information obtained from the intermediate task. the experiments show that the proposed model outperforms in all automatic evaluations comparing with the existing text-based multi-hop model and several qg models.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Cross-passage Hierarchical Memory Network (CHIME)\n",
            "\u001b[35mSection_title: 3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.2  CHIME\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Settings\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.3  Qualitative analysis\n",
            "bottom\n",
            "\u001b[32mconclusion: in this paper, we have proposed chime, a cross-passage hierarchical memory network for multi-passage generative review qa. it is built on the xlnet generator  by adding a memory module consisting of a context and a answer memory which guarantees a more accurate refining process for crosspassage evidence collection and answer generation. the sequential process adopted in chime makes it possible to elaborate longer text passages and some straightforward interpretability. we have assessed experimentally a significant quality improvement using different state-of-the-art metrics to measure the lexical and semantic coherence of the generated text. we plan to further extend chime to model with multiple ground truth simultaneously and leverage the available product attributes.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  OPEN-DOMAIN QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 2.2  IN-CONTEXT FEW-SHOT LEARNING\n",
            "\u001b[35mSection_title: 2.3  RATIONALE-AUGMENTED REASONING\n",
            "\u001b[35mSection_title: 2.4  MEMORIZATION IN LARGE LANGUAGE MODELS\n",
            "\u001b[35mSection_title: 3  LEARNING TO RECITE FOR CLOSED-BOOK QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 3.1  PROMPT-BASED RECITE-AND-ANSWER FOR QUESTION-ANSWERING\n",
            "\u001b[35mSection_title: 3.2  PASSAGE HINT-BASED DIVERSIFIED RECITATION WITH FINE-TUNING\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.1.1  EVALUATION DATASETS\n",
            "\u001b[35mSection_title: 4.1.2  PRE-TRAINED LANGUAGE MODELS\n",
            "\u001b[35mSection_title: 4.2  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.2.1  PROMPT-BASED RESULTS\n",
            "\u001b[35mSection_title: 4.2.2  RESULTS OF PASSAGE HINT-BASED DIVERSIFIED RECITATION\n",
            "\u001b[35mSection_title: 4.3.1  ON THE NUMBER OF SELF-CONSISTENCY PATHS\n",
            "\u001b[35mSection_title: 4.3.2  ON THE ROBUSTNESS OF FEW-SHOT EXEMPLARS\n",
            "\u001b[35mSection_title: 4.3.3  RECITATION V.S. RETRIEVAL V.S. GROUND-TRUTH\n",
            "\u001b[35mSection_title: 4.3.4  ERROR ANALYSIS\n",
            "\u001b[32mconclusion: in this paper, we propose a novel recitation-augmented generation framework to improve language models' performance in the closed-book question-answering setting. we hypothesize that for knowledge-intensive nlp tasks, encouraging the model to explicitly recite a specific knowledge source would be helpful in augmenting its memory. in addition, we found that diversifying the recitation process can be beneficial as well since usually there exists multiple knowledge sources that could be used to answer the same question. we show promising results over three large language models and across three different closed-book qa datasets, demonstrating the effectiveness of our proposed recite-and-answer approach.\n",
            "\u001b[35mSection_title: 5  CONCLUSION & DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Stanford Question Answering Dataset\n",
            "\u001b[35mSection_title: 2.2  A Model for Generalization\n",
            "\u001b[35mSection_title: 3  Related Work\n",
            "\u001b[35mSection_title: 4  Collecting New Test Sets\n",
            "\u001b[35mSection_title: 4.1  Passage Curation\n",
            "\u001b[35mSection_title: 4.2  Crowdsourcing Question-Answer Pairs\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[35mSection_title: 5  Main Results\n",
            "\u001b[35mSection_title: 5.1  Adaptive Overfitting\n",
            "\u001b[35mSection_title: 5.2  Robustness to Natural Distribution Shifts\n",
            "\u001b[35mSection_title: 6  Further Analysis\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction 1.The Core Concept of Cognitive Graph\n",
            "\u001b[35mSection_title: 1.2.  Basic Concept of Chinese Reading Comprehension Question Answering\n",
            "\u001b[35mSection_title: 1.3.  Triple Extraction\n",
            "\u001b[35mSection_title: 1.4.  Main Contribution\n",
            "\u001b[35mSection_title: 2.  Model Definition Core Concepts\n",
            "\u001b[35mSection_title: 3.  Model Framework\n",
            "\u001b[35mSection_title: 3.1.  Retrieval Reasoning Paths\n",
            "\u001b[35mSection_title: 3.1.1.  Paragraph Graph Construction\n",
            "\u001b[35mSection_title: 3.1.2.  Reasoning Path Modeling\n",
            "\u001b[35mSection_title: 3.1.3.  Model Optimization\n",
            "\u001b[35mSection_title: 3.2.  Reading and Answering Based on Reasoning Paths\n",
            "\u001b[35mSection_title: 3.3.  Joint Training\n",
            "\u001b[35mSection_title: 4.  Experimental Analysis\n",
            "\u001b[35mSection_title: 4.1.  Experimental Configuration\n",
            "\u001b[35mSection_title: 4.2.  Dataset\n",
            "\u001b[35mSection_title: 4.3.  Comparative Algorithms\n",
            "\u001b[35mSection_title: 4.4.  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.5.  Parameter Settings\n",
            "\u001b[35mSection_title: 4.6.  Algorithm Comparison\n",
            "\u001b[35mSection_title: 5.2.2.  Pretrained Models\n",
            "\u001b[35mSection_title: 5.2.3.  Top-K Inference Paths\n",
            "\u001b[35mSection_title: 5.3.  Data Augmentation Experiment\n",
            "\u001b[32mconclusion: in light of the current state of open-domain reading comprehension question-answering methods, this study proposed the qarcg method based on the dual-process theory of cognitive science.the qarcg approach views open-domain question answering as a combination of retrieval and reasoning systems.system 1, responsible for retrieval, extracts triples from given supporting text and iteratively retrieves information from wikipedia, constructing a cognitive graph with reasoning paths.system 2, responsible for reasoning, learns the interaction information between paragraphs using rnn based on the built cognitive graph.it reorders and scores different reasoning paths, and predicts the answer's span based on the highest-scoring reasoning path's paragraphs.the integration of retrieval and reasoning reduces the loss in graph construction and maintains graph structure, thereby enhancing interpretability and overcoming the lack of reasoning interpretability in traditional end-to-end reading comprehension methods.additionally, it addresses the requirement for existing large-scale knowledge graphs in knowledge graph question answering.\n",
            "\u001b[35mSection_title: 6.  Conclusions and Future Work 6.1. Conclusions\n",
            "\u001b[35mSection_title: 6.2.  Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model Architecture\n",
            "\u001b[35mSection_title: 3.1  A Base \"Paragraph\" Predictor Model\n",
            "\u001b[35mSection_title: 3.2  Gaussian Prior Optimization (GPO)\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[32mconclusion: in this work we introduce and examine a powerful yet simple model for reading comprehension on long texts which we call vault, based on the hypothesis that with a large sequence length long answers can be classified effectively without computationally heavy graph-based models. we validate our approach by showing it yields f1 scores competitive with heavier methods at a fraction of the decoding cost on two very different domain benchmark datasets that require reading long texts.  compare the correct answers produced by vault with the incorrect answers produced by the ablated model from the last row of table 3 (nq) and roberta baseline from the first row of table 2 (techqa).\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Solving Knowledge-intensive Tasks via\n",
            "\u001b[35mSection_title: 2.2  Aligning Language Model with Instructions via Human Feedback\n",
            "\u001b[35mSection_title: 2.3  Comparative Analysis of Concurrent Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Proposed Method: REFEED\n",
            "\u001b[35mSection_title: 3.1.1  Basic Pipeline\n",
            "\u001b[35mSection_title: 3.1.2  Enhanced Modules\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Backbone Language Model\n",
            "\u001b[35mSection_title: 4.2  Baseline Methods\n",
            "\u001b[35mSection_title: 4.3  Experimental Analysis\n",
            "\u001b[35mSection_title: 4.3.1  Zero/Few-shot Question Answering and Dialogue Evaluation\n",
            "\u001b[35mSection_title: 4.3.2  Ablation Study on Ensemble Method and Diverse Generation\n",
            "\u001b[35mSection_title: 4.3.3  Analysis on Chain-of-thought Reasoning on Multi-hop QA\n",
            "\u001b[35mSection_title: 4.3.4  Case Study\n",
            "\u001b[32mconclusion: in conclusion, this paper presents a novel pipeline, refeed, designed to improve large language models' performance in a plug-and-play framework, effectively addressing the challenges arising from knowledge-intensive tasks. by employing a retrieval method to provide automatic feedback on generated outputs and integrating this feedback to refine the outputs without the need for expensive fine-tuning, refeed offers a practical and efficient solution. we introduce two innovative modules within the refeed pipeline: diverse answer generation and an ensemble approach. these two modules further enhance refeed to produce more reliable and accurate answers by considering a wider array of retrieved documents and mitigating the risk of misleading retrieval feedback. our extensive experiments on four challenging knowledgeintensive benchmarks demonstrate the effectiveness of refeed in achieving state-of-the-art performance under the few-shot setting. we believe by continuing to refine and optimize the refeed pipeline, we can unlock its full potential and ex-pand its applicability across a diverse range of scenarios and applications.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Background Information\n",
            "\u001b[35mSection_title: 3.1  Binary Passage Retriever (BPR)\n",
            "\u001b[35mSection_title: 3.2  Joint Optimization of Query Encoder and Product Quantization (JPQ)\n",
            "\u001b[35mSection_title: 4  Adapting binary document encoders to new domains\n",
            "\u001b[35mSection_title: 4.1  Binary Passage Retriever (BPR)\n",
            "\u001b[35mSection_title: 4.2  Joint Optimization of Query Encoder and Product Quantization (JPQ)\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Experimental Results\n",
            "\u001b[35mSection_title: 7.2  Higher dimensional efficiency captures more information\n",
            "\u001b[35mSection_title: 7.3  GPL is robust across PQ centroids\n",
            "\u001b[32mconclusion: supervised dense compression algorithms have been popular and effective in-domain in recent times, however can have difficulties to generalize well to unseen domains. the algorithms are memory efficient, but lack in performance when evaluated in specialized domains which contain no training data. in order to adapt these compression algorithms under severe domain shifts, in this paper we propose a solution to jointly optimize domainadaption algorithms along with vector compression. the recent technique, gpl in combination with bpr and jpq provide a boost of 19.3 and 11.6 ndcg@10 points respectively.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: even though we find the gpl technique to provide a boost with memory compressed models: bpr and jpq. our work has a few limitations which we briefly mention them below and for future work: different compression algorithms: in our work, we considered jpq and bpr due to its popularity and effectiveness shown in our preliminary results. in future, we can work on extending our methods to more recent memory compression algorithms such as repconc (zhan et al., 2022).better backbone models: we suspect the performances on the beir can further improved with stronger backbone models in comparison with tas-b. due to the model agnostic nature of our method, we can easily extend our work to different state-ofthe-art dense retrievers in the upcoming future.requires separate models: bpr and jpq both require training separate models for each domain or task with our technique. this can be quite cumbersome for practical use-cases involving several hundreds of domains or retrieval tasks, for which one would need to train multiple models.compute intensive: our method gpl is compute intensive: (1) for bpr+gpl, every dense retriever requires to separately compute embeddings for the whole corpus for hard-negative mining.(2) cross-encoder teacher model although very effective, slows down the training significantly as required to label during training. in future, we can explore efficient and faster teachers instead of cross-encoders for gpl such as colbert (khattab and zaharia, 2020) or tilde (zhuang and zuccon, 2021) \n",
            "\u001b[35mSection_title: 9  Limitations and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Logical Chain Construction\n",
            "\u001b[35mSection_title: 2.2  Hierarchical Interaction Mechanism\n",
            "\u001b[35mSection_title: 2.3  Answer Selector\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2.2  Results on general NLI tasks\n",
            "\u001b[35mSection_title: 3.3  More Results\n",
            "\u001b[35mSection_title: 3.4  Ablation\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 4.1  Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 4.2  Logical Reasoning\n",
            "\u001b[32mconclusion: this paper presents a novel method to guide the mrc model to better perform logical reasoning tasks. we propose a holistic graph-based system to model hierarchical logical reasoning chains. to our best knowledge, we are the first to deal with context at both discourse level and phrase level as the basis for logical reasoning. to decouple the interaction between the node features and type features, we apply hierarchical interaction mechanism to yield the appropriate representation for reading comprehension. on the logical qa benchmarks (reclor, logiqa) and natural language inference benchmarks (snli and anli), our proposed model has been shown effective by significantly outperforming the strong baselines.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Task definition\n",
            "\u001b[35mSection_title: 2.2  Evaluation metrics\n",
            "\u001b[35mSection_title: 3  Data collection\n",
            "\u001b[35mSection_title: 3.1  Crowdsourcing interface\n",
            "\u001b[35mSection_title: 3.2  Workflow\n",
            "\u001b[35mSection_title: 3.3  Dataset\n",
            "\u001b[35mSection_title: 3.4  Statistics\n",
            "\u001b[35mSection_title: 4  Evaluation\n",
            "\u001b[35mSection_title: 4.1  Methodology\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[32mconclusion: towards evaluating rc systems' internal reasoning, we have proposed r 4 c that requires systems not only to output answers but also to give their derivations. for scalability, we have carefully developed a crowdsourced framework for annotating existing rc datasets with derivations. our experiments have demonstrated that our framework produces high-quality derivations, and that automatic evaluation metrics using multiple reference derivations can reliably capture oracle derivations. the experiments using two simple baseline models highlight the nature of r 4 c, namely that the derivation generation task is not simply the sf detection task. we make the dataset, automatic evaluation script, and baseline systems publicly available at https://naoya-i.github.io/r4c/.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Question Generation\n",
            "\u001b[35mSection_title: 3.2  Semi-diagonal Masking\n",
            "\u001b[35mSection_title: 3.3  Copy Mechanism\n",
            "\u001b[35mSection_title: 3.3.1  Normal Copy\n",
            "\u001b[35mSection_title: 3.3.2  Self-Copy\n",
            "\u001b[35mSection_title: 3.3.3  Two-Hop Self-Copy\n",
            "\u001b[35mSection_title: 3.4  Copy-Generate Probability\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Training Setup\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics and Models\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Training Speed\n",
            "\u001b[32mconclusion: we showed that having a unified encoder-decoder transformer model initialized with contextualized word embeddings and further extended with copy mechanism can already give state-of-the-art, without additional pre-training on generation tasks (dong et al., 2019). we also sped up the training of qg models that use bert by choosing predictions on output embeddings that are offset by one position ( §3.3). this work shows the significance of explicitly using self-attentions of bert like models. these models can further be used in other tasks such as abstractive summarization and machine translation to see qualitative improvements.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Material and Method\n",
            "\u001b[35mSection_title: 2.1.  Material\n",
            "\u001b[35mSection_title: 2.2.  DNA Extraction\n",
            "\u001b[35mSection_title: 2.3.  Design of PCR Primer\n",
            "\u001b[35mSection_title: 2.4.  Amplification of Toll-Like Gen Receptors (TLR3) gene\n",
            "\u001b[35mSection_title: 2.5.  Sequencing and Analysis\n",
            "\u001b[35mSection_title: 3.3.  Phylogenetic analysis\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[35mSection_title: 4.1.  Primer Design for TLR3 Gene on fish\n",
            "\u001b[35mSection_title: 4.2.  Amplification of Toll-Like Receptor (TLR-3) Gene in C. batrachus\n",
            "\u001b[35mSection_title: 4.3.  Detection of Toll-Like Receptor (TLR-3) Gene in Freshwater and Marine Fishes\n",
            "\u001b[32mconclusion: the results of phylogenetic analysis showed that the tlr3 gene of the catfish was closely related to i. punctatus and c. batracus as well as some freshwater fish and seawater.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Proposed Metric for Evaluating GenQA\n",
            "\u001b[35mSection_title: 3.1  KPQA\n",
            "\u001b[35mSection_title: 3.2  KPQA Metric\n",
            "\u001b[35mSection_title: 4.2  Collecting Human Judgments of Answer Correctness\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Implementation Details\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.4  Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we create high-quality human judgments on two genqa datasets, ms-marco and avsd, and show that previous evaluation metrics are poorly correlated with human judgments in terms of the correctness of an answer. we propose kpqa-metric, which uses the pre-trained model that can predict the importance weights of words in answers to a given question to be integrated with existing metrics. our approach has a dramatically higher correlation with human judgments than existing metrics, showing that our model-based importance weighting is critical to measure the correctness of a generated answer in genqa.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.1  THE HALLUCINATION CRITERIA IN HALLUQA\n",
            "\u001b[35mSection_title: 2.2  DATA COLLECTION\n",
            "\u001b[35mSection_title: 2.3  QUALITY ASSURANCE\n",
            "\u001b[35mSection_title: 2.4  DATA STATISTICS\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  MODELS\n",
            "\u001b[35mSection_title: 3.2  METRIC\n",
            "\u001b[35mSection_title: 5.  If correct answer examples include statements like \"this question cannot be answered\", then\n",
            "\u001b[35mSection_title: 3.3  EVALUATION METHOD\n",
            "\u001b[35mSection_title: 3.4  MAIN RESULTS AND ANALYSIS\n",
            "\u001b[35mSection_title: 4  DISCUSSION\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[35mSection_title: 5.1  CHINESE LARGE LANGUAGE MODELS\n",
            "\u001b[35mSection_title: 5.2  HALLUCINATIONS AND BENCHMARKS\n",
            "\u001b[35mSection_title: 5.3  EVALUATION WITH LLMS\n",
            "\u001b[32mconclusion: in this work, we create a chinese hallucination question-answering dataset named halluqa to evaluate hallucinations in chinese large language models.questions in halluqa can be used to measure imitative falsehoods and factual errors.we design a llm-based automated evaluation method and verify its effectiveness.we conduct extensive experiments on 24 large language models.all models achieve less than a 70% non-hallucination rate on halluqa, which proves the challenging nature of our dataset.according to the experimental results, we further analyze the primary hallucinations types of different models and discuss the types that different models need to prioritize and address.we hope that halluqa can help reduce hallucinations problems in chinese large language models and enhance the credibility of the models.the prompt used for evaluation is shown in figure 10.we utilize the conversational format of gpt-4.in the first turn, we include the evaluation criteria for hallucinations and evaluation guidance, and concatenate a response from the assistant indicating affirmation.in the second turn, we provide the question to be evaluated, the response, examples of correct answers, and an instruction to output \"yes\" or \"no\" as the evaluation reulst.during the evaluation, we need to replace the red placeholders with the question, examples of the correct answer, and the model's response to this question.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Knowledge Intensive NLP\n",
            "\u001b[35mSection_title: 2.2  Improving Retrieval\n",
            "\u001b[35mSection_title: 2.3  Damaging Retrieval\n",
            "\u001b[35mSection_title: 3  Motivating Pilot Study\n",
            "\u001b[35mSection_title: 3.1  Effects of Simulated Damaging Passages\n",
            "\u001b[35mSection_title: 4  Damaging Passages from Retrievers\n",
            "\u001b[35mSection_title: 5  Analysis Methodology\n",
            "\u001b[35mSection_title: 5.1  Identifying Passage Types\n",
            "\u001b[35mSection_title: 5.2  Passage Type Selection\n",
            "\u001b[35mSection_title: 5.3  Model Analysis\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Probe-based Selection Inference\n",
            "\u001b[35mSection_title: 6.2  Attention Analysis\n",
            "\u001b[35mSection_title: 6.3  Binary Classification\n",
            "\u001b[35mSection_title: 7.1  Probe-based Selection Inference\n",
            "\u001b[35mSection_title: 7.2  Attention Inference\n",
            "\u001b[35mSection_title: 7.3  Binary Inference\n",
            "\u001b[35mSection_title: 8  Ablations\n",
            "\u001b[35mSection_title: 8.1  Passage Type Classification\n",
            "\u001b[35mSection_title: 8.2  Qualitative Analysis on Definite Negative\n",
            "\u001b[35mSection_title: 8.3  Semantically Equivalent Answers\n",
            "\u001b[32mconclusion: the reader models in retrieve-then-read pipelines are sensitive to the retrieved contexts when generating answers.damaging passages in this set can lead to incorrect responses.filtering damaging passages results in increases in em scores without the need for architectural modifications.despite  shortcomings in evaluating qa with exact match, we demonstrate that by filtering passages, models can achieve 10% higher em scores using subsets of context that are 20x times smaller.\n",
            "\u001b[35mSection_title: 9  Conclusions\n",
            "\u001b[32mlimitation: identifying the behavior of black-box models is challenging.while we identify different subsets of evidence that the model reacts well to when generating a correct answer, there is no guarantee that these subsets of information correspond to what humans users would consider useful.furthermore, some of the reasons the model changed its prediction, such as generating a more specific answer, would be correct if multiple references were available for evaluating the models.however, these alternative answers are not available in the datasets, which means we are optimizing the models for a limited subset of truly valid answers.lastly, our approach may not be practical for decoder-only llms where the order of context/answer choices varies the outcome.to assess the answerability of the given n candidate contexts, o(n!) inferences are required for llms, while fid only needs one inference due to its order-invariance property.we report a limitation, evaluation and position of established modelling techniques that can help guide the community for future research.if models can effectively leverage external information, they should be capable of using text as an interpretable source of information rather than relying solely on knowledge that is stored within inaccessible model parameters.this approach may contribute to a future with nlp models that are more interpretable and controllable.\n",
            "\u001b[35mSection_title: 10  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Query Decomposition\n",
            "\u001b[35mSection_title: 2.2  Retrieval Experts\n",
            "\u001b[35mSection_title: 2.3  Combining retrieved results\n",
            "\u001b[35mSection_title: 3  Datasets\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Baseline models\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we study a real-world information-seeking settingtip of the tongue retrieval-in which users issue long, complex queries for re-finding items despite being unable to articulate identifying details about those items. we introduce a simple but effective approach to handling these complex queries that decomposes them into sub-queries or clues that are routed to expert retrievers for specialized scoring. our simple framework allows for modular composition of different retrievers and leveraging of pretrained models for specific modalities such as clip for document images. we observe improvements of up to 7% relative gain for recall@5 when incorporating query decomposition into existing retrievers on our newly-introduced whatsthatbook, a large challenging dataset of real-world, tip-of-thetongue queries for books.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  TASK DEFINITION\n",
            "\u001b[35mSection_title: 3  FREE-TEXT GRAPH CONSTRUCTION\n",
            "\u001b[35mSection_title: 3.1  Graph Construction\n",
            "\u001b[35mSection_title: 3.2  Question Grounding\n",
            "\u001b[35mSection_title: 3.3  Question Graph Pruning\n",
            "\u001b[35mSection_title: 4  FREE-TEXT GRAPH MODELING\n",
            "\u001b[35mSection_title: 4.1  Initial Representations\n",
            "\u001b[35mSection_title: 4.2  Graph Update\n",
            "\u001b[35mSection_title: 5.2  Implementation\n",
            "\u001b[35mSection_title: 6  EVALUATION RESULTS\n",
            "\u001b[35mSection_title: 6.1  Graph Coverage\n",
            "\u001b[35mSection_title: 6.2  Answer Accuracy\n",
            "\u001b[35mSection_title: 6.3  Ablation Study\n",
            "\u001b[35mSection_title: 6.4  Graph Visualization\n",
            "\u001b[35mSection_title: 6.5  Case Study\n",
            "\u001b[35mSection_title: 7  RELATED WORK: KNOWLEDGE REPRESENTATION FOR QA\n",
            "\u001b[35mSection_title: 7.1  Knowledge Graph Question Answering\n",
            "\u001b[35mSection_title: 7.2  Question Answering over Text\n",
            "\u001b[35mSection_title: 7.3  Graph Networks for qa\n",
            "\u001b[35mSection_title: 8  THE VIEW BEYOND DELFT\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Proposed Methodology\n",
            "\u001b[35mSection_title: 3.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Prediction Model\n",
            "\u001b[35mSection_title: 3.3  Adversarial Training\n",
            "\u001b[35mSection_title: 4  Experiments and Result\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Performance Comparison\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Span Refinement\n",
            "\u001b[35mSection_title: 5.2  Meta Learning\n",
            "\u001b[32mconclusion: we leverage adversarial learning to learn domaininvariant features. in our experiments, the proposed method consistently improves the performance of baseline and it is applicable to any qa model. in future work, we will try adversarial learning for pre-training model with diverse set of domains.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  GNNs for Multi-hop QA\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Paragraph Selection\n",
            "\u001b[35mSection_title: 3.2  Context Encoding\n",
            "\u001b[35mSection_title: 3.3  Weighted Graph Construction\n",
            "\u001b[35mSection_title: 3.4  Breadth First Reasoning\n",
            "\u001b[35mSection_title: 3.5  Multi-score Answer Prediction\n",
            "\u001b[35mSection_title: 3.6  Multi-task Joint Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  Ablations and Analysis\n",
            "\u001b[35mSection_title: 5.1  Evaluation on Breadth First Reasoning\n",
            "\u001b[35mSection_title: 5.2  Evaluation on Weights and Multi-score\n",
            "\u001b[35mSection_title: 5.3  Complexity Analysis\n",
            "\u001b[32mconclusion: in this paper, we proposed a novel gnn model of bfr-graph. specifically, the reasoning message starts from the question node and passes to the next sentences node hop by hop until all the edges have been passed. we also construct the reasoning graph as a weighted graph and present a more interpretable way to aggregate scores of different levels from gnn. on hotpotqa leaderboard, bfr-graph achieved state-of-the-art on answer span prediction.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Multilingual Question Answering\n",
            "\u001b[35mSection_title: 2.1  Pre-trained Language Model\n",
            "\u001b[35mSection_title: 2.2  Underlying QA model: MBERT QA\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 3.1  Data Augmentation with Translation\n",
            "\u001b[35mSection_title: 1.  Translate (Q+C):\n",
            "\u001b[35mSection_title: 3.3  Language Arbitration Framework\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Data and Evaluation Metric\n",
            "\u001b[35mSection_title: 4.2  Hyper-parameters\n",
            "\u001b[35mSection_title: 4.3  MLQA Results\n",
            "\u001b[35mSection_title: 4.4  TyDiQA Results\n",
            "\u001b[35mSection_title: 4.5  Error Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we highlight open challenges in the existing multilingual approach by (lewis et al. 2020) and (clark et al. 2020). specifically, we show that large pretrained multi-lingual lms are not enough for this task. we produce several novel strategies for multilingual qa that go beyond zero-shot training and outshine the previous baseline built on top of mbert. we present a translation model that has 14 times more training data. further, our at and laf strategies utilize translation as data augmentation to bring the language-specific embeddings of the lm closer to each other. these approaches help us significantly improve the cross-lingual transfer. empirically, our models demonstrate strong results and all approaches improve over the previous zs strategy. we hope these techniques spur further research in the field such as exploring other multilingual lms and invoking additional networks on top of large lms for multilingual nlp.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Counterfactual Generation\n",
            "\u001b[35mSection_title: 2.2  Model Calibration\n",
            "\u001b[35mSection_title: 3  Preliminaries\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Setup and Base Models\n",
            "\u001b[35mSection_title: 3.3  Generating Counterfactuals\n",
            "\u001b[35mSection_title: 3.3.1  Retrieve-Generate-Filter (RGF)\n",
            "\u001b[35mSection_title: 3.3.2  Solo-QAG and Duo-QAG\n",
            "\u001b[35mSection_title: 3.3.3  Quantifying Diversity of counterfactuals\n",
            "\u001b[35mSection_title: 3.4  Model Calibration\n",
            "\u001b[35mSection_title: 3.4.1  Baseline\n",
            "\u001b[35mSection_title: 3.4.2  Improving Explanations for Calibrators\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Generating Counterfactual Instances\n",
            "\u001b[35mSection_title: 4.2  Generalization of CF Augmented Models\n",
            "\u001b[35mSection_title: 4.3  Calibration\n",
            "\u001b[35mSection_title: 4.4  Desiderata of Explanations for Calibration\n",
            "\u001b[32mconclusion: in our paper, we present a novel approach for automatic data augmentation by llm generated counterfactual instances diverse in surface form and semantic content. our results show that augmenting training data of smaller models with llm generated cfs consistently improves generalization capabilities of the underlying models across six ood extractive qa datasets. we further show that models trained on cf augmented data are easier to calibrate, both when considering the standard confidence-based setup as well as the explanationaugmented calibration setup. finally, show that rationale-augmented calibrator models prefer concise explanations, rather than comprehensive ones. by highlighting the fact that more diverse cf instances improve the quality of the models' internal representations by covering a broader part of the input space we pave the way for future works exploring the relation between surface form and semantic diversity of data used for augmentation and the models' generalization performance.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1  Describing fine-grained labels\n",
            "\u001b[35mSection_title: 2.2  Extracting fine-grained labels from radiology reports\n",
            "\u001b[35mSection_title: 2.2.1  Core Findings Detection:\n",
            "\u001b[35mSection_title: 2.2.2  Association of modifiers with relevant core findings:\n",
            "\u001b[35mSection_title: 2.2.3  Negated instance detection of core findings:\n",
            "\u001b[35mSection_title: 2.2.4  Fine-grained finding pattern formation:\n",
            "\u001b[35mSection_title: 2.3  Building deep learning models with fine-grained labels\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1  Extent of coverage:\n",
            "\u001b[35mSection_title: 3.1  Accuracy of fine-grained label extraction:\n",
            "\u001b[35mSection_title: 3.2.  Deep learning model evaluation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Custom dataset generation process\n",
            "\u001b[35mSection_title: 2.2  Model Architecture\n",
            "\u001b[35mSection_title: 2.2.1  Multitask RoBERTa Encoder\n",
            "\u001b[35mSection_title: 2.2.2  MultiSpan Predictor\n",
            "\u001b[35mSection_title: 2.2.3  Answer Type Predictor\n",
            "\u001b[35mSection_title: 2.2.4  Multi-Span Tagger\n",
            "\u001b[35mSection_title: 2.2.5  Objective\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Main Result\n",
            "\u001b[32mconclusion: we present a simple multispan architecture, mseqa, for multi type question answering by classifying each sentence as a probable answer candidate. we show that when a combination of single/multi-span classifier with multispan tagging is used, the model provides robust answers for multi-span tasks without degrading its performance on single-span questions. as future work, we would like to further process selected sentences from the multi-span tagger and consolidate them into one fluent answer. we also plan to explore ways to put this question answering capability onto edge devices for various applications.  \n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Attack Methodology\n",
            "\u001b[35mSection_title: 3.1  Step 1: Markup Question\n",
            "\u001b[35mSection_title: 3.2  Step 2: Convert Question to Statement\n",
            "\u001b[35mSection_title: 3.3  Step 3: Generate Adversarial Statements\n",
            "\u001b[35mSection_title: 3.4  Step 4: Translate\n",
            "\u001b[35mSection_title: 3.5  Step 5: Insert Statements in Context\n",
            "\u001b[35mSection_title: 4  Data\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Hyper-parameters\n",
            "\u001b[35mSection_title: 6.1  English Only\n",
            "\u001b[35mSection_title: 6.2  Multilingual\n",
            "\u001b[35mSection_title: 6.3  Attack Strategies\n",
            "\u001b[35mSection_title: 7  Defense\n",
            "\u001b[35mSection_title: 8  Discussion and Analysis\n",
            "\u001b[32mconclusion: we have shown several novel adversaries that successfully attack mbert for mlqa. specifically, we show that the language of the adversarial statement impacts the attack with priority given to english and the language of the question regardless of the other languages in the qa pair. we also show that including such attack strategies while training our defense brings back performance without the need for complex neural network engineering. not only do the strategies improve results for their corresponding attack, they help for all our attacks indicating model robustness. in the future, we plan to expose vulnerabilities on other multilingual lms and datasets and explore more sophisticated defense strategies.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\u001b[35mSection_title: 10  Ethics\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Capturing the Tail of the Distribution\n",
            "\u001b[35mSection_title: 3.1  General Approach\n",
            "\u001b[35mSection_title: 3.2  Assigning Likelihood Scores s(x)\n",
            "\u001b[35mSection_title: 3.3  Controlling for Length\n",
            "\u001b[35mSection_title: 3.4  Dataset-specific Choices and Details\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Benchmarked Models\n",
            "\u001b[35mSection_title: 4.2  Alternative Splits for Semantic Parsing\n",
            "\u001b[35mSection_title: 4.3  Model Performance on Likelihood Splits\n",
            "\u001b[35mSection_title: 4.4  Effect of the LM on Likelihood Splits.\n",
            "\u001b[35mSection_title: 4.5  Are Reverse Likelihood Splits Difficult?\n",
            "\u001b[35mSection_title: 5  Analysis of Data Splits\n",
            "\u001b[35mSection_title: 5.1  Properties of the Proposed SPIDER Splits\n",
            "\u001b[35mSection_title: 5.2  Properties of the Proposed SNLI Splits\n",
            "\u001b[32mconclusion: with the saturation of static, single-metric leaderboards, there is growing consensus for the development of holistic evaluation benchmarks. this includes evaluation of systems on aspects of performance beyond just single error rate on indistribution data; aspects such as performance on out-of-distribution data (linzen, 2020), and evaluating generalizability, robustness and fairness (ethayarajh and jurafsky, 2020). in this work, we describe an approach to benchmark long-tail generalization, a necessary skill for nlp systems that truly understand language. we demonstrate the challenge posed by our splits to state-of-the-art models on several tasks; standard evaluation overestimates model performance on long-tail utterances. instead of releasing a random split as the only metric on official benchmarks, our simple method can be used, for a wide range of tasks, to expose additional challenges in the collected data at no annotation cost. benchmarking long-tail generalization, in this manner, can test model behavior on a broad set of generalization challenges, which may be missed by evaluations that test specific skills in isolation.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related work\n",
            "\u001b[35mSection_title: 3.  ExpMRC\n",
            "\u001b[35mSection_title: 3.1.  Subset selection\n",
            "\u001b[35mSection_title: 3.2.  Annotation process\n",
            "\u001b[35mSection_title: 3.3.  Data statistics\n",
            "\u001b[35mSection_title: 4.  Baselines\n",
            "\u001b[35mSection_title: 4.1.  Non-learning baselines\n",
            "\u001b[35mSection_title: 4.2.  Machine learning baselines\n",
            "\u001b[35mSection_title: 5.  Evaluation\n",
            "\u001b[35mSection_title: 5.1.  Evaluation metrics\n",
            "\u001b[35mSection_title: 5.2.  Human performance\n",
            "\u001b[35mSection_title: 6.  Experiments\n",
            "\u001b[35mSection_title: 6.1.  Setups\n",
            "\u001b[35mSection_title: 6.2.  Baseline results\n",
            "\u001b[35mSection_title: 6.3.  Answer and evidence balance\n",
            "\u001b[35mSection_title: 6.4.  Upper bound for evidence extraction\n",
            "\u001b[32mconclusion: in this paper, we propose a comprehensive benchmark for evaluating the explainability of mrc systems. the proposed expmrc benchmark contains four datasets, including squad, cmrc 2018, race + , c 3 , covering span-extraction mrc and multiple-choice mrc in both en- glish and chinese. expmrc aims to evaluate the mrc system to give not only correct predictions on the final answer but also extract correct evidence for the answer. we set up several baseline systems to thoroughly evaluate the difficulties of expmrc. the experimental results show that both traditional and state-of-the-art pre-trained language models still underperform human performance by a large margin on most of the subsets, indicating that more efforts should be made on designing an effective approach for evidence extraction. we hope the release of the dataset will further accelerate the research on the explainability and interpretability of mrc systems, especially for the unsupervised approaches.\n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Response-Dependent Interpretation\n",
            "\u001b[35mSection_title: 2.3  Crucial Problems with the Turing Test\n",
            "\u001b[35mSection_title: 3  Rethinking the Octopus Test\n",
            "\u001b[35mSection_title: 3.1  Octopus Test\n",
            "\u001b[35mSection_title: 3.2  Issues in Octopus Test\n",
            "\u001b[35mSection_title: 4  Reframing the Response-Dependent Interpretation of NLU Tests\n",
            "\u001b[35mSection_title: 2.  Using Behavioral Tests\n",
            "\u001b[35mSection_title: 3.  Response-Dependent Interpretation\n",
            "\u001b[35mSection_title: 5  Validity Argument for Testing NLU\n",
            "\u001b[35mSection_title: 1.  Domain Definition\n",
            "\u001b[35mSection_title: 3.  Generalization\n",
            "\u001b[35mSection_title: 4.  Explanation\n",
            "\u001b[35mSection_title: 6.  Utilization\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: the prevalence of exaggerated claims about the achievements of foundation models motivates us to reconsider how we define and evaluate nlu. our formulation of nlu using the response-dependent interpretation mitigates the issues of the turing and octopus tests; it stipulates that observers and target conditions, including tasks and abilities, must be specified. however, current practices for creating nlu datasets are yet to be aligned, which provides researchers with the freedom to choose convenient strategies. to organize essential practices using a standard guideline, we introduce the validity ar-gument, which guides stakeholders to collect and interpret evidence for validating that the test subject executes its intended behavior. our proposed checklist helps researchers find relevant practices for benchmarking nlu, but we continually revise it by investigating potential refutation to promote more credible nlu studies.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  MetaICL\n",
            "\u001b[35mSection_title: 3.1  Meta-training\n",
            "\u001b[35mSection_title: 3.2  Inference\n",
            "\u001b[35mSection_title: 3.3  Channel MetaICL\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Evaluation\n",
            "\u001b[35mSection_title: 4.4  Experiment Details\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Ablations\n",
            "\u001b[32mconclusion: in this paper, we introduced metaicl, a new fewshot learning method where an lm is meta-trained to learn to in-context learn, i.e. condition on training examples to recover the task and make predictions. we experiment with a large, diverse collection of tasks, consisting of 142 unique tasks in total and 52 unique target tasks, using seven different settings. metaicl outperforms a range of strong baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer, and outperforms or matches 8x bigger models. we identify ingredients for success of metaicl such as the number and diversity of meta-training tasks. we also demonstrate that, while metaicl is better than recent work using natural instructions, they are complementary and the best performance is achieved by integrating metaicl with instructions.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  VISUALIZATION OF TRANSFORMER REPRESENTATIONS\n",
            "\u001b[35mSection_title: 3  DEMONSTRATION OUTLINE\n",
            "\u001b[35mSection_title: 4  OBSERVATIONS\n",
            "\u001b[32mconclusion: visbert establishes a novel method to analyze the behavior of bert models, in particular regarding the question answering task. our method allows a fine-grained analysis of each of the bert layers and depicts how each input token changes in each step. additionally, visbert reveals four phases in bert's transformations that are common to all of the datasets we examined and that mirror the traditional nlp pipeline, cf. [10]. we establish this behaviour on three diverse question answering datasets and make all three models available for users to make their own analyses on their own data, as well as the code to reproduce this visualization.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  The Joy of Search: Only for Humans?\n",
            "\u001b[35mSection_title: 2  How Humans and Computers Search\n",
            "\u001b[35mSection_title: 2.1  Human Queries\n",
            "\u001b[35mSection_title: 2.2  Computer Systems\n",
            "\u001b[35mSection_title: 3  Cheater's Bowl: Gamified Data Collection For Human Searches\n",
            "\u001b[35mSection_title: 3.1  Motivation\n",
            "\u001b[35mSection_title: 3.2  Game Interface\n",
            "\u001b[35mSection_title: 3.3  The Player Community\n",
            "\u001b[35mSection_title: 3.4  A Question Answering Example\n",
            "\u001b[35mSection_title: 4  Human vs. Computer Search Strategies\n",
            "\u001b[35mSection_title: 4.1  Strategies in Common\n",
            "\u001b[35mSection_title: 4.2  Where Strategies Diverge\n",
            "\u001b[35mSection_title: 5  Existing Models and Future Design\n",
            "\u001b[35mSection_title: 5.1  Improve Existing Models with Human Actions\n",
            "\u001b[35mSection_title: 5.2  Design Suggestions for Future Models\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: open-domain and multi-hop qa is an important problem for both humans and computers. to compare how humans and computers search and answer complex questions, our interface collects human question answering data as agents search with traditional and neural search engines alongside question answering models that suggest queries and answers. humans often use shorter queries, apply dynamic search chains, and use world knowledge. future qa models should have the ability to generate novel queries, \"discard\" irrelevant results, and explicitly check answers. moreover, computer agents for qa should also be able to use diverse retrievers to find evidence to answer questions, learning from the insights found in human data. with an agent trained on our data, we could have the \"best of both worlds\" to combine the ingenuity and tacit knowledge of humans with an indefatigable agent with access to all the world's information.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Context Encoder\n",
            "\u001b[35mSection_title: 2.3  Heterogeneous Information Reasoning\n",
            "\u001b[35mSection_title: 2.4  Answer Decoder\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 3.3  Evaluation Measures\n",
            "\u001b[35mSection_title: 3.4  Implementation Details\n",
            "\u001b[35mSection_title: 3.5  Experimental Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we have proposed a new approach to multi-hop question answering over tabular and textual data. the approach, referred to as dehg, takes question answering as a problem of reasoning answers on the basis of a heterogeneous information graph. dehg employs bert in encoding of questions and passages respectively and generates pointers in decoding of answer generation. experimental results show that dehg significantly outperforms the state-of-the-art methods.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Answer-Based Claim Decomposition\n",
            "\u001b[35mSection_title: 3.2  Fine-grained Self-Evaluation\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Quantitative Results\n",
            "\u001b[35mSection_title: 5.2  Qualitative Analysis\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[35mSection_title: 5.3.1  Self-Consistency\n",
            "\u001b[35mSection_title: 5.3.2  Tense\n",
            "\u001b[35mSection_title: 5.3.3  Ambiguity\n",
            "\u001b[35mSection_title: 5.3.4  Reasoning\n",
            "\u001b[35mSection_title: 5.3.5  False Assumptions\n",
            "\u001b[35mSection_title: 5.4  Ground Truth Comparison\n",
            "\u001b[32mconclusion: we introduce answer-based claim decomposition, which aims to decompose a question into a series of true/false claims. through experiments on three datasets with gpt-3.5, including a new challenge dataset obscureqa, we show how our technique can be used to perform fine-grained self-evaluation. we find that there is a significant difference in the proportion of claims satisfied for incorrect and correct responses, but there is no indication that gpt-3.5 believes that the gold answer satisfies more abcd claims than its incorrect answers. finally, to investigate the reliability of our approach, we conduct an error analysis and based on our findings, suggest remedies to overcome these errors.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: in our preliminary experiments, we apply answerbased claim decomposition to factual trivia questions where answers are entities spanning a few words. however, we did not examine if our technique would be effective on other types of qa datasets, such as truthfulqa (lin et al., 2022b), which exploits imitative falsehoods and contains longer desired responses, or boolqa (clark et al., 2019), which has \"yes\" or \"no\" as the only possible answers.further, due to financial constrains, we test abcd and fine-grained self-evaluation through preliminary experiments on a subset of data from our three datasets. however, given that our results were statistically significant ( §5.1), we believe that the number of questions selected were sufficient for our study. in addition, since we only examined a subset of questions from our newly-collected dataset obscureqa, this opens up future research directions leveraging our dataset. we believe that obscureqa could be used to evaluate llms on a variety of facets, including benchmarking the academic knowledge of state-of-the-art llms, and given that this dataset frequently elicits untruthful responses, studying confidence and uncertainty calibration techniques.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\u001b[35mSection_title: 8  Ethics Statement\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Background\n",
            "\u001b[35mSection_title: 3.2  Proposed Model\n",
            "\u001b[35mSection_title: 3.2.1  Knowledge Graph Construction\n",
            "\u001b[35mSection_title: 3.2.2  KG-enriched Seq2Seq Ranking\n",
            "\u001b[35mSection_title: 4  Results and Analysis\n",
            "\u001b[32mconclusion: in this work, we proposed an effective approach to re-rank the documents by utilizing the knowledge graph and integrating the external knowledge into the plms. to effectively fuse the language and graph information in the knowledge-enriched framework, we introduced a mutual informationbased objective function, which ensures the fused representations are non-redundant and informative in nature. extensive experiments on biomedical and open-domain datasets show the effectiveness of the proposed approach. \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Open-Domain Question Answering\n",
            "\u001b[35mSection_title: 3  IRRR: Iterative Retriever, Reader, and Reranker\n",
            "\u001b[35mSection_title: 3.1  Retriever\n",
            "\u001b[35mSection_title: 3.2  Reader\n",
            "\u001b[35mSection_title: 3.3  Reranker\n",
            "\u001b[35mSection_title: 3.4  Training IRRR\n",
            "\u001b[35mSection_title: 3.4.1  Dynamic Oracle for Query Generation\n",
            "\u001b[35mSection_title: 3.4.2  Reducing Exposure Bias with Data Augmentation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Performance on Standard Benchmarks\n",
            "\u001b[35mSection_title: 5.2  Performance on the Unified Benchmark\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we presented iterative retriever, reader, and reranker (irrr), a system that uses a single model to perform subtasks to answer open-domain questions of arbitrary reasoning steps. irrr achieves competitive results on standard opendomain qa benchmarks, and establishes a strong baseline on b qa, the new unified benchmark we present, which features questions with mixed levels of complexity. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Crowdsourcing Tasks\n",
            "\u001b[35mSection_title: 3.1  Worker Qualification\n",
            "\u001b[35mSection_title: 3.2  Writing Task\n",
            "\u001b[35mSection_title: 3.3  Validation Task\n",
            "\u001b[35mSection_title: 4  Crowdsourcing Results\n",
            "\u001b[35mSection_title: 4.1  Dataset Construction\n",
            "\u001b[35mSection_title: 4.2  Human Performance\n",
            "\u001b[35mSection_title: 4.3  Machine Performance\n",
            "\u001b[35mSection_title: 4.4  Human-Model Performance Gap\n",
            "\u001b[35mSection_title: 5  Linguistic Analysis\n",
            "\u001b[35mSection_title: 5.1  Readability Measures\n",
            "\u001b[35mSection_title: 5.2  Question Types\n",
            "\u001b[32mconclusion: to make an nlu benchmark useful, it has to consist of examples that are linguistically diverse and difficult enough to discriminate among state-ofthe-art models. we crowdsource multiple-choice reading comprehension questions for passages extracted from seven different sources and analyze the effects of passage source on question difficulty and diversity. although we expect that the difficulty of a passage affects the difficulty of questions about that passage, the collected questions do not show any strong correlation between the human-machine performance gap and passage source, length, or readability measures. our manual annotation of comprehension types reveals that questions requiring numerical or logical reasoning are relatively difficult. we also find several trends between passage sources and comprehension types.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Multi-Step Generative Retrieval\n",
            "\u001b[35mSection_title: 3.2  Efficient Constrained Beam Search\n",
            "\u001b[35mSection_title: 3.3  Retrieval Corpus Memorization\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Bi-Encoder Retrieval Models\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metric\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 5.2  Do GRLS and BE behave differently?\n",
            "\u001b[35mSection_title: 5.3  Efficiency of GRLS\n",
            "\u001b[32mconclusion: we show that generative retrieval, which has been originally proposed for retrieving short sequences such as entities, can also be considered for retrieving longer sequences. we particularly find that generative retrieval can have an advantage over bi-encoder in certain situations, such as retrieving structured information (e.g., reasoning chains or graphs) and retrieving an arbitrary number of items. given that generative retrieval inherently has gpu memory and speed benefits, it can be a practical alternative for general retrieval tasks in the future. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 5  Baseline performance\n",
            "\u001b[32mconclusion: we present ropes, a new reading comprehension benchmark containing 14,322 questions, which aims to test the ability of systems to apply knowledge from reading text in a new setting. we hope that ropes will aide efforts in tying language and reasoning together for more comprehensive understanding of text.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 4.  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1.  Dataset\n",
            "\u001b[35mSection_title: 4.2.  Model Settings\n",
            "\u001b[35mSection_title: 4.3.  Model Output\n",
            "\u001b[35mSection_title: 4.4.  Evaluation Method\n",
            "\u001b[35mSection_title: 5.  Experiments\n",
            "\u001b[35mSection_title: 5.1.  Baseline Model\n",
            "\u001b[35mSection_title: 5.2.  Pre-trained Models and Fine-Tuning\n",
            "\u001b[35mSection_title: 5.3.  Data Augmentation\n",
            "\u001b[35mSection_title: 5.4.  Weight Decay Regularization\n",
            "\u001b[35mSection_title: 6.  Results and Discussions\n",
            "\u001b[32mconclusion: we attempted to solve quranqa shared task using bert (devlin et al., 2018) from scratch as well as fine-tuned over two different pre-trained variants. moreover we opted for data augmentation and weight-decay regularization techniques to improve performance over the task.\n",
            "\u001b[35mSection_title: 7.  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Deductive Reasoning\n",
            "\u001b[35mSection_title: 3  Evaluating LMs for Logical Robustness\n",
            "\u001b[35mSection_title: 3.1  Logical Robustness\n",
            "\u001b[35mSection_title: 3.2  Notations\n",
            "\u001b[35mSection_title: 3.3  Logical Contrast Sets\n",
            "\u001b[35mSection_title: 3.4  Logical Equivalence Sets\n",
            "\u001b[35mSection_title: 4  The ROBUSTLR Dataset\n",
            "\u001b[35mSection_title: 4.1  Dataset Domain\n",
            "\u001b[35mSection_title: 4.2  Dataset Sampling\n",
            "\u001b[35mSection_title: 4.3  Filtering Statistical Features\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  In-domain Performance\n",
            "\u001b[35mSection_title: 6.3  Performance on Logical Equivalence set\n",
            "\u001b[35mSection_title: 6.4  Human Evaluation\n",
            "\u001b[35mSection_title: 6.5  Analysis\n",
            "\u001b[35mSection_title: 7  Related Works\n",
            "\u001b[32mconclusion: in this paper, we proposed robustlr, a diagnostic benchmark to test the logical robustness of deductive reasoning models. in robustlr, we propose two evaluation sets, logical contrast and logical equivalence, each probing different logical reasoning abilities. overall, we find that fine-tuning lms such as roberta and t5 on deductive reasoning datasets is not sufficient to learn the semantics of the logical operators conjunction, disjunction, and negation. although well-aligned training dataset improves model performance, the models still find it challenging to understand negations, both in logical contrast and logical equivalence sets. we demonstrate some interesting shortcoming of lms designed for logical reasoning, that can eventually enable building better reasoning models.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: a key limitation of the work is the synthetic nature of the dataset. while it is ideal to explore more natural theories, it makes the systematic logical perturbation process very challenging. thus, in this work, we resort to using synthetic datasets, but aim to bridge this gap in future works. another limitation is the complexity of the datasets we explore. we use fairly simple logical rules and constructs for robustlr. some more complex forms of logical reasoning-based theories can potentially reveal even more limitations of deductive reasoning models. another interesting aspect we do not explore in this scope is potential techniques to improve these models on deductive reasoning tasks. this might involve trying different inductive biases in the form of architectural designs, more specialized datasets, etc.\n",
            "\u001b[35mSection_title: 9  Limitation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Problem Setup\n",
            "\u001b[35mSection_title: 2.2  Iterative Prompting Framework\n",
            "\u001b[35mSection_title: 2.3  Context-Aware Prompter\n",
            "\u001b[35mSection_title: 2.4  Learning and Inference\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Datasets & Preprocessing\n",
            "\u001b[35mSection_title: 3.2  Compared Methods\n",
            "\u001b[35mSection_title: 3.3  Evaluation Metric\n",
            "\u001b[35mSection_title: 3.4  Implementation Details\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Effectiveness of iCAP\n",
            "\u001b[35mSection_title: 4.2  Faithfulness of Prompting\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we explore an iterative prompting framework towards driving a \"chain of thought\" from plms for multi-step reasoning tasks. we show the superiority of this iterative scheme, and also the effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme. in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors. in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities. the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely, a human can track along the plm's chain of thought and make edits and corrections at different steps, similarly as in (mo et al., 2022a), which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process. we leave these investigations as future work.\n",
            "\u001b[35mSection_title: 6  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  | INTRODUC TI ON\n",
            "\u001b[35mSection_title: 2  | MATERIAL S AND ME THODS\n",
            "\u001b[35mSection_title: 2.1  | Routine culturing conditions, molecular biology techniques, and reagents\n",
            "\u001b[35mSection_title: 2.2  | Phage stock preparation, transductions, and transposon mutant selection\n",
            "\u001b[35mSection_title: 2.3  | Arrayed library preparation\n",
            "\u001b[35mSection_title: 2.4  | Morphology screen\n",
            "\u001b[35mSection_title: 2.5  | Southern blot hybridization analysis\n",
            "\u001b[35mSection_title: 2.6  | Identification of the transposon insertion site in Mycobacterium kansasii 13D6\n",
            "\u001b[35mSection_title: 2.7  | Construction of plasmids pML-B11 and pML1335 ΔxylE\n",
            "\u001b[35mSection_title: 2.8  | RNA isolation\n",
            "\u001b[35mSection_title: 2.9  | Northern blot analysis\n",
            "\u001b[35mSection_title: 2.10  | RT-qPCR analysis\n",
            "\u001b[35mSection_title: 2.11  | Biofilm formation\n",
            "\u001b[35mSection_title: 2.12  | Galleria mellonella larva killing assay\n",
            "\u001b[35mSection_title: 2.13  | Transposon insertion sequencing experiment\n",
            "\u001b[35mSection_title: 3  | RE SULTS AND D ISCUSS I ON\n",
            "\u001b[35mSection_title: 3.1  | Evaluation of the ϕMycoMarT7 transposon mutagenesis system in Mycobacterium kansasii\n",
            "\u001b[35mSection_title: 3.2  | A Mycobacterium kansasii transposon mutant library screen links a conserved sRNA gene of unknown function to colony morphology\n",
            "\u001b[35mSection_title: 3.3  | Disruption of B11 transcription leads to biofilm formation deficiency\n",
            "\u001b[35mSection_title: 3.4  | Disruption of B11 transcription does not cause attenuation in a Galleria mellonella larva killing assay\n",
            "\u001b[35mSection_title: 3.5  | A transposon insertion sequencing approach identifies 12,071 insertions that do not compromise\n",
            "\u001b[35mSection_title: 4  | CON CLUS IONS\n",
            "\u001b[35mSection_title: 30.  The final library is then sequenced as described in the Materials\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Default Logic and ASP\n",
            "\u001b[35mSection_title: 4  LogicNMR Benchmark\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Settings\n",
            "\u001b[35mSection_title: 5.2.1  Accuracy\n",
            "\u001b[35mSection_title: 5.2.2  Generalization\n",
            "\u001b[35mSection_title: 5.2.3  Proof-based Traceability\n",
            "\u001b[35mSection_title: 5.2.4  Robustness\n",
            "\u001b[35mSection_title: 5.3  Case Study\n",
            "\u001b[32mconclusion: in this paper, we construct a synthetic nonmonotonic reasoning benchmark, logicnmr, with explicit facts and rules, to capture the iterative update on the knowledge base.we probe whether the pre-trained language models have truly mastered the non-monotonic reasoning ability.the experimental results show that even though the fine-tuned language models all achieve a high accuracy, they perform worse on generalization, proof-based traceability and robustness to irrelevant information.consequently, we cannot give a positive answer to the research problem whether the language models master the non-monotonic reasoning ability.it suggests us to explore a better approach to take advantage of the language models to conduct non-monotonic reasoning tasks.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\u001b[32mlimitation: although we construct a dataset to probe the non-monotonic reasoning ability of language models and conduct some experiments, we have to admit that there are still some limitations.first, only three language models are used in this paper.more language models with different architectures should be evaluated.second, the synthetic rules of logicnmr are too strong.we will relax some restrictions of generating rules, such as query extraction way.third, we limit the default theory to only one extension to reduce reasoning complexity, resulting in simpler non-monotonic inference patterns.a future work is to probe non-monotonic reasoning ability in a more general and systematic way, such as by allowing plural extensions.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Related Work\n",
            "\u001b[35mSection_title: 2.1  Datasets\n",
            "\u001b[35mSection_title: 2.2  Models\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Mention Representation\n",
            "\u001b[35mSection_title: 3.2  Mention-Pair Coreference Scorer\n",
            "\u001b[35mSection_title: 3.3  Inference\n",
            "\u001b[35mSection_title: 3.4  Training\n",
            "\u001b[35mSection_title: 3.5  Implementation Details\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Baselines\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 7.2  Mention Representation Components\n",
            "\u001b[32mconclusion: we presented a neural approach for resolving cross-document event and entity coreference. we represent a mention using its text, context, andinspired by the joint model of lee et al. (2012)we make an event mention representation aware of coreference clusters of entity mentions to which it is related via predicate-argument structures, and vice versa. our model achieves state-of-the-art results, outperforming previous models by 10.5 conll f 1 points on events, and providing the first cross-document entity coreference results on ecb+. future directions include investigating ways to minimize the pipeline errors from the extraction of predicate-argument structures, and incorporating a mention prediction component, rather than relying on gold mentions. \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Fixed and Dynamic Multi-hop Retrieval\n",
            "\u001b[35mSection_title: 4.2  Datasets\n",
            "\u001b[35mSection_title: 4.3  Bi-Encoder Retrieval Models\n",
            "\u001b[35mSection_title: 4.4  Evaluation Metric\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 5.1  Results\n",
            "\u001b[35mSection_title: 5.2  Analysis\n",
            "\u001b[32mconclusion: in this paper, we show that the bi-encoder approach has limitations in multi-hop retrieval; the bottleneck problem becomes a more severe problem as the number of hops increases, and is more susceptible to error propagation. we present generative multi-hop retrieval (gmr), an encoder-decoder model that performs retrieval by generating the entire target sequences with the aid of constrained decoding. we show that gmr is more robust on multi-hop retrieval tasks where it achieves higher or comparable performance in five datasets. we also introduce two corpus memorization methods, lm memorization and multi-hop memorization, to further improve gmr's performance. our experimental results demonstrate that in multi-hop retrieval, a generative approach is highly competitive with bi-encoder methods and deserves further explorations in the community.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  UNICODE AND ITS ENCODINGS\n",
            "\u001b[35mSection_title: 2.1  UTF-16\n",
            "\u001b[35mSection_title: 2.2  UTF-8\n",
            "\u001b[35mSection_title: 3  RELATED WORK\n",
            "\u001b[35mSection_title: 4  NOTATIONAL CONVENTIONS\n",
            "\u001b[35mSection_title: 4.1  Mask operations\n",
            "\u001b[35mSection_title: 4.2  Vector operations\n",
            "\u001b[35mSection_title: 4.3  Special functions\n",
            "\u001b[35mSection_title: 5  AVX-512\n",
            "\u001b[35mSection_title: 5.1  Masking\n",
            "\u001b[35mSection_title: 5.2  Micro-architectural details\n",
            "\u001b[35mSection_title: 6  TRANSCODING FROM UTF-8 TO UTF-16\n",
            "\u001b[35mSection_title: 6.1  Classification and masks\n",
            "\u001b[35mSection_title: 6.2  Assembling characters\n",
            "\u001b[35mSection_title: 6.3  Processing the tail\n",
            "\u001b[35mSection_title: 6.4  Input validation\n",
            "\u001b[35mSection_title: 6.4.1  Overlong 2-byte sequences\n",
            "\u001b[35mSection_title: 6.4.2  Mismatched continuation bytes\n",
            "\u001b[35mSection_title: 6.4.3  Encodings out of range\n",
            "\u001b[35mSection_title: 6.5  Fast paths\n",
            "\u001b[35mSection_title: 6.5.1  ASCII only\n",
            "\u001b[35mSection_title: 6.5.2  1/2 byte only\n",
            "\u001b[35mSection_title: 6.5.3  1/2/3 byte only\n",
            "\u001b[35mSection_title: 7  TRANSCODING FROM UTF-16 TO UTF-8\n",
            "\u001b[35mSection_title: 7.1  Classification and fast paths\n",
            "\u001b[35mSection_title: 7.1.1  ASCII only\n",
            "\u001b[35mSection_title: 7.1.2  Default path\n",
            "\u001b[35mSection_title: 7.1.3  1/2 byte only\n",
            "\u001b[35mSection_title: 7.2  Surrogates\n",
            "\u001b[35mSection_title: 7.3  Encoding into UTF-8\n",
            "\u001b[35mSection_title: 7.4  Validation\n",
            "\u001b[35mSection_title: 7.5  Decoding failure and tail handling\n",
            "\u001b[35mSection_title: 8  EXPERIMENTS\n",
            "\u001b[32mconclusion: it is not a priori obvious that character transcoding is amenable to simd processing.earlier work achieved high speeds but it required kilobytes of lookup tables. 2 our work indicates that the avx-512 instruction-set extensions enables high speed for tasks such as character transcoding-without lookup tables and using few instructions.it suggests that some features of the avx-512 instruction-set extensions might serve as a reference for future instruction-set extensions.in particular, we find masked simd instructions (move, load, store, compress) with byte-level granularity useful.both intel and amd support avx-512 instructions.they also both offer specialized compilers, tuned for their processors.future work could compare the performance of our routines on more varied intel and amd processors (e.g., intel rocket lake and sapphire rapids, amd zen 4), using specialized compilers (e.g., from intel and amd) and hand-tuned assembly.we could extend our benchmarks to cover a wider range of string.\n",
            "\u001b[35mSection_title: 9  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 139.7  million mi²\n",
            "\u001b[35mSection_title: 3  mi\n",
            "\u001b[35mSection_title: 2  Fermi Problems\n",
            "\u001b[35mSection_title: 3  Related Work\n",
            "\u001b[35mSection_title: 4  Datasets and Tasks\n",
            "\u001b[35mSection_title: 4.1  Dataset Elements\n",
            "\u001b[35mSection_title: 4.2  Challenge Datasets\n",
            "\u001b[35mSection_title: 4.2.1  REALFP: Real-World Fermi Problems\n",
            "\u001b[35mSection_title: 4.2.2  SYNTHFP: Synthetic Fermi Problems\n",
            "\u001b[35mSection_title: 4.2.3  Challenge Tasks\n",
            "\u001b[35mSection_title: 4.3  Metrics\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[32mlimitation: our realfp dataset includes only one explanation program to a given fp whereas in practice, there can be multiple creative decompositions that lead to the correct answer. to encourage models that are capable of capturing this diversity in the output space, it would be interesting to (a) collect alternative solutions similar to say, image captioning datasets where it is the norm to train and evaluate against multiple ground truth candidates and (b) increasing the number of templates in the synthfp dataset, thereby biasing the model towards exploring multiple solutions by pre-training on a richer synthetic dataset. further, the work doesn't include other variants of fps -e.g. binary yes/no questions, comparisons, or fps involving probability and risk quantification. finally, note that our real-world dataset, by virtue of how it is collected, has a high us-centric bias, both in terms of cultural context and vocabulary.\n",
            "\u001b[35mSection_title: 6.2  Limitations of REALFP.\n",
            "\u001b[35mSection_title: 6.3  Modeling Improvements.\n",
            "\u001b[32mconclusion: in this work, we propose fermi problems (fps) as a reasoning challenge for ai systems. apart from introducing abstraction as a crucial reasoning skill, our work requires the combined application of various reasoning skills including creative decomposition of problems, commonsense reasoning, mathematical reasoning, etc. we collect two datasets -realfp with ∼1k real-world questions and syn-thfp with 10k templated questions. based on these datasets, we propose three concrete tasks of increasing difficulty that encompass the fp challenge. the baseline models we provide, despite being based on state-of-the-art language models and even with substantial fine-tuning, struggle on our challenge tasks. they are, on average, off by two orders of magnitude from the correct estimate and perform only slightly better than predicting a constant number. we thus hope to establish fermi problems as a hard reasoning challenge that motivates further advances in ai reasoning systems.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.  Change of Plans\n",
            "\u001b[35mSection_title: 3.  Point of No Return\n",
            "\u001b[35mSection_title: 4.  Major Setback\n",
            "\u001b[35mSection_title: 5.  Climax\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The TRIPOD Dataset\n",
            "\u001b[35mSection_title: 4  Turning Point Prediction Models\n",
            "\u001b[35mSection_title: 4.1  Identifying Turning Points in Synopses\n",
            "\u001b[35mSection_title: 4.2  Identifying Turning Points in Screenplays\n",
            "\u001b[35mSection_title: 4.3  End-to-end TP Identification\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[32mconclusion: we proposed the task of turning point identification in screenplays as a means of analyzing their narrative structure. we demonstrated that automatically identifying a sequence of key events and segmenting the screenplay into thematic units is feasible via an end-to-end neural network model. in future work, we will investigate the usefulness of tps for summarization and question answering. we will also scale the tripod dataset and move to a multi-modal setting where tps are identified directly in video data.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Agent Framework\n",
            "\u001b[35mSection_title: 2.2  Agent Ability\n",
            "\u001b[35mSection_title: 2.3  Agent Design\n",
            "\u001b[35mSection_title: 3  Evaluation\n",
            "\u001b[35mSection_title: 3.1  Preparations\n",
            "\u001b[35mSection_title: 3.1.1  Datasets\n",
            "\u001b[35mSection_title: 3.1.2  Tools\n",
            "\u001b[35mSection_title: 3.1.3  LLMs\n",
            "\u001b[35mSection_title: 3.2  Evaluation on Task Planning Ability\n",
            "\u001b[35mSection_title: 3.2.1  TPTU-OA: Tool Order Planning\n",
            "\u001b[35mSection_title: 3.2.2  TPTU-OA: Tool Order Planning and Subtask Description Generation\n",
            "\u001b[35mSection_title: 3.2.3  TPTU-OA: The Planning of Tool-Subtask Pair\n",
            "\u001b[35mSection_title: 3.2.4  TPTU-OA: The Planning of Tool-Subtask Pair with Unrelated Tools\n",
            "\u001b[35mSection_title: 3.2.5  TPTU-SA: The Planning of Tool-Subtask Pair Generation\n",
            "\u001b[35mSection_title: 3.3  Evaluation on Tool Usage Ability\n",
            "\u001b[35mSection_title: 3.3.1  The effectiveness of Single Tool Usage\n",
            "\u001b[35mSection_title: 3.3.2  TPTU-OA and TPTU-SA: Tool Usage for Multiple Tools\n",
            "\u001b[35mSection_title: 3.4  Insightful Observations\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 4.1  Tool Usage\n",
            "\u001b[35mSection_title: 4.2  Tool Creation\n",
            "\u001b[32mconclusion: in this paper, we have introduced a structured framework specially designed for llm-based ai agents, with an emphasis on their abilities in task planning and tool usage.this framework, coupled with our design of two distinct types of agents assigned for the inference process, allows for a comprehensive evaluation of the capabilities of current open-source llms, thereby yielding critical insights into their effectiveness.furthermore, our research highlights the significant potential of llms in managing complex tasks, revealing the exciting prospects they hold for future research and development.as we continue to explore and improve upon these models, we move closer to unlocking their full potential in a wide range of real-world applications.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.2  What is a secret?\n",
            "\u001b[35mSection_title: 2  Designing a system to keep secrets\n",
            "\u001b[35mSection_title: 3  Verifying that secrets are being kept\n",
            "\u001b[35mSection_title: 3.1  Phase 1. Baseline Assessment\n",
            "\u001b[35mSection_title: 3.2  Phase 2. Redacted Context Assessment\n",
            "\u001b[35mSection_title: 3.3  Phase 3. Secret Keeping\n",
            "\u001b[35mSection_title: 4  Evaluating the effectiveness of secret-keeping\n",
            "\u001b[35mSection_title: 4.1  Quantitative Results\n",
            "\u001b[35mSection_title: 4.2  Qualitative Evaluation\n",
            "\u001b[35mSection_title: 5  Is it right for machines to keep secrets?\n",
            "\u001b[35mSection_title: 6  Related Work in protecting sensitive information\n",
            "\u001b[35mSection_title: 6.1  Question Answering\n",
            "\u001b[35mSection_title: 6.2  Agent-based systems and reasoning\n",
            "\u001b[35mSection_title: 6.3  Content Moderation\n",
            "\u001b[35mSection_title: 6.4  Spoiler Detection\n",
            "\u001b[35mSection_title: 6.5  Memorization and Forgetting\n",
            "\u001b[35mSection_title: 6.6  Censorship, Sanitization and Anonymization\n",
            "\u001b[32mconclusion: we have introduced the task of secret-keeping as an important, and under-explored problem in question answering. we identify a lack of suitable secretkeeping metrics and define secrecy, paranoia and information leakage to address the gap. we design and implement a secret-keeping approach that is model-agnostic, only requiring access to predefined secrets, and the output of a qa system to, detect the disclosure of secrets. we have identified a rich field for future work in secret-keeping including: • reducing paranoia and information leakage.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Strategy Questions\n",
            "\u001b[35mSection_title: 2.1  Desiderata\n",
            "\u001b[35mSection_title: 2.2  Decomposing Strategy Questions\n",
            "\u001b[35mSection_title: 3  Data Collection Pipeline\n",
            "\u001b[35mSection_title: 3.1  Creative Question Writing (CQW)\n",
            "\u001b[35mSection_title: 3.2  Strategy Question Decomposition (SQD)\n",
            "\u001b[35mSection_title: 3.3  Evidence Matching (EVM)\n",
            "\u001b[35mSection_title: 3.4  Data Verification Mechanisms\n",
            "\u001b[35mSection_title: 4  The STRATEGYQA Dataset\n",
            "\u001b[35mSection_title: 4.1  Dataset Statistics\n",
            "\u001b[35mSection_title: 4.2  Data Quality\n",
            "\u001b[35mSection_title: 4.3  Data Diversity\n",
            "\u001b[35mSection_title: 4.4  Human Performance\n",
            "\u001b[35mSection_title: 5  Experimental Evaluation\n",
            "\u001b[35mSection_title: 5.1  Baseline Models\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we present strategyqa, the first dataset of implicit multi-step questions requiring a wide-range of reasoning skills. to build strategyqa, we introduced a novel annotation pipeline for eliciting creative questions that use simple language, but cover a challenging range of diverse strategies. questions in strategyqa are annotated with decomposition into reasoning steps and evidence paragraphs, to guide the ongoing research towards addressing implicit multi-hop reasoning.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Learning Method\n",
            "\u001b[35mSection_title: 3.3  Question Reconstructor\n",
            "\u001b[35mSection_title: 3.4  Solution Set\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Multi-Mention Reading Comprehension\n",
            "\u001b[35mSection_title: 4.3  Discrete Reasoning over Paragraphs\n",
            "\u001b[35mSection_title: 4.4  Semantic Parsing\n",
            "\u001b[35mSection_title: 5  Ablation Study\n",
            "\u001b[35mSection_title: 5.1  Performance on Test Data with Different\n",
            "\u001b[35mSection_title: 5.2  Effect of |Z| at Training\n",
            "\u001b[35mSection_title: 5.3  Effect of the Question Reconstructor\n",
            "\u001b[35mSection_title: 6  Evaluation of Solution Prediction\n",
            "\u001b[35mSection_title: 7  Case Study\n",
            "\u001b[32mconclusion: to alleviate the spurious solution problem in weakly supervised qa, we propose to explicitly exploit the semantic correlations between a question and its solution via mutual information maximization. during training, we pair a task-specific model with a question reconstructor which guides the task-specific model to predict solutions that are consistent with the questions. experiments on four qa datasets demonstrate the effectiveness of our learning method. as shown by automatic and manual analyses, models trained with our method are more resistant to spurious solutions during training, and are more precise in locating information that is relevant to the questions during inference, leading to higher accuracy of both answers and solutions.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[35mSection_title: 9  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Overview\n",
            "\u001b[35mSection_title: 3.3  QA and Strategy Prediction Module\n",
            "\u001b[35mSection_title: 3.4  Parent Node Prediction Module\n",
            "\u001b[35mSection_title: 3.5  Child Node Prediction Module\n",
            "\u001b[35mSection_title: 3.6  Training and Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  Results under Fully-Supervised Training\n",
            "\u001b[35mSection_title: 4.3  Using Fewer Training Samples\n",
            "\u001b[35mSection_title: 4.4  Evaluation of Out-of-Domain Data\n",
            "\u001b[35mSection_title: 4.5  Generalization Ability\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Ablation Study\n",
            "\u001b[35mSection_title: 5.2  Latency Analysis\n",
            "\u001b[32mconclusion: this paper presents ibr, a proof generation model via iterative backward reasoning for rule-based qa tasks. we equip the reasoning procedure with detailed hidden state tracking by predicting nodes and edges in the proof path iteratively backward from the question, and allow the model to reason on the elaborate representations of nodes and his- tory paths. our model is more interpretable than previous at-once models, and is also more effective and efficient than former iterative models. experiments also demonstrate the superiority of ibr to various baselines on proof generation under various settings.  we implement our model based on pytorch along with huggingface-transformers toolkit 6 . we use roberta large model 7 as our backbone encoder to generate token-level representations. table 10 shows the implementation details of ibr, including learning rates for different modules. all linear layers used in our model have one layer. the model trained after 8 epochs will be used in the evaluation. we remove functional words without lexical meaning like \"a\" and \"the\" from facts, rules, and questions to shorten the input length, so each training epoch takes about 2 hours. we select these hyper-parameters according to tuning them empirically based on the performance. all experiments are run on nvidia tesla-v100 gpus. the main experiment performance of ibr fluctuates by one point.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Construction of Evaluation Examples\n",
            "\u001b[35mSection_title: 2.1  HotpotQA\n",
            "\u001b[35mSection_title: 2.2  Sub-Question Generation\n",
            "\u001b[35mSection_title: 2.3  Intermediate Answer Extraction\n",
            "\u001b[35mSection_title: 2.4  Human Verification\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 4.2  Model Structure\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Test\n",
            "\u001b[35mSection_title: 5.2  Multi-hop QA Models on HotpotQA\n",
            "\u001b[35mSection_title: 5.3  Adversarial Evaluation for QA Datasets\n",
            "\u001b[32mconclusion: we propose a new way to evaluate whether multi-hop qa systems have learned the ability to perform reasoning over multiple documents by asking sub-questions. an automatic approach is designed to generate sub-questions for a multihop question. on a human-verified test set, we show that all three existing top models give worse performance on the subquestions compared to our proposed model with an explicit question type classification component and a single-hop qa component. as an initial step towards a more explainable qa system, we hope our work could motivate the construction of multi-hop qa datasets with explicit reasoning paths annotated and the development of better multi-hop qa models.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  METHOD\n",
            "\u001b[35mSection_title: 2.1  DATA AUGMENTATION FOR LOW-RESOURCE NLP\n",
            "\u001b[35mSection_title: 2.2  OVERVIEW OF KN O WDA\n",
            "\u001b[35mSection_title: 2.3  KNOWLEDGE MIXTURE TRAINING\n",
            "\u001b[35mSection_title: 2.4  GENERATING SYNTHETIC DATA USING KN O WDA\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  DATA AUGMENTATION ON FEWGLUE\n",
            "\u001b[35mSection_title: 3.3.1  TASK KNOWLEDGE TRANSFER\n",
            "\u001b[35mSection_title: 3.3.2  LONG TEXT GENERATION\n",
            "\u001b[35mSection_title: 3.3.3  COMPARISON WITH T0 AND RAW T5\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[32mconclusion: this paper explores multi-task learning paradigms at a massive scale for data augmentation in low-resource nlp tasks for the first time. we demonstrate that the proposed knowledge mixture training enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences). experiments verified the effectiveness of our knowda, and knowda outperforms state-of-the-art data augmentation approaches on the popular benchmarks fewglue, conll'03, and wikiann. we also perform ablation studies indicating the importance of including demonstrations and the impact of different keys. moreover, increasing the size of multi-task scaling and investigating more advanced training objectives for data augmentation is still a promising direction worthy of long-term exploration. \n",
            "\u001b[35mSection_title: 5  CONCLUSION AND FUTURE WORK\n",
            "\u001b[35mSection_title: 2.  Use\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  MULTI-HOP QUESTION ANSWERING TASK\n",
            "\u001b[35mSection_title: 3  DATASETS\n",
            "\u001b[35mSection_title: 3.1  Dataset creation\n",
            "\u001b[35mSection_title: 3.2  Existing datasets: statistics, comparisons and examples\n",
            "\u001b[35mSection_title: 3.3  Critiques and challenges\n",
            "\u001b[35mSection_title: 4  OVERVIEW OF METHODS\n",
            "\u001b[35mSection_title: 4.1  Retrieval\n",
            "\u001b[35mSection_title: 4.1.2  Final retrieval. (Re-ranking)\n",
            "\u001b[35mSection_title: 4.2  Reading Comprehension\n",
            "\u001b[35mSection_title: 4.2.1  Graph-based Techniques. -8 works\n",
            "\u001b[35mSection_title: 4.2.2  Question Decomposition Techniques. -3 works\n",
            "\u001b[35mSection_title: 4.2.3  Miscellaneous Techniques.\n",
            "\u001b[35mSection_title: 4.3  Answer Prediction Module\n",
            "\u001b[35mSection_title: 4.3.2  Span answering approach.\n",
            "\u001b[35mSection_title: 4.3.3  Generative answering approach.\n",
            "\u001b[35mSection_title: 4.4  Auxiliary Tasks\n",
            "\u001b[35mSection_title: 4.4.1  Reasoning Chain Prediction.\n",
            "\u001b[35mSection_title: 5  PROPOSED TAXONOMY\n",
            "\u001b[35mSection_title: 6.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 6.1.1  Answer Evaluation.\n",
            "\u001b[35mSection_title: 6.2  Adversarial evaluation\n",
            "\u001b[35mSection_title: 6.3  Verifying the extent of multi-hop reasoning\n",
            "\u001b[35mSection_title: 7  MULTI-HOP QUESTION GENERATION\n",
            "\u001b[35mSection_title: 7.1  Datasets\n",
            "\u001b[35mSection_title: 7.2  Evaluation\n",
            "\u001b[35mSection_title: 7.3  Methods\n",
            "\u001b[32mconclusion: multi-hop qa has been researched quite frequently in the recent years with multiple diverse models proposed that aim to model the multi-step retrieval-reasoning process and achieve promising improvements on existing datasets and benchmarks. such systems capable of performing multistep reasoning have a variety of applications ranging from chat-bot assistants that are capable of interactive conversations, to search engines that are capable to retrieve results that may be relevant but not reachable directly from the query text. at the same time the task of mhqa is significantly more challenging than its single hop counterpart. since paragraphs multiple hops away from the question could share few common words and little semantic relation with the question [34], the task to retrieve such contexts is challenging and suffers from semantic drift. the ability of current models to combine multiple contexts for reasoning is also limited. further challenges for solving mhqa is the difficult process of creating datasets that require the models to perform multi-hop reasoning, as well as the task of evaluating the models' abilities to do so without any hacks. some challenging benchmarks and evaluation methods have been recently proposed that bring out some surprising and interesting observations. these results point out to several limitations of existing systems and call for further research.\n",
            "\u001b[35mSection_title: 8  CONCLUSIONS AND FUTURE WORK\n",
            "\u001b[35mSection_title: 8.1  Flexible any-hop model\n",
            "\u001b[35mSection_title: 8.2  Explainable multi-hop QA\n",
            "\u001b[35mSection_title: 8.3  Better datasets\n",
            "\u001b[35mSection_title: 8.4  Better evaluation metrics\n",
            "\u001b[35mSection_title: 8.5  Methods to incorporate commonsense\n",
            "\u001b[35mSection_title: 8.6  Arithmetic questions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Splinter\n",
            "\u001b[35mSection_title: 3.1  Pretraining: Recurring Span Selection\n",
            "\u001b[35mSection_title: 3.2  Model: Question-Aware Span Selection\n",
            "\u001b[35mSection_title: 3.3  Fine-Tuning\n",
            "\u001b[35mSection_title: 4  A Few-Shot QA Benchmark\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Pretraining Implementation\n",
            "\u001b[35mSection_title: 5.3  Fine-Tuning Implementation\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Few-Shot Learning\n",
            "\u001b[35mSection_title: 6.2  High-Resource Regime\n",
            "\u001b[35mSection_title: 6.3  Ablation Study\n",
            "\u001b[35mSection_title: 6.4  Analysis\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we explore the few-shot setting of extractive question answering, and demonstrate that existing methods, based on fine-tuning large pretrained language models, fail in this setup. we propose a new pretraining scheme and architecture for span selection that lead to dramatic improvements, reaching surprisingly good results even when only an order of a hundred examples are available. our work shows that choices that are often deemed unimportant when enough data is available, again become crucial in the few-shot setting, opening the door to new methods that take advantage of prior knowledge on the downstream task during model development. \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  LM involved KG Reasoning\n",
            "\u001b[35mSection_title: 2.1.1  Relation Prediction.\n",
            "\u001b[35mSection_title: 2.1.2  Contextualized KG Random Walk\n",
            "\u001b[35mSection_title: 2.2  Knowledge-Injected LM\n",
            "\u001b[35mSection_title: 2.3  Pre-Train OREOLM to Reason\n",
            "\u001b[35mSection_title: 2.3.1  Weakly Supervised Training of KIL\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Evaluate for Closed-Book QA\n",
            "\u001b[35mSection_title: 3.2  Analyze KG Reasoning Module\n",
            "\u001b[35mSection_title: 3.3  Evaluate for Open-Book QA\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we presented oreolm, a novel model that incorporates symbolic kg reasoning with existing lms. we showed that oreolm can bring significant performance gain to open-domain qa benchmarks, both for closed-book and open-book settings, as well as encoder-only and encoder-decoder models. additionally, oreolm produces reasoning paths that helps interpret the model prediction. in future, we'd like to improve oreolm by training to conduct more reasoning steps, supporting locial reasoning, and apply oreolm to a broader range of knowledge-intensive nlp tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: limited reasoning steps in our experiments, we show that using reasoning step t = 2 has better performance to t = 1 on one-hop and multi-hop (mostly two) qa datasets. thus, it's a natural question about whether we could extending reasoning steps more? as previous kg reasoning mostly could support very long path (with lstm design)though we didn't spend much time exploring before the paper submission, we indeed try using t = 3, but currently it didn't get better results. we hypothesize the following reasons: 1) a large portion of our current model's improvement relies on the weakly supervised relation pre-training. to do it, we construct a k-hop (k=2 now) subgraph, and sample dependency graph based on it. the larger k we choose, the more noise is included into the generated relation label, in an exponential increasing speed. thus, it's harder to get accurate reasoning path ground-truth for high-order t . another potential reason is that within transformer model, the representation space in lower and upper layer might be very different, say, encode more syntax and surface knowledge at lower layers, while more semantic knowledge at upper layers. currently we adopt a mlp projection head, wishing to map integrated knowledge into the same space, but it might have many flaws and need further improvement. table requires pre-training and gpu resources our current design has a huge entity embedding table, which should be learned through additional supervision and could not directly fine-tune to downstream tasks. this is restricts our approach's usage.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Modeling\n",
            "\u001b[35mSection_title: 3.1  Sentence Markers\n",
            "\u001b[35mSection_title: 3.2  Fusion-in-Decoder Approach\n",
            "\u001b[35mSection_title: 3.3  Intermediate Fine-tuning (IFT)\n",
            "\u001b[35mSection_title: 4  Datasets\n",
            "\u001b[35mSection_title: 4.1  Intermediate Fine-Tuning Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Data\n",
            "\u001b[35mSection_title: 5  Evaluation Metrics\n",
            "\u001b[35mSection_title: 6  Implementation Details\n",
            "\u001b[35mSection_title: 7  Results and Discussion\n",
            "\u001b[35mSection_title: 7.1  Comparison with Prior Work\n",
            "\u001b[35mSection_title: 7.2  Universal Model\n",
            "\u001b[35mSection_title: 7.3  Error Analysis\n",
            "\u001b[32mconclusion: in this paper, we develop general methods to improve the performance of large pre-trained seq2seq models for jointly producing nl rationales and answer predictions. specifically, we introduce sentence markers into seq2seq models to tackle explanation fabrication, we enable larger input passage sizes using the fusion-in-decoder architecture, and we infuse knowledge by fine-tuning on restructured qa datasets. we show that a universal model can perform favourably compared to the best task-specific fine-tuned models. our methods improve the state of the art on rationale extraction metrics and task accuracy on multiple eraser benchmarks while reducing the extent to which seq2seq models fabricate explanations to justify incorrect predictions, thereby improving the reliability and verifiability of the generated rationales. \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methods\n",
            "\u001b[35mSection_title: 2.1  Sequence-to-Sequence via Transformers\n",
            "\u001b[35mSection_title: 2.1.2  Input annotations\n",
            "\u001b[35mSection_title: 2.2  Graph-Augmented Transformer Encoder\n",
            "\u001b[35mSection_title: 2.2.1  Graph Representation of Documents\n",
            "\u001b[35mSection_title: 2.2.2  GATE Sublayers\n",
            "\u001b[35mSection_title: 2.3  Training Losses\n",
            "\u001b[35mSection_title: 2.3.1  Negative Log-Likelihood Objective\n",
            "\u001b[35mSection_title: 2.3.2  Auxiliary Contrastive Objective\n",
            "\u001b[35mSection_title: 2.4  Data Filtering Approach\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Dataset Preprocessing and Evaluation\n",
            "\u001b[35mSection_title: 3.2  Training Protocols\n",
            "\u001b[35mSection_title: 4  Results and Analysis\n",
            "\u001b[35mSection_title: 4.1  Ablation Studies\n",
            "\u001b[35mSection_title: 4.2  Complementarity of TE and GATE\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose a series of strong transformer models for multi-hop qg. to effectively encode the context documents and the answer, we introduce answer type embeddings and a new sublayer to incorporate the extracted entity-centric graph. we also propose an auxiliary contrastive objective to identify the supporting facts and a data filtering approach to balance the training-test distribution mismatch. experiments on the hotpotqa dataset show that our models outperform the current best approaches by a substantial margin of 5 bleu points. our analysis further reveals that graph-based components may not be the most critical in improving the performance, but can render complementary strengths to the transformer.  \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Training Domain Invariant Representations for Dense Retrieval\n",
            "\u001b[35mSection_title: 3.1  Training the Dense Retrieval Model\n",
            "\u001b[35mSection_title: 3.2  Estimating the Domain Boundary with Momentum Domain Classifier\n",
            "\u001b[35mSection_title: 3.3  Adversarial Learning for Domain Invariant Representations\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Effectiveness of MoDIR\n",
            "\u001b[35mSection_title: 4.3  Effectiveness of Momentum Training and Ablation Studies\n",
            "\u001b[35mSection_title: 4.4  Convergence of Adversarial Training with Momentum\n",
            "\u001b[35mSection_title: 4.5  Impact of Domain Invariance\n",
            "\u001b[35mSection_title: 4.6  Case Study\n",
            "\u001b[32mconclusion: in this paper, we present modir, a new representation learning method that improves the zero-shot generalization ability of dense retrieval models. we first show that dense retrieval models differ from classification models in that they emphasize locality properties in the representation space. then we present a momentum-based adversarial training method that robustly pushes text encoders to provide a more domain invariant representation space for dense retrieval. our experiments demonstrate that, compared with ance, a recent sota dr model, modir's improvements are robust overall and significant on datasets where zerodr's evaluation is more accurate.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Competency Assessment of MRC Capabilities\n",
            "\u001b[35mSection_title: 2.1  Assessment Framework Formulation\n",
            "\u001b[35mSection_title: 2.2  Relationship Between Data Properties and Model Performance\n",
            "\u001b[35mSection_title: 3  Improve Learning Efficiency with Competency Assessment\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Results\n",
            "\u001b[35mSection_title: 4.2  Human Annotation\n",
            "\u001b[32mconclusion: we design a competency assessment framework for mrc capabilities, which describes model skills in an explainable and multi-dimensional manner. by leveraging the framework, we further uncover and disentangle the connections between various data properties and model performance on a specific task, as well as propose a capability boundary breakthrough curriculum (cbbc) strategy to maxi-mize the data value and improve training efficiency. the experiments performed on four benchmark datasets verified that our approach can significantly improve the performance of existing mrc models. our work shows a deep understanding of model capabilities and data properties helps monitor the model skills during training and improves learning efficiency. our framework and learning strategy are also generally applicable to other nlp tasks. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Predicting Bias in Question Answering\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Remove Coreferences\n",
            "\u001b[35mSection_title: 3.2  Perturb the Answer Sentence\n",
            "\u001b[35mSection_title: 3.3  Add a Distracting Answer Sentence\n",
            "\u001b[35mSection_title: 3.4  Beam Search and Filtering\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Studies\n",
            "\u001b[35mSection_title: 4.4  More Analysis\n",
            "\u001b[32mconclusion: we present tasa, an automatic adversarial attack method for qa models. it generates twin answer sentences, perturbed answer sentence (pas), and distracting answer sentence (das), to construct a new adversarial context in a qa sample. it can deceive models and misguide them to an incorrect answer based on their pitfalls that overly rely on matching sensitive keywords during predicting answers. in experiments, tasa achieves remarkable attack performance on five datasets and three victim models with satisfactory sample quality. our additional analysis also proves that it is possible to get more robust qa models via tasa in the future. during fine-tuning bert on different qa datasets, we set the maximum input sequence length as 384, using an adam optimizer whose initial learning rate is 6.25e−5 with the batch size 32. the epoch number is 3 and the final model after all epochs will be saved as the victim model. spanbert we also use the huggingfancetransformers to implement the model, along with spanbert-large-cased version 5 to initialize the weights. it contains 24 layers with a hidden size of 1024. a linear layer is added to predict the start and end positions of the answer span.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background: From Strings to Symbolic AGI\n",
            "\u001b[35mSection_title: 2.1  Production systems for string manipulation\n",
            "\u001b[35mSection_title: 2.2  Control flow: From strings to algorithms\n",
            "\u001b[35mSection_title: 2.3  Cognitive architectures: From algorithms to agents\n",
            "\u001b[35mSection_title: 2.4  Language models and agents\n",
            "\u001b[35mSection_title: 3  Connections between Language Models and Production Systems\n",
            "\u001b[35mSection_title: 3.1  Language models as probabilistic production systems\n",
            "\u001b[35mSection_title: 3.2  Prompt engineering as control flow\n",
            "\u001b[35mSection_title: 3.3  Towards cognitive language agents\n",
            "\u001b[35mSection_title: 4  Cognitive Architectures for Language Agents (CoALA): A Conceptual Framework\n",
            "\u001b[35mSection_title: 4.1  Memory\n",
            "\u001b[35mSection_title: 4.2  Grounding actions\n",
            "\u001b[35mSection_title: 4.3  Retrieval actions\n",
            "\u001b[35mSection_title: 4.4  Reasoning actions\n",
            "\u001b[35mSection_title: 4.5  Learning actions\n",
            "\u001b[35mSection_title: 4.6  Decision making\n",
            "\u001b[35mSection_title: 5  Case Studies\n",
            "\u001b[35mSection_title: 6  Actionable Insights\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\u001b[32mconclusion: we proposed cognitive architectures for language agents (coala), a conceptual framework to systematically understand and build language agents.our framework draws inspiration from the rich history of symbolic artificial intelligence and cognitive science, connecting decades-old insights to frontier research on large language models.we believe this approach provides a path towards developing more general and more human-like artificial intelligence.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Formulation\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Contextualization & Quantization (CQ)\n",
            "\u001b[35mSection_title: 3.1.1  Prompt Contextualization\n",
            "\u001b[35mSection_title: 3.1.2  Prompt Quantization\n",
            "\u001b[35mSection_title: 3.2  Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  In-domain performance\n",
            "\u001b[35mSection_title: 4.2  Out of domain performance\n",
            "\u001b[35mSection_title: 4.3  Multi-task performance\n",
            "\u001b[35mSection_title: 4.4  Further Experiments\n",
            "\u001b[35mSection_title: 5  Discussions\n",
            "\u001b[35mSection_title: 6  Related work\n",
            "\u001b[32mconclusion: we propose a novel extension of soft prompt tuning (pt) -vector quantized input-contextualized prompt tuning (vip), designed to have two desirable characteristics -(i) contextualizing the soft prompt tokens w.r.t input text using a learnable sentence encoder (ii) discretizing the contextual prompts using a vector quantization network. on an extensive set of language understanding tasks -superglue, qa, nli, ner, and relation classification, vip outperforms pt baseline. further, our generalization studies on out-of-domain evaluations of qa and nli and multi-task settings over 4 tasks also show that vip is able to learn richer and more robust prompt representations than pt.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[32mlimitation: in this section, we point out the limitations of vip and its potential future directions.• pretraining prompt contextualizer. the sentence encoder in vip is trained from scratch for each downstream task. however, following the prompt pre-training proposed in gu et al. (2021), a possible future work is to pretrain the prompt contextualizer in a task-agnostic way.• larger parameter size. vip framework demands a larger parameter size than the baseline soft prompt tuning, owing mainly to the codebook. in appendix a.2 we show that by reducing the number of vip-prompt tokens and codebooksize, we can reduce the parameter size to onethird while compromising performance slightly on superglue. more extensive experimental analysis and better techniques for compressing the codebook, we leave as future work.• more hyperparameters. other than the standard hyperparameters of the sentence encoder, the quantizer introduces new hyperparameters -codebook-size, multinomial sample size, and the temperature constant τ to scale logits. while vip needs additional hyperparameters, in all our experiments across 20 training datasets from 5 tasks, we fix all hyperparameters related to codebook and sentence-encoder. this shows that our model is indeed not sensitive to the hyperparameters and does not need very specific tuning for each task/setting.• training challenges. learning the codebook requires an ema style updating scheme instead of the standard gradient update. with the plm being frozen, this needs more careful handlingfor e.g. a critical hyperparameter is the value of the temperature constant τ . a very high value can lead to representation collapse of the codebook while very low values can lead to sparse codebook usage. however, as discussed above, τ is independent of the task and depends on the initial norm of codebook vectors.• impact on small-scale datasets. we posit that due to the larger parameter size of vip, it performs worse than pt in tasks with lesser training data, e.g. scores on the cb dataset in table 1. this is due to the larger parameter size of vip. indeed, by reducing the parameter size of vip (in appendix table 8), we achieve much better performance on cb.• t5-base as backbone plm. due to resource limitations, in all our experiments we use t5base as the backbone. following lester et al. (2021) where larger plms are shown to improve prompt-tuning performance, we speculate vip to showcase a similar effect. also, though we use t5 as plm in this work, our vip architecture can be used in bert or gpt style prediction or generation as well. however, a formal analysis of this is left as future work.• data and model bias. the language understanding tasks and datasets were predominantly in the english language, and thus limit our claims to the english language. gender, age, race, and other socioeconomic biases may exist in these datasets, and models trained on these datasets may propagate these biases. it is likely that additional biases are also embedded within the t5-base plm that was used as the backbone of vip. \n",
            "\u001b[35mSection_title: 8  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Chain-of-Query Generation\n",
            "\u001b[35mSection_title: 3.2  Interaction with Information Retrieval\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[32mconclusion: in this paper, we point out the challenges that should be considered in introducing ir into llm from perspectives of reasoning and knowledge.then, we propose a novel framework called searchain for enabling ir and llm to interact with each other effectively.searchain not only stimulates the knowledge-reasoning ability of llm but also uses ir to provide the knowledge that llm really needs based on the external knowledge base, which improves accuracy and credibility.besides, searchain can mark references to supporting documents for the knowledge involved in the generated contents, which improves the traceability of the contents.in addition, the interaction between ir and llm in searchain transforms the topology of reasoning from chain to tree, which enables llm to dynamically modify reasoning direction.experimental results on complex knowledge-intensive tasks show searchain performs better than all baselines.in future work, we will consider how to improve the efficiency of the framework and how to introduce more tools to interact with llm for more tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Chain-of-Query Generation\n",
            "\u001b[35mSection_title: 3.2  Interaction with Information Retrieval\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[32mconclusion: in this paper, we point out the challenges that should be considered in introducing ir into llm from perspectives of reasoning and knowledge.then, we propose a novel framework called searchain for enabling ir and llm to interact with each other effectively.searchain not only stimulates the knowledge-reasoning ability of llm but also uses ir to provide the knowledge that llm really needs based on the external knowledge base, which improves accuracy and credibility.besides, searchain can mark references to supporting documents for the knowledge involved in the generated contents, which improves the traceability of the contents.in addition, the interaction between ir and llm in searchain transforms the topology of reasoning from chain to tree, which enables llm to dynamically modify reasoning direction.experimental results on complex knowledge-intensive tasks show searchain performs better than all baselines.in future work, we will consider how to improve the efficiency of the framework and how to introduce more tools to interact with llm for more tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  DIFFERENTIABLE REASONING OVER A KB OF INDEXED TEXT\n",
            "\u001b[35mSection_title: 2.1  DIFFERENTIABLE MULTI-HOP REASONING\n",
            "\u001b[35mSection_title: 2.2  EFFICIENT IMPLEMENTATION\n",
            "\u001b[35mSection_title: 2.3  PRETRAINING THE INDEX\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  METAQA: MULTI-HOP QUESTION ANSWERING WITH TEXT\n",
            "\u001b[35mSection_title: 3.2  WIKIDATA: MULTI-HOP SLOT-FILLING\n",
            "\u001b[35mSection_title: 3.3  HOTPOTQA: MULTI-HOP INFORMATION RETRIEVAL\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[32mconclusion: we present drkit, a differentiable module that is capable of answering multi-hop questions directly using a large entity-linked text corpus. drkit is designed to imitate traversal in kb over the text corpus, providing ability to follow relations in the \"virtual\" kb over text. we achieve state-of-the-art results on the metaqa dataset for answering natural language questions, with a 9 point increase in the 3-hop case. we also developed an efficient implementation using sparse operations and inner product search, which led to a 10-100x increase in queries/sec over baseline approaches. a metaqa: implementation details we use p = 400 dimensional embeddings for the mentions and queries, and 200-dimensional embeddings each for the start and end positions. this results in an index of size 750mb. when computing a e→m , the entity to mention co-occurrence matrix, we only retain mentions in the top 50 paragraphs matched with an entity, to ensure sparsity. further we initialize the first 4 layers of the question encoder with the transformer network from pre-training. for the first hop, we assign z 0 as a 1-hot vector for the least frequent entity detected in the question using an exact match. the number of nearest neighbors k and the softmax temperature λ were tuned on the dev set of each task, and we found k = 10000 and λ = 4 to work best. we pretrain the index on a combination of the metaqa corpus, using the kb provided with metaqa for distance data, and the wikidata corpus.   indexing only entity-mentions in single-hop questions over all spans. note that drkit-entities has a high hits@1 performance on the rare relations subset, showing that there is generalization to less frequent data due to the natural language representations of entities and relations.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  DIFFERENTIABLE REASONING OVER A KB OF INDEXED TEXT\n",
            "\u001b[35mSection_title: 2.1  DIFFERENTIABLE MULTI-HOP REASONING\n",
            "\u001b[35mSection_title: 2.2  EFFICIENT IMPLEMENTATION\n",
            "\u001b[35mSection_title: 2.3  PRETRAINING THE INDEX\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  METAQA: MULTI-HOP QUESTION ANSWERING WITH TEXT\n",
            "\u001b[35mSection_title: 3.2  WIKIDATA: MULTI-HOP SLOT-FILLING\n",
            "\u001b[35mSection_title: 3.3  HOTPOTQA: MULTI-HOP INFORMATION RETRIEVAL\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[32mconclusion: we present drkit, a differentiable module that is capable of answering multi-hop questions directly using a large entity-linked text corpus. drkit is designed to imitate traversal in kb over the text corpus, providing ability to follow relations in the \"virtual\" kb over text. we achieve state-of-the-art results on the metaqa dataset for answering natural language questions, with a 9 point increase in the 3-hop case. we also developed an efficient implementation using sparse operations and inner product search, which led to a 10-100x increase in queries/sec over baseline approaches. a metaqa: implementation details we use p = 400 dimensional embeddings for the mentions and queries, and 200-dimensional embeddings each for the start and end positions. this results in an index of size 750mb. when computing a e→m , the entity to mention co-occurrence matrix, we only retain mentions in the top 50 paragraphs matched with an entity, to ensure sparsity. further we initialize the first 4 layers of the question encoder with the transformer network from pre-training. for the first hop, we assign z 0 as a 1-hot vector for the least frequent entity detected in the question using an exact match. the number of nearest neighbors k and the softmax temperature λ were tuned on the dev set of each task, and we found k = 10000 and λ = 4 to work best. we pretrain the index on a combination of the metaqa corpus, using the kb provided with metaqa for distance data, and the wikidata corpus.   indexing only entity-mentions in single-hop questions over all spans. note that drkit-entities has a high hits@1 performance on the rare relations subset, showing that there is generalization to less frequent data due to the natural language representations of entities and relations.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Concerns with YouTube Kids Videos\n",
            "\u001b[35mSection_title: 2.2  Reading Comprehension\n",
            "\u001b[35mSection_title: 2.3  Video Retrieval\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  Multi-modal Video Retriever Model\n",
            "\u001b[35mSection_title: 3.3  Reading Comprehension Model for Generating Answers\n",
            "\u001b[35mSection_title: 3.3.1  Input Embedding Block\n",
            "\u001b[35mSection_title: 3.3.2  Attention Block\n",
            "\u001b[35mSection_title: 3.3.3  Output Block\n",
            "\u001b[35mSection_title: 3.4  Multiple-Choice Answer Extraction Model\n",
            "\u001b[35mSection_title: 3.4.1  Neural Network for Multiple Choice (NNMC)\n",
            "\u001b[35mSection_title: 3.4.2  Closest Language Embedding Model (CLEM)\n",
            "\u001b[35mSection_title: 4  DATASET\n",
            "\u001b[35mSection_title: 4.1  YouTube Videos Dataset\n",
            "\u001b[35mSection_title: 4.2  Children's' Videos Question Answering (CVQA) Dataset\n",
            "\u001b[35mSection_title: 4.3  Textbook Question Answering (TQA) Dataset\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS AND RESULTS\n",
            "\u001b[35mSection_title: 5.1  Experiment 1: Evaluating RC Models on CVQA Question Answers Dataset\n",
            "\u001b[35mSection_title: 5.1.1  Training and Evaluation\n",
            "\u001b[35mSection_title: 5.1.2  Results\n",
            "\u001b[35mSection_title: 5.1.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.1.4  Error analysis\n",
            "\u001b[35mSection_title: 5.2  Experiment 2: Quantifying the Academic Quality of Videos using TQA dataset\n",
            "\u001b[35mSection_title: 5.2.1  Training and Evaluation\n",
            "\u001b[35mSection_title: 5.2.2  Results\n",
            "\u001b[35mSection_title: 5.3  Experiment 3: Comparing Different Channels\n",
            "\u001b[35mSection_title: 5.4  Discussion\n",
            "\u001b[32mconclusion: in this research, we demonstrate the ability of an rc model to assess academic quality by introducing a new dataset consisting of questions and answers from children's videos. we then determine the academic value of the top channels by measuring the number of textbook questions answered correctly by the model. our analysis of over 80,000 videos posted on the top 100 channels provides a comprehensive evaluation of the academic quality of content on ytk and utilizes a large dataset of middle school textbook questions on various topics. our findings reveal the academic topics covered in these children's videos, and we compare the quality of the channels.\n",
            "\u001b[35mSection_title: 6  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Notation\n",
            "\u001b[35mSection_title: 2.2  Question Generation\n",
            "\u001b[35mSection_title: 2.3  Teacher Re-labeling\n",
            "\u001b[35mSection_title: 2.4  Bi-encoder Training\n",
            "\u001b[35mSection_title: 3  Downstream Tasks\n",
            "\u001b[35mSection_title: 3.1  Paraphrase Ranking\n",
            "\u001b[35mSection_title: 3.2  Paraphrase Classification\n",
            "\u001b[35mSection_title: 3.3  Named Entity Recognition\n",
            "\u001b[35mSection_title: 3.4  Zero-shot Sentiment Analysis\n",
            "\u001b[35mSection_title: 4.2  Baselines and Ablations\n",
            "\u001b[35mSection_title: 4.3  Bi-encoder Question Answering\n",
            "\u001b[35mSection_title: 4.4  Zero-shot Paraphrase Ranking\n",
            "\u001b[35mSection_title: 4.5  Paraphrase Classification\n",
            "\u001b[35mSection_title: 4.6  Named Entity Recognition\n",
            "\u001b[35mSection_title: 4.7  Sentiment Analysis\n",
            "\u001b[35mSection_title: 4.8  Stability Analysis\n",
            "\u001b[35mSection_title: 5  Discussion and Related Work\n",
            "\u001b[35mSection_title: 5.1  Question Generation\n",
            "\u001b[35mSection_title: 5.2  Improving question answering\n",
            "\u001b[35mSection_title: 5.3  Learning contextual representations\n",
            "\u001b[35mSection_title: 5.4  Few-shot learning\n",
            "\u001b[32mconclusion: in this work, we pre-trained token-level contextual representations that are useful for downstream fewshot learning. our key idea was to use questionanswer pairs to define what information should be encoded in passage representations. we showed that these representations are useful for a variety of standard nlp tasks in zero-and few-shot settings, including paraphrase detection, named entity recognition, and sentiment analysis, across nine total datasets. looking forward, we hope to see more work on designing pre-training objectives that align with downstream needs for few-shot learning. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Dual-Encoders\n",
            "\u001b[35mSection_title: 3.3  Cross-Encoders\n",
            "\u001b[35mSection_title: 3.4  Geometry Alignment Mechanism\n",
            "\u001b[35mSection_title: 3.5  Model Training and Inference\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Compared Methods\n",
            "\u001b[35mSection_title: 4.4  Implementation Details\n",
            "\u001b[35mSection_title: 4.5  Results and Analysis\n",
            "\u001b[35mSection_title: 4.6  Case Study\n",
            "\u001b[32mconclusion: in this work, we propose a framework that enhances dual-encoders with cross-embeddings for answer retrieval. a novel geometry alignment mechanism is introduced to align the geometry of dual-encoders with cross-embeddings. extensive experimental results show that our method significantly improves dual-encoders model and outperforms the state-of-the-art method on multiple answer retrieval datasets.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Chain-of-Thought-Guided Retrieval and Open-Domain QA\n",
            "\u001b[35mSection_title: 3.1  Interleaving Retrieval with\n",
            "\u001b[35mSection_title: 3.2  Question Answering Reader\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[32mconclusion: chain-of-thought prompting has significantly improved llms' ability to perform multi-step reason-  [1984][1985][1986][1987][1988][1989] was performed by jane siberry. jane siberry was born in toronto. the castle in toronto is the casa loma. so the answer is: casa loma. table 2: example cots generated by gpt3 with different methods. since nor relies on parametric knowledge, it often makes a factual error in the first sentence derailing the full cot. oner can retrieve relevant information closest to the question and is less likely to make such errors early on, but it still makes errors later in the cot. as ircot performs retrieval after each step, it is often able to prevent such errors in each step. more examples are in app. d.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Datasets\n",
            "\u001b[35mSection_title: 3  Probing Multi-hop Datasets\n",
            "\u001b[35mSection_title: 3.1  Sentence-Factored Model Test\n",
            "\u001b[35mSection_title: 3.2  No Context Baseline\n",
            "\u001b[35mSection_title: 4  Span-based vs. Multiple-choice\n",
            "\u001b[35mSection_title: 4.1  Systems to Compare\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: there exist several other multi-hop reasoning datasets including worldtree , openbookqa (mihaylov et al., 2018), and mul-tirc (khashabi et al., 2018). these datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content. we leave a detailed investigation of these for future work.\n",
            "\u001b[35mSection_title: 5  Discussion and Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Datasets\n",
            "\u001b[35mSection_title: 3  Probing Multi-hop Datasets\n",
            "\u001b[35mSection_title: 3.1  Sentence-Factored Model Test\n",
            "\u001b[35mSection_title: 3.2  No Context Baseline\n",
            "\u001b[35mSection_title: 4  Span-based vs. Multiple-choice\n",
            "\u001b[35mSection_title: 4.1  Systems to Compare\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: there exist several other multi-hop reasoning datasets including worldtree , openbookqa (mihaylov et al., 2018), and mul-tirc (khashabi et al., 2018). these datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content. we leave a detailed investigation of these for future work.\n",
            "\u001b[35mSection_title: 5  Discussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Ability Modeling\n",
            "\u001b[35mSection_title: 2.2  Data Source and Selected Tasks\n",
            "\u001b[35mSection_title: 2.3  Contrastive Evaluation System\n",
            "\u001b[35mSection_title: 2.4  Availability\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[32mconclusion: this paper presents kola, a carefully designed knowledge-oriented llm assessment benchmark. we design a cognitive ability taxonomy for more helpful diagnostic results, adopt both known and evolving data sources for better fairness, and employ contrastive metrics for high applicability. in the  figure (b) shows the scatter plots of model performance on evolving tasks and its non-evolving counterparts (e.g., 3-5 v.s. [3][4][5][6]. the x-axis and y-axis of each subplot represent the standard scores (z value) correspondingly.\n",
            "\u001b[35mSection_title: 4  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary Background\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  VIR: Variable Integrity Ranking\n",
            "\u001b[35mSection_title: 3.2  VOP: Variable Operator Prediction\n",
            "\u001b[35mSection_title: 3.3  VKM: Variable Keyphrase Masking\n",
            "\u001b[35mSection_title: 3.4  Multi-task Pretraining\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metric\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Studies\n",
            "\u001b[35mSection_title: 5.3  Pretraining Task Performance\n",
            "\u001b[35mSection_title: 5.4  Case Study\n",
            "\u001b[35mSection_title: 6.2  Pretraining for Question Answering\n",
            "\u001b[32mconclusion: in this paper, we propose three solution program centric auxiliary pretraining tasks at both the whole program level and sub-program level. at the wholeprogram level, we propose the variable integrity ranking pretraining task, which guides the model to distinguish required and irrelevant variables in the noisy input. to further enhance the model's ability to learn the underlying reasoning process, we propose two additional pretraining tasks: variable operator prediction and variable keyphrase masking. these tasks help the model perform accurate sub-program construction. experimental results demonstrate the effectiveness of our method. variable integrity ranking achieves the most improvement on both the retriever and program solver. the sub-program level tasks substantially improve results on plms of different scales. our approach achieves 3.56% execution accuracy and 3.74% program accuracy improvement on the competitive roberta-large baseline.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary Background\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  VIR: Variable Integrity Ranking\n",
            "\u001b[35mSection_title: 3.2  VOP: Variable Operator Prediction\n",
            "\u001b[35mSection_title: 3.3  VKM: Variable Keyphrase Masking\n",
            "\u001b[35mSection_title: 3.4  Multi-task Pretraining\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metric\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Studies\n",
            "\u001b[35mSection_title: 5.3  Pretraining Task Performance\n",
            "\u001b[35mSection_title: 5.4  Case Study\n",
            "\u001b[35mSection_title: 6.2  Pretraining for Question Answering\n",
            "\u001b[32mconclusion: in this paper, we propose three solution program centric auxiliary pretraining tasks at both the whole program level and sub-program level. at the wholeprogram level, we propose the variable integrity ranking pretraining task, which guides the model to distinguish required and irrelevant variables in the noisy input. to further enhance the model's ability to learn the underlying reasoning process, we propose two additional pretraining tasks: variable operator prediction and variable keyphrase masking. these tasks help the model perform accurate sub-program construction. experimental results demonstrate the effectiveness of our method. variable integrity ranking achieves the most improvement on both the retriever and program solver. the sub-program level tasks substantially improve results on plms of different scales. our approach achieves 3.56% execution accuracy and 3.74% program accuracy improvement on the competitive roberta-large baseline.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Problems for Query Generation\n",
            "\u001b[35mSection_title: 1.2  Problems for Re-ranking\n",
            "\u001b[35mSection_title: 2  Proposed Approach\n",
            "\u001b[35mSection_title: 2.1  Query Generation Approach Proposed\n",
            "\u001b[35mSection_title: 2.2  Re-ranking Approach Proposed\n",
            "\u001b[35mSection_title: 3  Proposed Framework\n",
            "\u001b[35mSection_title: 3.1  NE Mention Detection\n",
            "\u001b[35mSection_title: 3.2  Query Generator\n",
            "\u001b[35mSection_title: 3.3  Reference Sentences Retrieval\n",
            "\u001b[35mSection_title: 3.4  Mention-Aware Re-Ranker\n",
            "\u001b[35mSection_title: 3.5  Masked Sequence Tagger\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Setup and Settings\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: to improve ner with non-local information from the internet, we propose a transformer-based query generation method and a mention-aware re-ranker, mentionscore. these can favor the recall of the retrieved results, select the non-local sentences specifically related to each mention in the local sentence and lead to a state-of-the-art performance of 61.56 micro-f1 score on wnut17 dataset is achieved.  table 1. the experimental results of adopting different query strategies and reranking settings on wnut17 dataset with reference sentences retrieved by google search retrieval. all our experiments report the average performance of 10 models trained with different seeds. the superscript \" * \" denotes the experiments reported in clner paper. clner w/ cl is the best setting of wang et al. (2020), which utilized retrieved sentences and was trained with the technique cooperative learning, while clner w/o cl only used the retrieved sentences. baseline#2 implemented in our system is the counterpart to clner w/o cl. m&s+ms is the setting of using mention as query and mentionscore as re-ranker to form the external context and evaluate with the model baseline#1. m_g&s+ms uses gold mention as query. compared to baseline#2, paired single-tail ttest for p-value < 0.05 is superscripted with † and p-value < 0.01 with ‡ to denote the t-test is passed.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 4  Experimental Results\n",
            "\u001b[32mconclusion: the use of ai is constantly evolving in diverse applications. this study investigates the potential advantages of a natural language processing approach for education. this research presented an education question generating (eqg) approach that augments the ebook content with generated edu-questions to provide students with an effective learning platform. through experiments, we assessed the model's performance on a question generation task both before and after contrastive training. we discovered that a contrastive trained model can produce more pertinent questions on the input text and can comprehend key concepts more effectively. experiments on qa dataset, prml (bishop, 2006) and ncert 2 ebook shows that our model succeeds to produces complex questions at scale.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future scope\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A Taxonomy for Correcting LLMs with Automated Feedback\n",
            "\u001b[35mSection_title: 2.1  Conceptual Framework\n",
            "\u001b[35mSection_title: 2.2  What gets corrected?\n",
            "\u001b[35mSection_title: 2.3  What is the source of the feedback?\n",
            "\u001b[35mSection_title: 2.4  What is the format of the feedback?\n",
            "\u001b[35mSection_title: 2.7  Summary of existing works\n",
            "\u001b[35mSection_title: 3  Training-Time Correction\n",
            "\u001b[35mSection_title: 3.1  Learning from Human Feedback\n",
            "\u001b[35mSection_title: 3.2  Learning with Automated Feedback\n",
            "\u001b[35mSection_title: 4  Generation-Time Correction\n",
            "\u001b[35mSection_title: 4.1  Generate-then-Rank\n",
            "\u001b[35mSection_title: 4.2  Feedback-Guided Decoding\n",
            "\u001b[35mSection_title: 5  Post-hoc Correction\n",
            "\u001b[35mSection_title: 5.1  Self-Correction\n",
            "\u001b[35mSection_title: 5.2  Models/Tools as Feedback\n",
            "\u001b[35mSection_title: 5.3  Multi-Agent Debate\n",
            "\u001b[35mSection_title: 6  Applications\n",
            "\u001b[35mSection_title: 6.1  Factual Correction\n",
            "\u001b[35mSection_title: 6.2  Reasoning Tasks\n",
            "\u001b[35mSection_title: 6.3  Code Synthesis\n",
            "\u001b[35mSection_title: 6.4  Other Applications\n",
            "\u001b[35mSection_title: 7  Research Gaps and Future Directions\n",
            "\u001b[35mSection_title: 7.1  Theoretical Justifications\n",
            "\u001b[35mSection_title: 7.2  Measuring the Ability of Self-Correction\n",
            "\u001b[35mSection_title: 7.3  Continual Self-Improvement\n",
            "\u001b[35mSection_title: 7.4  Self-Correction with Model Editing\n",
            "\u001b[35mSection_title: 7.5  Multi-Modal Self-Correction\n",
            "\u001b[32mconclusion: in this paper, we present a comprehensive survey of self-correcting large language models with automated feedback. we broadly categorize and analyze various self-correction strategies, including training-time, generation-time, and post-hoc corrections. we also discuss the major application areas of self-correction, including correcting factual errors, enhancing reasoning abilities, and improving code generation, among others. finally, we outline a number of potential future directions and associated challenges in this field. our goal with this paper is to provide a com-prehensive and useful resource for readers interested in the development of this rapidly evolving domain. to aid in this effort, we create a continually-updated reading list in a github repository: https://github.com/teacherpeterpan/ self-correction-llm-papers. \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Computational space reduction\n",
            "\u001b[35mSection_title: 1.1.  Words\n",
            "\u001b[35mSection_title: 1.2.  Hits\n",
            "\u001b[35mSection_title: 1.3.  Big-hits\n",
            "\u001b[35mSection_title: 2.  Modular design\n",
            "\u001b[35mSection_title: 2.1.  The global idea\n",
            "\u001b[35mSection_title: 2.2.  First Step: Building the dictionaries\n",
            "\u001b[35mSection_title: 3.3.  Execution time\n",
            "\u001b[35mSection_title: 4.  Programs usage\n",
            "\u001b[35mSection_title: 4.1.  Dictionary creation\n",
            "\u001b[35mSection_title: 4.4.  FilterHits\n",
            "\u001b[35mSection_title: 4.5.  FragHits\n",
            "\u001b[35mSection_title: 4.6.  Additional programs readDict\n",
            "\u001b[35mSection_title: 5.  Results quality\n",
            "\u001b[35mSection_title: 5.1.  Closely related sequences\n",
            "\u001b[35mSection_title: 5.4.  Alignathon dataset study\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Overview\n",
            "\u001b[35mSection_title: 2.2  Path Reranking\n",
            "\u001b[35mSection_title: 2.3  Instruction Search\n",
            "\u001b[35mSection_title: 2.4  Instruction Ensembling\n",
            "\u001b[35mSection_title: 2.5  In-context Learning\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Retrieval Performance\n",
            "\u001b[35mSection_title: 3.2  Full QA Performance\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[35mSection_title: 4.1  Comparison to Single-hop Reranking\n",
            "\u001b[35mSection_title: 4.2  Does the Instruction Matter?\n",
            "\u001b[35mSection_title: 4.3  Sensitivity to Instructions\n",
            "\u001b[35mSection_title: 4.4  Invariance to Document Order\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: this work introduces lepus, a method to perform unsupervised re-ranking of multi-document paths for question answering based on large language models. given a question, the document path is encoded into a prompt and the document path is scored as the probability of generating the question given the prompt. experiments on a standard multihop qa benchmark show the strong performance of lepus in the zero-shot setting, displaying comparable performance to fully-supervised retrievers. we also analyze our approach showing the utility of using multi-hop prompts as opposed to singlehop ones. lastly, our work shows that language models can indeed function as strong unsupervised re-rankers for multi-hop question answering.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 3.  Extraction and examination of dierences\n",
            "\u001b[35mSection_title: 3.  Synonyms\n",
            "\u001b[35mSection_title: 5.  Ellipsis (decrease of information)\n",
            "\u001b[35mSection_title: 6.  Complementation (increase of information)\n",
            "\u001b[35mSection_title: 7.  Error detection We s h o w some examples of error detection in\n",
            "\u001b[35mSection_title: 4.  Trial of automatic translation from the written to the spoken language\n",
            "\u001b[32mconclusion: in this study, w e extracted dierences between spoken and written languages and examined the extracted dierences by using spoken and written data constructed by the communications research laboratory and the national institute for japanese language. we also tried transforming written language into spoken language by using extracted dierences as the transformation rules.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Repression by RNA Antitoxins and Toxin Function\n",
            "\u001b[35mSection_title: 2.1.  Type I Antitoxins: Repression through Base Pairing Interactions\n",
            "\u001b[35mSection_title: 2.1.2.  Repressing Type I Toxins: Controlling Protein Synthesis\n",
            "\u001b[35mSection_title: 2.2.  Repressing Type I Toxins: Controlling Degradation and Translation\n",
            "\u001b[35mSection_title: 2.3.  Type III Antitoxins: Repression through Protein Sequestration\n",
            "\u001b[35mSection_title: 2.4.  Function of Type I and Type III Gene Pairs\n",
            "\u001b[35mSection_title: 2.4.1.  Plasmid Remnants?\n",
            "\u001b[35mSection_title: 2.4.2.  Impairment of Chromosomal Structure\n",
            "\u001b[35mSection_title: 2.4.3.  Persister Formation\n",
            "\u001b[35mSection_title: 2.4.4.  Chromosomal Stabilization and Recombination\n",
            "\u001b[35mSection_title: 2.4.5.  Protection from Foreign DNA\n",
            "\u001b[35mSection_title: 2.4.6.  Inhibition of Competitors\n",
            "\u001b[35mSection_title: 2.4.7.  Nucleic Acid Cleavage\n",
            "\u001b[35mSection_title: 3.  Regulating the Toxin mRNA versus the Protein\n",
            "\u001b[35mSection_title: 3.1.  Type I Antitoxins: Base Pairing to the Toxin mRNA\n",
            "\u001b[35mSection_title: 3.2.  Type III Antitoxins: Binding to the Toxic Proteins\n",
            "\u001b[32mconclusion: there are numerous toxin-antitoxin loci found within bacterial chromosomes; the type i and type iii pairs represent only a fraction of what has been described to date [1]. the type i and type iii loci are unique in that they utilize rna as antitoxins, yet they use their rna antitoxins very differently, illustrating the immense versatility of rna as a regulatory molecule. it is important to note that to date these antitoxins have been shown to act either through interaction with toxin mrna or protein.\n",
            "\u001b[35mSection_title: 4.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Adversarial Evaluation\n",
            "\u001b[35mSection_title: 2.1  The HotpotQA Task\n",
            "\u001b[35mSection_title: 2.2  Adversary Construction\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 3.1  Encoding\n",
            "\u001b[35mSection_title: 3.2  Single-Hop Baseline\n",
            "\u001b[35mSection_title: 3.3  Compositional Attention over Question\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 7  Related Works\n",
            "\u001b[32mconclusion: in this work, we identified reasoning shortcuts in the hotpotqa dataset where the model can locate the answer without multi-hop reasoning. we constructed adversarial documents that can fool the models exploiting the shortcut, and found that the performance of a state-of-the-art model dropped significantly under our adversarial examples. we showed that this baseline can improve on the adversarial evaluation after being trained on the adversarial data. we next proposed to use a control unit that dynamically attends to the question to guide the bi-attention in multi-hop reasoning. trained on the regular data, this 2-hop model is more robust against the adversary than the baseline; and after being trained with adversarial data, this model achieved further improvements on the adversarial evaluation and also outperforms the baseline. overall, we hope that these insights and initial improvements will motivate the development of new models that combine explicit compositional reasoning with adversarial training.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task: Table-based Semantic Parsing\n",
            "\u001b[35mSection_title: 3  SQUALL: Our New Dataset\n",
            "\u001b[35mSection_title: 3.1  Data Annotation\n",
            "\u001b[35mSection_title: 3.2  Post-processing\n",
            "\u001b[35mSection_title: 4  (State-of-the-Art) Base Model: Seq2seq with Attention and Copying\n",
            "\u001b[35mSection_title: 5  Using Alignments in Model Training\n",
            "\u001b[35mSection_title: 5.1  Supervised Attention\n",
            "\u001b[35mSection_title: 5.2  Oracle Experiments with Manual Alignments\n",
            "\u001b[35mSection_title: 5.3  Column Prediction\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we introduce squall, the first large-scale semantic parsing dataset with both hand-produced target logical forms and manually-derived lexical alignments between questions and sql queries. our dataset enables finer-grained supervision than existing datasets have previously supported. we incorporate the alignments into encoder-decoder-based neural models through supervised attention and an auxiliary task of column prediction. experiments confirm our intuition that finer-grained supervision is helpful to model training. our oracle studies also show that there is large unrealized further potential for our annotations. thus, it remains an exciting challenge for future research to use our lexical alignment annotations more effectively. our annotation cost analysis shows that collecting additional lexical alignments is more costeffective for improving model accuracy than having only logical forms. we hope that our findings will help future dataset design decisions and extensions of other existing datasets. one potential future direction is to further investigate the utility of lexical alignments in a cross-dataset/domain evaluation setting  \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  SPARSE RETRIEVAL MODELS\n",
            "\u001b[35mSection_title: 4  SPRINT TOOLKIT OVERVIEW\n",
            "\u001b[35mSection_title: 4.1  Inference Pipeline\n",
            "\u001b[35mSection_title: 4.2  Example Usage\n",
            "\u001b[35mSection_title: 4.3  Pyserini Integration\n",
            "\u001b[35mSection_title: 4.4  BEIR/Dataset Integration\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL DESIGN 5.1 Datasets\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Evaluation Settings\n",
            "\u001b[35mSection_title: 6  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 7  DEMYSTIFYING SPARSE RETRIEVAL\n",
            "\u001b[32mconclusion: in this work, we presented sprint, a unified python toolkit focused on sparse neural retrieval. the toolkit extends the evaluation of several neural sparse retrievers on a common interface and easily allows practitioners to search their custom datasets using sparse retrieval. evaluation of a custom dataset using our toolkit is straightforward, as we effectively use an inference pipeline to unify evaluation across all different sparse retrievers.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Approach\n",
            "\u001b[35mSection_title: 3.1  Multi-Hop Question Generation Model\n",
            "\u001b[35mSection_title: 3.1.1  Document and Answer Encoder\n",
            "\u001b[35mSection_title: 3.1.2  Multi-task Learning\n",
            "\u001b[35mSection_title: 3.1.3  Question Decoder\n",
            "\u001b[35mSection_title: 3.1.4  MultiHop-Enhanced QG\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Quantitative Analysis\n",
            "\u001b[35mSection_title: 5.2  Qualitative Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Animation in Data Visualization\n",
            "\u001b[35mSection_title: 2.2  Animation Generation\n",
            "\u001b[35mSection_title: 2.3  Natural Language Interface for Data Visualization\n",
            "\u001b[35mSection_title: 3  SYSTEM OVERVIEW\n",
            "\u001b[35mSection_title: 3.1  Design Requirements\n",
            "\u001b[35mSection_title: 3.2  System Architecture and Running Pipeline\n",
            "\u001b[35mSection_title: 4  DATA QUERY ANALYZER\n",
            "\u001b[35mSection_title: 4.1  Question Decomposition Meaning Representation\n",
            "\u001b[35mSection_title: 4.2  Decomposition Network (D)\n",
            "\u001b[35mSection_title: 4.3  Calibration Network (C)\n",
            "\u001b[35mSection_title: 5.1  User Interface and Interactions\n",
            "\u001b[35mSection_title: 5.2  Action-Oriented Unit Visualization\n",
            "\u001b[35mSection_title: 5.3  Datamation Generation\n",
            "\u001b[35mSection_title: 6  EVALUATION\n",
            "\u001b[35mSection_title: 6.1  Quantitative Evaluation\n",
            "\u001b[35mSection_title: 6.2  User Study\n",
            "\u001b[35mSection_title: 6.3  Interview with Experts\n",
            "\u001b[32mconclusion: in this paper, we presented the authoring tool datamator, developed for creating datamations. to the best of our knowledge, it is the first tool that supports datamation design and generation. given a dataset and a question, datamator can automatically decompose the question into a sequence of data analysis operators and generate a datamation based on unit visualization. datamator also allows the user to modify and edit the generated results. our user studies showed that datamator is highly rated for generating datamations to explain data analysis processes. its editing function also showed to be effective in correcting the automatically generated results.\n",
            "\u001b[35mSection_title: 7  CONCLUSION, LIMITATIONS, AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  VISION, LANGUAGE AND MULTI-MODAL TASKS\n",
            "\u001b[35mSection_title: 3  UNIFIED-IO\n",
            "\u001b[35mSection_title: 3.1  UNIFIED TASK REPRESENTATIONS\n",
            "\u001b[35mSection_title: 3.2  UNIFIED ARCHITECTURE\n",
            "\u001b[35mSection_title: 3.3  TRAINING\n",
            "\u001b[35mSection_title: 3.4  IMPLEMENTATION DETAILS\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  RESULTS ON GRIT\n",
            "\u001b[35mSection_title: 4.2  EVALUATION ON SAME CONCEPT AND NEW CONCEPT\n",
            "\u001b[35mSection_title: 4.3  ABLATIONS ON TASK GROUP\n",
            "\u001b[35mSection_title: 4.4  RESULTS ON ADDITIONAL TASKS\n",
            "\u001b[35mSection_title: 4.5  PROMPT GENERALIZATION CASE STUDY\n",
            "\u001b[32mlimitation: for object detection, while unified-io generally produces accurate outputs (see appendix a.4), we find the recall is often poor in cluttered images. prior work (chen et al., 2022b) has shown this can be overcome with extensive data augmentation techniques, but these methods are not currently integrated into unified-io. our use of a pre-trained vq-gan greatly simplifies our training and is surprisingly effective for dense prediction tasks. however, it does mean unified-io has limited image generation capabilities (recent works (yu et al., 2022b) have shown this method can be greatly improved but was not available at the time of development). we also found in a small-scale study that our model does not always understand prompts not in the training data (see appendix 4.5).\n",
            "\u001b[35mSection_title: 4.6  LIMITATIONS\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: we have presented unified-io, a unified architecture that supports a large variety of computer vision and nlp tasks with diverse inputs and outputs, including images, continuous maps, binary masks, segmentation masks, text, bounding boxes, and keypoints. this unification is made possible by homogenizing each of these modalities into a sequence of discrete tokens. the 2.9b parameter unified-io xl model is jointly trained on 90+ datasets, is the first model to perform all 7 tasks on the grit benchmark and obtains impressive results across 16 other vision and nlp benchmarks, with no benchmark fine-tuning or task-specific modifications.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  VISION, LANGUAGE AND MULTI-MODAL TASKS\n",
            "\u001b[35mSection_title: 3  UNIFIED-IO\n",
            "\u001b[35mSection_title: 3.1  UNIFIED TASK REPRESENTATIONS\n",
            "\u001b[35mSection_title: 3.2  UNIFIED ARCHITECTURE\n",
            "\u001b[35mSection_title: 3.3  TRAINING\n",
            "\u001b[35mSection_title: 3.4  IMPLEMENTATION DETAILS\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  RESULTS ON GRIT\n",
            "\u001b[35mSection_title: 4.2  EVALUATION ON SAME CONCEPT AND NEW CONCEPT\n",
            "\u001b[35mSection_title: 4.3  ABLATIONS ON TASK GROUP\n",
            "\u001b[35mSection_title: 4.4  RESULTS ON ADDITIONAL TASKS\n",
            "\u001b[35mSection_title: 4.5  PROMPT GENERALIZATION CASE STUDY\n",
            "\u001b[32mlimitation: for object detection, while unified-io generally produces accurate outputs (see appendix a.4), we find the recall is often poor in cluttered images. prior work (chen et al., 2022b) has shown this can be overcome with extensive data augmentation techniques, but these methods are not currently integrated into unified-io. our use of a pre-trained vq-gan greatly simplifies our training and is surprisingly effective for dense prediction tasks. however, it does mean unified-io has limited image generation capabilities (recent works (yu et al., 2022b) have shown this method can be greatly improved but was not available at the time of development). we also found in a small-scale study that our model does not always understand prompts not in the training data (see appendix 4.5).\n",
            "\u001b[35mSection_title: 4.6  LIMITATIONS\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: we have presented unified-io, a unified architecture that supports a large variety of computer vision and nlp tasks with diverse inputs and outputs, including images, continuous maps, binary masks, segmentation masks, text, bounding boxes, and keypoints. this unification is made possible by homogenizing each of these modalities into a sequence of discrete tokens. the 2.9b parameter unified-io xl model is jointly trained on 90+ datasets, is the first model to perform all 7 tasks on the grit benchmark and obtains impressive results across 16 other vision and nlp benchmarks, with no benchmark fine-tuning or task-specific modifications.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Definition\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Passage Ranking Model\n",
            "\u001b[35mSection_title: 3.2  Cooperative Reasoner\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Settings\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: in this paper we propose the problem of recovering reasoning chains in multi-hop qa from weak supervision signals. our model adopts an cooperative game approach where a ranker and a reasoner cooperate to select the most confident chains. experiments on the hotpotqa and medhop benchmarks show the effectiveness of the proposed approach. \n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Description\n",
            "\u001b[35mSection_title: 3.1  Query Formation\n",
            "\u001b[35mSection_title: 3.2  Decomposing Queries to Aspects\n",
            "\u001b[35mSection_title: 3.3  Candidate Pooling\n",
            "\u001b[35mSection_title: 4  Anno-GPT Framework\n",
            "\u001b[35mSection_title: 4.1  Annotation Guidelines\n",
            "\u001b[35mSection_title: 4.2  Optimization of Annotation Pipeline\n",
            "\u001b[35mSection_title: 4.3  Annotation Evaluations\n",
            "\u001b[35mSection_title: 5  Retrieval Results\n",
            "\u001b[35mSection_title: 5.1  DORIS-MAE Benchmarking Results\n",
            "\u001b[35mSection_title: 5.2  Additional Experiments\n",
            "\u001b[35mSection_title: 5.3  Supervised Learning on DORIS-MAE\n",
            "\u001b[32mconclusion: this paper introduces a novel task, scientific document retrieval using multi-level aspect-based queries (doris-mae), aimed at modeling the process of information retrieval in the context of scientific research.we also present a dataset for doris-mae generated using the anno-gpt framework.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\u001b[35mSection_title: 4.  CONCUR:\n",
            "\u001b[35mSection_title: 3.  Key component identification:\n",
            "\u001b[35mSection_title: 1  Sentence-level Embedding Models\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  LLMS AS ZERO-SHOT CRS 2.1 Task Formation\n",
            "\u001b[35mSection_title: 2.2  Framework\n",
            "\u001b[35mSection_title: 3  DATASET\n",
            "\u001b[35mSection_title: 4  EVALUATION\n",
            "\u001b[35mSection_title: 4.1  Evaluation Setup\n",
            "\u001b[35mSection_title: 4.2  Repeated Items Can Be Shortcuts\n",
            "\u001b[35mSection_title: 4.3  LLMs Performance\n",
            "\u001b[35mSection_title: 5  DETAILED ANALYSIS\n",
            "\u001b[35mSection_title: 5.1  Knowledge in LLMs\n",
            "\u001b[35mSection_title: 5.2  Information from CRS Data\n",
            "\u001b[32mlimitation: finding 10 -llm recommendations suffer from popularity bias in crs. popularity bias refers to a phenomenon that popular items are recommended even more frequently than their popularity would warrant [8]. figure 8 shows the popularity bias in llm recommendations, though it may not be biased to the popular items in the target datasets. on redial, the most popular movies such as avengers: infinity war appear around 2% of the time over all ground-truth items; on reddit, the most popular movies such as everything everywhere all at once appears less than 0.3% of the time over ground-truth items. but for the generated recommendations from gpt-4 (other llms share a similar trend), the most popular items such as the shawshank redemption appear around 5% times on redial and around 1.5% times on reddit. compared to the target datasets, llms recommendations are more concentrated on popular items, which may cause further issues like the bias amplification loop [8]. moreover, the recommended popular items are similar across different datasets, which may reflect the item popularity in the pre-training corpus of llms.finding 11 -recommendation performance of llms is sensitive to geographical regions. despite the effectiveness in general, it is unclear whether llms can be good recommenders across various cultures and regions. specifically, pre-trained language models' strong open-domain ability can be attributed to pre-training from massive data [5]. but it also leads to llms' sensitivity to data distribution. to investigate llms recommendation abilities for various regions, we take test instances from the reddit dataset and obtain the production region of 7,476 movies from a publicly available movie dataset 13 by exact title matching, then report the recall@1 for the linked movies grouped by region. we only report regions with more than 300 data points available to ensure enough data to support the result. as shown in figure 9 the current best model, gpt-4's performance on recommendation is higher for movies produced in english-speaking regions. this could be due to bias in the training data -the left of figure 9 show item on reddit forums are dominated by movies from english-speaking regions. such a result highlights large language model's recommendation performance varies by region and culture and demonstrates the importance of cross-regional analysis and evaluation for language model-based conversational recommendation models.\n",
            "\u001b[35mSection_title: 5.3  Limitations of LLMs as Zero-shot CRS\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[32mconclusion: we investigate large language models (llms) as zero-shot conversational recommendation systems (crs). through our empirical investigation, we initially address a repetition shortcut in previous standard crs evaluations, which can potentially lead to unreliable conclusions regarding model design. subsequently, we demonstrate that llms as zero-shot crs surpass all fine-tuned existing crs models in our experiments. inspired by their effectiveness, we conduct a comprehensive analysis from both the model and data perspectives to gain insights into the working mechanisms of llms, the characteristics of typical crs tasks, and the limitations of using llms as crs directly. our experimental evaluations encompass two publicly available datasets, supplemented by our newly-created dataset on movie recommendations collected by scraping a popular discussion website. this dataset is the largest public crs dataset and ensures more diverse and realistic conversations for crs research. we also discuss the future directions based on our findings in this section.\n",
            "\u001b[35mSection_title: 7  CONCLUSION AND DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  RNA-Mediated Gene Duplication and Retroposons\n",
            "\u001b[35mSection_title: 1.1.  Retrogenes and Processed Pseudogenes.\n",
            "\u001b[35mSection_title: 1.2.  Retroposons.\n",
            "\u001b[35mSection_title: 1.3.  Evolutionary Relationships of Various LINEs.\n",
            "\u001b[35mSection_title: 1.4.  SINEs and LINEs.\n",
            "\u001b[35mSection_title: 1.5.  Mechanism of RNA-Mediated Gene Duplication in Mammals.\n",
            "\u001b[35mSection_title: 2.  Retroposition Burst in Ancestral Primates\n",
            "\u001b[35mSection_title: 3.  A Primate Retrogene That Was Created by a Novel Mechanism\n",
            "\u001b[35mSection_title: 3.1.  Gene Creation by the Coupling of Gene Duplication and Domain\n",
            "\u001b[35mSection_title: 3.2.  Evolutionary\n",
            "\u001b[35mSection_title: 4.  RNA-Mediated Gene Duplication in Land Plants\n",
            "\u001b[35mSection_title: 4.1.  L1-Clade LINEs Are Predominant in the Genomes of Flowering Plants.\n",
            "\u001b[35mSection_title: 4.2.  Plant L1-Clade LINEs Consist of 3 Deeply Branching Lineages That Have Descended from the Common Ancestor of Monocots and Eudicots.\n",
            "\u001b[35mSection_title: 4.3.  A Conserved 3 -End\n",
            "\u001b[35mSection_title: 4.4.  Origin of Stringent and Relaxed 3 -End Recognition of\n",
            "\u001b[35mSection_title: 5.  Concluding Remarks\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  ROUTING POLICY GUIDES EXPERTS MERGING\n",
            "\u001b[35mSection_title: 3.2  MERGING ENCOURAGES EXPERT DECOMPOSITION\n",
            "\u001b[35mSection_title: 4.2  COMPETITIVE PERFORMANCE AND SUPERIOR EFFICIENCY OF MC-SMOE\n",
            "\u001b[32mconclusion: sparse mixture-of-experts (smoe) is a promising framework to scale up the model capacity, which enjoys roughly unchanged training and inference flops at the cost of significantly increased memory overheads.the memory requirements and expert redundancy highly limit its practical usage.\n",
            "\u001b[35mSection_title: 5  CONCLUSIONS\n",
            "\u001b[35mSection_title: 6  REPRODUCIBILITY STATEMENT\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary Study of Synonymic Relationship\n",
            "\u001b[35mSection_title: 2.1  Literal Meaning Coverage for Chinese Idioms\n",
            "\u001b[35mSection_title: 2.2  Mitigating the Inconsistency with Synonymic Relationship\n",
            "\u001b[35mSection_title: 3  Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Formulation\n",
            "\u001b[35mSection_title: 3.2  Construction of Synonym Graph\n",
            "\u001b[35mSection_title: 3.3  Synonym Knowledge Enhanced Reader\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Compared Methods\n",
            "\u001b[35mSection_title: 4.3  Implemented Details\n",
            "\u001b[35mSection_title: 5  Results and Discussion\n",
            "\u001b[35mSection_title: 5.1  Experimental Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Study\n",
            "\u001b[35mSection_title: 5.3  Case Study\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we give a quantitative analysis to prove that the literal meanings of many chinese idioms are far from their semantics, and also verify that the synonymic relationship can mitigate this inconsistency, which is beneficial for chinese idiom reading comprehension. we propose the synonym knowledge enhanced reader to fully utilize the relationship. experimental results show that our model achieves state-of-the-art performance among different settings of chid, a large-scale chinese idiom reading comprehension dataset. similar to chinese idioms, the inconsistency exists in a number of other language elements, e.g., slangs in english, where the potential use of the synonymic relationship among them requires a further exploration.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Background\n",
            "\u001b[35mSection_title: 2.  Objectives\n",
            "\u001b[35mSection_title: 3.  Materials and Methods\n",
            "\u001b[35mSection_title: 3.1.  Isolation of Microorganisms\n",
            "\u001b[35mSection_title: 3.2.  Microscopic Identification of Fungi\n",
            "\u001b[35mSection_title: 3.3.  Detection of Lipase Activity by Tributyrin Agar Plate Assay\n",
            "\u001b[35mSection_title: 3.4.  DNA Extraction\n",
            "\u001b[35mSection_title: 3.5.  Polymerase Chain Reaction Amplification and Sequencing of Partial Sequence of Lipase Gene\n",
            "\u001b[35mSection_title: 3.6.  Optimization Experiments\n",
            "\u001b[35mSection_title: 3.7.  Enzyme Activity Assay\n",
            "\u001b[35mSection_title: 3.8.  Effects of pH on Lipase Activity\n",
            "\u001b[35mSection_title: 3.9.  Effect of Temperature on Lipase Activity\n",
            "\u001b[35mSection_title: 4.  Results\n",
            "\u001b[35mSection_title: 4.1.  Isolation and Identification of Lipolytic Fungi\n",
            "\u001b[35mSection_title: 4.2.  Effect of pH and Temperature on Lipase Production\n",
            "\u001b[35mSection_title: 5.  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Collection\n",
            "\u001b[35mSection_title: 2.1  Question Annotation\n",
            "\u001b[35mSection_title: 2.2  Dataset Analysis\n",
            "\u001b[35mSection_title: 3  Socratic Question Generation\n",
            "\u001b[35mSection_title: 4.1  Classification Results\n",
            "\u001b[35mSection_title: 4.2  Question Generation Results\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation Studies\n",
            "\u001b[32mconclusion: we created a novel dataset socratiq to support research on automatic socratic question generation. we applied latest research in prompt-based conditional text generation to fine-tune existing large language models from gpt, t5, and prophetnet to learn soqg. through our study and the release of this novel dataset, we take a first step towards enabling future research on models for soqg as well as impactful applications in areas such as counseling and education (inkster et al., 2018;fitzpatrick et al., 2017).\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\u001b[32mlimitation: we note the following limitations in our work that also comprise our future research directions. first, while a human socratic method practitioner will know what type of socratic question to ask based only on context, our prompt-based models assume the availability of question-type for generating a type-sensitive question. in fact, when only contexts were used for qg (gpt, t5, prophetnet baselines in section 3), the generated questions matched the desired question-type (those of the available reference questions) in only 37-40% of the cases. furthermore, the question-type identification of automated methods using context alone was very poor with overall accuracy comparable to that of random assignments (section 4.1).secondly, though we showcased the potential use of soqg in designing chatbots and dialog systems for applications such as counseling, we note that the current evaluation has only been at the singleturn level. we hope to extend socratiq to capture back and forth discussions on cmv to provide multiturn data and also deduce via forum votes and other indicators if the discussion indeed resulted in changed minds and enabled alternate perspectives. furthermore, considering the special purpose of socratic questions in shaping perspectives and enabling introspection and reflection, a comprehensive evaluation would require measuring these aspects over the multi-turn sessions.finally, our dataset was created by re-purposing the cmv subreddit data available in english, a high-resource language for which large-scale pretrained language models (plms) are readily available. obtaining high classification and generation performances via fine-tuning of plms will be a challenge that needs addressing in low-resource languages. \n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Evidence Extraction\n",
            "\u001b[35mSection_title: 2.2  Evidence Filter\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets and Implementation\n",
            "\u001b[35mSection_title: 3.2  Results and Analysis\n",
            "\u001b[32mconclusion: we propose evidence filter to alleviate the effect of unrelated sentences and enhance the saliency of evidences potentially without human efforts. results on openbookqa indicate the effectiveness of our method. our future work is to enhance the evidence filter by more complex components.\n",
            "\u001b[35mSection_title: 4  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Preliminaries: Dense Retrieval\n",
            "\u001b[35mSection_title: 4  The SPAR Model\n",
            "\u001b[35mSection_title: 4.1  Training the Lexical Model Λ\n",
            "\u001b[35mSection_title: 4.2  Building SPAR with Λ\n",
            "\u001b[35mSection_title: 4.3  Comparing SPAR with Hybrid Retrievers\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Open-Domain Question Answering\n",
            "\u001b[35mSection_title: 5.2  MS Marco Passage Retrieval\n",
            "\u001b[35mSection_title: 5.3  Out-of-Domain Generalization of SPAR\n",
            "\u001b[35mSection_title: 5.3.1  Zero-shot performance on BEIR\n",
            "\u001b[35mSection_title: 5.3.2  SPAR on EntityQuestions\n",
            "\u001b[35mSection_title: 5.4  Implementation Details\n",
            "\u001b[35mSection_title: 6.2  Token-shuffled queries\n",
            "\u001b[35mSection_title: 6.3  Hybrid SPAR + BM25 model\n",
            "\u001b[32mconclusion: in this paper, we propose spar, a salient-phrase aware dense retriever, which can augment any dense retriever with the lexical matching capacity and out-of-domain generalization from a sparse retriever. this is achieved by training a dense lexical model λ to imitate the behavior of the teacher sparse retriever, the feasibility of which remained unknown until this work. we show that spar outperforms previous state-of-the-art dense and sparse retrievers, matching or even exceeding more complex hybrid systems, on various in-domain and outof-domain evaluation datasets. for future work we plan to explore if a dense retriever can be trained to learn lexical matching directly without relying on a teacher model. this way, we can avoid imitating the errors of the sparse retriever, and devise new ways of training dense retrievers that can potentially surpass hybrid models. moreover, there are several intriguing findings in this work that may warrant further study, such as why spar's acc@k improves relatively to the hybrid model as k increases, and why joint training is less effective than post-hoc vector concatenation.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 3  THE DSPY PROGRAMMING MODEL\n",
            "\u001b[35mSection_title: 3.1  NATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING & FINETUNING\n",
            "\u001b[35mSection_title: 3.2  PARAMETERIZED & TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES\n",
            "\u001b[35mSection_title: 3.3  TELEPROMPTERS CAN AUTOMATE PROMPTING FOR ARBITRARY PIPELINES\n",
            "\u001b[35mSection_title: 4  THE DSPY COMPILER\n",
            "\u001b[35mSection_title: 5  GOALS OF EVALUATION\n",
            "\u001b[35mSection_title: 7  CASE STUDY: COMPLEX QUESTION ANSWERING\n",
            "\u001b[32mconclusion: this paper introduced dspy, a new programming model for designing ai systems using pipelines of pretrained lms and other tools.we presented three new concepts introduced in this abstraction (dspy signatures, modules, and teleprompters), and showed in two very different case studies that it supports rapid development of highly effective systems that use relatively small lms.we have maintained open-source versions of this framework for close to a year.in this period, we have seen and created a large number of programs that were compiled to high-quality systems by dspy, spanning tasks from information extraction to low-resource synthetic data generation.in the interest of space and to maintain reasonable scope in this paper, we leave reporting on such tasks under controlled experimental conditions to future work.while in-context learning has proved transformative over the past 2-3 years of lm research, we argue that the true expressive power in this emerging paradigm is in building sophisticated text transformation graphs in which composable modules and optimizers (teleprompters) come together to leverage lms in more systematic and reliable ways.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Distant Supervision (DS) for\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Query-Evidence Pairing via DS\n",
            "\u001b[35mSection_title: 3.3  Pre-training Data Generation\n",
            "\u001b[35mSection_title: 3.4  Data Statistics and Analysis\n",
            "\u001b[35mSection_title: 4  Pre-training 4.1 Encoder\n",
            "\u001b[35mSection_title: 4.2  Span Reasoning Objective\n",
            "\u001b[35mSection_title: 4.3  Final Objective\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Few-shot Single-hop Text QA\n",
            "\u001b[35mSection_title: 5.4  Multi-hop Text QA\n",
            "\u001b[35mSection_title: 5.5  Table QA\n",
            "\u001b[35mSection_title: 5.6  Hybrid QA\n",
            "\u001b[35mSection_title: 6  Ablation Study\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we propose reasonbert, a novel pre-training method to enhance the reasoning ability of language models. the resulting model obtains substantial improvements on multi-hop and hybrid qa tasks that require complex reasoning, and demonstrates superior few-shot performance. in the future, we plan to use our query-evidence pairs collected by distant supervision to improve the retrieval performance for open-domain qa, as well as empower reasonbert to handle more types of reasoning, like comparison and numeric reasoning, in natural language understanding. \n",
            "\u001b[35mSection_title: 8  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Task Definition & Data Collection\n",
            "\u001b[35mSection_title: 3.1  Answer Extraction\n",
            "\u001b[35mSection_title: 3.2  Filtering of Answer Entities\n",
            "\u001b[35mSection_title: 3.3  Data Characteristics\n",
            "\u001b[35mSection_title: 3.3.1  Qualitative study\n",
            "\u001b[35mSection_title: 4  The Cluster-Select-Rerank Model\n",
            "\u001b[35mSection_title: 4.1  Training Objective\n",
            "\u001b[35mSection_title: 4.2  Cluster: Representative Entity Document Creation\n",
            "\u001b[35mSection_title: 4.3  Select: Shortlisting Candidate Answers\n",
            "\u001b[35mSection_title: 4.4  Rerank: Answering over Selected Candidates\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Models for comparison\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Answering Characteristics\n",
            "\u001b[32mconclusion: in the spirit of defining a question answering challenge that is closer to a real-world qa setting, we introduce the novel task of identifying the correct entity answer to a given user question based on a collection of unstructured reviews describing entities. we harvest a dataset of over 48,000 qa pairs, which enables end to end training of models.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.2  Approaches for Open-domain QA\n",
            "\u001b[35mSection_title: 2.3  Knowledge in Retrieval-based QA Models\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Retriever-Reader-Reranker Framework\n",
            "\u001b[35mSection_title: 3.2  Knowledge-Aided Model\n",
            "\u001b[35mSection_title: 3.3  Retriever\n",
            "\u001b[35mSection_title: 3.4  Reader\n",
            "\u001b[35mSection_title: 3.5  Reranker\n",
            "\u001b[35mSection_title: 3.6  Inference and Optimization\n",
            "\u001b[35mSection_title: 4  EXPERIMENT 4.1 Datasets and Preprocessing\n",
            "\u001b[35mSection_title: 4.3  Preliminary Experiments\n",
            "\u001b[35mSection_title: 4.4  Overall Performance\n",
            "\u001b[35mSection_title: 4.5  Analysis on the Retriever\n",
            "\u001b[35mSection_title: 4.6  Analysis on the Reranker\n",
            "\u001b[32mconclusion: this paper investigates how the performance of open-domain question answering can be improved through enhancing document retrieval and answer reranking. the central idea is to consider both question-document and document-document relationships in the document retriever and the answer reranker. more specifically, with the aid of external knowledge resources, we first construct question-document graphs and document-document graphs using knowledge triples, and then encode such relational knowledge in the document retrieval and answer ranking components. we evaluated our model on several open-domain question answering datasets including squad-open, quasar-t and triviaqaunfiltered. we observed that our method can boost the overall performance of open-domain question answering consistently on these datasets. extensive experiments show that modeling the questiondocument and document-document relationships can contribute to the improvement consistently.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Results and Discussion\n",
            "\u001b[35mSection_title: 2.1.  Testing for Horizontally-Acquired Genes-Microarray Analysis\n",
            "\u001b[35mSection_title: 2.2.  HRMA\n",
            "\u001b[35mSection_title: 2.2.  HRMA\n",
            "\u001b[35mSection_title: 2.2.1.  Francisella spp.\n",
            "\u001b[35mSection_title: 2.2.2.  Yersinia spp.\n",
            "\u001b[35mSection_title: 2.2.2.  Yersinia spp.\n",
            "\u001b[35mSection_title: 2.2.3.  Bacillus spp.\n",
            "\u001b[35mSection_title: 2.2.3.  Bacillus spp.\n",
            "\u001b[35mSection_title: 3.  Materials and Methods\n",
            "\u001b[35mSection_title: 3.1.  Materials\n",
            "\u001b[35mSection_title: 3.3.  High Resolution Melt Analysis (HRMA)\n",
            "\u001b[32mconclusion: both pcr and microarrays are valuable tools for the tracking the genetic underpinnings of amr resistance. here, we used two complementary technologies-microarray analysis and hrma-to survey 127 select agents, exempt strains, and near-neighbor species for a broad variety of resistance mechanisms acquired through both horizontal transfer and gene mutations. to our knowledge, this is the largest survey of category a agents, exempt strains, and near-neighbor species for genes covering multiple mechanisms of amr.\n",
            "\u001b[35mSection_title: 4.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Few-shot Knowledge Probing\n",
            "\u001b[35mSection_title: 3  Datasets\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 5  Related Works\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Task Formulation\n",
            "\u001b[35mSection_title: 2.2  Datasets\n",
            "\u001b[35mSection_title: 2.3  Model\n",
            "\u001b[35mSection_title: 2.4  Experimental Details\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 4  Explaining the QA-Retrieval Quality Independence\n",
            "\u001b[35mSection_title: 4.1  QA Performance by Subsets\n",
            "\u001b[35mSection_title: 4.2  Contrastive Evaluation\n",
            "\u001b[35mSection_title: 4.3  Are the Questions in StrategyQA\n",
            "\u001b[35mSection_title: 4.4  Why is StrategyQA hard?\n",
            "\u001b[32mconclusion: in this work we investigate the impact of evidence quality on question answering on two open-domain qa datasets, strategyqa and hotpotqa. results show that (1) strategyqa is less sensitive to the evidence quality than hotpotqa and (2) boolean questions are less sensitive to the evidence quality than extractive questions. further study shows this is mainly because the model does not yield a sufficiently high score on those examples with perfect evidence retrieval. finally, we recommend that for the implicit decomposition open-domain question answering problems with boolean questions such as strategyqa, researchers start by improving qa performance given the gold paragraphs rather than improving evidence quality.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Language Representation Learning\n",
            "\u001b[35mSection_title: 2.2  Neural Contextual Encoders\n",
            "\u001b[35mSection_title: 2.2.1  Sequence Models\n",
            "\u001b[35mSection_title: 2.2.2  Non-Sequence Models\n",
            "\u001b[35mSection_title: 2.2.3  Analysis\n",
            "\u001b[35mSection_title: 2.3  Why Pre-training?\n",
            "\u001b[35mSection_title: 2.  Pre-training provides a better model initialization,\n",
            "\u001b[35mSection_title: 2.4  A Brief History of PTMs for NLP\n",
            "\u001b[35mSection_title: 2.4.1  First-Generation PTMs: Pre-trained Word Embeddings\n",
            "\u001b[35mSection_title: 2.4.2  Second-Generation PTMs: Pre-trained Contextual Encoders\n",
            "\u001b[35mSection_title: 3  Overview of PTMs\n",
            "\u001b[35mSection_title: 3.1  Pre-training Tasks\n",
            "\u001b[35mSection_title: 3.1.1  Language Modeling (LM)\n",
            "\u001b[35mSection_title: 3.1.2  Masked Language Modeling (MLM)\n",
            "\u001b[35mSection_title: 3.1.3  Permuted Language Modeling (PLM)\n",
            "\u001b[35mSection_title: 3.1.4  Denoising Autoencoder (DAE)\n",
            "\u001b[35mSection_title: 3.1.5  Contrastive Learning (CTL)\n",
            "\u001b[35mSection_title: 3.1.6  Others\n",
            "\u001b[35mSection_title: 3.2  Taxonomy of PTMs\n",
            "\u001b[35mSection_title: 4.  Extensions:\n",
            "\u001b[35mSection_title: 3.3  Model Analysis\n",
            "\u001b[35mSection_title: 3.3.1  Non-Contextual Embeddings\n",
            "\u001b[35mSection_title: 3.3.2  Contextual Embeddings\n",
            "\u001b[35mSection_title: 4  Extensions of PTMs\n",
            "\u001b[35mSection_title: 4.1  Knowledge-Enriched PTMs\n",
            "\u001b[35mSection_title: 4.2  Multilingual and Language-Specific PTMs\n",
            "\u001b[35mSection_title: 4.2.1  Multilingual PTMs\n",
            "\u001b[35mSection_title: 4.2.2  Language-Specific PTMs\n",
            "\u001b[35mSection_title: 4.3  Multi-Modal PTMs\n",
            "\u001b[35mSection_title: 4.3.1  Video-Text PTMs\n",
            "\u001b[35mSection_title: 4.3.2  Image-Text PTMs\n",
            "\u001b[35mSection_title: 4.3.3  Audio-Text PTMs\n",
            "\u001b[35mSection_title: 4.4  Domain-Specific and Task-Specific PTMs\n",
            "\u001b[35mSection_title: 4.5  Model Compression\n",
            "\u001b[35mSection_title: 4.5.1  Model Pruning\n",
            "\u001b[35mSection_title: 4.5.2  Quantization\n",
            "\u001b[35mSection_title: 4.5.3  Parameter Sharing\n",
            "\u001b[35mSection_title: 4.5.4  Knowledge Distillation\n",
            "\u001b[35mSection_title: 4.5.5  Module Replacing\n",
            "\u001b[35mSection_title: 4.5.6  Early Exit\n",
            "\u001b[35mSection_title: 5  Adapting PTMs to Downstream Tasks\n",
            "\u001b[35mSection_title: 5.1  Transfer Learning\n",
            "\u001b[35mSection_title: 5.2  How to Transfer?\n",
            "\u001b[35mSection_title: 5.2.2  Choosing appropriate layers\n",
            "\u001b[35mSection_title: 5.2.3  To tune or not to tune?\n",
            "\u001b[35mSection_title: 5.3  Fine-Tuning Strategies\n",
            "\u001b[35mSection_title: 5.3.1  Prompt-based Tuning\n",
            "\u001b[35mSection_title: 6  Resources of PTMs\n",
            "\u001b[35mSection_title: 7  Applications\n",
            "\u001b[35mSection_title: 7.1  General Evaluation Benchmark\n",
            "\u001b[35mSection_title: 7.2  Question Answering\n",
            "\u001b[35mSection_title: 7.3  Sentiment Analysis\n",
            "\u001b[35mSection_title: 7.4  Named Entity Recognition\n",
            "\u001b[35mSection_title: 7.5  Machine Translation\n",
            "\u001b[35mSection_title: 7.6  Summarization\n",
            "\u001b[35mSection_title: 7.7  Adversarial Attacks and Defenses\n",
            "\u001b[35mSection_title: 8  Future Directions\n",
            "\u001b[32mconclusion: in this survey, we conduct a comprehensive overview of ptms for nlp, including background knowledge, model architecture, pre-training tasks, various extensions, adaption approaches, related resources, and applications. based on current ptms, we propose a new taxonomy of ptms from four different perspectives. we also suggest several possible future research directions for ptms.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction and Related Work\n",
            "\u001b[35mSection_title: 2  Efficiency Metrics\n",
            "\u001b[35mSection_title: 3  Local-Attention Speech Model\n",
            "\u001b[35mSection_title: 4  Methods and Implementation\n",
            "\u001b[35mSection_title: 4.1  Sequence Length Ranges\n",
            "\u001b[35mSection_title: 4.2  Implementational Details\n",
            "\u001b[35mSection_title: 5  Profiling Results\n",
            "\u001b[35mSection_title: 5.1  Layerwise Profiling Results\n",
            "\u001b[35mSection_title: 5.2  Overall Profiling Results\n",
            "\u001b[32mconclusion: we present an empirical efficiency analysis of vanilla transformers and their self-attention-based efficient variants across modalities, metrics and input context sizes. we find substantial differences across modalities and metrics when analyzing the tipping point for efficient variants. finally, the layerwise analysis finds that self-attention is not the only bottleneck. we recommend that all efficient model papers should report such cross-modal, layerwise profiling results on multiple efficiency metrics covering a variety of use-cases to provide a full picture of the benefits of the model.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  BERT and Review-based Tasks\n",
            "\u001b[35mSection_title: 3.1  BERT\n",
            "\u001b[35mSection_title: 3.2  Review Reading Comprehension (RRC)\n",
            "\u001b[35mSection_title: 3.3  Aspect Extraction\n",
            "\u001b[35mSection_title: 3.4  Aspect Sentiment Classification\n",
            "\u001b[35mSection_title: 4  Post-training\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  End Task Datasets\n",
            "\u001b[35mSection_title: 5.2  Post-training datasets\n",
            "\u001b[35mSection_title: 5.3  Hyper-parameters\n",
            "\u001b[35mSection_title: 5.4  Compared Methods\n",
            "\u001b[35mSection_title: 5.5  Evaluation Metrics and Model Selection\n",
            "\u001b[35mSection_title: 5.6  Result Analysis\n",
            "\u001b[32mconclusion: we proposed a new task called review reading comprehension (rrc) and investigated the possibility of turning reviews as a valuable resource for answering user questions. we adopted bert as our base model and proposed a joint post-training approach to enhancing both the domain and task knowledge. we further explored the use of this approach in two other review-based tasks: aspect extraction and aspect sentiment classification. experimental results show that the post-training approach before fine-tuning is effective.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Multi-hop Explanation Regeneration\n",
            "\u001b[35mSection_title: 1.  A facts bank of individual commonsense and scientific\n",
            "\u001b[35mSection_title: 3  Hybrid Autoregressive Inference\n",
            "\u001b[35mSection_title: 3.1  Explanatory Power\n",
            "\u001b[35mSection_title: 3.2  Dense Bi-encoder\n",
            "\u001b[35mSection_title: 3.3  Multi-hop Inference\n",
            "\u001b[35mSection_title: 4  Empirical Evaluation\n",
            "\u001b[35mSection_title: 4.1  Explanation Regeneration\n",
            "\u001b[35mSection_title: 4.2  Inference Time\n",
            "\u001b[35mSection_title: 4.3  Ablation Studies\n",
            "\u001b[35mSection_title: 4.4  Semantic Drift\n",
            "\u001b[35mSection_title: 4.5  Multi-hop Question Answering\n",
            "\u001b[35mSection_title: 4.6  Scalability\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: this work presented scar, a hybrid autoregressive architecture for scalable explanation regeneration. an extensive evaluation demonstrated that scar achieves performance comparable with that of state-of-the-art crossencoders while being ≈ 50 times faster and intrinsically scalable, and confirmed the impact of the hybridisation on semantic drift and question answering. this work demonstrated the effectiveness of hybrid architectures for explainable inference at scale, opening the way for future research at the intersection of latent and explicit models. as a future work, we plan to investigate the integration of relevance and explanatory power in an end-to-end differentiable architecture, and explore the applicability of the hybrid framework on additional natural language and scientific reasoning tasks, with a focus on real-world scientific inference problems.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Overview\n",
            "\u001b[35mSection_title: 2.1  Task Variations and Existing Datasets\n",
            "\u001b[35mSection_title: 2.2  Explanation Issues\n",
            "\u001b[35mSection_title: 3  Reading Comprehension from\n",
            "\u001b[35mSection_title: 3.1  Computational Model in Psychology\n",
            "\u001b[35mSection_title: 3.2  Skill Hierarchy for MRC\n",
            "\u001b[35mSection_title: 4  MRC on Psychometrics\n",
            "\u001b[35mSection_title: 4.1  Construct Validity in Psychometrics\n",
            "\u001b[35mSection_title: 4.2  Construct Validity in MRC\n",
            "\u001b[35mSection_title: 2.  Substantive\n",
            "\u001b[35mSection_title: 3.  Structural\n",
            "\u001b[35mSection_title: 5.  External\n",
            "\u001b[35mSection_title: 6.  Consequential\n",
            "\u001b[35mSection_title: 5  Future Directions\n",
            "\u001b[35mSection_title: 5.1  What side: Evaluating Situation Model\n",
            "\u001b[35mSection_title: 5.1.1  Context-dependent Situations\n",
            "\u001b[35mSection_title: 5.1.2  Grounding to Other Media\n",
            "\u001b[35mSection_title: 5.2  How side: Assuring Substantive Validity\n",
            "\u001b[35mSection_title: 5.2.1  Collecting High-quality Questions\n",
            "\u001b[35mSection_title: 5.2.2  Designing White-box Task Formulation\n",
            "\u001b[32mconclusion: in this position paper, we overviewed issues and future directions of mrc. we focused specifically on the situation model in psychology for what we should ask of reading comprehension and the substantive validity in psychometrics for how we should correctly evaluate it. we conclude that future datasets should (i) evaluate the capability of the situation model for understanding contextdependent situations and for grounding to nontextual information and (ii) ensure the substantive validity by improving the question quality and designing a white-box task formulation. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Framework for MRC Gold Standard Analysis\n",
            "\u001b[35mSection_title: 2.1.  Problem definition\n",
            "\u001b[35mSection_title: 2.2.  Dimensions of Interest\n",
            "\u001b[35mSection_title: 3.  Application of the Framework\n",
            "\u001b[35mSection_title: 3.1.  Candidate Datasets\n",
            "\u001b[35mSection_title: 3.2.  Annotation Task\n",
            "\u001b[35mSection_title: 3.3.  Qualitative Analysis\n",
            "\u001b[35mSection_title: 3.4.  Quantitative Results\n",
            "\u001b[35mSection_title: 4.  Related Work\n",
            "\u001b[32mconclusion: in this paper, we introduce a novel framework to characterise machine reading comprehension gold standards. this framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach. furthermore we applied the framework to analyse popular state-of-the-art gold standards for machine reading comprehension: we reveal issues with their factual correctness, show the presence of lexical cues and we observe that semantics-altering grammatical modifiers are missing in all of the investigated gold standards. studying how to introduce those modifiers into gold standards and observing whether state-of-the-art mrc models are capable of performing reading comprehension on text containing them, is a future research goal. a future line of research is to extend the framework to be able to identify the different types of exploitable cues such as question or entity typing and concrete overlap patterns. this will allow the framework to serve as an interpretable estimate of reading comprehension complexity of gold standards. finally, investigating gold standards under this framework where mrc models outperform the human baseline (e.g. squad) will contribute to a deeper understanding of the seemingly superb performance of deep learning approaches on them.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Definition\n",
            "\u001b[35mSection_title: 3.1  Hybrid Chain Notation\n",
            "\u001b[35mSection_title: 3.2  Hybrid Chain Extraction\n",
            "\u001b[35mSection_title: 3.2.1  Model Architecture\n",
            "\u001b[35mSection_title: 3.2.2  Model Training\n",
            "\u001b[35mSection_title: 3.2.3  Model Inference\n",
            "\u001b[35mSection_title: 3.3  Chain-centric Pre-training\n",
            "\u001b[35mSection_title: 3.3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.3.2  Corpus Construction\n",
            "\u001b[35mSection_title: 3.4  Hybrid Chain for QA\n",
            "\u001b[35mSection_title: 3.5  Knowledge Retrieval\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset and Evaluation\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Model Comparison\n",
            "\u001b[35mSection_title: 4.4  Evaluation of Chain-centric Reasoning\n",
            "\u001b[35mSection_title: 4.5  Evaluation of Chain-centric Pre-training\n",
            "\u001b[35mSection_title: 4.6  Qualitative Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present a chain-centric reasoning and pre-training (carp) framework for table-andtext question answering. when answering the questions given retrieved table and passages, carp first extracts explicit hybrid chain to reveal the intermediate reasoning process leading to the answer across table and text. the hybrid chain provides a guidance for qa, and explanation of the intermediate reasoning process. to enhance the extraction model with better reasoning ability and alleviate data sparsity problem, we design a novel chaincentric pre-training method. this method synthesizes the reasoning corpus in a larger scale and of higher reasoning complexity, which is achieved by automatically synthesizing heterogeneous reasoning paths from tables and passages in wikipedia and reversely generating multi-hop questions. we find that the pre-training task boosts performance on the hybrid chain extraction model, especially for questions requiring more complex reasoning, which leads to significant improvement on the performance of the qa model. the hybrid chain also provides better interpretability of the reasoning process. our system achieves the state-of-the-art result on a \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  The Role of Contextualized Language Model\n",
            "\u001b[35mSection_title: 2.1  From Language Model to Language Representation\n",
            "\u001b[35mSection_title: 2.2  CLM as Phenomenon 2.2.1 Revisiting the Definition.\n",
            "\u001b[35mSection_title: 2.2.2  Evolution of CLM Training Objectives.\n",
            "\u001b[35mSection_title: 2.2.3  Architectures of CLMs.\n",
            "\u001b[35mSection_title: 2.2.4  Derivative of CLMs.\n",
            "\u001b[35mSection_title: 2.3  Correlations Between MRC and CLM\n",
            "\u001b[35mSection_title: 3.  MRC as Phenomenon\n",
            "\u001b[35mSection_title: 3.1  Classic NLP Meets MRC\n",
            "\u001b[35mSection_title: 3.2  MRC Goes Beyond QA\n",
            "\u001b[35mSection_title: 3.3  Task Formulation\n",
            "\u001b[35mSection_title: 3.4  Typical Datasets\n",
            "\u001b[35mSection_title: 3.5  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.6  Towards Prosperous MRC\n",
            "\u001b[35mSection_title: 3.7  Related Surveys\n",
            "\u001b[35mSection_title: 4.  Technical Methods\n",
            "\u001b[35mSection_title: 4.1  Two-stage Solving Architecture\n",
            "\u001b[35mSection_title: 4.2  Typical MRC Architecture\n",
            "\u001b[35mSection_title: 4.2.1  Traditional RNN-based BiDAF.\n",
            "\u001b[35mSection_title: 4.2.2  Pre-trained CLMs for Fine-tuning.\n",
            "\u001b[35mSection_title: 4.3  Encoder\n",
            "\u001b[35mSection_title: 4.3.1  Multiple Granularity Features.\n",
            "\u001b[35mSection_title: 4.3.2  Structured Knowledge Injection.\n",
            "\u001b[35mSection_title: 4.3.3  Contextualized Sentence Representation.\n",
            "\u001b[35mSection_title: 4.4  Decoder\n",
            "\u001b[35mSection_title: 4.4.2  Answer Pointer.\n",
            "\u001b[35mSection_title: 4.4.3  Answer Verifier.\n",
            "\u001b[35mSection_title: 4.4.4  Answer Type Predictor.\n",
            "\u001b[35mSection_title: 4.5  Training Objectives\n",
            "\u001b[35mSection_title: 5.  Technical Highlights\n",
            "\u001b[35mSection_title: 5.1  Reading Strategy\n",
            "\u001b[35mSection_title: 5.2  CLMs Become Dominant\n",
            "\u001b[35mSection_title: 5.3  Data Augmentation\n",
            "\u001b[35mSection_title: 5.4  Decline of Matching Attention\n",
            "\u001b[35mSection_title: 5.5  Tactic Optimization\n",
            "\u001b[35mSection_title: 5.6  Empirical Analysis of Decoders\n",
            "\u001b[35mSection_title: 5.6.2  Verification.\n",
            "\u001b[35mSection_title: 5.6.3  Interaction.\n",
            "\u001b[35mSection_title: 5.6.4  Answer Dependency.\n",
            "\u001b[35mSection_title: 5.6.5  Findings.\n",
            "\u001b[35mSection_title: 6.2  Decomposition of Prerequisite Skills\n",
            "\u001b[35mSection_title: 6.3  Complex Reasoning\n",
            "\u001b[35mSection_title: 6.4  Large-scale Comprehension\n",
            "\u001b[35mSection_title: 6.5  Low-resource MRC\n",
            "\u001b[35mSection_title: 6.6  Multimodal Semantic Grounding\n",
            "\u001b[35mSection_title: 6.7  Deeper But Efficient Network\n",
            "\u001b[32mconclusion: this work comprehensively reviews the studies of mrc in the scopes of background, definition, development, influence, datasets, technical and benchmark highlights, trends, and opportunities. we first briefly introduced the history of mrc and the background of contextualized language models. then, we discussed the role of contextualized language models and the influence of mrc to the nlp community. the previous technical advances were summarized in the framework of encoder to decoder. after going through the mechanisms of mrc systems, we showed the highlights in different stages of mrc studies. finally, we summarized the trends and opportunities. the basic views we have arrived at are that 1) mrc boosts the progress from language processing to understanding; 2) the rapid improvement of mrc systems greatly benefits from the progress of clms; 3) the theme of mrc is gradually moving from shallow text matching to cognitive reasoning.\n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 1.2  Related concepts of fake news\n",
            "\u001b[35mSection_title: 1.3  An overview of this survey\n",
            "\u001b[35mSection_title: 2.1  Survey of researches about fake news\n",
            "\u001b[35mSection_title: 2.2  Fact-checking sites\n",
            "\u001b[35mSection_title: 3  DATASET OF FAKE NEWS DETECTION\n",
            "\u001b[35mSection_title: 3.1  News Articles\n",
            "\u001b[35mSection_title: 3.2  Social Media Posts\n",
            "\u001b[35mSection_title: 4  DATASET OF FACT VERIFICATION\n",
            "\u001b[35mSection_title: 5  OTHER DATASETS RELATED TO FAKE NEWS\n",
            "\u001b[35mSection_title: 5.1  Satire detection\n",
            "\u001b[35mSection_title: 5.2  News (Media) Credibility\n",
            "\u001b[35mSection_title: 5.3  Analysis of fake news\n",
            "\u001b[35mSection_title: 5.5  Guardian\n",
            "\u001b[35mSection_title: 5.6  Check-worthy claims\n",
            "\u001b[35mSection_title: 5.8  Other datasets\n",
            "\u001b[35mSection_title: 6  DISCUSSION AND FUTURE WORK\n",
            "\u001b[35mSection_title: 6.1  Topic domains\n",
            "\u001b[35mSection_title: 6.2  Languages\n",
            "\u001b[35mSection_title: 6.3  Labels\n",
            "\u001b[35mSection_title: 6.4  Functions and Types of platform\n",
            "\u001b[35mSection_title: 6.5  Intention of fake news\n",
            "\u001b[35mSection_title: 6.6  Benchmarks of fake news detection\n",
            "\u001b[35mSection_title: 6.7  Bias\n",
            "\u001b[35mSection_title: 6.8  Velocity\n",
            "\u001b[32mconclusion: our survey provides extensive reviews of fake news datasets by: (1) summarizing the definition of fake news, relevant concepts related to fake news, and the areas covered by existing survey papers on fake news research as a basis for discussion;\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.2  Graph Construction\n",
            "\u001b[35mSection_title: 3.3  Reasoning with Gated-RGCN\n",
            "\u001b[35mSection_title: 3.4  Output Layer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset and Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablations Studies\n",
            "\u001b[35mSection_title: 4.4  Analysis\n",
            "\u001b[32mconclusion: in this paper, we propose a novel approach for multi-hop reading comprehension across documents. our approach extends the entity graph by introducing reasoning entities, which can form the reasoning path from question to candidates. in addition, our approach incorporates the question in the multi-hop reasoning through a new gate mechanism to regulate how much useful information propagating from neighbors to the node. experiments show that our approach achieves state-of-the-art accuracy both for single and ensemble models.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Generative Question Answering\n",
            "\u001b[35mSection_title: 2.1  Generative QA Models\n",
            "\u001b[35mSection_title: 2.2  Multi-hop Questions and Decompositions\n",
            "\u001b[35mSection_title: 2.3  Answer Generation and Evaluation\n",
            "\u001b[35mSection_title: 3.1  Experimental Settings\n",
            "\u001b[35mSection_title: 3.2  Correlation of Correctness\n",
            "\u001b[35mSection_title: 3.3  Prediction Consistency\n",
            "\u001b[35mSection_title: 4  Improving Zero-shot Multi-hop Reasoning Capability\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Experimental Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we examined the multi-hop reasoning capabilities of generative qa models, finding that overall models take shortcuts when answering multi-hop questions, not demonstrating convincing multi-hop reasoning capability. when trained only on single-hop questions, models generalize poorly to multi-hop questions, while approximation using the concatenation of single-hop questions and sparql queries improves the multi-hop performance significantly. further directions include better approximations of multi-hop questions and advanced modeling techniques that encourage compositional ability.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Serving Users: The Cranfield Paradigm\n",
            "\u001b[35mSection_title: 3  Probing and Pushing Answerers: The Manchester Paradigm\n",
            "\u001b[35mSection_title: 3.1  Why the Name Manchester?\n",
            "\u001b[35mSection_title: 3.2  Examples\n",
            "\u001b[35mSection_title: 4  What Cranfield and Manchester Share\n",
            "\u001b[35mSection_title: 5  Ignore the Distinction at your Peril\n",
            "\u001b[35mSection_title: 6  Call to Action\n",
            "\u001b[32mconclusion: we identify two core motivations for qa research over the past twenty years. we link one to the usercentered goals of the cranfield paradigm and propose the manchester paradigm to describe research working towards building human-like, intelligent qa systems. in at least the short-term, this distinction is important as it illuminates the goals of industry and academic stakeholders; ultimately, this makes it easier to ensure that both research agendas are valued. in the long term, we suspect that the best qa agents will benefit from the insights of user-oriented tasks and the longer-range efforts towards natural language understanding (bender and koller, 2020; linzen, 2020). \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Adversarial Data Collection\n",
            "\u001b[35mSection_title: 2.2  Synthetic Question Generation\n",
            "\u001b[35mSection_title: 2.3  Self-training\n",
            "\u001b[35mSection_title: 2.4  Human Evaluation\n",
            "\u001b[35mSection_title: 3  Synthetic Data Generation\n",
            "\u001b[35mSection_title: 3.1  Data Generation Pipeline\n",
            "\u001b[35mSection_title: 3.1.1  Passage Selection\n",
            "\u001b[35mSection_title: 3.1.2  Answer Candidate Selection\n",
            "\u001b[35mSection_title: 3.1.3  Question Generation\n",
            "\u001b[35mSection_title: 3.1.4  Filtering and Re-labelling\n",
            "\u001b[35mSection_title: 3.2  End-to-end Synthetic Data Generation\n",
            "\u001b[35mSection_title: 3.3  Fine-tuning Setup\n",
            "\u001b[35mSection_title: 4  Measuring Model Robustness\n",
            "\u001b[35mSection_title: 4.1  Adversarially-collected Data\n",
            "\u001b[35mSection_title: 4.2  Comprehension Skills\n",
            "\u001b[35mSection_title: 4.3  Domain Generalisation\n",
            "\u001b[35mSection_title: 4.4  Adversarial Human Evaluation\n",
            "\u001b[32mconclusion: in this work, we develop a synthetic adversarial data generation pipeline for qa, identify the best components, and evaluate on a variety of robustness measures. we propose novel approaches for answer candidate selection, adversarial question generation, and synthetic example filtering and relabelling, demonstrating improvements over existing methods. furthermore, we evaluate the final models on three existing robustness measures and achieve state-of-the-art results on adversarialqa, improved learnability of various comprehension skills for checklist, and improved domain generalisation for the suite of mrqa tasks.\n",
            "\u001b[35mSection_title: 5  Discussion and Conclusion\n",
            "\u001b[35mSection_title: 6  Ethical Considerations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Causal Question Typologies\n",
            "\u001b[35mSection_title: 2.2  Causal Question Answering\n",
            "\u001b[35mSection_title: 2.3  Question Answering Datasets\n",
            "\u001b[35mSection_title: 2.4  Question Answering Systems\n",
            "\u001b[35mSection_title: 3  A Typology of Causal Questions\n",
            "\u001b[35mSection_title: 3.1  The Causal Question Category\n",
            "\u001b[35mSection_title: 3.2  The Semantic Dimension\n",
            "\u001b[35mSection_title: 3.3  The Pragmatic Dimension\n",
            "\u001b[35mSection_title: 3.4  Causal Questions in Web Search\n",
            "\u001b[35mSection_title: 4  The Webis-CausalQA-22 Corpus\n",
            "\u001b[35mSection_title: 4.1  Corpus Construction\n",
            "\u001b[35mSection_title: 4.2  Corpus Analysis\n",
            "\u001b[35mSection_title: 5  Evaluation\n",
            "\u001b[32mconclusion: we constructed webis-causalqa-22, the first large benchmark dataset of 1.1 million causal questionanswer pairs, which serves to advance research in causal question answering. to ensure diversity of questions, we extracted them using seven hand-crafted high-precision lexical rules to capture as many subtypes of causal questions as possible. these rules were derived from a new typology of causal questions, which in turn is based on relevant related work on question typologies. a manual analysis of a sample of questions was used to characterize causal questions in terms of two dimensions: (1) their semantic properties, i.e., according to which element of the causal structure the question is asked (antecedent, consequent, or the causal chain) and (2) their pragmatic interpretation, i.e., the underlying intention or assumed information need of the questioner (e.g., prevention of medical problems). furthermore, a subsequent analysis of the causal questions contained in a search engine log showed that a significant proportion of 5% of question queries are causal. finally, we evaluated the state-of-the-art model unifiedqa on our corpus as an initial baseline for causal question answering.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Open Knowledge Graphs\n",
            "\u001b[35mSection_title: 3  Open Link Prediction\n",
            "\u001b[35mSection_title: 3.1  Evaluation protocol\n",
            "\u001b[35mSection_title: 4  Creating the Open Link Prediction\n",
            "\u001b[35mSection_title: 4.1  Source Dataset\n",
            "\u001b[35mSection_title: 4.2  Evaluation Data\n",
            "\u001b[35mSection_title: 4.3  Training Data\n",
            "\u001b[35mSection_title: 5  Open Knowledge Graph Embeddings\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Models and Training\n",
            "\u001b[35mSection_title: 6.2  Results\n",
            "\u001b[32mconclusion: we proposed the olp task and a method to create an olp benchmark. we created the large olp benchmark olpbench, which will be made publicly available 4 . we investigated the effect of leakage of evaluation facts, non-relational information, and entity-knowledge during model selection using a prototypical open link prediction model. our results indicate that most predicted true facts are genuinely new. jointly in the context of lp.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Multi-stage retrieval pipelines\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Model structure\n",
            "\u001b[35mSection_title: 3.2  Training data generation\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  MS MARCO Passage Ranking\n",
            "\u001b[35mSection_title: 4.2  Zero-shot Retrieval\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 5  Results and Discussion\n",
            "\u001b[35mSection_title: 5.1  Results on MS MARCO\n",
            "\u001b[35mSection_title: 5.2  Results on BEIR\n",
            "\u001b[35mSection_title: 5.3  Ablation\n",
            "\u001b[35mSection_title: 5.4  Discussion\n",
            "\u001b[32mconclusion: we proposed a generic training framework for rerankers based on a hybrid retriever. while the hybrid retriever is composed of term-based and neural models, the reranker is a neural cross-attention model which learns from negatives examples generated by the hybrid retriever. the proposed approach is robust and outperforms several strong baselines on ms marco passage ranking task and beir benchmark dataset, which demonstrates that it is practical and generalized. we observe that a model trained with robust training instances (in this case, from the hybrid retriever) produces a reranker that outperforms matched-training rerankers for termbased or neural retrievers.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The Transformer with Synchronization (TRANSYNC ) Framework\n",
            "\u001b[35mSection_title: 3  Evaluating Tasks\n",
            "\u001b[35mSection_title: 3.1  NarrativeQA\n",
            "\u001b[35mSection_title: 3.2  Wild Multi-hop Reasoning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[32mconclusion: in this work, we propose transync framework with flexible synchronization mechanisms for encoding long sequences. we demonstrate the feasibility of our method in reasoning tasks with long context, and also show its high adaptability to different scenarios. we consider our work to be valuable as an easy solution to address the long context issue in qa, and to be potentially applicable to other long sequence modeling tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Data Collection\n",
            "\u001b[35mSection_title: 2.1.  Wikipedia Graph\n",
            "\u001b[35mSection_title: 2.2.  Feasible Titles List\n",
            "\u001b[35mSection_title: 2.3.  Paragraph Pairs Selection\n",
            "\u001b[35mSection_title: 2.4.  Annotation by Crowd Workers\n",
            "\u001b[35mSection_title: 2.5.  Processing and Normalizing\n",
            "\u001b[35mSection_title: 3.  Data Analysis\n",
            "\u001b[35mSection_title: 3.1.  Question Analysis\n",
            "\u001b[35mSection_title: 3.2.  Answer Analysis\n",
            "\u001b[35mSection_title: 3.3.  Multi-hop Reasoning Type Analysis\n",
            "\u001b[35mSection_title: 4.2.  Benchmark settings\n",
            "\u001b[35mSection_title: 5.  Experiments\n",
            "\u001b[35mSection_title: 5.1.  Experimental Settings\n",
            "\u001b[35mSection_title: 5.2.  Human Performance\n",
            "\u001b[35mSection_title: 5.3.  Results\n",
            "\u001b[32mconclusion: in this work, we propose vimqa, a multi-hop vietnamese qa dataset. it is highly necessary and important to facilitate the development of vietnamese qa models that can perform advanced reasoning and provide explainable answers with supporting facts. then, we also propose a pipeline for collecting multi-hop qa examples that can be generalized for all languages. we also prove the efficiency of our pipeline via the detailed analysis in our vimqa dataset. the experimental results indicate that vimqa is challenging for competitive approaches in both single and multiple hop qa. it reveals that our vimqa dataset is a good resource for vietnamese and cross-lingual qa models, especially in vietnamese multi-hop qa tasks for reasoning and explaining the comprehension and coherence of text understanding.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Data Source\n",
            "\u001b[35mSection_title: 3.2  Question Generation using ChatGPT\n",
            "\u001b[35mSection_title: 3.3  Evaluation of Generated Question Complexity\n",
            "\u001b[35mSection_title: 4.1  Experiment Setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: with the emergence of llms like chatgpt and open-source successful llms, it is extremely important to understand the capabilities and limitations of different llms.in order to test deeper reasoning abilities of llms by referring to longer contexts, we evaluate answers generated by llms on questions generated by chatgpt on summaries of long documents.results show that our proposed method of question generation poses a challenging setup for llms and shed light on performance gaps between massive llms and open-source llms.we hope our analysis motivates future research directions such as leveraging longer contexts in a constrained sequence length setting and developing better long-form text generation for smaller llms.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Datasets\n",
            "\u001b[35mSection_title: 3  Probing Multi-hop Datasets\n",
            "\u001b[35mSection_title: 3.1  Sentence-Factored Model Test\n",
            "\u001b[35mSection_title: 3.2  No Context Baseline\n",
            "\u001b[35mSection_title: 4  Span-based vs. Multiple-choice\n",
            "\u001b[35mSection_title: 4.1  Systems to Compare\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: there exist several other multi-hop reasoning datasets including worldtree , openbookqa (mihaylov et al., 2018), and mul-tirc (khashabi et al., 2018). these datasets are more complex to analyze since the answers may not appear directly in the passage and may simply be entailed by passage content. we leave a detailed investigation of these for future work.\n",
            "\u001b[35mSection_title: 5  Discussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHOD\n",
            "\u001b[35mSection_title: 3.1  UNSUPERVISED TRAINING ON UNALIGNED DOCUMENTS\n",
            "\u001b[35mSection_title: 3.1.1  CONTRASTIVE LEARNING\n",
            "\u001b[35mSection_title: 3.1.2  BUILDING POSITIVE PAIRS FROM A SINGLE DOCUMENT\n",
            "\u001b[35mSection_title: 3.1.3  BUILDING LARGE SET OF NEGATIVE PAIRS\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  BEIR BENCHMARK\n",
            "\u001b[35mSection_title: 4.2  BASELINES\n",
            "\u001b[35mSection_title: 4.3  RESULTS\n",
            "\u001b[35mSection_title: 5  ABLATION STUDIES\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  OUR APPROACH\n",
            "\u001b[35mSection_title: 3.1  Retrieval task description\n",
            "\u001b[35mSection_title: 3.2  Overview of the approach\n",
            "\u001b[35mSection_title: 3.3  N-gram-based identifiers\n",
            "\u001b[35mSection_title: 3.4  Prompt engineering\n",
            "\u001b[35mSection_title: 3.5  Training and inference\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Evaluation metrics\n",
            "\u001b[35mSection_title: 4.4  Implementation details\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 5.1  Evaluation on in-domain and out-of-domain datasets\n",
            "\u001b[35mSection_title: 5.1.2  Out-of-domain performance.\n",
            "\u001b[35mSection_title: 5.2  Adaptability to unseen tasks\n",
            "\u001b[35mSection_title: 5.3  Analysis of n-gram-based identifiers\n",
            "\u001b[35mSection_title: 5.4  Downstream performance\n",
            "\u001b[35mSection_title: 5.5  Memory and inference efficiency\n",
            "\u001b[32mconclusion: we have proposed ugr, a novel unified generative retriever, which can robustly serve different retrieval tasks for knowledge-intensive language tasks. to unify retrieval tasks, we formulated the retrieval problem as a conditional generation problem and introduced an n-gram-based identifier for relevant contexts at different levels of granularity. to learn different retrieval tasks with a single model, we mapped the descriptions of tasks to a few prompt tokens for keeping task specifications. empirical results on the kilt benchmark demonstrated the superiority of the proposed method. efficiently integrating knowledge from different retrieval tasks in ugr has the potential to save significant time and computational resources in both academic and industrial environments. however, ugr needs a complex scoring function to solve the identifier repetition problem; we encourage future work that explores other effective and efficient semantic identifiers for generative retrieval. beyond kilt, training a more general unified generative retrieval model to serve different retrieval applications under multiple corpora and modalities seems a promising future direction.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Relation Module\n",
            "\u001b[35mSection_title: 3.1  Augmenting Inputs\n",
            "\u001b[35mSection_title: 3.2  Object Extractor\n",
            "\u001b[35mSection_title: 3.2.1  Object Extraction Regularization\n",
            "\u001b[35mSection_title: 3.3  Relation Networks\n",
            "\u001b[35mSection_title: 4  Question Answering Baselines\n",
            "\u001b[35mSection_title: 4.1  BiDAF\n",
            "\u001b[35mSection_title: 4.2  BERT\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Ablation Study\n",
            "\u001b[35mSection_title: 7  Analysis\n",
            "\u001b[32mconclusion: in this work we propose a new relation module that can be applied on any mrc reader and help increase the prediction accuracy on non-answerable questions. we extract high level semantics from multi-head self-attentive pooling. the semantic object pairs are fed into the relation network which makes a guided decision as to whether a question is answerable. in addition we augment the context vector with plausible answers, allowing us to extract objects focused on the proposed answer span, and differentiate from other objects that are not as relevant in the context. our results on the squad 2.0 dataset using the relation module on both bidaf and bert models show improvements from the relation module. these results prove the effectiveness of our relation module.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Contrastive Learning with Anchor-Document Pairs\n",
            "\u001b[35mSection_title: 3.2  Anchor Filtering\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  Performance Analysis\n",
            "\u001b[32mconclusion: we train an unsupervised dense retrieval model, anchor-dr, leveraging the rich web anchors. in particular, we design a contrastive learning task: anchor-document prediction to continuously pretrain anchor-dr. additionally, we apply predefined rules and train a query classifier to filter out uninformative anchors. experiments on two public datasets: msmarco and beir show that anchor-dr significantly outperforms the state-of-the-art dense retrievers on unsupervised retrieval. our analyses provide a further comparison of the patterns of information contained in our contrastive learning pairs and query-document pairs in test datasets.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Length-based Vulnerability of Contrastive Text Encoders\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 5  Ablation Analysis\n",
            "\u001b[35mSection_title: 5.2  Importance of Self-referential Elongation\n",
            "\u001b[35mSection_title: 6.1  Positional Invariance\n",
            "\u001b[32mconclusion: in this work, we questioned the length generalizability of contrastive learning-based text encoders.we observed that, despite their seemingly strong representational power, this ability is strongly vulnerable to length-induced semantic shifts.we formalized length attack, demystified it, and defended against it with la(ser) 3 .we found that, teaching the models \"my longer-self = myself\" provides a standalone semantic signal for more robust and powerful unsupervised representation learning.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Dataset Creation\n",
            "\u001b[35mSection_title: 3.1  Bilingual Parallel Passage Collection\n",
            "\u001b[35mSection_title: 3.2  Question-Answer Pair Crowdsourcing\n",
            "\u001b[35mSection_title: 3.3  Multiple Answers Construction\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.1  Prefixes of Questions\n",
            "\u001b[35mSection_title: 4.2  Answers Types\n",
            "\u001b[35mSection_title: 4.3  Relationships of Questions with Passages and Reading Comprehension Skills\n",
            "\u001b[35mSection_title: 5  MRC Task Formulation on BiPaR\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Evaluation Metric\n",
            "\u001b[35mSection_title: 6.2  Human Performance Evaluation\n",
            "\u001b[35mSection_title: 6.3  Baseline models\n",
            "\u001b[35mSection_title: 6.4  Experimental Setup\n",
            "\u001b[35mSection_title: 6.5  Evaluation Results\n",
            "\u001b[32mconclusion: in this paper, we have presented the bipar, a bilingual parallel machine reading comprehension dataset on novels. from bilingual parallel passages of chinese and english novels, we manually created diversified parallel questions and answers of different types via crowdsourced workers with a multi-layer quality control system. although bipar is an extractive mrc dataset, in-  table 6: fine-grained results in terms of different answer types and question categories on the monolingual task. the left side of the slash is the f1 score on the english data, while the right is on chinese. all f1 scores are calculated on the 100 questions as described in section 4.3.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Knowledge Obtaining\n",
            "\u001b[35mSection_title: 2.2  Scene Graph Building\n",
            "\u001b[35mSection_title: 2.3  Basic Encoding\n",
            "\u001b[35mSection_title: 2.4  Dimensional-Iteration Encoding\n",
            "\u001b[35mSection_title: 2.5  Task-Specific Input and Output\n",
            "\u001b[35mSection_title: 3  Experiments and Analysis\n",
            "\u001b[35mSection_title: 3.1  Datasets and Metrics\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Baselines\n",
            "\u001b[35mSection_title: 3.4  Overall Performance\n",
            "\u001b[35mSection_title: 3.5  Effectiveness of the Event Knowledge\n",
            "\u001b[35mSection_title: 3.6  Effectiveness of the Scene Graph\n",
            "\u001b[35mSection_title: 3.7  Effectiveness of Iterable Encoding\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we focus on narrative machine reading comprehension. inspired by human behaviors, we propose a novel method to restore the scene for the narrative passage. specifically, we introduce the event knowledge from atomic and build a three-dimensional graph to describe the scene. to encode the scene graph, we propose graph dimensional-iteration network (gdin). we conduct experiments on two relevant datasets, rocstories and cosmosqa. the result shows our method achieves state-of-the-art. further experimental investigation shows that (1) compared with concept knowledge, the event knowledge we choose is more suitable for narrative mrc; (2) our proposed graph models the scene more effectively than the unstructured text and the unified plane graph do; (3) our proposed gdin encodes the scene graph efficiently by iterating multiple steps.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND AND PRELIMINARIES\n",
            "\u001b[35mSection_title: 2.1  Formal Definition of Backdoor\n",
            "\u001b[35mSection_title: 3  TAXONOMY OF BACKDOOR LEARNING\n",
            "\u001b[35mSection_title: 4  THREAT MODEL\n",
            "\u001b[35mSection_title: 4.1  Model Architecture and Hyperparameter Configuration\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Threat Model\n",
            "\u001b[35mSection_title: 4.4  Benchmark Datasets\n",
            "\u001b[35mSection_title: 4.5  Backdoor Attacks Granularity\n",
            "\u001b[35mSection_title: 4.5.2  Word-level attacks.\n",
            "\u001b[35mSection_title: 4.6  End-to-End Backdoor Learning Attacks\n",
            "\u001b[35mSection_title: 4.7  Basic Learning Attacks\n",
            "\u001b[35mSection_title: 4.8  Clean-label Attacks\n",
            "\u001b[35mSection_title: 4.9  Embedding Backdoors for Pre-trained Models\n",
            "\u001b[35mSection_title: 4.10  Backdoor Attacks and Transfer Learning\n",
            "\u001b[35mSection_title: 4.11  Challenges Associated with Backdoor Attacks\n",
            "\u001b[35mSection_title: 5  DEFENCE AGAINST BACKDOOR ATTACKS\n",
            "\u001b[35mSection_title: 5.0.1  Poisoned Data\n",
            "\u001b[35mSection_title: 5.1  Input Space Outliers\n",
            "\u001b[35mSection_title: 5.2  Latent Space Outliers\n",
            "\u001b[35mSection_title: 5.3  Identifying Backdoored Models\n",
            "\u001b[35mSection_title: 5.4  Reconstructing Triggers\n",
            "\u001b[35mSection_title: 5.5  Trigger Agnostic Detection\n",
            "\u001b[35mSection_title: 5.6  Trigger Detection During Deployment\n",
            "\u001b[35mSection_title: 5.7  Reparing Models Post-training\n",
            "\u001b[35mSection_title: 5.8  Trigger Patching\n",
            "\u001b[35mSection_title: 5.9  Trigger-Agnostic Backdoors\n",
            "\u001b[35mSection_title: 6  INSIGHTS AND OPEN CHALLENGES\n",
            "\u001b[35mSection_title: 6.1  Perceivability\n",
            "\u001b[35mSection_title: 6.2  Stealthiness of Backdoor Triggers\n",
            "\u001b[35mSection_title: 6.3  Transferebility of Defense Techniques Beyond the Text Domain\n",
            "\u001b[35mSection_title: 6.4  Certified Defenses Against Backdoor Attacks\n",
            "\u001b[35mSection_title: 6.5  Develop Generic Attack and Defense Methods\n",
            "\u001b[35mSection_title: 6.6  Develop New Benchmarks\n",
            "\u001b[35mSection_title: 7  RELATED SURVEYS\n",
            "\u001b[32mconclusion: this work extensively covers research efforts on backdoor learning for nlp. to this end, we systematically and comprehensively survey state-of-the-art research studies on backdoor attacks and defenses. additionally, we thoroughly review and analyze various aspects of backdoor learning, including techniques, model architectures, evaluation metrics, and benchmark datasets. we argue that for backdoor learning to contribute to actual robustness, research studies should take into account an expansive view and strive to answer questions related to why such attacks and defenses are successful. it is crucial to determine whether any given technique is booming due to limitations and weaknesses associated with the target model (inherent incapability arising from intrinsic properties of a target model) or whether it is due to weaknesses or limitations in the dataset itself. finally, we offer insights into open challenges and future research directions worth pursuing.  [98] 2022 gradient-based adversarial attacks amazon, trojanai sentiment analysis generic bert, gpt, lstm, gru azizi et al. [6] 2021 sequence-to-sequence (seq-2-seq) generative model mr, yelp, ag news, hs sentiment analysis, topic classification black-box bert, cnn, lstm chan et al. [14] 2020 conditional adversarially regularized autoencoder snli, yelp, mnli sentiment analysis, nli generic bert, roberta, xlnet chan et al. [16] 2021 backdoor sentence insertion imdb, dbpedia sa, sc, pr grey-box lstm chan et al. [21] 2021 trigger construction imdb, sst-5 sa white-box bert,lstm eger et al. [35] 2019 visual text perturbations imdb, mr sa black-box bert,lstm nguyen et al. [119] 2020 input-aware trigger generator via diversity loss mnist, object recognition sa, sc resnet,lstm acc, asr qi et al. [131] 2021 syntactic trigger-based attack sst-2, ag news, olid sa, sc white-box bert,lstm qi et al. [132] 2021 invisible triggers via learnable combination of word substitution sst-2, ag news, olid sa, sc white-box bert,lstm wallace et al. [159] 2019 gradient guided search over token snli, squad, olid nli black-box bert, gpt-2 yang et al. [180] 2021 poisoned word embeddings sst-2, imdb, snli sa, sc, nli black-box bert yang et al. [182] 2021 negative data augmentation and modifying word embeddings yelp, imdb, twitter sa, toxic detection black-box bert zhang et al. [192] 2021 re-weighted training of language models webtext, twitter toxic detection, qa white-box bert, gpt-2, xlnet zhang et al. [195] 2021 neuron-level backdoor attack olid, gtsrb, sst-2, enron toxic and spam detection, sa black-box bert, roberta, vggnet li et al. [83] 2021 knowledge distillation gtsrb, cfair-10 image recognition black-box resnet, vggnet garg et al. [44] 2020 backdoor injection by adversarial weight perturbation mr, cfair-10 generic black-box wordcnn, lstm chen et al. [19] 2021 task-agnostics label replacement foundation model sst-2, qnli, rte generic white-box bert, gpt-2 gan et al. [39] 2021 triggerless genetic clean-labels sentence generation sst-2, ag news, olid sc, sa black-box bert\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\u001b[35mSection_title: 9  ACKNOWLEDGEMENT\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Works\n",
            "\u001b[35mSection_title: 2.1.  Search Engine and Information Retrieval\n",
            "\u001b[35mSection_title: 2.  Related Works\n",
            "\u001b[35mSection_title: 2.1.  Search Engine and Information Retrieval\n",
            "\u001b[35mSection_title: 2.2.  Single-Hop and Multi-Hop QA\n",
            "\u001b[35mSection_title: 2.3.  Open-Domain Retrieval Task\n",
            "\u001b[35mSection_title: 2.4.  Similarity Matching Based Method\n",
            "\u001b[35mSection_title: 2.5.  Multi-Hop Reasoning for Retrieval Task\n",
            "\u001b[35mSection_title: 2.2.  Single-Hop and Multi-Hop QA\n",
            "\u001b[35mSection_title: 2.3.  Open-Domain Retrieval Task\n",
            "\u001b[35mSection_title: 2.4.  Similarity Matching Based Method\n",
            "\u001b[35mSection_title: 2.5.  Multi-Hop Reasoning for Retrieval Task\n",
            "\u001b[35mSection_title: 2.6.  Pageview\n",
            "\u001b[35mSection_title: 2.7.  Peasoning Chains\n",
            "\u001b[35mSection_title: 3.  System Architecture\n",
            "\u001b[35mSection_title: 3.1.  Overview\n",
            "\u001b[35mSection_title: 3.2.  Gold Paragraph Selection\n",
            "\u001b[35mSection_title: 3.2.  Gold Paragraph Selection\n",
            "\u001b[35mSection_title: 3.3.  Encoder and Attention Module\n",
            "\u001b[35mSection_title: 3.4.  Construction of Cognitive Graph\n",
            "\u001b[35mSection_title: 3.5.  Multi-Step Reasoning\n",
            "\u001b[35mSection_title: 3.6.  Producing Plausible Evidence Chains\n",
            "\u001b[35mSection_title: 3.7.  Example\n",
            "\u001b[35mSection_title: 4.  Experiments\n",
            "\u001b[35mSection_title: 4.1.  Datasets\n",
            "\u001b[35mSection_title: 4.2.  Implementation Details\n",
            "\u001b[35mSection_title: 4.3.  Baseline\n",
            "\u001b[35mSection_title: 4.4.  Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.  Results\n",
            "\u001b[35mSection_title: 5.1.  Overall Results\n",
            "\u001b[35mSection_title: 5.2.  Performance of Evidence Chain Retrieval\n",
            "\u001b[35mSection_title: 5.3.  Ablation Study\n",
            "\u001b[35mSection_title: 6.  Case Study\n",
            "\u001b[32mconclusion: we present a new framework 2scr-ir sensor to tackle multi-hop retrieval problems on a large scale, which retrieves reasoning paths over the cognitive graph to provide users with useful explicit evidence chains. our retriever model learns to sequentially retrieve evidence paragraphs to construct reasoning paths, which is subsequently re-ranked by the sensor that determines the final information presented as the one extracted from the best reasoning path. our retriever obtains state-of-the-art results using the hotpotqa dataset, which shows the efficiency of our framework. the state-of-theart performance on squad is achieved, demonstrating the robustness of our method. besides, our analysis shows that 2scr-ir can produce reliable and explainable reasoning chains. in the future, we may incorporate new advances in building cognitive graphs from the web context to solve more difficult reasoning problems.\n",
            "\u001b[35mSection_title: 7.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Results\n",
            "\u001b[35mSection_title: 2.1.  Complete Sequences and Phylogenetic Analysis of Signal Transducers and Activators of Transcription (STATs)\n",
            "\u001b[35mSection_title: 2.2.  STATs Structure and Sequence Analysis\n",
            "\u001b[35mSection_title: 2.2.  STATs Structure and Sequence Analysis\n",
            "\u001b[35mSection_title: 2.3.  Expression of S. hasta STATs in Various Tissues\n",
            "\u001b[35mSection_title: 2.3.  Expression of S. hasta STATs in Various Tissues\n",
            "\u001b[35mSection_title: 2.4.  Cell Viability and Intracellular Triglyceride (TG) Content\n",
            "\u001b[35mSection_title: 2.4.  Cell Viability and Intracellular Triglyceride (TG) Content\n",
            "\u001b[35mSection_title: 2.5.  Enzymatic Activities\n",
            "\u001b[35mSection_title: 2.6.  Transcriptional Regulation of the Genes by Leptin in Vitro\n",
            "\u001b[35mSection_title: 2.5.  Enzymatic Activities\n",
            "\u001b[35mSection_title: 2.4.  Cell Viability and Intracellular Triglyceride (TG) Content\n",
            "\u001b[35mSection_title: 2.5.  Enzymatic Activities\n",
            "\u001b[35mSection_title: 2.6.  Transcriptional Regulation of the Genes by Leptin in Vitro\n",
            "\u001b[35mSection_title: 2.6.  Transcriptional Regulation of the Genes by Leptin in Vitro\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[35mSection_title: 4.  Materials and Methods\n",
            "\u001b[35mSection_title: 4.1.  STAT cDNAs Cloning and mRNA Expression Patterns of Various Tissues\n",
            "\u001b[35mSection_title: 4.1.1.  Fish Rearing and Sampling\n",
            "\u001b[35mSection_title: 4.1.2.  RNA Isolation and cDNAs Synthesis\n",
            "\u001b[35mSection_title: 4.1.3.  Cloning and Sequencing of STATs\n",
            "\u001b[35mSection_title: 4.2.  Hepatocytes Treatment in Vitro\n",
            "\u001b[35mSection_title: 4.2.1.  Hepatocytes Culture and Treatment\n",
            "\u001b[35mSection_title: 4.2.2.  Cell Viability Assay, TG Content and Enzyme Activity Determination\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Cross-encoder Approach\n",
            "\u001b[35mSection_title: 2.2  Bi-encoder Approach\n",
            "\u001b[35mSection_title: 2.3  Uni-encoder Approach\n",
            "\u001b[35mSection_title: 3  PROPOSED MODEL\n",
            "\u001b[35mSection_title: 3.1  Model Architecture\n",
            "\u001b[35mSection_title: 3.2  Dual Document Encoder\n",
            "\u001b[35mSection_title: 3.3  Co-training Strategy\n",
            "\u001b[35mSection_title: 3.4  Learning-based Pruning\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 5  RESULTS AND ANALYSIS\n",
            "\u001b[35mSection_title: 5.1  Effectiveness vs. Efficiency\n",
            "\u001b[35mSection_title: 5.2  In-depth Analysis\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[32mconclusion: in this paper, we proposed a novel uni-encoder model, sparse retriever using a dual document encoder (spade), to alleviate the trade-off between effectiveness and efficiency of the ir system.we adopted a dual document encoder for lexical and semantic matching and developed a co-training strategy to mitigate the training intervention between encoders.we also utilized document-and corpus-level pruning during model training, enabling efficient retrieval using the inverted index.experimental results showed that spade achieves state-of-the-art performance among uni-encoder models with acceptable query latency, notably preferable for commercial ir systems.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 2.1  App Review Classification in Software Engineering\n",
            "\u001b[35mSection_title: 2.1.1  Topic Modeling Based Models\n",
            "\u001b[35mSection_title: 2.1.2  Machine Learning Based Approaches\n",
            "\u001b[35mSection_title: 2.1.3  Deep Learning Based Approaches\n",
            "\u001b[35mSection_title: 2.2  Pre-trained Models\n",
            "\u001b[35mSection_title: 3  Study Overview\n",
            "\u001b[35mSection_title: 3.1  Research Overview\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.3  Prior approaches\n",
            "\u001b[35mSection_title: 3.4  Pre-Trained Models (PTM)\n",
            "\u001b[35mSection_title: 3.5  Implementation\n",
            "\u001b[35mSection_title: 3.5.1  Prior approaches\n",
            "\u001b[35mSection_title: 3.6  Experimental Setup\n",
            "\u001b[35mSection_title: 3.7  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4  Approach for Research Questions\n",
            "\u001b[35mSection_title: 4.1  Approach for Research Question 1\n",
            "\u001b[35mSection_title: 4.2  Approach for Research Question 2\n",
            "\u001b[35mSection_title: 4.3  Approach for Research Question 3\n",
            "\u001b[35mSection_title: 4.3.1  Binary vs. Multi-Class Setting\n",
            "\u001b[35mSection_title: 4.3.2  Zero-Shot Classification Setting\n",
            "\u001b[35mSection_title: 4.3.3  Multi-Task Setting\n",
            "\u001b[35mSection_title: 4.3.4  Multiple Resources\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  RQ1: Accuracy and Efficiency of PTMs Compared to Prior approaches\n",
            "\u001b[35mSection_title: 5.2  RQ2: Domain-Specific PTMs vs. General PTMs\n",
            "\u001b[35mSection_title: 5.3  RQ3: Experimenting PTMs in Different Settings\n",
            "\u001b[35mSection_title: 5.3.1  RQ3-1: Binary vs. Multi-class Setting\n",
            "\u001b[35mSection_title: 5.3.2  RQ3-2: Zero-Shot Setting\n",
            "\u001b[35mSection_title: 5.3.3  RQ3-3: Multi-task Setting\n",
            "\u001b[35mSection_title: 5.3.4  RQ3-4: Multi-resource Setting\n",
            "\u001b[35mSection_title: 6  Discussions\n",
            "\u001b[35mSection_title: 6.1  Implications For Users\n",
            "\u001b[35mSection_title: 6.2  Implications For Researchers\n",
            "\u001b[35mSection_title: 7  Threats to Validity\n",
            "\u001b[32mconclusion: we conducted an extensive exploratory study comparing app issue classification tools and pre-trained transformer-based models in various settings. we conducted the experiments on six available datasets and a highly imbalanced dataset, which is a combination of the six datasets. domain-specific ptms were trained using different sizes of app review data we collected from google play and these customized ptms were also studied here. our results confirm that ptms are achieving higher scores in binary and multi-class classification compared to prior approaches, but the over-the-shelf ptms are not always the best models to be used in all scenarios. instead, cptms have the highest scores and are able to perform better than other models in all settings. moreover, incorporating app specific data in the pre-training of ptms reduces the prediction time. one of the future directions of this research is assessing domain-specific ptms in other areas of app reviews and exploring ways to increase performance in zero-shot setting.            \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset\n",
            "\u001b[35mSection_title: 3.1  Crowdsourcing\n",
            "\u001b[35mSection_title: 3.2  Discussion\n",
            "\u001b[35mSection_title: 3.3  Meta-information\n",
            "\u001b[35mSection_title: 4  Tasks and Methods\n",
            "\u001b[35mSection_title: 4.1  Causal Detection\n",
            "\u001b[35mSection_title: 4.2  Fine-grained Event Causality Extraction\n",
            "\u001b[35mSection_title: 4.3  Causal QA\n",
            "\u001b[35mSection_title: 4.4  Models\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Settings\n",
            "\u001b[35mSection_title: 5.2  Causality Detection\n",
            "\u001b[35mSection_title: 5.3  Fine-grained Event Causality Extraction\n",
            "\u001b[35mSection_title: 5.4  Causal QA\n",
            "\u001b[35mSection_title: 5.4.1  Quantitative Analysis\n",
            "\u001b[35mSection_title: 5.4.2  Challenges by Causal QA\n",
            "\u001b[35mSection_title: 5.4.3  Error Analysis\n",
            "\u001b[32mconclusion: we explored the efficacy of current state-of-the-art methods for causal reasoning tasks by considering a novel fine-grained reasoning setting and developing a dataset with rich human labels. experimental results using the state-of-the-art pre-trained language models provide the evidence that there is much room for improvement on causal reasoning tasks, and a need for designing better solutions to correlation discovery related to event causality analysis and why/what-if qa tasks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 7  Ethical Statement\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The Approach\n",
            "\u001b[35mSection_title: 2.1  Model Description\n",
            "\u001b[35mSection_title: 2.2  Model Results\n",
            "\u001b[35mSection_title: 3  Graph Structure May Not Be Necessary\n",
            "\u001b[35mSection_title: 4  Understanding Graph Structure\n",
            "\u001b[35mSection_title: 4.1  Graph Attention Versus Self Attention\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Experimental Results\n",
            "\u001b[32mconclusion: this study set out to investigate whether graph structure is necessary for multi-hop reasoning tasks and what role it plays. we established that with the proper use of pre-trained models, graph structure may not be necessary for multi-hop reasoning. in addition, we point out that the adjacency matrix and graph structure can be regarded as some kind of task-related prior knowledge. we find both graph-attention and graph structure can be replaced by self-attention or transformer.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHODOLOGY 3.1 Problem Setup\n",
            "\u001b[35mSection_title: 3.2  Observations\n",
            "\u001b[35mSection_title: 3.3  Historical Query Expansion\n",
            "\u001b[35mSection_title: 3.3.1  Keyword extractor (KE).\n",
            "\u001b[35mSection_title: 3.3.2  Query performance predictor (QPP).\n",
            "\u001b[35mSection_title: 3.4  Neural Transfer Reformulation\n",
            "\u001b[35mSection_title: 4.2  Baseline Query Reformulation Methods\n",
            "\u001b[35mSection_title: 4.3  Evaluation and Settings\n",
            "\u001b[35mSection_title: 5  RESULTS\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Component Evaluation\n",
            "\u001b[35mSection_title: 6  DISCUSSION\n",
            "\u001b[35mSection_title: 6.1  Query characteristics\n",
            "\u001b[35mSection_title: 6.2  Retrieval characteristics\n",
            "\u001b[35mSection_title: 6.3  Case Study\n",
            "\u001b[32mconclusion: we present hqe and ntr, both conversational query reformulation methods stacked on a successful multi-stage ir pipeline. the effectiveness of our methods are attested by experiments on the cast benchmark dataset, the results of which suggest that the two methods have different advantages in fusing context information into conversational user utterances for downstream ir models. finally, this work elevates the state of the art in cast benchmarks and provides simple but effectives baselines for future research.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Question Decomposition\n",
            "\u001b[35mSection_title: 3.2  Single-hop question answering\n",
            "\u001b[35mSection_title: 3.3  Operation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Results and Discussion\n",
            "\u001b[32mconclusion: we propose using reasoning templates for tackling reading comprehension tasks that involve reasoning over multiple paragraphs. we show that this approach is competitive with state of the art models on a subset of drop's subtraction questions, while requiring much less training data and providing better visibility of the model's decision making. in future work, we plan on extending to further templates and investigate how to learn templates instead of working from a predefined set.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Problem Statement\n",
            "\u001b[35mSection_title: 3.2  Model Overview\n",
            "\u001b[35mSection_title: 3.3  Query Generation\n",
            "\u001b[35mSection_title: 3.4  Deriving Supervision Signal for Query Generation\n",
            "\u001b[35mSection_title: 3.5  Question Answering Component\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Data and Setup\n",
            "\u001b[35mSection_title: 4.2  End-to-end Question Answering\n",
            "\u001b[35mSection_title: 4.3  Analysis of Query Generation\n",
            "\u001b[32mconclusion: in this paper, we presented golden (gold entity) retriever, an open-domain multi-hop question answering system for scalable multi-hop reasoning. through iterative reasoning and retrieval, golden retriever greatly improves the recall of gold supporting facts, thus providing the question answering model a much better set of context documents to produce an answer from, and demonstrates competitive performance to the state of the art. generating natural languages queries for each step of reasoning, golden retriever is also more interpretable to humans compared to previous neural retrieval approaches and affords better understanding and verification of model behavior. we start from the wikipedia dump file containing the introductory paragraphs used in hotpotqa that yang et al. (2018) provide, 10 and add the fields corresponding to wikipedia page titles and the introductory paragraphs (text) into the index.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Literature Review\n",
            "\u001b[35mSection_title: 3.  Methodology\n",
            "\u001b[35mSection_title: 3.1.  Qualitative Approach\n",
            "\u001b[35mSection_title: 3.2.  Quantitative Approach\n",
            "\u001b[35mSection_title: 3.3.  Questionnaire Attributes [1]\n",
            "\u001b[35mSection_title: 4.  Strategy Project (Plan)\n",
            "\u001b[35mSection_title: 5.  Resources and Evaluation of Change\n",
            "\u001b[32mconclusion: the research aims to reach a point where a benefit for change is actually happening at the end of the research's period.this could be measured through the tools that are mentioned above.thus if (pm-em) was negative and a shift in the training procedures and the management's policies had an impact on service n. saadeh\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary\n",
            "\u001b[35mSection_title: 2.1  Task Definition\n",
            "\u001b[35mSection_title: 2.2  DOM Trees of HTML codes\n",
            "\u001b[35mSection_title: 3  TIE\n",
            "\u001b[35mSection_title: 3.1  Architecture of the Whole SRC System\n",
            "\u001b[35mSection_title: 3.2  Construction of GAT Graphs\n",
            "\u001b[35mSection_title: 3.2.1  DOM Trees\n",
            "\u001b[35mSection_title: 3.2.2  NPR Graphs\n",
            "\u001b[35mSection_title: 3.3  Design of TIE\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Metrics\n",
            "\u001b[35mSection_title: 4.3  Baselines & Setup\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 4.5  Case Study\n",
            "\u001b[35mSection_title: 4.6  Ablation Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we proposed a tag-level qa model called tie to better understand the topological information contained in the structured web pages. our model explicitly captures two of the most informative topological structures of the web pages, logical and spatial structures, by dom trees and npr graphs, respectively. with the proposed twostage pipeline, we conduct extensive experiments on the websrc dataset. our tie successfully achieves sota performances and the contributions of its key components are validated.\n",
            "\u001b[35mSection_title: 6  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  QAConv Dataset\n",
            "\u001b[35mSection_title: 2.1  Data Collection\n",
            "\u001b[35mSection_title: 2.1.1  Selection and Segmentation\n",
            "\u001b[35mSection_title: 2.1.2  Question Generation\n",
            "\u001b[35mSection_title: 2.1.3  Crowdsourcing QA Pairs\n",
            "\u001b[35mSection_title: 2.1.4  Quality Verification and Data Splits\n",
            "\u001b[35mSection_title: 2.2  QA Analysis\n",
            "\u001b[35mSection_title: 2.2.1  Question Analysis\n",
            "\u001b[35mSection_title: 2.2.2  Answer Analysis\n",
            "\u001b[35mSection_title: 2.3  Chunk Mode and Full Mode\n",
            "\u001b[35mSection_title: 3  Experimental Results\n",
            "\u001b[35mSection_title: 3.1  State-of-the-art Baselines\n",
            "\u001b[35mSection_title: 3.1.1  Span-based Models\n",
            "\u001b[35mSection_title: 3.1.2  Free-form Models\n",
            "\u001b[35mSection_title: 3.1.3  Retrieval Models\n",
            "\u001b[35mSection_title: 3.1.4  Computational Details\n",
            "\u001b[35mSection_title: 3.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.3  Performance Analysis\n",
            "\u001b[35mSection_title: 3.3.1  Chunk Mode\n",
            "\u001b[35mSection_title: 3.3.2  Full Mode\n",
            "\u001b[35mSection_title: 4  Error Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: qaconv is a new dataset that conducts qa on informative conversations such as emails, panels, and channels. we show the unique challenges of our tasks in both chunk mode with oracle partial conversations and full mode with a retrieval stage. we find that state-of-the-art qa models have limited dialogue understanding and tend to predict our answerable qa pairs as unanswerable. we provide a new testbed for qa on conversation tasks to facilitate future research.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  PMR\n",
            "\u001b[35mSection_title: 2.1  Pre-training of PMR\n",
            "\u001b[35mSection_title: 2.2  Fine-tuning PMR for Extraction Tasks\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 4  Main Results\n",
            "\u001b[35mSection_title: 5.1  Explainable Sequence Classification\n",
            "\u001b[35mSection_title: 5.2  Unifying Extraction and Classification with PMR\n",
            "\u001b[35mSection_title: 5.3  Better Comprehending capability\n",
            "\u001b[32mconclusion: this work presents a novel mrc-style pre-training model called pmr.pmr can fully resolve the learning objective and model architecture gaps that frequently appear in fine-tuning existing mlms.experimental results from multiple dimensions, including effectiveness in solving few-shot tasks and ood generalization, show the benefits of bridging the gap between pre-training and fine-tuning for span extraction tasks.pmr also shows promising potential in explaining the sequence classification process and unifying nlu tasks.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  PRELIMINARIES\n",
            "\u001b[35mSection_title: 2.1  Task: Logical Reasoning QA\n",
            "\u001b[35mSection_title: 2.2  Logic Theories for Logical Reasoning QA\n",
            "\u001b[35mSection_title: 2.2.1  Informal Logic\n",
            "\u001b[35mSection_title: 2.2.2  Formal Logic\n",
            "\u001b[35mSection_title: 3  DISCOURSE-AWARE GRAPH NETWORKS\n",
            "\u001b[35mSection_title: 3.1  Logic Graph Construction\n",
            "\u001b[32mlimitation: it is studied that clause-like text spans delimited by discourse relations can be discourse units that reveal the rhetorical structure of texts [34], [41]. we further observe that such discourse units are essential logical propositions in logical reasoning, such as premise or conclusion. as the example shown in figure 3, the \"while\" in the passage indicates a comparison between the attributes of the fig. 2. the discourse-aware graph networks (dagns) pipeline mainly consists of (1) logic graph construction (2) logic representation learning. the logic graph construction module takes a logical qa data point as input and constructs logic graphs. the logic representation learning module then performs graph reasoning upon the constructed logic graphs. besides, the encoder provides fundamental embeddings for the pipeline.\"analog system\" and that of the \"digital system\". the \"because\" in the option uncovers that \"error cannot occur in the emission of digital signals\" as a premise to the conclusion \"digital systems are the best information systems\".this observation is agreed with informal logic theories [35], [36], which study uncovering logical structure from the texts and have conventional in-line logical indicators. for example, acknowledged premise indicators include \"since\", \"because\", \"given that\". conclusion indicators include \"therefore\", \"so\", \"consequently\", and so forth. most of these indicators are discourse connectives. some discourse parsers [42], [43] perform discourse unit segmentation. however, discourse parsing is still challenging, and the parsers are not general to new data, such as logical reasoning questions. for example, segbot [43] is good on the rst-dt dataset but does not work well on the standardized exam texts as in the reclor dataset. thus, we customize discourse unit delimitation strategy for logical texts.we use the penn discourse treebank (pdtb 2.0) [34] to help draw discourse connectives. pdtb 2.0 contains discourse relations that are manually annotated on the 1 million wall street journal (wsj) corpus and are broadly characterized into \"explicit\" and \"implicit\" connectives. the former ones are explicitly present in sentences such as discourse adverbial \"instead\" or subordinating conjunction \"because\", whereas the latter ones are inferred by pdtb annotators between successive pairs of text spans split by punctuation marks such as \".\" or \";\". we take all the \"explicit\" connectives as well as common punctuation marks to form our discourse-aware delimiter library, presented in table 1. each logical text is split into elementary discourse units (edus) by all the delimiters in the library. the edus are taken as graph nodes v.nodes with topic-related terms. the desired key terms are those real nouns or phrases that repeatedly appear in the text. such nouns or phrases are instantiations of logical variables in propositions. as a result, replacing such terms with abstract variables or terms in other topics does not change the process of reasoning. for example, in figure 3, the first two sentences indicate a comparison of \"signal\" between \"analog system(s)\" and \"digital system(s)\". performing abstraction by replacing \"signal\" with variable γ, \"analog system(s)\" with variable α, and \"digital system(s)\" with variable β, the propositions are free from the topic of electronics, but the comparison relation is retained.we use a sliding window to collect the recurring phrases. given the input logical text, stemming is first applied to handle morphological diversity. then, the sliding window loops over ngrams and records the reoccurrence. next, all the stop words and overlapped substrings are filtered. the resulting topic-related terms are attached to the nodes according to which text segment they belong.binary node types. the text of logical reasoning qa consists of two possible structures: (passage, question, options) or (dialogue context, candidate responses). we regard passage or dialogue context as context texts that carry the main logical reasoning structure, whereas regard (question, options) or candidate responses as candidate texts that are added to the context texts and should remain their logical consistency.according to the discourse unit delimitation, the graph nodes are naturally from the context texts or the candidate texts. therefore, we define two disjoint and independent node sets: context node set v u and candidate node setv v . v u ∪v v = v and v u ∩v v = ∅.the interplay between the two node sets formulates logical consistency between the context and the candidate texts.\n",
            "\u001b[35mSection_title: 3.1.1  Nodes via Discourse Unit Delimitation\n",
            "\u001b[35mSection_title: 3.1.2  Edge Definition\n",
            "\u001b[35mSection_title: 3.2  Logic Representation Learning\n",
            "\u001b[35mSection_title: 3.2.1  The End-to-End Learning Pipeline\n",
            "\u001b[35mSection_title: 3.2.2  Logic Edge Reasoning\n",
            "\u001b[35mSection_title: 3.2.3  Logic Graph Reasoning\n",
            "\u001b[35mSection_title: 3.2.4  Feature Fusion\n",
            "\u001b[35mSection_title: 4  EXPERIMENT\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Results in Supervised Scenarios\n",
            "\u001b[35mSection_title: 4.3.1  ReClor Dataset\n",
            "\u001b[35mSection_title: 4.3.2  LogiQA Dataset\n",
            "\u001b[35mSection_title: 4.3.3  MuTual Dataset\n",
            "\u001b[35mSection_title: 4.4  Results in Zero-shot Scenarios\n",
            "\u001b[35mSection_title: 4.4.1  Zero-shot Transfer between ReClor and LogiQA\n",
            "\u001b[35mSection_title: 4.4.2  Zero-shot Transfer to MuTual\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[35mSection_title: 4.5.1  Importance of Graph Components\n",
            "\u001b[35mSection_title: 4.5.2  Model Components\n",
            "\u001b[35mSection_title: 4.5.3  Effect of GNN Layer Stacks\n",
            "\u001b[35mSection_title: 4.6  Question Types\n",
            "\u001b[35mSection_title: 4.7  Visualization\n",
            "\u001b[35mSection_title: 5  RELATED WORKS\n",
            "\u001b[35mSection_title: 5.1  Textual Reasoning\n",
            "\u001b[35mSection_title: 5.2  Discourse Applications\n",
            "\u001b[32mconclusion: this paper explores a structure-based solution to textual logical reasoning that explicitly models the logical reasoning process. the challenges include: (1) uncovering the inference structure from plain texts for effective structural constraints. (2) learning the inference processes rather than the knowledge for effective logical reasoning.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Invariance of MPPIs\n",
            "\u001b[35mSection_title: 3.1.1  Random Seed\n",
            "\u001b[35mSection_title: 3.1.2  Pretraining and Architecture\n",
            "\u001b[35mSection_title: 3.1.3  Cross-Domain Similarity\n",
            "\u001b[35mSection_title: 3.2  Cross-Domain Transferability of MPPIs\n",
            "\u001b[35mSection_title: 3.3  Human-Sufficient MPPIs do not\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[32mconclusion: we empirically verify the surprising invariance of mppis to random seed, model architecture, and pretraining, as well as their wide transferability across domains. these results suggest that mppis may not be best explained by poorly calibrated neural estimates of confidence or dataset-specific bias. examining their relationship to generalization and adversarial robustness, we highlight the ability to maintain in-domain performance but significantly alter out-domain performance and robustness. we hope our results encourage a more systematic analysis of hypotheses regarding model behavior outside the human interpretable distribution of examples.  (2017), we borrowed the implementation and hyper-parameters from hitvoice (https://github.com/hitvoice/drqa) and train on 1 nvidia tesla v100 gpu. 8\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Results\n",
            "\u001b[35mSection_title: 2.1.  Singleplex Detection\n",
            "\u001b[35mSection_title: 2.2.  Analytical Specificity and Sensitivity\n",
            "\u001b[35mSection_title: 2.3.  Allele-Specific Detection\n",
            "\u001b[35mSection_title: 2.4.  Multiplex Detection\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[35mSection_title: 4.  Materials and Methods\n",
            "\u001b[35mSection_title: 4.1.  Bacterial Strains, DNA Extraction, and Quantification\n",
            "\u001b[35mSection_title: 4.2.  Diagnostic Targets, Oligonucleotides, and Synthetic Templates\n",
            "\u001b[35mSection_title: 4.3.  Singleplex Detection\n",
            "\u001b[35mSection_title: 4.4.  Analytical Specificity and Sensitivity\n",
            "\u001b[35mSection_title: 4.5.  Allele-Specific Detection\n",
            "\u001b[35mSection_title: 4.6.  Multiplex Detection\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  INPUT-DEPENDENT PROMPT TUNING FOR MULTI-TASKING A FROZEN LM\n",
            "\u001b[35mSection_title: 2.1  THE ID-PT ARCHITECTURE\n",
            "\u001b[35mSection_title: 2.2  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 2.3  RESULTS\n",
            "\u001b[35mSection_title: 3  A FROZEN LM READER FOR OPEN-DOMAIN QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 3.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.2  RESULTS\n",
            "\u001b[35mSection_title: 4  RECURSIVELY APPLYING A FROZEN LM\n",
            "\u001b[35mSection_title: 4.1  TEXTUAL LM RECURSION\n",
            "\u001b[35mSection_title: 4.2  NEURAL LM RECURSION\n",
            "\u001b[32mconclusion: while fine-tuning huge lms can often yield excellent performance, this approach is expensive at training time, requires serving a plethora of models at runtime, and provides poor adaptability in the face of variations in the targeted task. this paper has shown that a better alternative exists: freezing a single, huge pretrained lm and learning much smaller neural modules that specialize the lm to different tasks. while prompt tuning, prefix tuning, and other existing frozen model methods cited above can be seen as a simple instantiations of this idea, this paper shows that much more complex architectures can achieve much stronger performance.\n",
            "\u001b[35mSection_title: 5  CONCLUSIONS\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  INPUT-DEPENDENT PROMPT TUNING FOR MULTI-TASKING A FROZEN LM\n",
            "\u001b[35mSection_title: 2.1  THE ID-PT ARCHITECTURE\n",
            "\u001b[35mSection_title: 2.2  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 2.3  RESULTS\n",
            "\u001b[35mSection_title: 3  A FROZEN LM READER FOR OPEN-DOMAIN QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 3.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.2  RESULTS\n",
            "\u001b[35mSection_title: 4  RECURSIVELY APPLYING A FROZEN LM\n",
            "\u001b[35mSection_title: 4.1  TEXTUAL LM RECURSION\n",
            "\u001b[35mSection_title: 4.2  NEURAL LM RECURSION\n",
            "\u001b[32mconclusion: while fine-tuning huge lms can often yield excellent performance, this approach is expensive at training time, requires serving a plethora of models at runtime, and provides poor adaptability in the face of variations in the targeted task. this paper has shown that a better alternative exists: freezing a single, huge pretrained lm and learning much smaller neural modules that specialize the lm to different tasks. while prompt tuning, prefix tuning, and other existing frozen model methods cited above can be seen as a simple instantiations of this idea, this paper shows that much more complex architectures can achieve much stronger performance.\n",
            "\u001b[35mSection_title: 5  CONCLUSIONS\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  INPUT-DEPENDENT PROMPT TUNING FOR MULTI-TASKING A FROZEN LM\n",
            "\u001b[35mSection_title: 2.1  THE ID-PT ARCHITECTURE\n",
            "\u001b[35mSection_title: 2.2  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 2.3  RESULTS\n",
            "\u001b[35mSection_title: 3  A FROZEN LM READER FOR OPEN-DOMAIN QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 3.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.2  RESULTS\n",
            "\u001b[35mSection_title: 4  RECURSIVELY APPLYING A FROZEN LM\n",
            "\u001b[35mSection_title: 4.1  TEXTUAL LM RECURSION\n",
            "\u001b[35mSection_title: 4.2  NEURAL LM RECURSION\n",
            "\u001b[32mconclusion: while fine-tuning huge lms can often yield excellent performance, this approach is expensive at training time, requires serving a plethora of models at runtime, and provides poor adaptability in the face of variations in the targeted task. this paper has shown that a better alternative exists: freezing a single, huge pretrained lm and learning much smaller neural modules that specialize the lm to different tasks. while prompt tuning, prefix tuning, and other existing frozen model methods cited above can be seen as a simple instantiations of this idea, this paper shows that much more complex architectures can achieve much stronger performance.\n",
            "\u001b[35mSection_title: 5  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Text-to-Text Transfer Learning for DST\n",
            "\u001b[35mSection_title: 2.2  Unanswerable Questions\n",
            "\u001b[35mSection_title: 3.2  Evaluation\n",
            "\u001b[35mSection_title: 3.3  Implementation\n",
            "\u001b[35mSection_title: 3.4  Baselines\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Zero-Shot\n",
            "\u001b[35mSection_title: 4.2  Few-Shot\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Impact of Unanswerable Questions\n",
            "\u001b[35mSection_title: 5.2  Error Analysis\n",
            "\u001b[35mSection_title: 5.3  Oracle Study\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present transferqa, a unified generative model that performs dst without using any dst training data. transferqa uses the textto-text transfer learning framework that seamlessly combines extractive qa and multi-choice qa for tracking both categorical slots and non-categorical slots. to enable our model to zero-shot \"none\" value slots, we introduce two effective ways to construct unanswerable questions, i.e., negative question sampling and context truncation. the experimental results on the multiwoz and sgd datasets demonstrate the effectiveness of our approach in both zero-shot and few-shot settings. we also show that improving the \"none\" value slot accuracy has\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Video retrieval methods\n",
            "\u001b[35mSection_title: 2.3  Report generation using online sources\n",
            "\u001b[35mSection_title: 3  Dataset\n",
            "\u001b[35mSection_title: 3.1  Current event curation\n",
            "\u001b[35mSection_title: 3.2  Video collection\n",
            "\u001b[35mSection_title: 4  Data analysis\n",
            "\u001b[35mSection_title: 4.1  Semantic information in video\n",
            "\u001b[35mSection_title: 4.2  Video content by domain\n",
            "\u001b[35mSection_title: 4.3  Information evolution\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Approach\n",
            "\u001b[35mSection_title: 5.2  Model architecture and training\n",
            "\u001b[35mSection_title: 5.3  Retrieval baselines\n",
            "\u001b[35mSection_title: 5.4  MultiVENT retrieval\n",
            "\u001b[32mconclusion: we introduce multivent, a multimodal, multilingual dataset grounded in natural language documents for event-centric video retrieval and information acquisition. this dataset consists of 2,396 videos covering 260 current events reported in five target languages (arabic, chinese, english, korean, and russian) paired with multilingual natural language video descriptions and long-form event-centric text documents. we use this dataset to characterize online news coverage and how models can use this online content for information acquisition. we propose a multilingual video retrieval benchmark using multivent and present multiclip, multilingual video retrieval model to serve as a baseline for the task. we evaluate this model and related retrieval approaches on msr-vtt and multivent to illustrate the importance of pretraining on multilingual data for evaluation on multivent. in future work, we aim to explore the effect that joint vision-ocr embeddings can have on video retrieval in text-heavy contexts. also in future work, a repaq-adjacent system [25] for automatically extracting question-answer pairs from video content and video-document pairs could be developed and applied to multivent. through this, a framework for teaching models to perform open-domain question-answering tasks with multimodal background corpora could be established, expanding the domain of questions a model can answer.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 2.1  PLMs for Dense Retrieval\n",
            "\u001b[35mSection_title: 2.2  PLMs for Sparse Retrieval\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Multi-view Representations\n",
            "\u001b[35mSection_title: 3.2  Contextual Masked Auto-encoder with Multi-view Decoding Paradigm\n",
            "\u001b[35mSection_title: 3.3  Fine-tuning and Inference\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETTINGS\n",
            "\u001b[35mSection_title: 4.1  Pre-training\n",
            "\u001b[35mSection_title: 4.2  Fine-tuning and Inference\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 5  EXPERIMENT RESULTS\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Out-of-domain Evaluation on BEIR Benchmarks\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 6.1  Impact of Multi-view Representations and Multi-view Decoding Paradigm\n",
            "\u001b[35mSection_title: 6.2  Performances of Reducing Dimensions of Multi-view Representations\n",
            "\u001b[35mSection_title: 6.4  Case Study\n",
            "\u001b[32mconclusion: this paper proposes a multi-view contextual masked auto-encoding pre-training architecture for better passage retrieval. experiment results show that multi-view representation and multi-view decoding paradigms significantly contribute to effec-tive retrieval performance. our method also shows good robustness and stability. in the future, we will further explore incorporating new pre-training paradigms to get more effective and robust retrievers.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Defining Reading Comprehension\n",
            "\u001b[35mSection_title: 3  What kinds of questions?\n",
            "\u001b[35mSection_title: 4  Ways to combat shortcuts\n",
            "\u001b[35mSection_title: 4.1  Question / passage mismatch\n",
            "\u001b[35mSection_title: 4.2  \"No answer\" option\n",
            "\u001b[35mSection_title: 4.3  Dialog\n",
            "\u001b[35mSection_title: 4.4  Complex reasoning\n",
            "\u001b[35mSection_title: 4.5  Context construction\n",
            "\u001b[35mSection_title: 4.6  Adversarial construction\n",
            "\u001b[35mSection_title: 4.7  Minimal question pairs\n",
            "\u001b[35mSection_title: 4.8  Free-form answers\n",
            "\u001b[35mSection_title: 4.9  Multi-task evaluation\n",
            "\u001b[35mSection_title: 4.10  Explainability\n",
            "bottom\n",
            "\u001b[35mSection_title: 5  Recommendations for future research\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Literature Review\n",
            "\u001b[35mSection_title: 3.  Intelligent Decision Prediction and\n",
            "\u001b[35mSection_title: 3.1.2.  Bidirectional Transformer Convolution Network Model.\n",
            "\u001b[35mSection_title: 3.2.  Reselection Mechanism.\n",
            "\u001b[35mSection_title: 3.3.  Experimental Results and Analysis\n",
            "\u001b[35mSection_title: 3.3.1.  Data and Preprocessing.\n",
            "\u001b[35mSection_title: 3.3.2.  Experimental Setup and Evaluation Index.\n",
            "\u001b[35mSection_title: 3.3.3.  Result Analysis.\n",
            "\u001b[35mSection_title: 4.  Evidence Prediction Method Based on Sentence Selection for Legal Documents\n",
            "\u001b[35mSection_title: 4.1.  Method Introduction.\n",
            "\u001b[35mSection_title: 4.2.  Tightly Connected Encoder Stack.\n",
            "\u001b[35mSection_title: 4.3.  Multihead Self-Attention Layer.\n",
            "\u001b[35mSection_title: 4.4.  Binary Cross-Entropy Loss Function.\n",
            "\u001b[35mSection_title: 4.5.2.  Model Test. As shown in\n",
            "\u001b[32mconclusion: for legal documents with clear structure and rigorous expression, it is helpful to improve human work efficiency to let machines understand and read legal documents. e purpose of reading comprehension in the legal field is to train the machine model through legal documents so that it can answer various questions according to the given case description. an excellent reading and understanding system in the legal field can assist judges, lawyers, and other professionals in their work and also make it easy for people to understand the basic situation of each case. it has a wide range of application prospects, such as crime prediction, evidence prediction, legal provisions recommendation, and intelligent court trial. is paper mainly studies the evidence prediction in the legal field. taking the prediction of reading and understanding evidence in the legal field as the research task, this paper puts forward a prediction method of evidence based on sentence selection for legal documents. a sentence selection module is designed to remove irrelevant sentences, and questions and answers are used to infer evidence, which has achieved good results. rough experiments, it is found that the score of joint f1 proposed in this paper is 70.07%, which is more accurate than the mainstream model.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 4  Retrieval Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  Embedding Quality Analysis\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we introduce a method to train bi-encoders on unstructured text and knowledge graph types through a type-enforced contrastive loss. our loss can improve retrieval of rare entities for ambiguous mentions, while maintaining strong overall performance on open-domain nlp tasks. we hope our work inspires future work on integrating structured data into pretrained models. s. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon. any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of nih, onr, or the u.s. government.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Perturbation Types\n",
            "\u001b[35mSection_title: 2.2  Entropy Maximization\n",
            "\u001b[35mSection_title: 2.3  Conditional Independence Assumption for Extractive QA\n",
            "\u001b[35mSection_title: 2.4  Recognizing Multiple Types of Perturbations\n",
            "\u001b[35mSection_title: 2.5  Interpretation from the Perspective of Causality\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Experimental Setups\n",
            "\u001b[35mSection_title: 3.2  Human Evaluation\n",
            "\u001b[35mSection_title: 3.3  Cross-Perturbation Evaluation\n",
            "\u001b[35mSection_title: 3.4  Out-of-Distribution Generalization\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 4.1  Insensitivity to Large Perturbations\n",
            "\u001b[35mSection_title: 4.2  Sensitivity to Small Perturbations\n",
            "\u001b[32mconclusion: we first showed that entropy maximization often fails to transfer to unseen perturbation types. maximizing the entropy terms for various types of perturbations is effective in mitigating this problem. the failure of entropy maximization to improve out-of-distribution generalization may be caused by the unnaturalness of the perturbed inputs. modifying the perturbation functions to effectively improve outof-distribution generalization is future work.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data Collection and Annotation\n",
            "\u001b[35mSection_title: 3.1  Text Sources\n",
            "\u001b[35mSection_title: 3.2  Data Annotation\n",
            "\u001b[35mSection_title: 3.3  Quality Control\n",
            "\u001b[35mSection_title: 4  Data Analysis\n",
            "\u001b[35mSection_title: 4.1  Dataset Statistics\n",
            "\u001b[35mSection_title: 4.2  Question types by words\n",
            "\u001b[35mSection_title: 4.3  Question types by reasoning\n",
            "\u001b[35mSection_title: 4.4  Prompt types\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Settings\n",
            "\u001b[35mSection_title: 5.2  Models\n",
            "\u001b[35mSection_title: 5.3  Automatic Evaluation\n",
            "\u001b[35mSection_title: 5.4  Human Evaluation\n",
            "\u001b[35mSection_title: 5.5  Analysis of the effects of prompt\n",
            "\u001b[32mconclusion: in this paper, we propose khanq, a dataset for generating in-depth educational questions. each sample in khanq is carefully annotated as context, prompt, and question to form a clean dataset. we evaluate the performance of state-of-the-art question generation models on khanq. we find that although it is feasible for the model to generate fluent and complex questions, the ability to understand and reason over the context and the prompt is still far from reaching the human level.\n",
            "\u001b[35mSection_title: 6  Conclusions and Future Works\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Task Formulation\n",
            "\u001b[35mSection_title: 4  Approach\n",
            "\u001b[35mSection_title: 4.1  Knowledge-Intensive Tasks Unification\n",
            "\u001b[35mSection_title: 4.2  Web-based Knowledge Retrieval\n",
            "\u001b[35mSection_title: 4.2.1  LLM Knowledge Evaluation\n",
            "\u001b[35mSection_title: 4.2.2  Web Knowledge Retrieval\n",
            "\u001b[35mSection_title: 4.3  Knowledge-based Model Pre-training\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental\n",
            "\u001b[35mSection_title: 5.2  Main Results\n",
            "\u001b[35mSection_title: 5.3  Detailed Analysis\n",
            "\u001b[35mSection_title: 5.4  Case Study\n",
            "\u001b[32mconclusion: this paper presented a unified web-augmented framework for a wide range of knowledge-intensive tasks, called uniweb. we convert 16 tasks into a text-to-text generation task for training. we propose a search engine assisted learning method to selectively retrieve documents from the web through google search. furthermore, to reduce the discrepancy between the encoded and retrieved knowledge, we design a pre-training task, i.e., continual knowledge learning, to integrate the retrieved knowledge into llms. experiments on 16 tasks show the effectiveness of our web-augmented model compared to previous retrieval-augmented models. in future work, we will investigate the effect of web content in detail and consider applying our model to more types of downstream tasks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: for web-augmented models including our work, the deterioration of search results from search engine highlights the importance of deriving an effective method to interact with the huge web.search engines are often perceived as black-box and non-transparent for end users. therefore, many works proposed \"leaning to search\" to decompose complex questions into simpler queries, which may improve the performance of web-based models (nakano et al., 2021;komeili et al., 2021). in our model, we used a commercial search engine as the retriever to work with the whole web as a knowledge source. since the web is not curated and well-structured like wikipedia, we may encounter unexpected safety issues, including misinformation and harmful contents. while we have relied on the security control of the search engine, more attention should be paid to better understand the risks and provide effective ways to mitigate them. we hope our simple approach and strong results could encourage more future work by the community to tackle these questions. to encourage the community to investigate the question and ensure reproducibility, after the reviewing process, we will release the search urls used in our experiments.as for the potential concern, since we use the search engine to access real-time information, we do not have a tight control over retrieved results as traditional end-to-end retrieval (guu et al., 2020;lewis et al., 2020b). not only the changes of search engine logic, but also the newly published information, might create discrepancies over the course of time. this is also an issue we have to tackle to build a stable web-based solution for llms. • commonsense reasoning is intended to utilize commonsense knowledge to reason about certain aspects of the given text . therefore, we consider the given text as input and the prediction as output.• natural language inference is the task of determining whether the given \"hypothesis\" logically follows from the \"premise\" (storks et al., 2019).it acquires deep knowledge about the relationship between hypothesis and premise. we consider the premise as input and the hypothesis as output.for each category, we choose several representative tasks to construct our pretraining corpus. the detailed information of these included tasks is listed in table 6. to mitigate the huge disparity between dataset sizes, we follow  to use the temperature-scaled mixing strategy with a rate of t = 2 for setting the proportion of data coming from each task. during pretraining, for each task example, we use bm25 to retrieve top-10 passages from ccnet as our external knowledge. the input texts are concatenated with the retrieved passages using manually-written prompts. the final input is constructed in the following format: the \"option\" string is applied only when the input text is provided with several candidate answers. the blanks \"[passage n ]\" and \"[option n ]\" is filled with the retrieved passages and candidate answers. the blank \"[task instruction]\" aims to indicate the task for our model, which is task-specific and detailed in table 7.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Pretraining from Scratch\n",
            "\u001b[35mSection_title: 3.1  Pretraining\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  RQ1\n",
            "\u001b[35mSection_title: 4.2  RQ2: Do models pretrained in MSMARCO generalize well on other collections?\n",
            "\u001b[35mSection_title: 4.4  RQ4: Impact of Architectures\n",
            "\u001b[32mconclusion: foundation models come with the promise to be highly general and modular. it is believed that they contain a wide \"knowledge\" due to their pretraining on a large collection, which is then believed to be the source of their improved performance. we have examined how this pretraining collection influence the performance of ir models. our research question was to assess how much of this implicit knowledge, beneficial to the final performance, comes from pretraining on a large external collection. this is why we have experimented on a variety of collections, domains and languages to study how pretraining from scratch actually performed compared to their de facto approach of simple finetuning. while we were expecting the standard pretrained models to work better, we surprisingly revealed that pretraining from scratch works better for first-stage retrieval on msmarco, tripclick and several non-english languages on the mr.  tydi benchmark. in particular, the flops regularization played a critical role in those results, suggesting that regularization or better pretraining techniques could further improve the results. furthermore, pretrained models from scratch also behave well in the zero shot scenario for sparse models such as splade. nevertheless, pretraining from a large collection has a slight advantage when training rerankers. overall, these results, specific to ir, challenge the foundation model hypothesis for small models, ie that a more general model encapsulating the world knowledge would be better than a smaller one in a specific domain application. furthermore, our study makes a contribution to the debate between general purpose and specific purpose models. in a way, our experiments showed that less is more. in addition, pretrained language models come also with many challenges such as the societal bias in the data they have been trained on. we hope that our study could convince practitioners, both from industry and academia, to reconsider specific purpose models by pretraining from scratch. last but not least, doing so enable to better control efficiency, data bias and replicability, which are key research questions for the ir community.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Pretraining from Scratch\n",
            "\u001b[35mSection_title: 3.1  Pretraining\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  RQ1\n",
            "\u001b[35mSection_title: 4.2  RQ2: Do models pretrained in MSMARCO generalize well on other collections?\n",
            "\u001b[35mSection_title: 4.4  RQ4: Impact of Architectures\n",
            "\u001b[32mconclusion: foundation models come with the promise to be highly general and modular. it is believed that they contain a wide \"knowledge\" due to their pretraining on a large collection, which is then believed to be the source of their improved performance. we have examined how this pretraining collection influence the performance of ir models. our research question was to assess how much of this implicit knowledge, beneficial to the final performance, comes from pretraining on a large external collection. this is why we have experimented on a variety of collections, domains and languages to study how pretraining from scratch actually performed compared to their de facto approach of simple finetuning. while we were expecting the standard pretrained models to work better, we surprisingly revealed that pretraining from scratch works better for first-stage retrieval on msmarco, tripclick and several non-english languages on the mr.  tydi benchmark. in particular, the flops regularization played a critical role in those results, suggesting that regularization or better pretraining techniques could further improve the results. furthermore, pretrained models from scratch also behave well in the zero shot scenario for sparse models such as splade. nevertheless, pretraining from a large collection has a slight advantage when training rerankers. overall, these results, specific to ir, challenge the foundation model hypothesis for small models, ie that a more general model encapsulating the world knowledge would be better than a smaller one in a specific domain application. furthermore, our study makes a contribution to the debate between general purpose and specific purpose models. in a way, our experiments showed that less is more. in addition, pretrained language models come also with many challenges such as the societal bias in the data they have been trained on. we hope that our study could convince practitioners, both from industry and academia, to reconsider specific purpose models by pretraining from scratch. last but not least, doing so enable to better control efficiency, data bias and replicability, which are key research questions for the ir community.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  The QC4QA Framework\n",
            "\u001b[35mSection_title: 3.2.1  Overall Framework\n",
            "\u001b[35mSection_title: 3.2.2  Question Classification\n",
            "\u001b[35mSection_title: 3.2.3  Pseudo Labeling & Sampling\n",
            "\u001b[35mSection_title: 3.2.4  Self-supervised Adaptation\n",
            "\u001b[35mSection_title: 3.2.5  Overall Objective\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Baselines\n",
            "\u001b[35mSection_title: 4.2  Training and Evaluation\n",
            "\u001b[35mSection_title: 4.3  Main Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[35mSection_title: 4.4.1  Question Classification\n",
            "\u001b[35mSection_title: 4.4.2  Distribution-aware Sampling\n",
            "\u001b[35mSection_title: 4.4.3  Sensitivity of Hyperparameter λ\n",
            "\u001b[35mSection_title: 4.4.4  Confidence Threshold in Pseudo labeling\n",
            "\u001b[32mconclusion: in this paper, we propose a novel framework for qa domain adaptation. the proposed qc4qa combines question classification with self-supervised adaptation techniques. qc4qa leverages question classes to reduce domain discrepancies and resemble target data distribution in training. different from existing works, qc4qa achieves superior performance by introducing a simple question classifier and incorporating the question class information in the training objective. we demonstrate the efficiency and effectiveness of qc4qa compared to state-of-the-art approaches by achieving a substantially better performance on multiple datasets.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Task Definition and Data Collection\n",
            "\u001b[35mSection_title: 3.1  Questions and Evidence Documents\n",
            "\u001b[35mSection_title: 3.2  Human Identification of Answers\n",
            "\u001b[35mSection_title: 3.3  Data Statistics\n",
            "\u001b[35mSection_title: 4  Evaluation of Annotation Quality\n",
            "\u001b[35mSection_title: 4.1  Preliminaries: The Sampling Distribution\n",
            "\u001b[35mSection_title: 4.2  Expert Evaluations of Correctness\n",
            "\u001b[35mSection_title: 4.3  Results for Precision Measurements\n",
            "\u001b[35mSection_title: 4.4  Variability of Annotations\n",
            "\u001b[35mSection_title: 5  Evaluation Measures\n",
            "\u001b[35mSection_title: 5.1  Definition of an Evaluation Measure Based on 5-Way Annotations\n",
            "\u001b[35mSection_title: 5.2  Super-Annotator Upper Bound\n",
            "\u001b[35mSection_title: 6  Baseline Performance\n",
            "\u001b[35mSection_title: 6.1  Untrained Baselines\n",
            "\u001b[35mSection_title: 6.2  Document-QA\n",
            "\u001b[35mSection_title: 6.3  Custom Pipeline (DecAtt + DocReader)\n",
            "\u001b[35mSection_title: 6.4  Results\n",
            "\u001b[32mconclusion: we argue that progress on qa has been hindered by a lack of appropriate training and test data. to address this, we present the natural questions corpus. this is the first large publicly available data set to pair real user queries with high-quality annotations of answers in documents. we also present metrics to be used with nq, for the purposes of evaluating the performance of question answering systems. we demonstrate a high upper bound on these metrics and show that existing methods do not approach this upper bound. we argue that for them to do so will require significant advances in nlu. figure 5 shows example questions from the data set. figure 6 shows example question/answer pairs from the data set, together with expert judgments and statistics from the 25-way annotations.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Task Variations and Existing Datasets\n",
            "\u001b[35mSection_title: 2.2  Benchmarking Issues\n",
            "\u001b[35mSection_title: 3  Reading Comprehension from\n",
            "\u001b[35mSection_title: 3.1  Computational Model in Psychology\n",
            "\u001b[35mSection_title: 3.2  Skill Hierarchy for MRC\n",
            "\u001b[35mSection_title: 4  MRC on Psychometrics\n",
            "\u001b[35mSection_title: 4.1  Construct Validity in Psychometrics\n",
            "\u001b[35mSection_title: 4.2  Construct Validity in MRC\n",
            "\u001b[35mSection_title: 2.  Substantive\n",
            "\u001b[35mSection_title: 5.  External\n",
            "\u001b[35mSection_title: 6.  Consequential\n",
            "\u001b[35mSection_title: 5  Future Directions\n",
            "\u001b[35mSection_title: 5.1  What Question: Situation Model\n",
            "\u001b[35mSection_title: 5.1.1  Context-dependent Situations\n",
            "\u001b[35mSection_title: 5.1.2  Grounding to Other Media\n",
            "\u001b[35mSection_title: 5.2  How Question: Substantive Validity\n",
            "\u001b[35mSection_title: 5.2.1  Shortcut-proof Questions\n",
            "\u001b[35mSection_title: 5.2.2  Explanation by Design\n",
            "\u001b[32mconclusion: in this paper, we outlined current issues and future directions for benchmarking machine reading comprehension. we visited the psychology study to analyze what we should ask of reading comprehension and the construct validity in psychometrics to analyze how we should correctly evaluate it. we deduced that future datasets should evaluate the capability of the situation model for understanding context-dependent situations and for grounding to non-textual information and ensure the substantive validity by creating shortcut-proof questions and designing an explanatory task formulation. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Definition of \"understanding\"\n",
            "\u001b[35mSection_title: 3.  Benchmarks\n",
            "\u001b[35mSection_title: 3.1.  Image analytics benchmarks\n",
            "\u001b[35mSection_title: 3.2.  Natural language processing (NLP) benchmarks\n",
            "\u001b[35mSection_title: 3.3.  Visual Question Answering (VQA) benchmarks\n",
            "\u001b[35mSection_title: 3.4.  Commonsense inference benchmarks\n",
            "\u001b[35mSection_title: 3.5.  Commonsense inference benchmarks that need Natural Language Processing\n",
            "\u001b[35mSection_title: 4.  Research streams\n",
            "\u001b[35mSection_title: 4.1.  Visual Question Answering\n",
            "\u001b[35mSection_title: 4.2.  Compositional and hierarchical representations\n",
            "\u001b[35mSection_title: 4.2.1.  Hierarchical representations in multi-modal data\n",
            "\u001b[35mSection_title: 4.2.2.  Tree-LSTMs and tree structures\n",
            "\u001b[35mSection_title: 4.2.3.  Hyperbolic embeddings\n",
            "\u001b[35mSection_title: 4.2.4.  Knowledge graph learning\n",
            "\u001b[35mSection_title: 4.3.  Scene and document understanding\n",
            "\u001b[35mSection_title: 4.3.1.  Object detection and instance segmentation\n",
            "\u001b[35mSection_title: 4.3.2.  Inverse graphics\n",
            "\u001b[35mSection_title: 4.3.3.  Image decomposition\n",
            "\u001b[35mSection_title: 4.3.4.  Document understanding\n",
            "\u001b[35mSection_title: 4.4.  Neuro-symbolic computing\n",
            "\u001b[35mSection_title: 4.4.1.  Neuro-symbolic reasoning\n",
            "\u001b[35mSection_title: 4.4.2.  Program synthesis\n",
            "\u001b[35mSection_title: 4.5.  Commonsense inference\n",
            "\u001b[35mSection_title: 5.  Discussion\n",
            "\u001b[35mSection_title: 5.1.  AI that understands capabilities\n",
            "\u001b[35mSection_title: 5.2.  Benchmarks and understanding\n",
            "\u001b[35mSection_title: 5.3.  Research streams\n",
            "bottom\n",
            "\u001b[32mconclusion: the paper considers the components an artificial intelligence system that understands should have. that is, a system that not only learns statistical relationships within the data, but is capable of forming a human-like understanding ✓ ✗ ✗ ✗ table 1: cap1: hierarchical and compositional knowledge representation, cap2: multimodal structure-to-structure mapping, cap3: integrates symbolic and non-symbolic knowledge, cap4: supports symbolic reasoning with uncertainties of the input data. this is most certainly a truly difficult problem to solve and the purpose of the paper is not to claim a general solution for solving it, but to look at several research streams and some of their latest developments. furthermore, several benchmarks are described, which have been used to study certain characteristics of an ai that understands. the work also contributes to a growing interest in artificial intelligence systems that are interpretable and transparent; properties that are crucial in domains, such as medical diagnosis.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  QUADRo\n",
            "\u001b[35mSection_title: 4  Database and Dataset Construction\n",
            "\u001b[35mSection_title: 4.1  The Database\n",
            "\u001b[35mSection_title: 4.2  Question Ranking dataset\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Model Training\n",
            "\u001b[35mSection_title: 5.3  Metrics\n",
            "\u001b[35mSection_title: 5.4  Answer Retrieval and Ranking Performance\n",
            "\u001b[35mSection_title: 5.5  Retrieval and reranker evaluation\n",
            "\u001b[35mSection_title: 5.6  Comparison with Web-based QA\n",
            "\u001b[35mSection_title: 5.7  Ablated end-to-end comparisons\n",
            "\u001b[32mconclusion: in this paper, we have described our study to scale qa-based on q/a db to open domain applications. this required to build a large db, which we built only using publicly available q/a pairs, reaching a significant size of ≈ 6.3m items. to enable retrieval from these large dbs, inspired by the latest neural ir technology, we modeled neural retrieval for q/a pairs. we proposed two different methods based on only questions, and on q/a pairs, where questions and answers can be seen as context. we analyzed the significant impact of using architectures with separate encoder versus dual encoders, in accuracy and efficiency.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Task Generalization with\n",
            "\u001b[35mSection_title: 2.2  Source Task Retrieval\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Training Source Prompt Embeddings\n",
            "\u001b[35mSection_title: 3.2  Zero-Shot Embedding Retrieval\n",
            "\u001b[35mSection_title: 3.3  Interpolation of Prompt Embeddings\n",
            "\u001b[35mSection_title: 3.4  Variance-based Ranking\n",
            "\u001b[35mSection_title: 4  Experimental Settings\n",
            "\u001b[35mSection_title: 4.1  Source Tasks\n",
            "\u001b[35mSection_title: 4.2  Evaluation Tasks\n",
            "\u001b[35mSection_title: 4.3  Baseline Models\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 6  Analysis of ROSPR\n",
            "\u001b[35mSection_title: 7  Ablation Studies\n",
            "\u001b[32mconclusion: in this paper, we introduce rospr, a method that efficiently enhances zero-shot generalization capabilities of a meta-trained lm by retrieving promptspecific source prompt embeddings (soft prompts) for a given target task.we accomplish this by first training the soft prompts for hard prompt of the source tasks.after training source prompt embeddings, we construct the source prompt library by storing the mean representation of training instances as keys and the corresponding prompt embeddings as values.at inference, we search for training instances stored in the library similar to sample instances from the target task, retrieve the corresponding prompt embedding, select the most frequently retrieved embedding, and append it to each of the target task instances for prediction.our results show that rospr efficiently enhances the zero-shot performance of the backbone model while introducing minimal additional parameters during inference.we additionally provide analysis of which factors attribute to the performance of rospr and find that heuristic cues such as the answer choice format are critical for generalization performance, implying that it may play a role similar to demonstrations in in-context learning.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: although we show the effectiveness of rospr by applying it on t0-3b (sanh et al., 2021), we did not evaluate our method on different model scales such as the t0-11b variant and other lm architectures such as decoder-only lms due to limited computational resources.this leaves future works on applying rospr to even larger lms and diverse lm architectures (wang et al., 2022a).moreover, it is hard to apply var to target tasks without answer choices such as free-form generation because variance among options cannot be obtained.however, rospr and rospr+inter can still be utilized and we leave applying rospr on zero-shot task location of free-form generation as future work (scialom et al., 2022).\n",
            "\u001b[35mSection_title: 9  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The FOLLOWUPQG Dataset\n",
            "\u001b[35mSection_title: 3.1  Data Sources\n",
            "\u001b[35mSection_title: 3.2  Data Collection\n",
            "\u001b[35mSection_title: 3.3  Crowd-Sourced Data Validation\n",
            "\u001b[35mSection_title: 4  Data Analysis\n",
            "\u001b[35mSection_title: 4.1  Categories of follow-up questions\n",
            "\u001b[35mSection_title: 4.2  Comparison with existing datasets\n",
            "\u001b[35mSection_title: 5  Follow-up Question Generation\n",
            "\u001b[35mSection_title: 5.1  Models\n",
            "\u001b[35mSection_title: 5.3  Human Evaluation\n",
            "\u001b[35mSection_title: 6  Controllable Follow-up QG\n",
            "\u001b[35mSection_title: 6.1  Experimental Setting\n",
            "\u001b[35mSection_title: 6.2  Result Analysis\n",
            "\u001b[35mSection_title: 6.3  Case Study\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Explanation Datasets\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Candidate Chain Construction\n",
            "\u001b[35mSection_title: 3.3  eQASC -Explanations for QASC\n",
            "\u001b[35mSection_title: 3.4  eQASC-perturbed -Testing Robustness\n",
            "\u001b[35mSection_title: 3.5  eOBQA -Testing Generalization\n",
            "\u001b[35mSection_title: 4.1  Chain Representation\n",
            "\u001b[35mSection_title: 4.2  Model Training\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Metrics\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Results: Performance on eQASC\n",
            "\u001b[35mSection_title: 5.4  Results:Consistency in eQASC-perturbed\n",
            "\u001b[35mSection_title: 5.5  Results: Generalization to eOBQA\n",
            "\u001b[35mSection_title: 6  Analysis and Discussions\n",
            "\u001b[35mSection_title: 6.1  GRC as Explicit Reasoning Rationale\n",
            "\u001b[35mSection_title: 6.2  Error Analysis\n",
            "\u001b[35mSection_title: 6.3  Chains for Negative Answer Options\n",
            "\u001b[32mlimitation: our focus in this paper has been on recognizing valid chains of reasoning, assuming a retrieval step that retrieves a reasonable pool of candidates to start with (section 3.2). however, the retrieval step itself is not perfect: for qasc, designed so that at least one valid chain always exists, the retrieved pool of 10 contains no valid chains for 24% of the questions (upper bound in table 2), capping the overall system's performance. to gauge the performance of our model when coupled with an improved retrieval system, we ran an experiment where, at test time, we explicitly add the gold chain to the candidate pool if it does not get retrieved (and even if there is some other valid chain already in the pool). we find the p@1 score rises from 0.54 (table 2) to 0.82 (upper bound is now 1.0). this indicates the model scoring algorithm is performing well, and that improving the retrieval system, e.g., by considering may more chains per question or modifying the search algorithm itself, is likely to have the biggest impact on improving the overall system. note also that the corpus itself is an important component: finding valid chains requires the corpus to contain a broad diversity of general facts to build chains from, hence expanding/filtering the corpus itself is another avenue for improvement.\n",
            "\u001b[35mSection_title: 6.4  Limitations of Retrieval\n",
            "\u001b[35mSection_title: 6.5  Future Directions\n",
            "\u001b[32mconclusion: explaining answers to multihop questions is important for understanding why an answer may be correct, but there is currently a dearth of suitable, annotated data. to address this, and promote progress in explanation, we contribute three new explanation datasets, including one with over 98k annotated reasoning chains -by far the largest repository of annotated, corpus-derived explanations to date. we also have shown this data can significantly improve explanation quality on both in-domain (qasc) and out-of-domain (obqa) tasks. finally, we have proposed and explored using a lightweight method to achieve a delexicalized representation of reasoning chains. while preserving explanation quality (despite removing details), this representation appears to be more robust to certain perturbations.\n",
            "\u001b[35mSection_title: 7  Summary and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  ECG Indices for SCD Risk Stratification\n",
            "\u001b[35mSection_title: 2.2.  Clinical Interoperability Standards\n",
            "\u001b[35mSection_title: 3.  The Proposed Interoperable System and Results\n",
            "\u001b[35mSection_title: 3.1.  System Specifications and Functional Scheme\n",
            "\u001b[35mSection_title: 3.2.  Subdomain Definitions for CRS\n",
            "\u001b[35mSection_title: 3.3.  Database Description\n",
            "\u001b[35mSection_title: 3.4.  System Validation\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[32mconclusion: close cooperation with cardiologists was assessed to define the chosen cardiac risk subdomains. in the long term, the system will allow one to not only include other indices but also work on a large scale of a number of patients. achieving multicentric connectivity in different hospitals will represent an increase in the scale of collected data. this approach paves the way toward intelligent systems that will take advantage of the current state of knowledge and advances on machine learning models and big data systems so that the crs in large patient databases can be analyzed.\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Retrieval Augmentation\n",
            "\u001b[35mSection_title: 2.2  Cooperation with Black-box LLMs\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Rewrite-Retrieve-Read\n",
            "\u001b[35mSection_title: 3.2  Trainable Scheme\n",
            "\u001b[35mSection_title: 3.2.1  Rewriter Warm-up\n",
            "\u001b[35mSection_title: 3.2.2  Reinforcement Learning\n",
            "\u001b[35mSection_title: 4  Implementation\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Task Settings\n",
            "\u001b[35mSection_title: 5.1.1  Open-domain QA\n",
            "\u001b[35mSection_title: 5.1.2  Multiple-choice QA\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 6.1  Training Process\n",
            "\u001b[35mSection_title: 6.2  Retrieval Result\n",
            "\u001b[35mSection_title: 6.3  Case Study\n",
            "\u001b[32mconclusion: this paper introduces the rewrite-retrieve-read pipeline, where a query rewriting step is added for the retrieval-augmented llm.this approach is applicable for adopting a frozen large language model as the reader and a real-time web search engine as the retriever.further, we propose to apply a tuneable small language model the rewriter, which can be trained to cater to the frozen retriever and reader.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Setup\n",
            "\u001b[35mSection_title: 2.1  Generalizablility Across Reasoning Types\n",
            "\u001b[35mSection_title: 2.2  Selective Prediction\n",
            "\u001b[35mSection_title: 3  Mixture of Reasoning Experts\n",
            "\u001b[35mSection_title: 3.1  Specialized Reasoning Experts\n",
            "\u001b[35mSection_title: 3.2  Ensembling via Answer Selection\n",
            "\u001b[35mSection_title: 4  Sanity Check: MORE Improves Generalizability\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Specialization and Loss of Generalizability\n",
            "\u001b[35mSection_title: 4.3  MORE Improves Generalizability\n",
            "\u001b[35mSection_title: 4.4  Question-Only Routing\n",
            "\u001b[35mSection_title: 5  MORE Improves Selective QA\n",
            "\u001b[35mSection_title: 5.1  Automatic Abstention\n",
            "\u001b[35mSection_title: 5.2  Human Abstention\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we proposed the more framework where we construct a pool of specialized qa models that excel at different reasoning types, and then train an answer selector to select the best answer among them.experiments on 12 datasets covering four reasoning types demonstrate that more achieve better generalizability than all baselines.more importantly, the inter-expert agreement features in more offer useful signals for training effective calibrators that improve selective qa and also improve human verification of the system's final predictions.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 1.1  Related Work\n",
            "\u001b[35mSection_title: 2.  BigBird Architecture\n",
            "\u001b[35mSection_title: 3.  Theoretical Results about Sparse Attention Mechanism\n",
            "\u001b[35mSection_title: 3.1  Notation\n",
            "\u001b[35mSection_title: 3.2  Universal Approximators\n",
            "\u001b[35mSection_title: 3.3  Turing Completeness\n",
            "\u001b[32mlimitation: we demonstrate a natural task which can be solved by the full attention mechanism in o(1)layers. however, under standard complexity theoretic assumptions, this problem requires ω(n)-layers for any sparse attention layers withõ(n) edges (not just bigbird). (hereõ hides poly-logarthmic factors.)consider the simple problem of finding the corresponding furthest vector for each vector in the given sequence of length n. formally, task 1. given n unit vectors {u 1 , . . . , u n }, find f (u 1 , . . . , u n ) → (u 1 * , . . . , u n * ) where for a fixed j ∈ [n], we define j * = arg max k u k − u j 2 2 . finding vectors that are furthest apart boils down to minimize inner product search in case of unit vectors. for a full-attention mechanism with appropriate query and keys, this task is very easy as we can evaluate all pair-wise inner products.the impossibility for sparse-attention follows from hardness results stemming from orthogonal vector conjecture(ovc) [1,2,7,97]. the ovc is a widely used assumption in fine-grained complexity. informally, it states that one cannot determine if the minimum inner product among n boolean vectors is 0 in subquadratic time. in app. c, we show a reduction using ovc to show that if a transformer g ∈ t h=1,m=2d,q=0 d for any sparse directed graph d can evaluate the task 1, it can solve the orthogonal vector problem. proposition 1. there exists a single layer full self-attention g ∈ t h=1,m=2d,q=0 that can evaluate task 1, i.e. g(u 1 , ..., u n ) = [u 1 * , . . . , u n * ], but for any sparse-attention graph d with o(n) edges (i.e. inner product evaluations), would requireω(n 1−o(1) ) layers.we give a formal proof of this fact in app. c.\n",
            "\u001b[35mSection_title: 3.4  Limitations\n",
            "\u001b[35mSection_title: 4.  Experiments: Natural Language Processing\n",
            "\u001b[35mSection_title: 4.1  Encoder Only Task\n",
            "\u001b[35mSection_title: 4.2  Encoder-Decoder Tasks\n",
            "\u001b[35mSection_title: 5.  Experiments: Genomics\n",
            "\u001b[32mconclusion: we propose bigbird: a sparse attention mechanism that is linear in the number of tokens.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\u001b[35mSection_title: 2.  A cross-attention mechanism\n",
            "\u001b[35mSection_title: 3.  Global attention:\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  WE BAR E N A : WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS\n",
            "\u001b[35mSection_title: 2.1  CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE\n",
            "\u001b[35mSection_title: 2.2  WEBSITE SELECTION\n",
            "\u001b[35mSection_title: 2.3  OBSERVATION SPACE\n",
            "\u001b[35mSection_title: 2.4  ACTION SPACE\n",
            "\u001b[35mSection_title: 3  BENCHMARK SUITE OF WEB-BASED TASKS\n",
            "\u001b[35mSection_title: 3.1  INTENT COLLECTION\n",
            "\u001b[35mSection_title: 3.2  EVALUATION ANNOTATION\n",
            "\u001b[35mSection_title: 4  BASELINE WEB AGENTS\n",
            "\u001b[35mSection_title: 5  RESULTS\n",
            "\u001b[35mSection_title: 5.1  MAIN RESULTS\n",
            "\u001b[35mSection_title: 5.2  ANALYSIS\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[32mconclusion: we present webarena, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents.webarena includes fully functional web applications and genuine data from four major categories, providing a realistic platform for agent interaction.it further supports a wide range of tools and external knowledge bases, fostering a focus on human-like problem-solving.additionally, we curate a comprehensive benchmark consisting of 812 examples that focus on translating high-level natural language intents into specific web interactions.we also offer metrics to programmatically ascertain whether tasks have been completed according to the desired objectives.our experiments show that even gpt-4 only achieves a limited end-to-end task success rate of 14.41%, significantly lagging behind the human performance of 78.24%.these findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within webarena environment.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Extended Transformer Construction\n",
            "\u001b[35mSection_title: 3.1  Relative Position Encoding\n",
            "\u001b[35mSection_title: 3.2  Global-Local Attention\n",
            "\u001b[35mSection_title: 3.3  Handling Long Inputs with Global-Local Attention\n",
            "\u001b[35mSection_title: 3.4  Handling Structured Inputs with Global-Local Attention\n",
            "\u001b[35mSection_title: 3.5  Pre-training Tasks\n",
            "\u001b[35mSection_title: 3.6  Lifting Weights from Existing Models\n",
            "\u001b[35mSection_title: 4  Empirical Evaluation\n",
            "\u001b[35mSection_title: 4.1  Data Set\n",
            "\u001b[35mSection_title: 4.2  Training Configuration\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[32mconclusion: this paper introduced the extended transformer construction, or etc, a novel extension of the original transformer model designed specifically to (1) scale up the input length to sequences longer than 512 tokens (scaling linearly in the size of the input), and (2) allow ingesting structured inputs. etc also allows lifting weights from existing bert models, saving significant computational resources while training. the key ideas that enable etc to achieve these are a new global-local attention mechanism, coupled with relative position encodings.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Creating the Dataset from ELI5\n",
            "\u001b[35mSection_title: 3.2  Dataset Analysis\n",
            "\u001b[35mSection_title: 4  Evaluation Methods\n",
            "\u001b[35mSection_title: 5  Models\n",
            "\u001b[35mSection_title: 5.1  Extractive and Retrieval Models\n",
            "\u001b[35mSection_title: 5.2  Abstractive Models\n",
            "\u001b[35mSection_title: 6.1  Overview of Model Performance\n",
            "\u001b[35mSection_title: 6.2  Quantitative and Qualitative Analysis\n",
            "\u001b[32mconclusion: we introduce the first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers. we show that abstractive models generate coherent answers and are competitive with extractive models in human evaluation. proposed models are far from human performance, in part due to the inability to exploit the long full web text. we hope eli5 will inspire future work in all aspects of long-form qa, from the information extraction problem of obtaining information from long, multi-document input to generating more coherent and accurate paragraph-length answers.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A Naive Interpretation of RNAi in Minsky Register Machine\n",
            "\u001b[35mSection_title: 3  RNAi as Chemical Reaction and Register Machines\n",
            "\u001b[35mSection_title: 4  Recursive RNAi and Probabilistic Termination\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  NEURAL MODULE NETWORKS\n",
            "\u001b[35mSection_title: 2.1  COMPONENTS OF A NMN FOR TEXT\n",
            "\u001b[35mSection_title: 2.2  LEARNING CHALLENGES IN NMN FOR TEXT\n",
            "\u001b[35mSection_title: 3  MODULES FOR REASONING OVER TEXT\n",
            "\u001b[35mSection_title: 3.1  DATA TYPES\n",
            "\u001b[35mSection_title: 3.2  NEURAL MODULES FOR QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 4  AUXILIARY SUPERVISION\n",
            "\u001b[35mSection_title: 4.1  UNSUPERVISED AUXILIARY LOSS FOR IE\n",
            "\u001b[35mSection_title: 4.2  QUESTION PARSE AND INTERMEDIATE MODULE OUTPUT SUPERVISION\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  DATASET\n",
            "\u001b[35mSection_title: 5.2  RESULTS\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[35mSection_title: 7  FUTURE DIRECTIONS\n",
            "\u001b[32mconclusion: we show how to use neural module networks to answer compositional questions requiring symbolic reasoning against natural language text. we define probabilistic modules that propagate uncertainty about symbolic reasoning operations in a way that is end-to-end differentiable. additionally, we show that injecting inductive bias using unsupervised auxiliary losses significantly helps learning.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Our Method: Visconde\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  IIRC\n",
            "\u001b[35mSection_title: 4.2  Qasper\n",
            "\u001b[35mSection_title: 4.3  StrategyQA\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[32mconclusion: this paper describes a system for multi-document question answering that uses a passage reranker to retrieve documents and large language models to reason over them and compose an answer. our system rivals state-of-the-art supervised models in three datasets: iirc, qasper, and strategyqa. our results suggest that using gpt-3 as a reader is close to human-level performance as long as relevant passages are provided, while current retrievers are the main bottleneck. we also show that inducing the model to give explanations before answering a question improves effectiveness.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Input Encoding\n",
            "\u001b[35mSection_title: 2.2  Memory Graph Network\n",
            "\u001b[35mSection_title: 2.3  Module Networks\n",
            "\u001b[35mSection_title: 2.4  Answer Decoding\n",
            "\u001b[35mSection_title: 3  Dataset\n",
            "\u001b[35mSection_title: 4  Empirical Evaluation\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we introduce episodic memory qa, the task of answering personal user questions grounded on memory graph (mg). the dataset is generated with synthetic memory graphs with simulated attributes, and accompanied with 100k qa pairs composed via bootstrapped scripts and manual annotations. several novel model components are proposed for unique challenges for the episodic memory qa: 1) memory graph networks (mgn) extends the conventional memory networks by enabling dynamic expansion of memory slots through graph traversals, which also naturally allows for explainable predictions. 2) several neural module networks are proposed for the proposed task, each of which takes queries and memory graphs as input to infer answers. 3) the main episodic memory qa net aggregates answer prediction from each neural module to generate final answer candidates. the empirical results demonstrate the efficacy of the proposed model in the memory qa reasoning.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Main contributions of this survey for QA\n",
            "\u001b[35mSection_title: 3  The Question Answering Framework\n",
            "\u001b[35mSection_title: 3.1  Question Answering Algorithms\n",
            "\u001b[35mSection_title: 3.2  Knowledge Source\n",
            "\u001b[35mSection_title: 3.2.1  Closed-domain knowledge source\n",
            "\u001b[35mSection_title: 3.2.2  Open-domain knowledge source\n",
            "\u001b[35mSection_title: 3.3  Question and Answer Types\n",
            "\u001b[35mSection_title: 3.4  Definition of a question answering system\n",
            "\u001b[35mSection_title: 4  Question Answering Paradigms\n",
            "\u001b[35mSection_title: 4.1  Information Retrieval-Based Question Answering\n",
            "\u001b[35mSection_title: 4.1.1  Document-retriever\n",
            "\u001b[35mSection_title: 4.1.2  Document-reader\n",
            "\u001b[35mSection_title: 4.2  Knowledge Base Question Answering\n",
            "\u001b[35mSection_title: 4.3  Generative Question Answering\n",
            "\u001b[35mSection_title: 5  Benchmark Data and Evaluation Scores\n",
            "\u001b[35mSection_title: 5.1  Benchmark Datasets\n",
            "\u001b[35mSection_title: 5.2  Taxonomy of Evaluation Scores\n",
            "\u001b[35mSection_title: 6  Quantitative error scores\n",
            "\u001b[35mSection_title: 6.1  Simple Untrained Automatic Evaluation Scores (S-UAES)\n",
            "\u001b[35mSection_title: 6.2  Advanced Untrained Automatic Evaluation Scores (A-UAES)\n",
            "\u001b[35mSection_title: 6.2.1  Bilingual Evaluation Understudy (BLEU)\n",
            "\u001b[35mSection_title: 6.2.2  National Institute of Standards and Technology (NIST)\n",
            "\u001b[35mSection_title: 6.2.3  Recall-Oriented Understudy for Gisting Evaluation (ROUGE)\n",
            "\u001b[35mSection_title: 6.2.4  Metric for Evaluation of Translation with Explicit\n",
            "\u001b[35mSection_title: 6.3  Machine-trained Evaluation Scores (MTES)\n",
            "\u001b[35mSection_title: 6.3.1  Automatic Dialogue Evaluation Model (ADEM)\n",
            "\u001b[35mSection_title: 6.3.2  Referenced metric and Unreferenced metric Blended\n",
            "\u001b[35mSection_title: 6.3.3  Metric for automatic Unreferenced dialogue evaluation (MaUdE)\n",
            "\u001b[35mSection_title: 6.3.4  Regressor Using Sentence Embeddings (RUSE)\n",
            "\u001b[35mSection_title: 6.3.5  BLEURT\n",
            "\u001b[35mSection_title: 6.3.6  BERTScore\n",
            "\u001b[35mSection_title: 6.3.7  Learning-based Composite Metrics\n",
            "\u001b[35mSection_title: 7  Challenges for evaluation scores\n",
            "\u001b[35mSection_title: 7.1  Drawback of word-overlap scores\n",
            "\u001b[35mSection_title: 7.2  Drawback of learning-based scores\n",
            "\u001b[35mSection_title: 7.3  Drawback of scores based on a pre-trained model\n",
            "\u001b[35mSection_title: 7.4  Drawback of scores based on human judgement\n",
            "\u001b[35mSection_title: 7.5  Major problem\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\u001b[32mconclusion: the question-answering task is among the oldest challenges in artificial intelligence and it is still one of the most important tasks in natural language processing to this day as it enables humans to interact with a machine in a natural way. for dealing with domain-specific information and different formats of data which could be either structured or unstructured data there are three main paradigms for constructing qa systems: (1) information retrieval-based question answering, (2) knowledge base question answering, and (3) generative question answering.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Main contributions of this survey for QA\n",
            "\u001b[35mSection_title: 3  The Question Answering Framework\n",
            "\u001b[35mSection_title: 3.1  Question Answering Algorithms\n",
            "\u001b[35mSection_title: 3.2  Knowledge Source\n",
            "\u001b[35mSection_title: 3.2.1  Closed-domain knowledge source\n",
            "\u001b[35mSection_title: 3.2.2  Open-domain knowledge source\n",
            "\u001b[35mSection_title: 3.3  Question and Answer Types\n",
            "\u001b[35mSection_title: 3.4  Definition of a question answering system\n",
            "\u001b[35mSection_title: 4  Question Answering Paradigms\n",
            "\u001b[35mSection_title: 4.1  Information Retrieval-Based Question Answering\n",
            "\u001b[35mSection_title: 4.1.1  Document-retriever\n",
            "\u001b[35mSection_title: 4.1.2  Document-reader\n",
            "\u001b[35mSection_title: 4.2  Knowledge Base Question Answering\n",
            "\u001b[35mSection_title: 4.3  Generative Question Answering\n",
            "\u001b[35mSection_title: 5  Benchmark Data and Evaluation Scores\n",
            "\u001b[35mSection_title: 5.1  Benchmark Datasets\n",
            "\u001b[35mSection_title: 5.2  Taxonomy of Evaluation Scores\n",
            "\u001b[35mSection_title: 6  Quantitative error scores\n",
            "\u001b[35mSection_title: 6.1  Simple Untrained Automatic Evaluation Scores (S-UAES)\n",
            "\u001b[35mSection_title: 6.2  Advanced Untrained Automatic Evaluation Scores (A-UAES)\n",
            "\u001b[35mSection_title: 6.2.1  Bilingual Evaluation Understudy (BLEU)\n",
            "\u001b[35mSection_title: 6.2.2  National Institute of Standards and Technology (NIST)\n",
            "\u001b[35mSection_title: 6.2.3  Recall-Oriented Understudy for Gisting Evaluation (ROUGE)\n",
            "\u001b[35mSection_title: 6.2.4  Metric for Evaluation of Translation with Explicit\n",
            "\u001b[35mSection_title: 6.3  Machine-trained Evaluation Scores (MTES)\n",
            "\u001b[35mSection_title: 6.3.1  Automatic Dialogue Evaluation Model (ADEM)\n",
            "\u001b[35mSection_title: 6.3.2  Referenced metric and Unreferenced metric Blended\n",
            "\u001b[35mSection_title: 6.3.3  Metric for automatic Unreferenced dialogue evaluation (MaUdE)\n",
            "\u001b[35mSection_title: 6.3.4  Regressor Using Sentence Embeddings (RUSE)\n",
            "\u001b[35mSection_title: 6.3.5  BLEURT\n",
            "\u001b[35mSection_title: 6.3.6  BERTScore\n",
            "\u001b[35mSection_title: 6.3.7  Learning-based Composite Metrics\n",
            "\u001b[35mSection_title: 7  Challenges for evaluation scores\n",
            "\u001b[35mSection_title: 7.1  Drawback of word-overlap scores\n",
            "\u001b[35mSection_title: 7.2  Drawback of learning-based scores\n",
            "\u001b[35mSection_title: 7.3  Drawback of scores based on a pre-trained model\n",
            "\u001b[35mSection_title: 7.4  Drawback of scores based on human judgement\n",
            "\u001b[35mSection_title: 7.5  Major problem\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\u001b[32mconclusion: the question-answering task is among the oldest challenges in artificial intelligence and it is still one of the most important tasks in natural language processing to this day as it enables humans to interact with a machine in a natural way. for dealing with domain-specific information and different formats of data which could be either structured or unstructured data there are three main paradigms for constructing qa systems: (1) information retrieval-based question answering, (2) knowledge base question answering, and (3) generative question answering.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Ranking of Candidate Justification Sets\n",
            "\u001b[35mSection_title: 3.2  Answer Classification\n",
            "\u001b[35mSection_title: 4  Empirical Evaluation\n",
            "\u001b[35mSection_title: 4.1  Justification Results\n",
            "\u001b[35mSection_title: 4.2  Question answering results\n",
            "\u001b[35mSection_title: 4.3  Domain Robustness Analysis\n",
            "\u001b[35mSection_title: 4.4  Ablation Analysis\n",
            "\u001b[35mSection_title: 4.5  Error Analysis\n",
            "\u001b[35mSection_title: 4.6  Alignment ROCC\n",
            "\u001b[32mconclusion: we introduced rocc, a simple unsupervised approach for selecting justification sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer. we coupled this method with a state-of-the-art bert-based supervised question answering system, and achieved a new state-ofthe-art on the multirc and arc datasets among approaches that do not use external resources during training. we showed that rocc-based qa approaches are more robust across domains, and generalize better to other related tasks like entailment. in the future, we envision that rocc scores can be used as distant supervision signal to train supervised justification selection methods.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Framework Overview\n",
            "\u001b[35mSection_title: 3.2  Offline QA-pair Generation\n",
            "\u001b[35mSection_title: 3.3  Online Question Matching\n",
            "\u001b[35mSection_title: 3.4  Enhanced Question Generation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Real-Time QA Experiments\n",
            "\u001b[35mSection_title: 4.3  Question Generation Experiments\n",
            "\u001b[35mSection_title: 4.4  Analysis\n",
            "\u001b[32mconclusion: in this paper, we present ocean-q, a novel method that uses qg to reduce the computational cost of rtqa, and address the bottleneck of encoding coming question which cannot be cached offline. experiments on squad-open demonstrate that ocean-q is able to accelerate the fastest rtqa system by 4 times, while only losing the accuracy by 3+%. to further improve qg quality, we introduce a new data augmentation method and leverage multi-task learning and diverse beam search to boost rtqa performance. currently, there still exists a gap between ocean-q and the state-of-the-art rtqa models. future work that includes designing a better qg model may help closing the gap.  data augmentation, s min , s max and n are set to 0.5, 1.0 and 2 respectively. for multi-task learning, we use squad, hotpotqa and newsqa dataset, with mixture ratio 0.4 by searching from 0.1 to 0.9. for diverse beam search, the diverse strength rate γ is set to 4.0. for rtqa, we generate 20 different questions with each answer.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Retriever\n",
            "\u001b[35mSection_title: 2.2  Unsupervised Passage Re-ranking (UPR)\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Open-Domain QA Datasets\n",
            "\u001b[35mSection_title: 3.2  Keyword-centric Datasets\n",
            "\u001b[35mSection_title: 3.3  Retrievers\n",
            "\u001b[35mSection_title: 3.3.1  Unsupervised Retrievers\n",
            "\u001b[35mSection_title: 3.3.2  Supervised Retrievers\n",
            "\u001b[35mSection_title: 3.4  Pre-Trained Language Models (PLMs)\n",
            "\u001b[35mSection_title: 3.5  Implementation Details\n",
            "\u001b[35mSection_title: 4  Experiments: Passage Retrieval\n",
            "\u001b[35mSection_title: 4.1  Main Task\n",
            "\u001b[35mSection_title: 4.2  Ablation Studies\n",
            "\u001b[35mSection_title: 4.2.1  Importance of Question Generation\n",
            "\u001b[35mSection_title: 4.2.2  Impact of Pre-trained Language Models\n",
            "\u001b[35mSection_title: 4.2.3  Passage Candidate Size vs Latency\n",
            "\u001b[35mSection_title: 4.3  Zero-Shot Supervised Transfer\n",
            "\u001b[35mSection_title: 4.4  Evaluation on Keyword-centric Datasets\n",
            "\u001b[35mSection_title: 4.4.1  Entity Questions\n",
            "\u001b[35mSection_title: 4.4.2  BEIR Benchmark\n",
            "\u001b[35mSection_title: 5  Experiments: Question Answering\n",
            "\u001b[35mSection_title: 5.1  Method\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose upr, an approach to perform unsupervised passage re-ranking for opendomain retrieval. to re-rank, upr computes a relevance score for question generation conditioned on each retrieved passage using pre-trained language models. extensive experiments across a wide range of qa datasets show that an unsupervised pipeline consisting of retriever and upr greatly outperforms strong supervised retriever models. in addition, upr further improves the performance of supervised retrievers. on the open-domain qa task, by just performing inference using re-ranked passages and a pre-trained reader model, we achieve new state-of-the-art results.\n",
            "\u001b[35mSection_title: 7  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction & Motivation\n",
            "\u001b[35mSection_title: 2  Our Approach and Model\n",
            "\u001b[35mSection_title: 2.1  Domain Adaptability of the Model\n",
            "\u001b[35mSection_title: 3  Experiments & Results\n",
            "\u001b[35mSection_title: 3.1  Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we presented a simple approach for converting question answering over knowledge graphs into an inference problem. leveraging existing models of natural language inference, as well as proposing a new model, we have shown state of the art results on metaqa dataset. our model is simple and amenable for domain adaptation, to solve the problem of qa over kgs from newer domains with lesser training data. to the best of our knowledge, this is the first attempt in treating qa over kg as an inferencing problem and the results are exciting. the work is preliminary and provides a good starting point for discussion and further research.\n",
            "\u001b[35mSection_title: 5  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  n-templates, 1d, 2d and 3d\n",
            "\u001b[35mSection_title: 3.1  iso-rule advantages\n",
            "\u001b[35mSection_title: 3.2  iso-rule sizes\n",
            "\u001b[35mSection_title: 4  iso-subsets expressed as iso-rules\n",
            "\u001b[35mSection_title: 4.1  k-totalistic rules (kcode)\n",
            "\u001b[35mSection_title: 4.2  t-totalistic rules (tcode)\n",
            "\u001b[35mSection_title: 4.4  reaction-diffusion rules\n",
            "\u001b[35mSection_title: 4.5  survival/birth rules\n",
            "\u001b[35mSection_title: 5  rule-table size summary\n",
            "\u001b[35mSection_title: 6  input-frequency histogram (IFH)\n",
            "\u001b[35mSection_title: 7  filtering\n",
            "\u001b[35mSection_title: 8  the IFH mutation/filter game\n",
            "\u001b[35mSection_title: 9  input-entropy and min-max variability\n",
            "\u001b[35mSection_title: 10  automatically classifying rule-space\n",
            "\u001b[35mSection_title: 11  summary\n",
            "\u001b[35mSection_title: 12  acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Setup and Background\n",
            "\u001b[35mSection_title: 2.1  Multi-hop Question Answering\n",
            "\u001b[35mSection_title: 2.2  Fusion-in-Decoder Model (FID)\n",
            "\u001b[35mSection_title: 3  PATHFID Reader for Multi-hop QA\n",
            "\u001b[35mSection_title: 3.1  Overview of PATHFID\n",
            "\u001b[35mSection_title: 3.2  Extending Multi-hop QA beyond Answer Generation\n",
            "\u001b[35mSection_title: 3.2.1  Global Input Representation\n",
            "\u001b[35mSection_title: 3.2.2  Hierarchical Reasoning Path as a Sequence\n",
            "\u001b[35mSection_title: 3.3  Encoding Cross-Passage Interactions (PATHFID+)\n",
            "\u001b[35mSection_title: 3.4  Training and Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and General Setup\n",
            "\u001b[35mSection_title: 4.2  Main Experiments: HotpotQA\n",
            "\u001b[35mSection_title: 4.2.1  Overall Results\n",
            "\u001b[35mSection_title: 4.2.2  Analysis\n",
            "\u001b[35mSection_title: 4.3  Experiments: IIRC\n",
            "\u001b[35mSection_title: 4.4  Analyzing the Benefit of Joint Training\n",
            "\u001b[32mconclusion: in this work, we propose a generative question answering (qa) approach that models multi-hop qa as a single sequence prediction task. it learns to generate an answer along with a reasoning path to improve its capability of multi-hop reasoning. our experiments on prominent multi-hop qa benchmarks, hotpotqa and iirc, validate the promise and effectiveness of our proposed method path-fid and its extension pathfid+. future work will explore (1) our pathfid approach more closely with text retrieval models in open-domain qa scenarios and (2) more explicit grounding on the input information to make our approach even more interpretable and controllable.   in figure 4 and 5, we visualize the correlation between supporting evidence and answer prediction performances for comparison and bridge question types, respectively. to obtain these plots, we first split the examples into 10 buckets where n-th bucket contains the examples with support-f1 score in (10 * (n − 1), 10 * n] percentile for n = {1, 2, . . . , 10}. then, we take the average answer prediction accuracy (both em and f1) over these examples for each bucket, and report this number on the y-axis of the plot at the corresponding support-f1 bucket on the x-axis, while dropping the empty buckets. note that x = 0 corresponds to examples with support-f1 score of 0. also note that the size of a data point on the figure reflects the number of examples in the corresponding bucket as also indicated by the legend. from figures 4 and 5, we can observe that the accuracy of the generated answers is significantly lower, 30% for bridge and 10% for comparison, for the first bucket with zero support-f1 compared to buckets with positive support-f1 score. this suggests that the model has a difficult time figuring out the an-swer when the supporting evidence prediction is poor. another observation that holds for both categories is the general trend of increased answer quality as the supporting fact prediction improves. combining these two points provide additional evidence (in addition to table 2 in the main paper) implicitly supporting the answer generation process of pathfid being grounded on the generated supporting facts, which is generated as the prefix of the answer segment in the full decoded reasoning path sequence during inference. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Supporting document retriever\n",
            "\u001b[35mSection_title: 2.2  Document reader\n",
            "\u001b[35mSection_title: 2.3  Generator\n",
            "\u001b[35mSection_title: 2.4  Reader-before-generator\n",
            "\u001b[35mSection_title: 3  Experiment Setups\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 4  Experiment Results\n",
            "\u001b[35mSection_title: 4.1  Automatic Evaluation\n",
            "\u001b[35mSection_title: 4.2  Human evaluation\n",
            "\u001b[35mSection_title: 4.3  Ablation\n",
            "\u001b[35mSection_title: 5  Further analysis\n",
            "\u001b[35mSection_title: 5.3  How does pre-training help?\n",
            "\u001b[32mconclusion: we propose a new end-to-end framework rbg that jointly models answer generation and machine reading to tackle the faithfulness issue in lfqa. experiments on two lfqa datasets, eli5 and ms marco, demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics. the detailed analysis further proves the competency of our method in generating fluent, relevant, and more faithful answers. we also propose to evaluate the factual correctness of lfqa model by answering questions of extractive qa tasks (e.g., natural questions), which may be helpful to evaluate the faithfulness of lfqa model efficiently.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 3  1 3\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Benchmark\n",
            "\u001b[35mSection_title: 3.1  Formatting\n",
            "\u001b[35mSection_title: 3.2  Mathematics\n",
            "\u001b[35mSection_title: 3.3  Physics\n",
            "\u001b[35mSection_title: 3.4  MCAT\n",
            "\u001b[35mSection_title: 3.5  Law\n",
            "\u001b[35mSection_title: 4  Evaluation\n",
            "\u001b[35mSection_title: 4.1  Results\n",
            "\u001b[35mSection_title: 5  Model-based Rubric Evaluation\n",
            "\u001b[32mconclusion: in this paper, we presented arb, a novel benchmark for evaluating advanced reasoning capabilities in large language models. our dataset is composed of various problems from the sciences and law, sourced from graduate-level exams and professional resources. despite advancements in current llms, their performance remains very low on the quantitative subjects, in arb's tasks. we also proposed a rubric-based self-evaluation method, enabling llms to grade their own reasoning. this method is not yet reliable enough to replace human grading. we hope this method can be extended to more reliable and cheap testing of complex model outputs.\n",
            "\u001b[35mSection_title: 6  Limitations and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[32mlimitation: the large n c limit is inherent in all holographic qcd methods, and this poses a problem for the aspects of nuclear physics that are different between the small n c and the large n c regimes. in particular, the bulk nuclear matter at zero temperature and pressure (but finite density) forms a quantum liquid for small n c -such as real-life n c = 3 -but becomes a crystalline solid for large n c .to see how this works, consider a condensed matter analogy -some atoms which attract to each other at long or medium distances but have repulsive hard cores. semi-classically, at zero temperature and pressure such atoms always form some kind of a crystal; it takes strong quantum effects to put the atoms into some other phase such as liquid or super-solid.of particular importance is the kinetic energy of the zero-point quantum motion of atoms confined to narrow potential wells,k ∼ π 2 2 2m atom (well diameter) 2 ,(2.1)or rather its ratio k/u to the potential binding energy u per atom. according to newton bernardes [18], this ratio is related to the de bour parameter λ b of the inter-atomic potential ask u ≈ 11λ 2 b , λ b = r c √ 2mǫ , (2.2)where r c is the radius of the atomic hard core and ǫ is the maximal depth of the potential. for small de bour parameters, the quantum corrections to the semi-classical approximation are weak and the crystal remains stable at zero pressure. for larger λ b , the quantum corrections due to kinetic energy become important, and when λ b exceeds a critical value somewhere between 0.2 and 0.3 [19], the crystal melts into a quantum liquid. 4 for example, helium atoms have λ b = 0.306 and hence k/u ≈ 1 while neon atoms have λ b = 0.063 and hence k/u ≈ 0.05; consequently, at zero temperature and zero pressure helium is a quantum liquid while neon is a crystalline solid.to see how the k/u ratio of the nuclear matter depends on the number of colors, we note that in the large n c limit, the leading nuclear forces are proportional to n c . specifically, according to kaplan and manohar [20],v ( r, i 1 , i 2 , j 2 , j 2 ; n c ) = n c × a c (r) + n c × a s (r)(i 1 i 2 )(j 1 j 2 ) + n c × a t (r)(i 1 i 2 ) 3(nj 1 )(nj 2 ) − (j 1 j 2 ) (2.3) + o(1/n c ).for the same n c -independent radial profiles a c , a s , a t of the central, spin-spin, and tensor potentials. classically, such potentials would like to arrange a many-nucleon system in some kind of a crystal with n c -independent nearest-neighbor distance ∼ 1 fm, while the binding energy of a nucleon in such a crystal would be proportional to the n c . indeed, all models of nuclear matter based on semi-classical models of nucleons form such crystals, for example skyrmion crystals of ref. [6]. in the quantum theory, nucleons in such a lattice have zeropoint kinetic energies (2.1) where the well diameter is independent on n c while the nucleon's mass m ∝ n c , hence k ∝ 1/n c andk u ∝ n −1 c n +1 c = 1 n 2 c . (2.4)we may estimate the coefficient of this proportionality using the de bour parameter λ b . the maximal depth of the central potential between two nucleons is about 100 mev for n c = 3, so we take it to be ǫ ∼ n c × 30 mev for large n c . likewise, we take the nucleon mass to be m n ∼ n c × 300 mev and hard-core radius r c ∼ 0.7 fm regardless of n c . consequently,λ b = r c √ 2mǫ ∼ 2 n c =⇒ k u ∼ 45 n 2 c (2.5)and hence liquid nuclear matter for n c 8 and solid nuclear matter for n c 8.the numerical coefficient in eq. (2.5) and hence our estimate n crit c ∼ 8 for the dividing line between liquid and solid bulk nuclear matter (at low pressures and temperatures) should be taken with a large grain of salt. also, the transition between liquid nuclear matter for n c = 3 and crystalline nuclear matter for large n c may go through some exotic phases at intermediate values of n c , perhaps something like a quantum supersolid, perhaps something more exotic without known condensed-matter analogues. but regardless of the details of this transition, in the large n c limit the potential energy of interacting near-static nucleons becomes much larger than the nucleons' kinetic energies, and the bulk nuclear matter at t = 0, p = 0 conditions becomes a conventional semi-classical crystal. the structure of such crystals can be modeled holographically -and indeed there is active research in this direction (for instance [7]) -but we have no experimental data to compare to the models because real-life nuclei with n c = 3 are liquid rather than solid.meanwhile, instead of trying do build holographic models of complete nuclei we focus on holographic models of the nuclear forces. but even at the level of the two-body forces, the large n c limit maybe different from the real-life case of just 3 colors. of particular concern is the isoscalar attractive force due to exchanges of the σ(600) scalar mesons between the nucleons. in real life, this is a major component of the net attractive force -especially at the medium-long distances between the nucleons -but in the large n c limit this component may weaken or disappear because the σ(600) meson itself may become heavier or even disappear from the scalar meson spectrum.the σ(600) (also known as f 0 (600)) is the lightest isoscalar true-scalar meson. in real life, it appears as a very broad resonance of two pions -so broad that its central mass is somewhat controversial and different experimentalists locate it anywhere between 400 mev and 700 mev, and sometimes even higher, cf. references in the particle data group's listing [21]. but the real controversy about the σ(600) resonance is its physical origin. unlike the heavier i g = 0 + , j cp = 0 ++ mesons f 0 (980), f 0 (1370), etc., the σ(600) meson does not exist in the non-relativistic quark model 5 so for many years r. l. jaffe and others [22,23,24,25] were claiming that the σ(600) is not a true qq meson but a qqqq tetraquark. specifically, it's a molecule-like bound state of two pions which exists because the ρ-meson exchanges in the t-channel induce an attractive s-channel force between the pions. if this claim is true, then the σ resonance goes away in the large n c limit because the forces between pions become weak as 1/n c . but many other authors (see [26] for a sample) identify the σ(600) with the σ field of the linear sigma model of the chiral symmetry breaking. or rather, the massive σ(x) field parametrizing fluctuations of magnitude of the symmetry-breaking vev ψψ gives rise to primordial sigma quanta, while the real sigma mesons σ(600) are quantum mixtures of those primordial quanta with the |ππ states (and to lesser extent with the other i g = 0 + , j p c = 0 ++ mesons). from this point of view, the non-relativistic quark model is irrelevant because the quarks do not become non-relativistic until after the chiral symmetry has already been broken. indeed, the nrqm does not see that the pions are (pseudo) goldstone bosons, so the fact that it does not see the sigma meson at all is simply another limitation of the nrqm as far as the chiral symmetry breaking is concerned. if this point of view is right, then the sigma meson exits for all n c . for large n c limit, this meson is mostly a quantum of the σ(x) field -its mixing with |ππ and other states becomes weak -and it's a narrow resonance rather than a broad hump we have for n c = 3, but it remains a dominant resonance in the i g = 0 + , j p c = 0 ++ ππ channel, and its mass should not be too different from the real-life 600 mev.the other mesons -scalar or vector, isoscalar or isovector -are unlikely to be disturbed by the large n c limit, so their contributions to the nuclear forces would be similar to real-life qcd. if the σ(600) meson remains in the spectrum in the large n c limit and if its mass remains similar to the real-life 600 mev, then the entire nuclear potential (2.3) for n c → ∞ would be similar to what it is in real life, except for the overall factor n c . in particular, the net central potential v c (r) would be repulsive at short distances (the hard core) but 5 in the non-relativistic quark model, all 0 ++ mesons have s = 1 and l = 1. consequently, the lightest 0 ++ meson should be heavier than the lightest 1 −− mesons ρ(770) or ω(787) that have s = 1 but l = 0. depending on the assumptions one makes about the forces between the quark and the antiquark, this argument identifies the lightest true qq meson with i g = 0 + and j p c = 0 ++ as either f 0 (980) or f 0 (1370). in any case, the σ(600) resonance is way too light to be a p-wave qq state, so it has to be something else.attractive at medium and long distances:r v c (r) n c → ∞ limit with a light σ meson (2.6)on the other hand, if the σ(600) meson disappears from the spectrum for large n c , or if it becomes heavier than the lightest vector meson, then the dominant attractive force would become shorter-ranged than the repulsive force, and the net force at medium and long distances would be repulsive rather than attractive:r v c (r) n c → ∞ limit without the σ meson (2.7)in this scenario, at large n c the nuclear force is repulsive at all distances, and there are no bound nuclei at all, liquid or crystalline.so what really happens to the sigma-meson and to the nuclear forces at large n c ? the best way to settle this controversy would be to find the σ resonance and its mass in a lattice qcd calculation for several values of n c . such a calculation would require a realistic pion mass (unlike most present-day lattice calculations extrapolating from m π ≥ 350 mev) andrather large lattices to distinguish the sigma resonance from the two-pion continuum, so it may be too hard for the present-day computers. but thanks to the moore's law, finding the σ resonance on a lattice should become possible in a not-too-distant future.alternatively, we may try to resolve the issue using holography. although a holographic model of real qcd -or rather, of qcd with large n c -is yet to be constructed, several known models seem to be qualitatively similar, so we can compare their predictions for the meson spectra in general, and for the lightest true scalar meson in particular. however, the models that seem qualitatively similar to qcd may not be similar enough, and their predictions could be widely off target. indeed, the predictions of different models have turned out to be quite different from each other. for example, in the sakai-sugimoto model which we use in this article, the lightest true scalar meson is more than twice as heavy as the lightest vector meson. consequently -as we shall see in painful detail in section 5 -the net nuclear force is everywhere repulsive and looks like (2.7) rather than like (2.6). on the other hand, in the highly-non-antipodal version of the dymarsky-kuperstein-sonnenschein model [27], the lightest j cp = 0 ++ meson is much lighter than any other mesons (except the pions) [29]. however, this lightest scalar is a pseudo-goldstone boson of the approximate conformal symmetry of the flavor sector, so it is not clear how much attractive force it can mediate. as of this writing, it is not clear if the net nuclear potential in this model looks like the real-life potential (2.6) or like the everywhere-repulsive potential (2.7) we calculate in this paper for the sakai-sugimoto model. but suppose tomorrow somebody discovers a holographic model of the real qcd andmiracle of miracles -it has a realistic spectrum of mesons, including the σ(600) resonance, and even the realistic yukawa couplings of those mesons to the baryons. even for such a model, the two-body nuclear forces would not be quite as in the real world because the semi-classical holography limits n c → ∞, λ → ∞ suppress the multiple meson exchanges between baryons. although in this case, the culprit is not the large number of colors but the large 't hooft coupling λ = n c g 2 ym . indeed, from the hadronic point of view, nuclear forces arise from the mucleons exchanging one, two, or more mesons, and in real life the double-meson exchanges are just as important as the single-meson exchanges. in particular, since the lightest mesonic state with i g = 0 + , j cp = 0 ++ quantum numbers is a pair of un-bound pions, the longest-range isoscalar attractive force between nucleons comes from exchanges of two pions rather than of any single mesons. in holography, the single-meson exchanges happen at the tree level of the string theory while the multiple meson exchanges involve string loops (k − 1 loops for k mesons), and the loop amplitudes are suppressed by the powers of 1/λ relative to the tree amplitudes.naively, one would expect the loop amplitudes to carry additional factors of 1/n c (which is dual to the string coupling) rather than 1/λ, or maybe both 1/λ and 1/n c factors, but the naive power-of-n c counting does not work for loop amplitudes involving baryons made of n c quarks. 6 indeed, in honest qcd with a large number of colors, the multi-meson-exchange contributions to the non-relativistic effective potential for the baryons are not suppressed by powers of 1/n c [27]. however, the extra powers of n c due to n c quarks in a baryon are not accompanied by the extra powers of λ, so in holography, the contributions of the multiple meson exchanges are suppressed, albeit by powers of 1/λ rather than 1/n c .to see how this works in a general holographic model of qcd with n c ≫ n f , note that such a model starts with a string-theoretic construction where the colors and the flavors live on separate branes. for large n c and large λ, the color branes become black branes producing curvature and fluxes through the bulk, which provide a non-trivial background for degrees of freedom living in the bulk itself as well as on the flavor branes. the bulk degrees of freedom are dual to the pure-color sector of qcd (glueballs, etc.), while the vector and scalar fields living on the flavor branes are dual to the qq mesons. the flavor fields have rather weak couplings to each other: in 5d terms,g 5d,flavor ∼ √ r kk √ λn c (2.8)so the 4d mesons -which are modes of the 5d vector and scalar fields with wave functionsψ ∼ r −1/2kk -have couplings to each other of the orderg m m m ∼ 1 √ λn c . (2.9)a holographic baryon is made from some brane spanning only the compact dimensions that is connected to the flavor branes by n c strings, although this construction is often equivalent to an instanton of the 5d flavor gauge fields. consequently, the baryon-meson coupling is enhanced by an extra factor of n c ,g m bb ∼ n c × 1 √ λn c = √ n c √ λ . (2.10)at the tree level of the baryon-meson theory, scattering of two baryons proceeds through a single-meson exchange, which produces a o(n c /λ) amplitude,a tree ∼ g 2 m bb ∼ n c λ . (2.11)at the one-loop level, there are two types of diagrams, the triangle diagrams such asa ∆ ∼ g 3 m bb × g m m m ∼ n c λ 2 (2.12)and the box and crossed-box diagrams (2.13) with amplitudesa ∼ g 4 m bb ∼ n 2 c λ 2 . (2.14)that carry an extra power of n c . however, banerjee et al showed [27] that for non-relativistic baryons, the box and the crossed-box diagrams almost cancel each other from the effective potential between the baryons, with the un-canceled part having a lower power of the n c . banerjee et al did not pay any attention to the powers of λ, but clearly the un-canceled sub-leading terms in the box and crossed-box diagrams cannot carry higher powers of the 't hooft coupling than the leading terms (2.14), thusa uncanceled ∼ n c λ 2 ∼ a ∆ ∼ 1 λ a tree . (2.15)in other words, the contribution of the double-meson exchange and other one-loop processes to the 2-body nuclear potential carries the same power of n c but is suppressed by a factor 1/λ compared to the tree-level singe-meson exchange.to be precise, the large λ limit suppresses exchanges of the un-bound meson pairs but not of the meson-meson resonances -which become narrow (because of weak g m m m ) and act as single mesons exchanged between the two baryons. in particular, in the isoscalar 0 ++ channel that gives rise to the dominant attractive force between nucleons, the λ → ∞ limit suppresses the contribution of the unbound two-pion continuum, but it replaces it with a discrete set of f 0 resonances. in a good holographic model of qcd (which alas has not been found yet), the overall strength of the 0 ++ channel should be similar to the real qcd, so it would produce a similar isoscalar attractive force at short distances. however, the range of this attractive force would be significantly shorter: instead of decaying with distance like exp(−2m π r) as in real life, the holographic attractive force decays as exp(−m 0 r) where m 0 is the mass of the lightest isoscalar 0 ++ meson, presumably σ(600 mev).in principle, the isoscalar 1 −− channel that gives rise to the dominant repulsive force suffers from similar corrections in the λ → ∞ limit. but in practice, the strongest and the longest-range contribution to this channel comes from exchanges of a single ω(787) meson, so suppressing the multi-meson exchanges in this channel would not make a qualitative difference. thus altogether, the net effect of large 't hooft coupling on the central nuclear potential -besides the overall 1/λ factor -is the shortening of the attractive tail at long distances:r λ × v c (r)blue: real qcd, λ ∼ 1 red: best possibility for holographic qcd, λ ≫ 1 (2.16) however, this optimistic picture presumes a holographic model of qcd that correctly reproduces (a) the overall strength of the isoscalar 0 ++ and 1 −− channels, and (b) the mass spectra of vector and scalar mesons, especially the masses of the lightest 0 ++ and 1 −− mesons σ(600) and ω(787). but thus far, no known model satisfies these requirements, not even approximately, so the nuclear forces they produce could be much more different from the real life than (2.16). in particular, the nuclear force we calculate in this paper for the sakai-sugimoto model turns out to be everywhere repulsive:r v c (r)/n c blue: real qcd red: sakai-sugimoto model (2.17)now let's go back to the large n c limit -in holography or in honest qcd -and consider yet another general problem with baryons made from many quarks: how to separate the nucleons with i = j = 1 2 from the other kinds of baryons such as ∆ with i = j = 3 2 ? in real life, there is a large mass gap between the nucleons and the ∆ baryons -almost 300 mev -but for large n c this gap shrinks as 1/n c . at the same time, the two-baryon potential grows like n c , so for large n c it becomes stronger than the gap. consequently, two interacting nucleons may \"forget\" their individual spins and isospins and mix up with other baryonic species such as ∆. in fact, for large n c there is a whole lot of baryonic species with i = j ranging from 1 2 (for odd n c ) or 0 (for even n c ) all the way up to n c /2, and a strongly-interacting nucleon might mix up with all of them. while such mixing would not affect the isoscalar spin-blind central force between two baryons, it might significantly enhance the isovector spin-spin and tensor forces.therefore, comparing the two-baryon forces in the large n c limit to the real-life twonucleon forces is rather tricky. one has to carefully keep track of the spin and isospin degrees of freedom of the two baryons, expand the interaction hamiltonian into central, spinspin, and tensor forces as in eq. (2.3), and then compare the radial profiles a c (r), a s (r), and a t (r). moreover, the spin and isospin degrees of freedom require quantum mechanical treatment because semi-classically, we do not get definite spins or isospins even for standalone single baryons. instead, we get skyrmions, or instantons, or some other kind of solitons with a definite orientation of the su(2) isospin relative to the su(2) spin ; in quantum terms, they become superpositions of baryons with all possible i = j = 1 2 , 3 2 , . . . , ∞. consequently, a force between two such semiclassical baryons is not a force between two nucleons but rather a superposition of forces between different baryonic species.this problem affected the first holographic calculation of the nuclear forces by k. y. kim and i. zahed [30]. their baryons were semiclassical instantons in the sakai-sugimoto model, so instead of definite |i, i z , j, j z they had a definite direction n in s 3 = su(2) isospin × su(2) spin /su(2) common . consequently, kim and zahed [30] found that the force between two baryons depends on the angle between n 1 and n 2 -it was attractive for some angles and repulsive for other -but they could not interpret this angular dependence in terms of the isovector spin-spin and tensor forces. by comparison, hashimoto, sakai, and sugimoto [16] made a similar calculation using properly quantized collective coordinates for each instanton.consequently, they obtained the force between two nucleons rather than some mixed-up baryons, and they could see how this force depends on each nucleon's i z and j z . in particular, they saw that at medium-short distances, the net force between two nucleons is always repulsive. evidently, the attraction kim and zahed saw for some relative orientations of semiclassical baryons happens only for high spins and isospins, but not for nucleons withi = j = 1 2 .on the other hand, the analysis of hashimoto et al was limited to the first-order perturbation theory for nucleons that are far enough from each other to avoid the strong mixing of spins and isospins. this approach will not work for the hard-core region at very short distances where the interactions are much stronger than the gaps between states of the individual baryons. in the hard core, the semiclassical analysis of kim and zahed might work better than the perturbative expansion of hashimoto et al, although comparing the semi-classical large-n c results to the real-life nuclear forces might be problematic.to summarize, the large n c limit of nuclear physics suffers from three major problems. the third problem of baryon mixing is only technical, and it can be solved -at least for the medium and long distances between the nucleons -by following hashimoto et al rather than kim and zahed. but there are no ways around the first problem of different phase structures of nuclear matter with n c = 3 and with n c → ∞. even at high pressures and densities, there is a difference: for n c = 3, squeezing nucleons together makes them merge into a quark liquid, while for n c → ∞ the nucleons always retain their individual identities and a would-be quark liquid suffers from the \"chiral density wave\" instability [4]. it is possible that at some intermediate pressures and densities the n c = 3 nucleons form a crystal -just like helium solidifies at high pressures -before merging into a quark liquid. if this intermediate-pressure phase of real nuclear matter is ever observed in a lab, or can be reliably shown to exist in some exotic but observable places like inferiors of neutron stars, it would be very interesting to compare its properties to the holographic models. until then, we can only speculate.finally, the second problem -concerning the fate of the σ(600) resonance in the large n c limit and its effect on the attractive nuclear force -is solvable in principle, but it has not been solved yet. in holography, this problem is aggravated by using qcd-like models in lieu of the presently unknown holographic dual of the real qcd. the meson spectra of such models are not quite realistic; for example, in the sakai-sugimoto model (both antipodal and non-antipodal versions) there is no σ resonance and the lightest scalar meson has more than twice the mass of the lightest vector meson. consequently, we shall see in section 5 that in this model, the attractive force is both weaker and shorter-ranged than the repulsive force, so the net nuclear force is always repulsive. this could be a peculiar failing of the sakai-sugimoto model, or it could be the general problem of holography or even of the large n c limit. hopefully, future research will resolve this issue.\n",
            "\u001b[35mSection_title: 2  Limitations of the N c → ∞ Limit and Holography\n",
            "\u001b[35mSection_title: 3  Baryons in the Non-antipodal Sakai-Sugimoto Model\n",
            "\u001b[35mSection_title: 4  Summary of the Repulsive Force\n",
            "\u001b[35mSection_title: 5.2  Attractive Forces in the Intermediate Zone\n",
            "\u001b[35mSection_title: 5.3  Attractive Forces in the Far Zone\n",
            "\u001b[35mSection_title: 6  Full DBI Action in the Near Zone\n",
            "\u001b[35mSection_title: 7  Summary and Open Questions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Data Collection\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Model Architecture and Training\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Establishing Human Performance\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we present hotpotqa, a large-scale question answering dataset aimed at facilitating the development of qa systems capable of performing explainable, multi-hop reasoning over diverse natural language. we also offer a new type of factoid comparison questions to test systems' ability to extract and compare various entity properties in text.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  PiC Dataset Construction\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Data Annotation\n",
            "\u001b[35mSection_title: 3.3  Annotation Verification\n",
            "\u001b[35mSection_title: 4  Three Phrase Understanding Tasks\n",
            "\u001b[35mSection_title: 4.1  Phrase Similarity (PS)\n",
            "\u001b[35mSection_title: 4.2  Phrase Retrieval (PR)\n",
            "\u001b[35mSection_title: 4.3  Phrase Sense Disambiguation (PSD)\n",
            "\u001b[35mSection_title: 5  Experiments and Results\n",
            "\u001b[35mSection_title: 5.1  Phrase Similarity: Contextualized phrase embeddings improve accuracy Q:\n",
            "\u001b[35mSection_title: 5.2  Human Baselines and Upperbound (95% Exact Match) on Phrase Retrieval\n",
            "\u001b[35mSection_title: 5.3  Phrase Retrieval: In ranking, context only helps BERT embeddings but not others\n",
            "\u001b[35mSection_title: 5.5  Phrase Sense Disambiguation: Best models also perform poorly\n",
            "\u001b[32mconclusion: while wic and english wsd rely exclusively on dictionaries (pilehvar and camacho-collados, 2019) to obtain word senses and example sentences, our data collection depends on wikipedia, wic, & nlp models and our annotation depends on experts.\n",
            "\u001b[35mSection_title: 6  Discussion and Conclusion\n",
            "\u001b[32mlimitation: our dataset is currently limited to multi-word, english noun-phrases. furthermore, it is expected to contain around a 5% error on pr-pass (i.e. the best human performance is 95% em). on pr-page, there may be more than one correct target phrase; however, we only label one phrase as the correct answer per document. we use only phrases that contain at least one wic word. hyperparameters we train each bert-based classifier for a maximum of 100 epochs with early stopping monitored on validation accuracy (patience of 10 epochs). we use a batch size of 200 and adam optimizer with learning rate α = 0.0001, β 1 = 0.9, β 2 = 0.999, and ϵ = 10 −8 .\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\u001b[35mSection_title: 2.  In\n",
            "\u001b[35mSection_title: 3.  In\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Reading comprehension in other languages\n",
            "\u001b[35mSection_title: 3  Dataset Collection\n",
            "\u001b[35mSection_title: 3.1  Contexts collection\n",
            "\u001b[35mSection_title: 3.2  Question and answer pairs generation\n",
            "\u001b[35mSection_title: 3.3  Quality management\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.1  Answer type analysis\n",
            "\u001b[35mSection_title: 4.2  Question type analysis\n",
            "\u001b[35mSection_title: 4.3  Context and answer length\n",
            "\u001b[35mSection_title: 5  Dataset Evaluation\n",
            "\u001b[35mSection_title: 6.1  Experimental setup\n",
            "\u001b[35mSection_title: 6.2  Model performance\n",
            "\u001b[35mSection_title: 6.2.1  Performance by answer types\n",
            "\u001b[35mSection_title: 6.2.2  Performance by question types\n",
            "\u001b[35mSection_title: 6.2.3  Performance by answer lengths\n",
            "\u001b[32mconclusion: in this paper, we proposed the japanese question answering dataset, jaquad. we collected the contexts from japanese wikipedia articles and 39k+ questions were manually annotated by fluent japanese speakers. jaquad has the same format as squad, and the characteristics of the data are generally similar to korquad 1.0. in the experiments, we fine-tuned a japanese pre-trained language model with jaquad as a baseline and achieved 78.92% for f1 score and 63.38% for em on test set. the baseline reaches promising results, but there is plenty of room for improvement. extension of the dataset, such as covering longer answers, is left for future work. the dataset and our experiments are available at https://github.com/skelterlabsinc/jaquad.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Statement\n",
            "\u001b[35mSection_title: 3.1  Joint graph representation\n",
            "\u001b[35mSection_title: 3.2  KG node relevance scoring\n",
            "\u001b[35mSection_title: 3.3  GNN architecture\n",
            "\u001b[35mSection_title: 3.4  Inference & Learning\n",
            "\u001b[35mSection_title: 3.5  Computation complexity\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Knowledge graphs\n",
            "\u001b[35mSection_title: 4.3  Implementation & training details\n",
            "\u001b[35mSection_title: 4.4  Baselines\n",
            "\u001b[35mSection_title: 4.5  Main results\n",
            "\u001b[35mSection_title: 4.6  Analysis\n",
            "\u001b[35mSection_title: 4.6.1  Ablation studies\n",
            "\u001b[35mSection_title: 4.6.2  Model interpretability\n",
            "\u001b[35mSection_title: 4.6.3  Structured reasoning\n",
            "\u001b[35mSection_title: 4.6.4  Effect of KG node relevance scoring\n",
            "\u001b[32mconclusion: we presented qa-gnn, an end-to-end question answering model that leverages lms and kgs.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Hierarchical Graph Network\n",
            "\u001b[35mSection_title: 3.1  Graph Construction\n",
            "\u001b[35mSection_title: 3.2  Context Encoding\n",
            "\u001b[35mSection_title: 3.3  Graph Reasoning\n",
            "\u001b[35mSection_title: 3.4  Multi-task Prediction\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Experimental Results\n",
            "\u001b[35mSection_title: 4.3  Case Study\n",
            "\u001b[32mconclusion: in this paper, we propose a new approach, hierarchical graph network (hgn), for multi-hop question answering. to capture clues from different granularity levels, our hgn model weaves heterogeneous nodes into a single unified graph. experiments with detailed analysis demonstrate the effectiveness of our proposed model, which achieves state-of-the-art performance on hotpotqa benchmark. currently, in the fullwiki setting, an off-theshelf paragraph retriever is adopted for selecting relevant context from large corpus of text. future work includes investigating the interaction and joint training between hgn and paragraph retriever for performance improvement.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  PRELIMINARIES\n",
            "\u001b[35mSection_title: 2.1  Conventional Training\n",
            "\u001b[35mSection_title: 2.2  Network Pruning\n",
            "\u001b[35mSection_title: 2.3  Context Selection\n",
            "\u001b[35mSection_title: 2.4  Ablation\n",
            "\u001b[35mSection_title: 3  METHODLOGY\n",
            "\u001b[35mSection_title: 3.1  Comparison Principle\n",
            "\u001b[35mSection_title: 3.2  Comparative Loss\n",
            "\u001b[35mSection_title: 3.2.2  CmpCrop\n",
            "\u001b[35mSection_title: 3.3  Discussion\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  Classification: Application to Text Classification\n",
            "\u001b[35mSection_title: 4.1.2  Models & Training.\n",
            "\u001b[35mSection_title: 4.1.3  Results.\n",
            "\u001b[35mSection_title: 4.2  Extraction: Application to Reading Comprehension\n",
            "\u001b[35mSection_title: 4.2.2  Models & Training.\n",
            "\u001b[35mSection_title: 4.3  Ranking: Application to Pseudo-Relevance Feedback\n",
            "\u001b[35mSection_title: 4.3.3  Results.\n",
            "\u001b[35mSection_title: 5  ANALYSIS\n",
            "\u001b[35mSection_title: 5.1  Effect of Weighting Strategy\n",
            "\u001b[35mSection_title: 5.2  Effect of Comparison Strategy\n",
            "\u001b[35mSection_title: 5.3  Effect of Model Parameters\n",
            "\u001b[35mSection_title: 5.4  Effect of Input Context\n",
            "\u001b[35mSection_title: 5.5  Loss Visualization\n",
            "\u001b[35mSection_title: 5.6  Training Efficiency\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[35mSection_title: 6.1  Contrastive Learning\n",
            "\u001b[35mSection_title: 6.2  Dropout-based Comparison\n",
            "\u001b[32mconclusion: in this paper, we propose cross-model comparative loss, a simple task-agnostic loss function, to improve the utility of neurons in nlu models. comparative loss is essentially a ranking loss based on the comparison principle between the full model and its ablated models, with the expectation that the less ablation there is, the smaller the task-specific loss. to ensure comparability among multiple ablated models, we progressively ablate the models and provide two controlled ablation methods based on dropout and context cropping, applicable to a wide range of tasks and models.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Experimental Settings\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we examined the three main strategies for transfer learning in natural language processing: training on an intermediate supporting task to aid the target task (stilts), training on the target and supporting task simultaneously (mtl), or training on multiple supporting tasks alongside the target task (mtl all ). we provide the first comprehensive comparison between these three methods using the glue dataset suite and show that there is a simple rule for when to use one of these techniques over the other. this simple heuristic, which holds true in more than 92% of applicable cases, states that multi-task learning is better than intermediate fine tuning when the target task is smaller than the supporting task and vice versa. additionally, we showed that these pairwise transfer learning techniques outperform the mtl all approach in almost every case.  pairwise oracle uses the best supplementary task for the given target task using the best pairwise method (stilts or mtl). all scores are the average of 5 random seeds. note that mtl all was run with three different sampling methods (top half). we find that on almost every task, pairwise approaches are better than mtl all . bold scores indicate the best score in the column for the given section.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  METHODOLOGY\n",
            "\u001b[35mSection_title: 2.1  TASK CONDITIONED TRANSFORMER\n",
            "\u001b[35mSection_title: 2.1.4  CONDITIONAL BOTTLENECK\n",
            "\u001b[35mSection_title: 2.2  MULTI-TASK UNCERTAINTY SAMPLING\n",
            "\u001b[35mSection_title: 3  RELATED WORK\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS AND RESULTS\n",
            "\u001b[35mSection_title: 4.1  MULTI-TASK UNCERTAINTY SAMPLING\n",
            "\u001b[35mSection_title: 4.2  ABLATION AND MODULE ANALYSIS\n",
            "\u001b[35mSection_title: 4.3  JOINTLY TRAINING ON 8 TASKS: GLUE\n",
            "\u001b[35mSection_title: 4.4  TRANSFER TO NEW TASKS\n",
            "\u001b[35mSection_title: 4.5  JOINTLY TRAINING\n",
            "\u001b[32mconclusion: we believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. in a large-scale 24-task nlp experiment, ca-mtl outperforms fully tuned single task models by 2.3% for bert large and by 1.2% for roberta large using 1.12 times the number of parameters, while single task fine-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. when a bert vanilla mtl model sees its performance drop as the number of tasks increases, ca-mtl scores continue to climb. performance gains are not driven by a single task as it is often the case in mtl. each ca-mtl module that adapts a transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. this evidence shows that ca-mtl is able to mitigate task interference and promote more efficient parameter sharing. we showed that mt-uncertainty is able to avoid degrading performances of low resource tasks. tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. overall, ca-mtl offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. extending such ideas will be an objective for future work.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Arbitrary Topologies\n",
            "\u001b[35mSection_title: 1.2  Advantages to non-naïve representation.\n",
            "\u001b[35mSection_title: 2  Model\n",
            "\u001b[35mSection_title: 2.1  Architecture\n",
            "\u001b[35mSection_title: 2.2  Separate Encoding of Sentences\n",
            "\u001b[35mSection_title: 2.3  Encoder\n",
            "\u001b[35mSection_title: 2.4  Decoder\n",
            "\u001b[35mSection_title: 2.4.1  Supporting Sentences\n",
            "\u001b[35mSection_title: 2.4.2  Answer Start\n",
            "\u001b[35mSection_title: 2.4.3  Answer End\n",
            "\u001b[35mSection_title: 2.4.4  Answer Type\n",
            "\u001b[35mSection_title: 3  Preprocessing\n",
            "\u001b[35mSection_title: 4  Results\n",
            "bottom\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task\n",
            "\u001b[35mSection_title: 4  Data Analysis\n",
            "\u001b[35mSection_title: 5  RGNet\n",
            "\u001b[35mSection_title: 6.2  Results and Discussion\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: in this work, we present a new qa datasets with complex numerical questions and interpretable reasoning graph. we also introduce an automatic evaluation metric for the generated reasoning process. we finally present an initial model producing the reasoning process while answering questions. the experiments show that noahqa is challenging and will become an interesting direction in both numerical qa and explainable qa.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[35mSection_title: 9  Ethics\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task and Background\n",
            "\u001b[35mSection_title: 2.1  Task Definition\n",
            "\u001b[35mSection_title: 2.2  Base Model\n",
            "\u001b[35mSection_title: 3  Logic-Driven Context Extension\n",
            "\u001b[35mSection_title: 3.1  Logic Identification\n",
            "\u001b[35mSection_title: 3.2  Logic Extension\n",
            "\u001b[35mSection_title: 3.3  Logic Verbalization\n",
            "\u001b[35mSection_title: 4  Logic-Driven Data Augmentation\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Dataset\n",
            "\u001b[35mSection_title: 5.2  Overall Performance\n",
            "\u001b[35mSection_title: 5.3  Further Analysis\n",
            "\u001b[35mSection_title: 5.4  Generalizability Discussion\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we focus on the task of logical reasoning of text. following a three-step logical reasoning paradigm, we first propose a neuro-symbolic logicdriven context extension framework. it identifies logical expressions as elementary units of logical inference and symbolically deduces the implicitly mentioned expressions, and verbalizes them as an extended context into a pre-trained model to match the answer. we also introduce a logic-driven data augmentation algorithm, which augments literally similar but logically different instances and employs contrastive learning to help our model better capture logical information. experimental results confirm the general effectiveness of our lreasoner, and it even surpasses human performance on the reclor dataset. in the future, we will explore to model different logical reasoning types and directly incorporate symbolic logic into the model structure.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND\n",
            "\u001b[35mSection_title: 3  SEMI-PARAMETRIC INDUCING POINT NETWORKS\n",
            "\u001b[35mSection_title: 3.1  SEMI-PARAMETRIC LEARNING BASED ON NEURAL INDUCING POINTS\n",
            "\u001b[35mSection_title: 3.2  SEMI-PARAMETRIC INDUCING POINT NETWORKS\n",
            "\u001b[35mSection_title: 3.2.1  ARCHITECTURE OF THE ENCODER AND PREDICTOR\n",
            "\u001b[35mSection_title: 3.3  INDUCING POINT NEURAL PROCESSES\n",
            "\u001b[35mSection_title: 3.4  OBJECTIVE FUNCTION\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  UCI DATASETS\n",
            "\u001b[35mSection_title: 4.2  NEURAL PROCESSES FOR META-LEARNING\n",
            "\u001b[35mSection_title: 4.3  GENOTYPE IMPUTATION\n",
            "\u001b[35mSection_title: 4.4  SCALING GENOTYPE IMPUTATION VIA META-LEARNING\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: in this paper, we introduce a domain-agnostic general-purpose architecture, the semi-parametric inducing point network (spin) and use it as the basis for induced point neural process (ipnps). unlike previous semi-parametric approaches whose computational cost grows quadratically with the size of the dataset, our approach scales linearly in the size and dimensionality of the data by leveraging a cross attention mechanism between datapoints and induced latents. this allows our method to scale to large datasets and enables meta learning with large contexts. we present empirical results on 10 uci datasets, a gaussian process meta learning task, and a real-world important task in genomics, genotype imputation, and show that our method can achieve competitive, if not better, performance relative to state-of-the-art methods at a fraction of the computational cost.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\u001b[35mSection_title: 7  ACKNOWLEDGMENTS\n",
            "\u001b[35mSection_title: 8  REPRODUCIBILITY\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Tutorial Overview\n",
            "\u001b[35mSection_title: 2  Problem Introduction\n",
            "\u001b[35mSection_title: 4  Instructor information\n",
            "\u001b[35mSection_title: 1  Tutorial Overview\n",
            "\u001b[35mSection_title: 2  Problem Introduction\n",
            "bottom\n",
            "\u001b[35mSection_title: 4  Instructor information\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem definition\n",
            "\u001b[35mSection_title: 3  Data preparation\n",
            "\u001b[35mSection_title: 3.1  Data acquisition\n",
            "\u001b[35mSection_title: 3.2  Data augmentation by back translation\n",
            "\u001b[35mSection_title: 4  Text encoder\n",
            "\u001b[35mSection_title: 5  Relation learning\n",
            "\u001b[35mSection_title: 5.1  Semantic matching\n",
            "\u001b[35mSection_title: 5.2  Contrastive learning\n",
            "\u001b[35mSection_title: 5.3  Joint learning\n",
            "\u001b[35mSection_title: 6  Experiment\n",
            "\u001b[35mSection_title: 6.1  Datasets and experimental settings\n",
            "\u001b[35mSection_title: 6.2  Baselines\n",
            "\u001b[35mSection_title: 6.3  Evaluation criteria\n",
            "\u001b[35mSection_title: 6.4  Results and analysis\n",
            "\u001b[35mSection_title: 6.5  Distance distribution of samples\n",
            "\u001b[35mSection_title: 7  Deployment\n",
            "\u001b[35mSection_title: 8  Related work\n",
            "\u001b[35mSection_title: 8.1  Legal information retrieval\n",
            "\u001b[35mSection_title: 8.2  CNN-based methods for legal information retrieval\n",
            "\u001b[35mSection_title: 8.3  Pre-trained language model based methods for legal information retrieval\n",
            "\u001b[35mSection_title: 8.4  COVID-19 information retrieval\n",
            "\u001b[35mSection_title: 8.5  Deep neural network based information retrieval\n",
            "\u001b[35mSection_title: 8.6  BERT-based information retrieval\n",
            "\u001b[35mSection_title: 8.7  Intelligent legal system\n",
            "\u001b[32mconclusion: we propose a semantic matching network for pairwise relation learning. moreover, we introduce auxiliary contrastive learning to help network better distinguish the sentences. our experiments show that our method leads to performance improvements under a variety of encoder designs. based on the network we proposed, we design and implement a legal ir system for the covid-19 pandemic. the system can identify: (1) the crime cases entered by the user and find the most similar cases to be pushed to the user as answers, and (2) the crime cases documented by the user and give the reference legal gist applicable to the case. meanwhile, the study could benefit developing a more comprehensive legal ir system and similar systems. in the future, it is worth doing more experiments on more data set to analyse the effect of various neural models for such tasks and accordingly improving the system.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  | INTRODUCTION\n",
            "\u001b[35mSection_title: 2  | MATERIALS AND METHODS\n",
            "\u001b[35mSection_title: 2.1  | Patient selection\n",
            "\u001b[35mSection_title: 2.2  | Treatment and follow-up\n",
            "\u001b[35mSection_title: 2.3  | Statistical analysis\n",
            "\u001b[35mSection_title: 3  | RESULTS\n",
            "\u001b[35mSection_title: 3.1  | Patient demographics and survival outcomes\n",
            "\u001b[35mSection_title: 3.2  | Effect of corpus uteri invasion on prognostication\n",
            "\u001b[35mSection_title: 3.3  | Development of refined staging systems using corpus uteri status and FIGO 2018 classification and 9th edition TNM schema\n",
            "\u001b[35mSection_title: 3.4  | Performances of the proposed RPAbased staging systems against conventional FIGO and TNM classifications\n",
            "\u001b[35mSection_title: 4  | DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Material and methods\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 3.1  Morphology and phenotypic plasticity of the green algal endosymbionts of freshwater sponges\n",
            "\u001b[35mSection_title: 3.2  Molecular phylogeny of the sponge endosymbionts\n",
            "\u001b[35mSection_title: 3.3  Distribution of the sponge endosymbionts\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[35mSection_title: 4.1  Multiple origin of symbionts in freshwater sponges\n",
            "\u001b[35mSection_title: 4.2  Distribution patterns of sponge endosymbionts\n",
            "\u001b[35mSection_title: 4.3  Taxonomic consequences and remarks to the genus Choricystis\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Unsupervised Domain Adaptation of Question Answering\n",
            "\u001b[35mSection_title: 2.2  Source-free Unsupervised Domain Adaptation\n",
            "\u001b[35mSection_title: 3  Problem Definition\n",
            "\u001b[35mSection_title: 4  Method\n",
            "\u001b[35mSection_title: 4.1  Base Model\n",
            "\u001b[35mSection_title: 4.2  Mask Module\n",
            "\u001b[35mSection_title: 4.3  Training on Source Domain\n",
            "\u001b[35mSection_title: 4.4  Training on Target Domain\n",
            "\u001b[35mSection_title: 4.4.1  Mask adaptation\n",
            "\u001b[35mSection_title: 4.4.2  Self-training\n",
            "\u001b[35mSection_title: 5  Experiment Setup\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Implementation Details\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Performance on Target Domains\n",
            "\u001b[35mSection_title: 6.2  Ablation Study\n",
            "\u001b[35mSection_title: 6.3  Sensitivity Analysis for Available Target Domain Sample Numbers\n",
            "\u001b[35mSection_title: 6.4  Impact of Threshold\n",
            "\u001b[32mconclusion: in this paper, we explore the possibility of transferring knowledge for unsupervised domain adaptation on question answering, without access to initial domain data. we proposed a novel self-trainingbased approach, mdaqa. we specially design an attention mask module to automatically keep key knowledge from the source domain and learn to mitigate domain shift between source and target domains. the module can be easily integrated into existing language models. our comprehensive experiments on well-known benchmark datasets demonstrate that mdaqa outperforms previous methods by a clear margin. it can also achieve decent performance even when the available target domain data is highly limited. this makes mdaqa have a very wide range of application scenarios.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[32mlimitation: since the use of source-free uda is mostly discussed in the medical field at the moment, ideally, more experiments on medical datasets would be more convincing. however, since available medical qa datasets are extremely limited and hard to access, currently we still conduct experiments on commonly used general-purpose qa datasets.\n",
            "\u001b[35mSection_title: 8  Limitation\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Non English resources\n",
            "\u001b[35mSection_title: 3  A new Hungarian question answering benchmark dataset\n",
            "\u001b[35mSection_title: 3.1  Creation of the corpus\n",
            "\u001b[35mSection_title: 3.2  Features of the corpus\n",
            "\u001b[35mSection_title: 4  Models and performance\n",
            "\u001b[35mSection_title: 4.1  Document retrieval models\n",
            "\u001b[35mSection_title: 4.2  Reader models\n",
            "\u001b[32mconclusion: we presented a new qa benchmark database in hungarian, that in several aspects, goes beyond squad-type datasets: it is not limited to single contiguous short extractive answer spans, contains yes/no questions, non-contiguous multispan short answers, long answers, questions requiring arith-model short answers long answers with multispan no multispan with multispan no multispan  table 5: performance of extractive reader models on short and long answer spans with and without multispan answers. metic reasoning, and other questions where the answer cannot be simply copied from the text. the annotation was created using a customized label-studio-based annotation platform. the annotators were encouraged to get actively involved in selecting the texts to be annotated and to abandon annotation of uninteresting or low quality texts in order to make the annotation task less boring and demotivating. we also trained and evaluated baseline models for document retrieval and reader models for answer span extraction. cross-lingual knowledge transfer naturally facilitated by multilingual transformer models was found to be beneficial for the quality of the trained models.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  PROGRAMFC\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  Program-Guided Reasoning\n",
            "\u001b[35mSection_title: 3.3  Reasoning Program Generation\n",
            "\u001b[35mSection_title: 3.4  Sub-Task Functions\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Main Results\n",
            "\u001b[35mSection_title: 4.2  How Does the Reasoning Program Help?\n",
            "\u001b[35mSection_title: 4.3  Interpretability of Reasoning Programs\n",
            "\u001b[35mSection_title: 4.4  Closed-Book Fact-Checking\n",
            "\u001b[32mconclusion: we proposed programfc, a few-shot neurosymbolic model for fact-checking that learns to map input claims to a reasoning program consisting of a sequence of sub-task function calls for answering a question, for fact-checking a simple claim, and for computing a logical expression. then factchecking is performed by executing that program. programfc combines the advantages of symbolic programs, such as explainability, with the flexibility of end-to-end neural models. using codex as the program generator, programfc demonstrates promising performance on hover and feverous with only a small number of incontext demonstrations and no additional training. we also investigated the impact of model size and the benefits of programs for retrieval, and we analyzed the errors. the results indicated that pro-gramfc effectively balances model capability, learning efficiency, and interpretability.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  IR for MultiHop QA\n",
            "\u001b[35mSection_title: 3.2  Performance on HOTPOTQA\n",
            "\u001b[35mSection_title: 3.3  Zero-shot experiment on Wikihop\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we introduce an entity-centric approach to ir that finds relevant evidence required to answer multihop questions from a corpus containing millions of paragraphs leading to significant improvement to an existing qa system.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Edge Probing\n",
            "\u001b[35mSection_title: 2.1  Formulation\n",
            "\u001b[35mSection_title: 2.2  Edge Probing Tasks and Datasets\n",
            "\u001b[35mSection_title: 3  Annotation Artifacts in EP Test Datasets\n",
            "\u001b[35mSection_title: 4  Do the EP Models Use Heuristics?\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results and Analysis\n",
            "\u001b[35mSection_title: 5  EP Test Results: Random vs\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: ep tests are classification tasks to measure an llm's ability to encode syntactic and semantic knowledge.however, in many ep datasets, there is not a significant difference between the random vs pre-trained encoders, which raises questions about the validity of the tests (the \"classifier knowledge\" problem).we analyze 17 datasets across 10 datasets to find various biases and show that the ep classifiers are more prone to use heuristic mechanisms when random encoders are used instead of the pre-trained ones.when the dataset biases are removed, the pre-trained encoders do show a significant difference from the random ones as expected.information-theoretic probes have been proposed before to solve the \"classifier knowledge\" problem, we show why they might not be necessary.future work would extend the findings of this study to fine-tuned models.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Experiments\n",
            "\u001b[35mSection_title: 2.1  Datasets\n",
            "\u001b[35mSection_title: 2.2  Models\n",
            "\u001b[35mSection_title: 2.3  Training Data\n",
            "\u001b[35mSection_title: 2.4  Implementation\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 3.1  Main Trends\n",
            "\u001b[35mSection_title: 3.2  Same Dataset, Different Format\n",
            "\u001b[35mSection_title: 3.3  Quantifying the Relative Impact of Parameters versus Examples\n",
            "\u001b[35mSection_title: 3.4  Comparison with Massive Models\n",
            "\u001b[35mSection_title: 3.5  Discussion\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this work, we present an empirical investigation on the relationships between (1) a task's format, (2) the number of labeled examples available for said task, and (3) the number of parameters the model tackling the task has. through our extensive experiments, we determine that task format greatly affects the relative performance improvement that can be expected from increased training set size and parameter count. for tasks that do not require the recollection of specific external information -i.e. classification, multiple choice, and extractive qa -we find that more labeled data and larger models both reliably improve performance. in fact, for some of these tasks, adding a few hundred labeled examples is more beneficial than scaling up the model size by billions of parameters. it seems then, from a practitioner's perspective, that for many tasks where data is very sparse, the tried-and-true strategy of simply collecting more training data will often be a more effective strategy than attempting to scale to larger, more computationally-demanding models. however, the picture is very different for open qa tasks; for such tasks, we find that increasing the size of the training data barely improves performance, leaving parameter inflation as the only reliable approach to improve accuracy. finally, we provide a hypothesis to explain these results and conclude with a practical corollary -when possible, changing the format from open qa into a more \"self-contained\" one will allow labeled data to bridge performance gaps between moderately-sized models and much larger ones.  table 2 provides the results from our main experiment (section 3, figure 2) in tabular form.  \n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Preliminaries\n",
            "\u001b[35mSection_title: 2.2  Retriever\n",
            "\u001b[35mSection_title: 2.3  Reranker\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Passage Retrieval\n",
            "\u001b[35mSection_title: 3.2  Document Retrieval\n",
            "\u001b[35mSection_title: 4  Ablation Study and Analysis\n",
            "\u001b[35mSection_title: 4.2  Input Sequence Length\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 5.1  Large Language Models\n",
            "\u001b[35mSection_title: 5.2  Multi-Stage Text Retrieval\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Pipeline Model: QUARK\n",
            "\u001b[35mSection_title: 3.1  Sentence Scoring Module\n",
            "\u001b[35mSection_title: 3.2  Question Answering Module\n",
            "\u001b[35mSection_title: 3.3  Bringing it Together\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Ablation\n",
            "\u001b[32mconclusion: our work shows that on the hotpotqa tasks, a simple pipeline model can do as well as or better than more complex solutions, such as graph networks, cross-document attention, or ner. powerful pre-trained models allow us to score sentences one at a time, without looking at other paragraphs. by operating jointly over these sentences chosen from multiple paragraphs, we arrive at answers and supporting sentences on par with state-of-theart approaches. this result shows that supporting sentence identification in hotpotqa is itself not a multi-hop problem, and suggests focusing on other multi-hop datasets to demonstrate the value of more complex retrieval techniques.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 3.  Empirical Study and Findings:\n",
            "\u001b[35mSection_title: 2.1  INSTRUCTION TUNING\n",
            "\u001b[35mSection_title: 2.2  LLM AGENTS\n",
            "\u001b[35mSection_title: 3  LEARNING THROUGH COMMUNICATION\n",
            "\u001b[35mSection_title: 3.1  EXPLORATION PHASE\n",
            "\u001b[35mSection_title: 3.2  TRAINING PHASE\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  DATASETS\n",
            "\u001b[35mSection_title: 4.2  SETTINGS\n",
            "\u001b[35mSection_title: 4.3  RESULTS\n",
            "\u001b[35mSection_title: 5  DISCUSSION\n",
            "\u001b[32mconclusion: we introduced learning-through-communication (ltc), a paradigm that adapts llm agents to new tasks and environments via communication-based iterative learning.within this ltc framework, we have designed three communication modes for common tasks including decision-making, knowledgeintensive reasoning, and numeric reasoning.these communication modes facilitate interactions between llm agents and their environments, as well as other agents such as gpt-4 and humans.the history of these interactions can be autonomously organized into training data for ppo training so that the agent can adapt to the new task.our approach represents a closed loop where the agent self-interacts with the environment or other agents, and learning to improve itself with minimal human intervention.empirically, we have demonstrated that ltc performs strongly in success rate and efficiency across three different tasks: alfworld, hotpotqa, and gsm8k.it consistently outperforms existing llm agent and instruction tuning baselines, showing the promise of the ltc paradigm in adapting llm agents to new tasks and environments with minimal human effort.as for future work, we plan to explore more diverse communication patterns for different tasks, and involve the communication with human during the iterative learning process.we will open source our code to facilitate further research in this line.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Embedding-based Methods\n",
            "\u001b[35mSection_title: 2.2  GNN-based Methods\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Model Framework\n",
            "\u001b[35mSection_title: 3.2  Graph Partition\n",
            "\u001b[35mSection_title: 3.3  Subgraph Encoder\n",
            "\u001b[35mSection_title: 3.3.1  Attributed Value Encoder\n",
            "\u001b[35mSection_title: 3.3.2  Mean Aggregator\n",
            "\u001b[35mSection_title: 3.4  Graph Alignment\n",
            "\u001b[35mSection_title: 3.5  Channel Ensemble\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Overall Performance\n",
            "\u001b[35mSection_title: 4.2.1  Regular Setting\n",
            "\u001b[35mSection_title: 4.2.2  Hard Setting\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  Case Study of Attributes and Values\n",
            "\u001b[32mconclusion: we propose a novel ea model (attrgnn) and contribute a hard experimental setting for practical evaluation. attrgnn can integrate both attribute and relation triples with varying importance for better performance. experimental results under the regular and hard settings present significant improvements of our proposed model, and the severe dataset bias can be effectively alleviated in our proposed hard setting.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Basic concepts\n",
            "\u001b[35mSection_title: 2.1  Rewriting logic\n",
            "\u001b[35mSection_title: 2.2  Maude\n",
            "\u001b[35mSection_title: 3.  Approach\n",
            "\u001b[35mSection_title: 3.1  Construction of the graph-based compositions model\n",
            "\u001b[35mSection_title: 3.2  Construction of the composition plan\n",
            "\u001b[35mSection_title: 4.  Case study\n",
            "\u001b[32mconclusion: in this work, we showed how the formalism of graphs can be used to improve the composition of web services and make it automatic. more precisely, we have proposed rewriting logic and its maude language as a support for a graph-based approach for automatic composition of web services. the proposed model has made possible the exploration of different composition schemas as well as the formal analysis of service compositions. our contribution has broadly followed two main steps:\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Contextual Encoding\n",
            "\u001b[35mSection_title: 3.2  Interpretable Reasoning\n",
            "\u001b[35mSection_title: 3.3  Answer Prediction\n",
            "\u001b[35mSection_title: 3.4  Model Training\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Baseline\n",
            "\u001b[35mSection_title: 5.2  Case Study of Interpretability\n",
            "\u001b[35mSection_title: 5.3  Neural Network Module Performance\n",
            "\u001b[35mSection_title: 5.4  Error Analysis\n",
            "\u001b[35mSection_title: 5.4.1  Type One Error\n",
            "\u001b[35mSection_title: 5.4.2  Type Two Error\n",
            "\u001b[32mconclusion: in this paper, we aim to answer ropes questions in an interpretable way by leveraging five neural network modules. these modules are trained in an end-to-end manner and each module provides transparent intermediate outputs. experimental results demonstrate the effectiveness of each module, and analysis on intermediate outputs presents good interpretability for the inference process in contrasted with \"black box\" models. moreover, we find that with explicitly designed compositional modeling of inference process, our approach with a few training examples achieves similar accuracy to strong baselines with full-size training data which indicates a better generalization capability. meanwhile, extending these models to a larger scope of question types or more complex scenarios is still a challenge, and we will further investigate the trade-off between explainability and scalability.   table 8 shows one labelled example, and the process of adding auxiliary supervision label contain the following steps: table 7: detailed parameters used in answer prediction, we provide search bounds for each hyperparameter and list out the hyperparameters combination for out best model and baseline model. other unmentioned parameters keep same as the one used in bert.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Annotation Framework\n",
            "\u001b[35mSection_title: 3.1  Framework Overview\n",
            "\u001b[35mSection_title: 3.2  Annotation Schemes\n",
            "\u001b[35mSection_title: 3.3  Annotation Costs\n",
            "\u001b[35mSection_title: 4  Learning a Cost-Effective Policy\n",
            "\u001b[35mSection_title: 4.1  Objective\n",
            "\u001b[35mSection_title: 4.2  Annotation Procedure and Learning\n",
            "\u001b[35mSection_title: 4.3  Policy Updates\n",
            "\u001b[35mSection_title: 4.4  Model Updates\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Implementation Details\n",
            "\u001b[35mSection_title: 6  Experimental Results\n",
            "\u001b[35mSection_title: 6.1  Performance on Answer Span Annotations\n",
            "\u001b[35mSection_title: 6.2  Cost-Performance Sensitivity Analysis\n",
            "\u001b[35mSection_title: 6.3  Performance on Full Dataset Annotation\n",
            "\u001b[35mSection_title: 7  Discussion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  ASQA Task and Data\n",
            "\u001b[35mSection_title: 3.1  ASQA Annotation Objectives\n",
            "\u001b[35mSection_title: 3.2  ASQA Annotation Process\n",
            "\u001b[35mSection_title: 3.3  ASQA Dataset\n",
            "\u001b[35mSection_title: 4  ASQA Metrics\n",
            "\u001b[35mSection_title: 4.1  Automated Evaluation\n",
            "\u001b[35mSection_title: 4.2  Human Evaluation\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Models\n",
            "\u001b[35mSection_title: 5.2  Human Performance\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 7  Analysis\n",
            "\u001b[32mconclusion: in contrast to existing datasets for long-form qa, asqa admits a clear notion of correctness that we use to define an overall metric of performance (dr). our empirical evaluations demonstrate that dr correlates well with the human judgment; and there is a large gap between human performance and the strong baselines. thus, we believe that asqa is an appealing task for the qa community. our analysis suggests that strong performance on asqa is contingent upon both high-quality retrieval and summarization. these aspects constitute important directions for future work on asqa.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: we now make two remarks that we urge the reader to consider when interpreting the results of this work.inter-annotator agreement in section 3.3, we observed that inter-annotator agreement in asqa is higher than in eli5. we note, however, that the high inter-annotator agreement in asqa is contingent upon the high inter-annotator agreement in the ambigqa dataset. indeed, ambigqa disambiguations serve as a shared source of information between the two asqa annotators working on the same instance, potentially inflating the level of agreement.that said, min et al. (2020) observe that human annotators have a decent level of agreement in constructing the disambiguations in ambigqa, thereby supporting the observation that asqa is more objective than eli5.evaluation metrics second, we caveat that our accuracy metrics (str-em and disambig-f1) only measure the recall of the required information in the long answers. in cases where the long answer hallucinates incorrect disambiguations or facts, the accuracy metrics may still be high as long as the correct disambiguations are included. we note, however, that this unnecessary extra information may still be penalized by the rouge-l metric. moreover, in the presence of distractors, we also expect the accuracy of the roberta model used for reading comprehension to degrade, thereby effectively penalizing a low precision.on a separate note, the disambig-f1 metric requires a high-accuracy qa system. hence, for domains that are significantly different from wikipedia, fine-tuning the roberta squadv2 model on the task might be important to ensure the effectiveness of the disambig-f1 metric. and receive an answer within a day. to support this mechanism, we allowed annotators to \"park\" an annotation task they were unsure about and return to it after they have their concerns resolved.annotators' well-being for this study, we recruited annotators who were fully dedicated to our task (8 hours a day for 5 days a week). to reduce the pressure on annotators and allow them to work at a comfortable pace, we gave annotators one hour to answer each question and recommended answering ten or more questions per day. on average, it took annotators 15 minutes to answer each question with the time consumption slightly decreasing as annotators get familiar with the task. the compensation rate for the task was set to be $17.8/hour which is higher than the minimum hourly wage in the us.\n",
            "\u001b[35mSection_title: 9  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Simulated data\n",
            "\u001b[35mSection_title: 2  Conflicting Joint Genotyping Calls\n",
            "\u001b[35mSection_title: 3  Example complex variant\n",
            "\u001b[35mSection_title: 4  Software versions and command lines\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Problem definition\n",
            "\u001b[35mSection_title: 4  QUARTET model\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Explanation accuracy\n",
            "\u001b[35mSection_title: 6  Downstream Task\n",
            "\u001b[35mSection_title: 6.1  Accuracy on a QA task\n",
            "\u001b[35mSection_title: 6.2  Correlation between QA and Explanation\n",
            "\u001b[35mSection_title: 7  Error analysis\n",
            "\u001b[35mSection_title: 1.  Multiple explanations:\n",
            "\u001b[32mconclusion: explaining the effects of a perturbation is critical, and we have presented the first system that can do this reliably. quartet not only predicts meaningful explanations, but also achieves a new state-of-the-art on the end-task itself, leading to an interesting finding that models can make better predictions when forced to explain. our work opens up new directions for future research: 1) can additional background context from the web improve explainable reasoning? 2) can such structured explanations be applied to other nlp tasks? we look forward to future progress in this area.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.2  Gene identification and cloning\n",
            "\u001b[35mSection_title: 1.5  Materials\n",
            "\u001b[35mSection_title: 1.6  Solubility test of BHET and TPA mixture\n",
            "\u001b[35mSection_title: 1.7  Assays and mechanoenzymatic reactions\n",
            "\u001b[35mSection_title: 2.2  Synthesis\n",
            "\u001b[35mSection_title: 2.3  NMR spectra\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Text Reasoning Meaning Representation\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this work, we present r 3 , a large-scale reading comprehension dataset in which a qa system is required to give answers to questions over diverse natural language, but also needed to present the reasoning processes. we hope this dataset can facilitating the development of explainable qa systems. \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Approach\n",
            "\u001b[35mSection_title: 4.  Answer selection:\n",
            "\u001b[35mSection_title: 4  Experimental Results\n",
            "\u001b[35mSection_title: 4.1  Evidence Retrieval Results\n",
            "\u001b[35mSection_title: 4.2  Answer Selection Results\n",
            "\u001b[35mSection_title: 5  Representational Analysis\n",
            "\u001b[35mSection_title: 5.1  Attention Analysis\n",
            "\u001b[35mSection_title: 5.2  Learned Embedding Analysis\n",
            "\u001b[32mconclusion: we introduced a simple unsupervised approach for retrieving candidate evidence chains that after reranking achieves state-of-the-art evidence retrieval performance on two multi-hop qa datasets: qasc and multirc. we highlight the importance of generating and feeding candidate evidence chains by showing several benefits over the widely followed approach that retrieves evidence sentences individually. further, we introduced few attention and embedding analyses demonstrating that jointly retrieving and reranking chains assist in learning compositional information, which is also beneficial to the downstream qa task. overall, our work highlights the strengths and potential of joint retrieval+reranking approaches for future works.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Scientific Explanation: The Epistemological Perspective\n",
            "\u001b[35mSection_title: 2.1  Explanation as an Argument\n",
            "\u001b[35mSection_title: 2.1.1  Deductive-Inductive Arguments\n",
            "\u001b[35mSection_title: 2.1.2  Explanatory Unification and Argument Patterns\n",
            "\u001b[35mSection_title: 2.2  Fitting the Explanandum into a Discernible Pattern\n",
            "\u001b[35mSection_title: 2.2.1  Statistical-Relevance\n",
            "\u001b[35mSection_title: 2.2.2  Causes and Mechanisms\n",
            "\u001b[35mSection_title: 2.3  Summary\n",
            "\u001b[35mSection_title: 3  Scientific Explanation: The Linguistic Perspective\n",
            "\u001b[35mSection_title: 3.1  Biology Why Questions\n",
            "\u001b[35mSection_title: 3.1.1  Recurring Explanatory Sentences\n",
            "\u001b[35mSection_title: 3.2  Science Questions\n",
            "\u001b[35mSection_title: 3.2.1  Distribution and Reuse of Explanatory Sentences\n",
            "\u001b[35mSection_title: 3.2.2  Abstraction and Patterns of Unification\n",
            "\u001b[35mSection_title: 3.3  Summary\n",
            "\u001b[35mSection_title: 5  Implications for Explanation-based AI\n",
            "\u001b[32mconclusion: in order to provide an epistemologically grounded characterisation of natural language explanations, this paper attempted to bridge the gap in the notion of scientific explanation (salmon, 2006;salmon, 1984), studying it as both a formal object and as a linguistic expression. the combination of a systematic survey with a corpus analysis on natural language explanations (jansen et al., 2014;jansen et al., 2018),\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Our Approach\n",
            "\u001b[35mSection_title: 3.1  Word Transformation Types\n",
            "\u001b[35mSection_title: 3.2  Reforming Pointer Generator Network\n",
            "\u001b[35mSection_title: 3.2.1  The Encoder\n",
            "\u001b[35mSection_title: 3.2.2  The Decoder\n",
            "\u001b[35mSection_title: 3.3  Reforming UNILM\n",
            "\u001b[35mSection_title: 4.2  Performance Evaluation\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[32mconclusion: in this paper, we discover two major issues in the existing neural qg models. to tackle the two issues, we propose this enhancing approach for qg and apply the approach to two typical sequenceto-sequence models, i.e., the pointer generator network and unilm. we further conduct extensive experiments using squad and marco datasets. the experimental results show that improved versions of models can significantly enhance the quality of qg and speed up the decoding.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Understanding in Humans\n",
            "\u001b[35mSection_title: 2.2  'Understanding' in Machines\n",
            "\u001b[35mSection_title: 2.3  Reasoning an RC Model should Perform\n",
            "\u001b[35mSection_title: 2.4  Reasoning an RC Model does Perform\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Datasets and Models\n",
            "\u001b[35mSection_title: 3.2  Counterfactual Explanations\n",
            "\u001b[35mSection_title: 3.3  Saliency-based Explanations\n",
            "\u001b[35mSection_title: 4  Results & Analysis\n",
            "\u001b[35mSection_title: 4.1  Base Model Performance\n",
            "\u001b[35mSection_title: 4.2  Counterfactual Explanations\n",
            "\u001b[35mSection_title: 4.3  Alignment Score\n",
            "\u001b[35mSection_title: 4.4  Generalization Tests\n",
            "\u001b[35mSection_title: 4.5  Heuristics for Coreference Questions\n",
            "\u001b[35mSection_title: 5  Discussion and Related Work\n",
            "\u001b[32mconclusion: making progress towards trustworthy nlp models requires specific definitions for the behavior expected of these models in different situations. we propose a framework for rc model analysis that involves: (a) the definition of the expected 'reasoning' steps; (b) analysis of model behavior. we contribute such definitions for two linguistic 'skills' (comparison and coreference resolution), and use parallel explainability techniques to investigate whether rc models based on bert family encoders answer such questions correctly for the right reasons. we find that to be the case for comparison, but not for coreference. moreover, we find that, even for comparison, the models 'break' when encountering out-of-distribution counterfactual perturbations, suggesting that they memorize specific lexical patterns rather than learn more general reasoning 'skills'. as such, more research is needed on developing definitions and tests for specific 'skills' expected of nlu models, as well as on more faithful interpretability techniques.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 7  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 2.1  Cross-lingual transfer learning\n",
            "\u001b[35mSection_title: 2.2  Self-training\n",
            "\u001b[35mSection_title: 3  Proposed Approach\n",
            "\u001b[35mSection_title: 3.1  Self-training process\n",
            "\u001b[35mSection_title: 3.2  Labeling process\n",
            "\u001b[35mSection_title: 4.1  Model\n",
            "\u001b[35mSection_title: 4.2  Datasets\n",
            "\u001b[35mSection_title: 4.3  Pseudo-labeling\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Quantitative results\n",
            "\u001b[35mSection_title: 5.2  Qualitative analysis\n",
            "\u001b[32mconclusion: this paper presents the first self-training approach to improve cross-lingual machine reading comprehension. the experiments were conducted on largescale datasets in four different languages. the results showed that our approach improved the perfor-mance significantly compared to the baseline with 1 -16 em and 1 -8 f1 scores. we also analyzed how self-training improves cross-lingual reading comprehension in several aspects and found that improvements are correlated to zero-shot performance but not the number of pseudo-labels. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Research Area\n",
            "\u001b[35mSection_title: 2.2.  Romanization and Viticulture\n",
            "\u001b[35mSection_title: 2.2.  Romanization and Viticulture\n",
            "\u001b[35mSection_title: 2.3.  Dataset\n",
            "\u001b[35mSection_title: 2.3.1.  Archaeological Dataset\n",
            "\u001b[35mSection_title: 2.3.2.  Topographic Dataset and Socio-Economic Dataset\n",
            "\u001b[35mSection_title: 2.4.  Predictive Modelling\n",
            "\u001b[35mSection_title: 2.4.1.  Modelling Modules and Automatization Procedure\n",
            "\u001b[35mSection_title: 2.4.2.  Variable Selection by Expert Knowledge\n",
            "\u001b[35mSection_title: 2.4.3.  Automated Variable Selection by Statistical Dispersion\n",
            "\u001b[35mSection_title: 2.5.  Visualization of Results in an Interactive Web Map\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Spatial Distribution of Archaeological Sites\n",
            "\u001b[35mSection_title: 3.2.  Predictive Modelling\n",
            "\u001b[35mSection_title: 3.2.1.  Variable Selection by Expert Knowledge\n",
            "\u001b[35mSection_title: 3.2.2.  Automated Variable Selection by Statistical Dispersion\n",
            "\u001b[35mSection_title: 3.3.  Interactive Web Map\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[32mconclusion: the general objective of the present paper was to calculate the suitabilities for archeological sites in the laetanian region related to intensive and specialized roman viticulture in order to determine the underlying factors of their distribution. therefore, the archaeological dataset of 82 documented wine-pressing facilities was used as response variable in a predictive modelling approach. furthermore, 15 topographical and 6 socio-economic location characteristics were taken into account, which may have had, as stated by other authors, an influence on the distribution of the sites. in a first step, two models were developed where the variable selection was based on expert knowledge from previous studies on roman agriculture and viticulture. however, this work additionally aimed at semi-automating the process of variable selection. hence, the variables were selected using statistical distribution metrics. the resulting models with automated variable selection showed good performance. they serve well as a first approximation of suitability modelling when no a priori knowledge on operating processes and interactions influencing the spatial distribution of sites is available. consequently, this modeling approach can be used for theory building. however, it is of special interest that the best prediction performance was obtained by an expert knowledge model utilizing a combination of predictor variables that is based on the specific recommendations on viticulture by lucius junius moderatus columella, the prominent ancient roman agronomist. the model was used to make the first assumptions and theories about the underlying factors that had an impact on the development of viticulture in the laetanian region. the results indicate that the accessibility of a location and its connectivity to the local and regional distribution centres and trade routes, determined by terrain steepness, was decisive for the settlement of winemaking facilities. moreover, on the basis of the predictive model and findings on experimental archaeology, the maximum number of winegrowing facilities that could have existed in the laetanian region in roman times was extrapolated to 360. as the applied modeling approach focusses on the average 50% and 75% of wine pressing facilities, some site locations could not be explained by the predictive model. thus, future research should particularly focus on those locations to find reliable hypotheses for settlement under these \"marginal\" and \"less suitable\" conditions. this is expected to provide more detailed insights into the complexity of ancient rural settlement and viticulture in the laetanian region.\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Dataset\n",
            "\u001b[35mSection_title: 3.1  Data Crawling\n",
            "\u001b[35mSection_title: 3.2  Data Processing\n",
            "\u001b[35mSection_title: 3.3  Question-Answer Writing\n",
            "\u001b[35mSection_title: 4  Dataset Statistics\n",
            "\u001b[35mSection_title: 4.1  Judgement\n",
            "\u001b[35mSection_title: 4.2  Calculation\n",
            "\u001b[35mSection_title: 4.3  Reasoning\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  MeDia-BERT\n",
            "\u001b[35mSection_title: 5.2  Compared Methods\n",
            "\u001b[35mSection_title: 5.3  Experiments Results\n",
            "\u001b[32mconclusion: in this paper, we construct mediaqa, a novel dataset for qa on medical dialogues and propose a method media-bert based on the state-of-the-art pretrained language model. the proposed dataset shows the distinctiveness of medical dialogues compared with other normal domains in the context of qa. due to the unique characteristics, it is challenging for existing qa models compared with human performance. we hope our dataset will lead a deeper research on machine reading comprehension in medical domain.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Thought Flow Networks\n",
            "\u001b[35mSection_title: 2.1  Inspiration: Hegel's Dialectics\n",
            "\u001b[35mSection_title: 2.2  Formalization of Thought Flow Concept\n",
            "\u001b[35mSection_title: 2.3  Implementation in Transformers for QA\n",
            "\u001b[35mSection_title: 3  Question Answering Experiments\n",
            "\u001b[35mSection_title: 3.1  Data, Model and Training\n",
            "\u001b[35mSection_title: 3.2  Performance Improvements\n",
            "\u001b[35mSection_title: 3.3  Thought Flow Patterns\n",
            "\u001b[35mSection_title: 4  Human Evaluation\n",
            "\u001b[35mSection_title: 4.1  Experiment Design\n",
            "\u001b[35mSection_title: 4.2  Quantitative Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we introduced a task-agnostic self-correction formalism that turns a model's single output prediction into an evolving sequence of predictions-the thought flow. we take inspiration from hegel's dialectics and propose a correction module along with a gradient-based update rule that sequentially updates a model's output distributions in the direction of an increasing self-estimate of correctness. we apply our method to question answering models and conduct extensive experiments including human evaluation. we find that thought flows (i) can increase f 1 -scores up to 9.3%, (ii) exhibit complex self-correction patterns and (iii) provide significant improvements in human interaction and system perception including task performance and perceived system correctness and naturalness. a potential next step to further improve performance is learning to stop. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary Studies\n",
            "\u001b[35mSection_title: 2.1  Passage Retrieval with Dual-Encoders\n",
            "\u001b[35mSection_title: 2.2  Discrimination Issue in Dual-Encoders\n",
            "\u001b[35mSection_title: 3  Mixture-of-Textual-Experts\n",
            "\u001b[35mSection_title: 3.1  Mixture-of-Textual-Experts\n",
            "\u001b[35mSection_title: 3.2  Pre-training\n",
            "\u001b[35mSection_title: 3.3  Fine-tuning and Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Pre-training\n",
            "\u001b[35mSection_title: 4.2  Fine-tuning\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Comparision of Different Dual-encoders\n",
            "\u001b[35mSection_title: 5.2  Effect of Switching Pre-training Corpus of FFN-Experts\n",
            "\u001b[35mSection_title: 5.3  Quantitive Analysis for Embeddings\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[32mconclusion: this paper proposes to pre-train with the mixtureof-textual-experts to counter the imbalanced discrimination issue in existing dual encoders. textual-specific experts are introduced for individual modeling of the distinct traits of queries and passages. results on large-scale web bench-marks show steady improvement in retrieval performances. quantitive analysis shows a more balanced distribution of query-passage embeddings for dual-encoders.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Encoder\n",
            "\u001b[35mSection_title: 3.2  State Update Predictor\n",
            "\u001b[35mSection_title: 3.3  Multi-Perspective Dialogue Collaborative Selector\n",
            "\u001b[35mSection_title: 3.4  State Generator\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Metrics\n",
            "\u001b[35mSection_title: 4.2  Baseline Models\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Is It Beyond the Granularity?\n",
            "\u001b[35mSection_title: 5.2  Domain-Specific Dialogue Requirements\n",
            "\u001b[32mconclusion: we introduce an effective dicos-dst that dynamically selects the relevant dialogue contents corresponding to each slot from a combination of three perspectives. the dialogue collaborative selector module performs a comprehensive selection for each turn dialogue based on its relation to the slot name, its connection to the current turn dialogue, and the implicit mention oriented reasoning. then only the selected dialogue contents are fed into state generator, which explicitly minimizes the distracting information passed to the downstream state prediction. our dicos-dst model achieves new state-of-the-art performance on the multiwoz benchmark, and achieves competitive performance on most other dst benchmark datasets. the potential relationship among the above perspectives is a promising research direction, and we will explore it for more than dialogue selection in the future. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  List QA Dataset Generation\n",
            "\u001b[35mSection_title: 2.1  Answer Extraction\n",
            "\u001b[35mSection_title: 2.2  Question Generation\n",
            "\u001b[35mSection_title: 2.3  Iterative Filtering\n",
            "\u001b[35mSection_title: 2.4  Answer Expansion\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  List QA Models\n",
            "\u001b[35mSection_title: 3.3  Metrics\n",
            "\u001b[35mSection_title: 3.4  Results\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[35mSection_title: 4.1  Ablation Study\n",
            "\u001b[35mSection_title: 4.2  Answer Distribution\n",
            "\u001b[35mSection_title: 4.3  Question Types\n",
            "\u001b[35mSection_title: 4.4  Error Analysis\n",
            "\u001b[32mconclusion: herein, we introduced liquid, a framework that automatically generates list qa datasets from unlabeled corpora to alleviate the data scarcity problem in this field. our synthetic data significantly improved the performance of the current supervised models on five benchmark datasets. we thoroughly analyzed the effect of each component in liq-uid and generated data quantitatively and qualitatively.   table 10: distribution of answer types for the synthetic and bioasq 9b data. table 9 presents the distribution of the number of answers. similar to the results in the general domain (table 5), the synthetic data were more skewed toward smaller numbers of answers than the labeled data, but some answers (14.2%) had four or more spans.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  List QA Dataset Generation\n",
            "\u001b[35mSection_title: 2.1  Answer Extraction\n",
            "\u001b[35mSection_title: 2.2  Question Generation\n",
            "\u001b[35mSection_title: 2.3  Iterative Filtering\n",
            "\u001b[35mSection_title: 2.4  Answer Expansion\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  List QA Models\n",
            "\u001b[35mSection_title: 3.3  Metrics\n",
            "\u001b[35mSection_title: 3.4  Results\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[35mSection_title: 4.1  Ablation Study\n",
            "\u001b[35mSection_title: 4.2  Answer Distribution\n",
            "\u001b[35mSection_title: 4.3  Question Types\n",
            "\u001b[35mSection_title: 4.4  Error Analysis\n",
            "\u001b[32mconclusion: herein, we introduced liquid, a framework that automatically generates list qa datasets from unlabeled corpora to alleviate the data scarcity problem in this field. our synthetic data significantly improved the performance of the current supervised models on five benchmark datasets. we thoroughly analyzed the effect of each component in liq-uid and generated data quantitatively and qualitatively.   table 10: distribution of answer types for the synthetic and bioasq 9b data. table 9 presents the distribution of the number of answers. similar to the results in the general domain (table 5), the synthetic data were more skewed toward smaller numbers of answers than the labeled data, but some answers (14.2%) had four or more spans.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Setup\n",
            "\u001b[35mSection_title: 3.1  Problem Statement and Evaluation\n",
            "\u001b[35mSection_title: 3.2  Data\n",
            "\u001b[35mSection_title: 3.3  Uncompressed Retrieval Peformance\n",
            "\u001b[35mSection_title: 4  Compression Methods\n",
            "\u001b[35mSection_title: 4.1  Random Projection\n",
            "\u001b[35mSection_title: 4.2  Principal Component Analysis\n",
            "\u001b[35mSection_title: 4.3  Autoencoder\n",
            "\u001b[35mSection_title: 4.4  Precision Reduction\n",
            "\u001b[35mSection_title: 4.5  Combination of PCA and Precision Reduction\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Model Comparison\n",
            "\u001b[35mSection_title: 5.2  Data size\n",
            "\u001b[35mSection_title: 5.3  Retrieval errors\n",
            "\u001b[35mSection_title: 5.4  Pitfalls of Reconstruction Loss\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[35mSection_title: 7  Summary\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Nonparametric Decoding\n",
            "\u001b[35mSection_title: 3.1  Key Benefits\n",
            "\u001b[35mSection_title: 3.2  BASE Nonparametric Decoding\n",
            "\u001b[35mSection_title: 3.3  ASYNC Nonparametric Decoding\n",
            "\u001b[35mSection_title: 3.4  CONTRASTIVE Nonparametric Decoding\n",
            "\u001b[35mSection_title: 3.5  Clustering\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Datasets & Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Details of Constructing CE\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 5.1  Vanilla Decoding vs. Np Decoding\n",
            "\u001b[35mSection_title: 5.2  Benefits of Nonparametric Decoding\n",
            "\u001b[35mSection_title: 5.3  What is Well-Constructed Contextualized Embedding Matrix (CE)?\n",
            "\u001b[32mlimitation: np decoding uses k-means clustering to reduce the number of contextualized embeddings, the performance varies by how the contextualized embeddings are clustered. as the process is relatively inconsistent, reducing the number with other methods would make the model performance more consistent. also, as it is not trivial to add new contextualized token embeddings on top of preconstructed ce due to the clustering step, we did not perform on dynamic corpus setup where new items are added or updated. np decoding is applicable to all generative retrieval models including gmr or seal which needs all token embeddings, however, we focused on generative retrieval models with representative output as the retrieval target in this work. also, while it is a general approach applicable to all encoder-decoder models, we focused on applying the method to t5.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  CRITIC: CORRECTING WITH TOOL-INTERACTIVE CRITIQUING\n",
            "\u001b[35mSection_title: 3.1  IN-CONTEXT LEARNING FOR LLMS\n",
            "\u001b[35mSection_title: 3.2  INTERACTION WITH EXTERNAL TOOLS\n",
            "\u001b[35mSection_title: 3.3  VERIFICATION WITH TOOL-INTERACTION\n",
            "\u001b[35mSection_title: 3.4  CORRECTION WITH CRITIQUES\n",
            "\u001b[35mSection_title: 4.2  MATHEMATICAL PROGRAM SYNTHESIS\n",
            "\u001b[35mSection_title: 4.3  TOXICITY REDUCTION\n",
            "\u001b[35mSection_title: 4.4  ADDITIONAL ABLATIONS AND ANALYSIS\n",
            "\u001b[32mconclusion: we propose critic, a novel plug-and-play framework that empowers frozen llms to self-verify and self-correct by interacting with the external environment. leveraging the intuition of critical thinking with external feedback, critic enables llms to validate their knowledge and improve their answers through introspection without requiring further training. experiments on diverse tasks and datasets have consistently shown the effectiveness, generality, and interoperability of critic. moreover, we shed light on the unreliability of llms in self-verification, highlighting the potential of external tool interaction to solve this problem. we hope our findings will inspire further exploration into the truthfulness of language models, ultimately leading to more trustworthy ai systems. \n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\u001b[35mSection_title: 2.  Truthfulness:\n",
            "\u001b[35mSection_title: 2.  Truthfulness:\n",
            "\u001b[35mSection_title: 2.  Truthfulness:\n",
            "\u001b[35mSection_title: 2.  Truthfulness:\n",
            "\u001b[35mSection_title: 2.  Truthfulness:\n",
            "\u001b[35mSection_title: 2.  Truthfulness:\n",
            "\u001b[35mSection_title: 2.  Truthfulness:\n",
            "\u001b[35mSection_title: 2.  Let's check the code:\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  METHOD\n",
            "\u001b[35mSection_title: 2.2  MULTI-HOP DENSE RETRIEVAL\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  EXPERIMENTS: RETRIEVAL\n",
            "\u001b[35mSection_title: 3.1.1  DIRECT\n",
            "\u001b[35mSection_title: 3.1.2  RERANKING\n",
            "\u001b[35mSection_title: 3.1.3  ANALYSIS\n",
            "\u001b[35mSection_title: 3.2  EXPERIMENTS: HOTPOTQA\n",
            "\u001b[35mSection_title: 3.2.1  RESULTS\n",
            "\u001b[35mSection_title: 3.3  EXPERIMENTS: MULTI-EVIDENCE FEVER\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[32mconclusion: in this work, we generalized the recently proposed successful dense retrieval methods by extending them to the multi-hop setting. this allowed us to handle complex multi-hop queries with much better accuracy and efficiency than the previous best methods. we demonstrated the versatility of our approach by applying it to two different tasks, using a variety of downstream modules. in addition, the simplicity of the framework and the fact that it does not depend on a corpus-dependent graph structure opens the possibility of applying such multi-hop retrieval methods more easily and broadly cross different domains and settings.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  METHOD\n",
            "\u001b[35mSection_title: 2.2  MULTI-HOP DENSE RETRIEVAL\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  EXPERIMENTS: RETRIEVAL\n",
            "\u001b[35mSection_title: 3.1.1  DIRECT\n",
            "\u001b[35mSection_title: 3.1.2  RERANKING\n",
            "\u001b[35mSection_title: 3.1.3  ANALYSIS\n",
            "\u001b[35mSection_title: 3.2  EXPERIMENTS: HOTPOTQA\n",
            "\u001b[35mSection_title: 3.2.1  RESULTS\n",
            "\u001b[35mSection_title: 3.3  EXPERIMENTS: MULTI-EVIDENCE FEVER\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[32mconclusion: in this work, we generalized the recently proposed successful dense retrieval methods by extending them to the multi-hop setting. this allowed us to handle complex multi-hop queries with much better accuracy and efficiency than the previous best methods. we demonstrated the versatility of our approach by applying it to two different tasks, using a variety of downstream modules. in addition, the simplicity of the framework and the fact that it does not depend on a corpus-dependent graph structure opens the possibility of applying such multi-hop retrieval methods more easily and broadly cross different domains and settings.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The PullNet Model\n",
            "\u001b[35mSection_title: 3.1  The Question Subgraph\n",
            "\u001b[35mSection_title: 3.2  Iterative subgraph construction\n",
            "\u001b[35mSection_title: 3.2.1  Overview\n",
            "\u001b[35mSection_title: 3.2.2  Pull Operations\n",
            "\u001b[35mSection_title: 3.2.3  Classify Operations\n",
            "\u001b[35mSection_title: 3.2.4  The Update Operation\n",
            "\u001b[35mSection_title: 3.3  Training\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Tasks\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Subgraph Retrieval for Baseline Models\n",
            "\u001b[35mSection_title: 4.5  Main Results\n",
            "\u001b[35mSection_title: 4.5.1  MetaQA\n",
            "\u001b[35mSection_title: 4.5.2  WebQuestionsSP\n",
            "\u001b[35mSection_title: 4.5.3  Complex WebQuestions\n",
            "\u001b[35mSection_title: 4.6  Additional Results\n",
            "\u001b[35mSection_title: 4.6.1  Retrieval Performance of PullNet\n",
            "\u001b[35mSection_title: 4.6.2  Training Time\n",
            "\u001b[32mconclusion: pullnet is a novel integrated qa framework for (1) learning what to retrieve from a kb and/or corpus and (2) reasoning with this heterogeneous data to find the best answer. unlike prior work, pullnet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. in each iteration, a graph cnn is used to identify subgraph nodes that should be expanded using \"pull\" operations on the corpus and/or kb. this iterative process makes it possible to retrieve a small graph that contains just the information relevant to a multi-hop question.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Datasets of reasoning from text\n",
            "\u001b[35mSection_title: 2.2  Reasoning using large language models\n",
            "\u001b[35mSection_title: 3  FOLIO Corpus Construction\n",
            "\u001b[35mSection_title: 3.1  Example collection\n",
            "\u001b[35mSection_title: 3.2  NL-FOL alignment review\n",
            "\u001b[35mSection_title: 3.3  FOL verification\n",
            "\u001b[35mSection_title: 3.4  NL review\n",
            "\u001b[35mSection_title: 3.5  Dataset statistics\n",
            "\u001b[35mSection_title: 4.2  NL-FOL translation\n",
            "\u001b[35mSection_title: 5  Methods\n",
            "\u001b[35mSection_title: 5.1  Logical reasoning\n",
            "\u001b[35mSection_title: 5.2  NL-FOL translation\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Dataset split\n",
            "\u001b[35mSection_title: 6.2  Evaluation metrics\n",
            "\u001b[35mSection_title: 6.3  Main results\n",
            "\u001b[35mSection_title: 7  Analysis\n",
            "\u001b[32mconclusion: we show the accuracy of the examples with different numbers of premises needed to reach the conclusions in figure 3. under the few-shot prompting setting, gpt-3 and   \n",
            "\u001b[35mSection_title: 7.1  Number of premises needed for conclusions\n",
            "\u001b[35mSection_title: 7.2  Comparison of the results between WikiLogic and HybLogic\n",
            "\u001b[32mconclusion: confusion matrices in figure 4 for the fine-tuning and 8-shot nl prompt results both show that lms are significantly better at making the correct predictions for conclusions with labels of true than the conclusions with labels of false or unknown. the accuracy on examples with false or unknown conclusions is 54.41% with fine-tuning and 36.91% with few-shot prompting. they also tend to make more predictions of true than the other labels.\n",
            "\u001b[35mSection_title: 7.3  Performance on conclusions with different labels\n",
            "\u001b[35mSection_title: 7.4  Shuffling the input premises\n",
            "\u001b[35mSection_title: 7.5  Prompt design\n",
            "\u001b[35mSection_title: 7.6  Number of shots\n",
            "\u001b[35mSection_title: 7.7  Case Study\n",
            "\u001b[32mconclusion: we introduced folio, an expert-written dataset for first-order logic (fol) reasoning equipped with parallel fol formulas. the examples in folio are created based on real-world knowledge with natural language. it exhibits a large number of distinct logic patterns and a large vocabulary. experiments show that the performance of one of the most capable llms publicly available is only slightly better than chance with few-shot prompting on hyblogic, a subset of folio, and llms are especially bad at predicting the correct truth values for false and unknown conclusions.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Datasets of reasoning from text\n",
            "\u001b[35mSection_title: 2.2  Reasoning using large language models\n",
            "\u001b[35mSection_title: 3  FOLIO Corpus Construction\n",
            "\u001b[35mSection_title: 3.1  Example collection\n",
            "\u001b[35mSection_title: 3.2  NL-FOL alignment review\n",
            "\u001b[35mSection_title: 3.3  FOL verification\n",
            "\u001b[35mSection_title: 3.4  NL review\n",
            "\u001b[35mSection_title: 3.5  Dataset statistics\n",
            "\u001b[35mSection_title: 4.2  NL-FOL translation\n",
            "\u001b[35mSection_title: 5  Methods\n",
            "\u001b[35mSection_title: 5.1  Logical reasoning\n",
            "\u001b[35mSection_title: 5.2  NL-FOL translation\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Dataset split\n",
            "\u001b[35mSection_title: 6.2  Evaluation metrics\n",
            "\u001b[35mSection_title: 6.3  Main results\n",
            "\u001b[35mSection_title: 7  Analysis\n",
            "\u001b[32mconclusion: we show the accuracy of the examples with different numbers of premises needed to reach the conclusions in figure 3. under the few-shot prompting setting, gpt-3 and   \n",
            "\u001b[35mSection_title: 7.1  Number of premises needed for conclusions\n",
            "\u001b[35mSection_title: 7.2  Comparison of the results between WikiLogic and HybLogic\n",
            "\u001b[32mconclusion: confusion matrices in figure 4 for the fine-tuning and 8-shot nl prompt results both show that lms are significantly better at making the correct predictions for conclusions with labels of true than the conclusions with labels of false or unknown. the accuracy on examples with false or unknown conclusions is 54.41% with fine-tuning and 36.91% with few-shot prompting. they also tend to make more predictions of true than the other labels.\n",
            "\u001b[35mSection_title: 7.3  Performance on conclusions with different labels\n",
            "\u001b[35mSection_title: 7.4  Shuffling the input premises\n",
            "\u001b[35mSection_title: 7.5  Prompt design\n",
            "\u001b[35mSection_title: 7.6  Number of shots\n",
            "\u001b[35mSection_title: 7.7  Case Study\n",
            "\u001b[32mconclusion: we introduced folio, an expert-written dataset for first-order logic (fol) reasoning equipped with parallel fol formulas. the examples in folio are created based on real-world knowledge with natural language. it exhibits a large number of distinct logic patterns and a large vocabulary. experiments show that the performance of one of the most capable llms publicly available is only slightly better than chance with few-shot prompting on hyblogic, a subset of folio, and llms are especially bad at predicting the correct truth values for false and unknown conclusions.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 4.4.  CONSTRUÇÃO DOS AMPLICONS SINGLE-CHAIN\n",
            "\u001b[35mSection_title: 6.  CONCLUSÕES\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Task Definition and Dataset\n",
            "\u001b[35mSection_title: 3.2  Baseline\n",
            "\u001b[35mSection_title: 3.3  Metrics\n",
            "\u001b[35mSection_title: 4  Analysis Part I: Experimental Study\n",
            "\u001b[35mSection_title: 4.2  Passage Ranker\n",
            "\u001b[35mSection_title: 5  Evaluation Part I: QA System Ablation\n",
            "\u001b[35mSection_title: 5.1  Overall Performance of Book QA\n",
            "\u001b[35mSection_title: 5.2  Ranker Ablation\n",
            "\u001b[35mSection_title: 5.3  Reader Ablation\n",
            "\u001b[35mSection_title: 6  Analysis Part II: Human Study\n",
            "\u001b[35mSection_title: 6.1  Question Categorization\n",
            "\u001b[35mSection_title: 6.2  Annotation Details\n",
            "\u001b[35mSection_title: 6.3  Performance of Question Type Classification on the Annotated Data\n",
            "\u001b[32mconclusion: we conduct a comprehensive analysis on the book qa task, taking the representative narrativeqa dataset as an example. firstly, we design the book qa techniques by borrowing the wisdom from the cutting-edge open-domain qa research and demonstrate through extensive experiments that (1) evidence retrieval in book qa is difficult even with the state-of-the-art pre-trained lms, due to the factors of rich writing style, recurrent book plots and characters, and the requirement of high-level story understanding; (2) our proposed approaches that adapt pre-trained lms to books, especially the prereading technique for the reader training, are consistently helpful.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Implicit Relations\n",
            "\u001b[35mSection_title: 3  The IMPLICITRELATIONS Benchmark\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Data Analysis\n",
            "\u001b[35mSection_title: 4  Experimental Setting\n",
            "\u001b[35mSection_title: 4.1  Task and Model Specification\n",
            "\u001b[35mSection_title: 4.2  Evaluation\n",
            "\u001b[35mSection_title: 5  Large LMs Can Infer Implicit Relations\n",
            "\u001b[35mSection_title: 5.1  Effect of In-Context Examples\n",
            "\u001b[35mSection_title: 5.2  Effect of Model Size\n",
            "\u001b[35mSection_title: 6  Implicit Relations for QA\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we propose the task of implicit relation inference, which decouples inference of reasoning steps from their execution. we introduce implicitrela-tions, a benchmark that includes more than 2,000 annotated implicit relations. we show large lms can infer implicit relations across multiple types of questions and reasoning skills, but this success does not translate to an improvement in answering implicit reasoning questions. our work sheds light on capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for improving the ability of models to infer implicit relations.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Implicit Relations\n",
            "\u001b[35mSection_title: 3  The IMPLICITRELATIONS Benchmark\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Data Analysis\n",
            "\u001b[35mSection_title: 4  Experimental Setting\n",
            "\u001b[35mSection_title: 4.1  Task and Model Specification\n",
            "\u001b[35mSection_title: 4.2  Evaluation\n",
            "\u001b[35mSection_title: 5  Large LMs Can Infer Implicit Relations\n",
            "\u001b[35mSection_title: 5.1  Effect of In-Context Examples\n",
            "\u001b[35mSection_title: 5.2  Effect of Model Size\n",
            "\u001b[35mSection_title: 6  Implicit Relations for QA\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we propose the task of implicit relation inference, which decouples inference of reasoning steps from their execution. we introduce implicitrela-tions, a benchmark that includes more than 2,000 annotated implicit relations. we show large lms can infer implicit relations across multiple types of questions and reasoning skills, but this success does not translate to an improvement in answering implicit reasoning questions. our work sheds light on capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for improving the ability of models to infer implicit relations.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Limited tokens embedded\n",
            "\u001b[35mSection_title: 2.  Limited training data\n",
            "\u001b[35mSection_title: 3.  Limited corpora diversity\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Distant Supervision for Decomposition\n",
            "\u001b[35mSection_title: 3.1  Intuitions\n",
            "\u001b[35mSection_title: 3.2  Parallel News Extraction\n",
            "\u001b[35mSection_title: 3.3  Comparisons with Similar Data Sources\n",
            "\u001b[35mSection_title: 3.4  Pre-training with Distant Supervision\n",
            "\u001b[35mSection_title: 4  Decomposition-based QA Pipeline\n",
            "\u001b[35mSection_title: 4.1  Intuitions and Design Choices\n",
            "\u001b[35mSection_title: 4.2  Factual Correction for Generated Facts\n",
            "\u001b[35mSection_title: 4.3  DECOMPENTAIL QA Pipeline\n",
            "\u001b[35mSection_title: 4.4  Inference\n",
            "\u001b[35mSection_title: 5  Intrinsic Experiments\n",
            "\u001b[35mSection_title: 5.1  Overnight\n",
            "\u001b[35mSection_title: 5.2  TORQUE\n",
            "\u001b[35mSection_title: 6  Sanity Check Experiments\n",
            "\u001b[35mSection_title: 6.1  Dataset and Settings\n",
            "\u001b[35mSection_title: 6.2  Training and Results\n",
            "\u001b[35mSection_title: 7  Decomposition QA Experiments\n",
            "\u001b[35mSection_title: 7.1  Datasets\n",
            "\u001b[35mSection_title: 7.2  Settings and Baselines\n",
            "\u001b[35mSection_title: 7.3  Results\n",
            "\u001b[35mSection_title: 7.4  Ablation Studies\n",
            "\u001b[35mSection_title: 7.5  Manual Analysis\n",
            "\u001b[32mconclusion: this work proposes a novel method that extracts distant and incidental signals from parallel news to facilitate general question representation. such parallel news signals intuitively bridge the reasoning gap in pre-trained language models due to reporting biases. to support this intuition, we train a model named decompt5 on such distant supervision and show that it improves 20%-30% on two semantic parsing benchmarks, namely overnight and torque, that directly evaluate query understanding. with decompt5 as the basis, we design a well-motivated question-answering pipeline decompentail that follows a decomposition, correction, and entailment scheme. we show that decompentail improves on strategyqa and hotpotqa by 3.7% and 8%, respectively.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: in this section, we discuss some of the limitations of our work, and motivate future works. limited question formats. our proposed qa pipeline operates on binary yes/no questions. while binary questions are very general, as most other questions can be re-written into similar forms, such transformations have not been designed or evaluated, which motivates future works. limited factual correction coverage. we use gpt-3 as the backbone for our factual correction step. although it is shown to be effective, it is not as deterministic as wikipedia-based ir approaches, and we cannot easily interpret why it makes mistakes and understand how to improve.\n",
            "\u001b[35mSection_title: 9  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  System Overview\n",
            "\u001b[35mSection_title: 2.1  Pre-trained Models\n",
            "\u001b[35mSection_title: 2.2  Fine-tuning MRC Models with Multi-Task Learning\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 3.1  Experimental Settings\n",
            "\u001b[35mSection_title: 3.2  Experimental Results\n",
            "\u001b[35mSection_title: 3.2.1  The Main Results and the Effects of\n",
            "\u001b[35mSection_title: 3.2.2  The Effects of Multi-Task Learning\n",
            "\u001b[35mSection_title: 3.2.3  Summary\n",
            "\u001b[35mSection_title: 3.3  Analysis\n",
            "\u001b[32mconclusion: in this paper, we describe a simple baseline system that baidu submitted for the mrqa 2019 shared task. our system is built on a framework of pre-training and fine-tuning, namely d-net. d-net employs the techniques of pre-trained lan-guage models and multi-task learning to improve the generalization of mrc models and we conduct the experiments to examine the effectiveness of these strategies.\n",
            "\u001b[35mSection_title: 4  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Pilot Analysis\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Model Architecture\n",
            "\u001b[35mSection_title: 3.2  Model Training\n",
            "\u001b[35mSection_title: 3.3  Complexity Analysis\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Datasets\n",
            "\u001b[35mSection_title: 4.3  Experimental Settings\n",
            "\u001b[35mSection_title: 4.4  Overall Results\n",
            "\u001b[35mSection_title: 4.5  Fuse Layer-wise and Token-wise Pruning\n",
            "\u001b[35mSection_title: 4.6  Results on Long-text Tasks\n",
            "\u001b[35mSection_title: 4.7  Case Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we propose a novel method for accelerating bert inference, called tr-bert, which prunes bert at token-level granularity. specifically, tr-bert utilizes reinforcement learning to learn a token selection policy, which is able to select general meaningful tokens in the bottom layers and select task-relevant tokens in the top layers. experiments on eleven nlp tasks demonstrate the effectiveness of tr-bert as it accelerates bert inference by 2-5 times for various performance demand. besides, tr-bert achieves a better quality and speed trade-off on long-text tasks, which shows its potential to process large amounts of information in the real-world applications.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  BENCHMARKS FOR DR MODELS\n",
            "\u001b[35mSection_title: 4  TRAIN-TEST LABEL OVERLAP\n",
            "\u001b[35mSection_title: 4.1  Query Overlap\n",
            "\u001b[35mSection_title: 4.2  Passage Overlap\n",
            "\u001b[35mSection_title: 5  RESAMPLING EVALUATION METHODS\n",
            "\u001b[35mSection_title: 5.1  Resampling Training Queries\n",
            "\u001b[35mSection_title: 5.2  Resampling Training and Test Queries\n",
            "\u001b[35mSection_title: 6  EVALUATING EXTRAPOLATION\n",
            "\u001b[35mSection_title: 6.1  Comparing Model Architectures\n",
            "\u001b[35mSection_title: 6.1.1  Baselines.\n",
            "\u001b[35mSection_title: 6.1.2  Implementation.\n",
            "\u001b[35mSection_title: 6.1.3  Evaluation Results and Discussion.\n",
            "\u001b[35mSection_title: 6.1.4  Answer to RQ1.\n",
            "\u001b[35mSection_title: 6.2  Comparing DR Training Methods\n",
            "\u001b[35mSection_title: 6.2.1  Advanced DR Training Methods.\n",
            "\u001b[35mSection_title: 6.2.2  Implementation.\n",
            "\u001b[35mSection_title: 6.2.3  Comparing Pretraining and Finetuning.\n",
            "\u001b[35mSection_title: 6.2.4  Investigation of Pretraining.\n",
            "\u001b[35mSection_title: 6.2.5  Answer to RQ2.\n",
            "\u001b[35mSection_title: 7  RELATIONSHIP WITH TRANSFER ABILITY\n",
            "\u001b[35mSection_title: 7.1  Domain Transfer and BEIR dataset\n",
            "\u001b[35mSection_title: 7.2  Correlation Investigation\n",
            "\u001b[35mSection_title: 7.2.1  Implementation.\n",
            "\u001b[35mSection_title: 7.2.2  Discussion.\n",
            "\u001b[32mconclusion: in this paper, we propose a simple yet effective method that evaluates the extrapolation performance of dr models, i.e., how dr models perform on queries that are distinct from the training queries. with the proposed evaluation method, we first revisit how existing dr models perform in the extrapolation regimes. results lead to several non-trivial findings that have been concealed by the existing evaluation protocol. concretely, dr is substantially more vulnerable to extrapolation than the interaction-based deep neural ranking models, and pretraining is a more effective method to improve the extrapolation ability of dr than finetuning techniques. then we further interpret our extrapolation performance by investigating its relationship with the domain transfer ability. results suggest that the extrapolation performance is a potential indicator of the domain transfer ability, further highlighting the feasibility of our methods to evaluate the generalization ability of dr models. although this paper focuses on evaluating how dr models extrapolate, the methodologies can also be used for other models in the future.\n",
            "\u001b[35mSection_title: 8  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Question Answering Datasets\n",
            "\u001b[35mSection_title: 2.2  Multi-span Models\n",
            "\u001b[35mSection_title: 3  Dataset Construction and Composition\n",
            "\u001b[35mSection_title: 3.1  Data Collection and Preprocessing\n",
            "\u001b[35mSection_title: 3.2  Issues in Existing Dataset\n",
            "\u001b[35mSection_title: 3.3  High Quality Re-annotation\n",
            "\u001b[35mSection_title: 3.4  Dataset Statistics\n",
            "\u001b[35mSection_title: 3.5  Dataset Expansion\n",
            "\u001b[35mSection_title: 4  Models\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Proposed Model\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Setup\n",
            "\u001b[35mSection_title: 5.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.3  Results and Analysis\n",
            "\u001b[32mconclusion: we present multispanqa, a reading comprehension dataset where answers consist of multiple discrete spans. as part of this, we proposed a method for classifying the semantic structure of answers, based on the semantic relation between answer spans. we also provide an expanded version of the dataset which includes unanswerable questions and single-answer questions, to make it both more challenging and more realistic. we additionally presented a number of models for multi-span qa extraction, and found that the best-performing model was sequence tagging-based, augmented by a span number prediction module and span adjustment module.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Answer Relevance Generation\n",
            "\u001b[35mSection_title: 3.2  Answer Relevance Attention\n",
            "\u001b[35mSection_title: 3.3  QFS-BART\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results & Analysis\n",
            "\u001b[35mSection_title: 5.1  Case Study\n",
            "\u001b[32mconclusion: in this work, we propose qfs-bart, an abstractive summarization model for query focused summarization. we use a generalizing qa model to make explicit answer relevance scores for all words in the document and combine them to the encoder-decoder attention. we also leverage pretrained model (e.g. bart) and two-stage finetuning method which further improve the summarization performance significantly. experimental results show the proposed model achieves state-ofthe-art performance on debatepedia dataset and outperforms several comparable baselines on duc 2006-7 datasets. hal daumé iii and daniel marcu. 2006  in this paper, we introduce a two-step architecture: 1) retrieve answer-related sentences given the query, rank them by the confidence score (generated from equation 4) and concatenate them. 2) use our qfs-bart to produce an abstractive summary.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 1.1  Textual Novelty Detection: An Entailment Perspective\n",
            "\u001b[35mSection_title: 1.2  Multiple Premise Entailment (MPE) for Novelty Detection\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 2.1  Existing Literature\n",
            "\u001b[35mSection_title: 2.1.4  Diversity and Novelty.\n",
            "\u001b[35mSection_title: 2.2  Our Explorations So Far\n",
            "\u001b[35mSection_title: 2.2.2  RDV-CNN Method for Document Novelty.\n",
            "\u001b[35mSection_title: 2.2.3  Detecting Document Novelty via Decomposable Attention.\n",
            "\u001b[35mSection_title: 3.  Current Methodology: Encompassing Multiple Premises for Document-Level Novelty Detection\n",
            "\u001b[35mSection_title: 3.1  Relevance Detection\n",
            "\u001b[35mSection_title: 3.2  Selection Module and Relevance Scores\n",
            "\u001b[35mSection_title: 3.3  Novelty Detection Module\n",
            "\u001b[35mSection_title: 3.3.3  Cross-Attention\n",
            "\u001b[35mSection_title: 4.  Dataset Description\n",
            "\u001b[35mSection_title: 4.1  TAP-DLND 1.0 Corpus\n",
            "\u001b[35mSection_title: 4.2  APWSJ Dataset\n",
            "\u001b[35mSection_title: 4.3  TAP-DLND 2.0 Corpus\n",
            "\u001b[35mSection_title: 4.3.1  About the Annotators.\n",
            "\u001b[35mSection_title: 4.3.2  Annotation Example.\n",
            "\u001b[35mSection_title: 4.4  Datasets for Allied Tasks\n",
            "\u001b[35mSection_title: 4.4.1  Webis Crowd Paraphrase\n",
            "\u001b[35mSection_title: 4.4.3  Wikipedia Rewrite Corpus.\n",
            "\u001b[35mSection_title: 5.  Evaluation\n",
            "\u001b[35mSection_title: 5.1  Baselines and Ablation Study\n",
            "\u001b[35mSection_title: 5.1.3  Baseline 3: Single Premise.\n",
            "\u001b[35mSection_title: 5.2  Comparing Systems\n",
            "\u001b[35mSection_title: 5.2.2  Comparing System-2.\n",
            "\u001b[35mSection_title: 5.2.3  Comparing System-3.\n",
            "\u001b[35mSection_title: 5.2.4  Comparing System-4.\n",
            "\u001b[35mSection_title: 5.3  BERT-NLI Variant of the Proposed Architecture\n",
            "\u001b[35mSection_title: 5.4  Hyperparameter Details\n",
            "\u001b[35mSection_title: 5.5  Results\n",
            "\u001b[35mSection_title: 5.5.1  Evaluation Metrics.\n",
            "\u001b[35mSection_title: 5.5.3  On APWSJ Dataset.\n",
            "\u001b[35mSection_title: 5.6  Results on Related Tasks\n",
            "\u001b[35mSection_title: 5.7  On Using Contextual Language Models\n",
            "\u001b[35mSection_title: 5.8  Analysis\n",
            "\u001b[35mSection_title: 5.9  Error Analysis\n",
            "\u001b[32mconclusion: textual novelty detection has an array of use-cases starting from search and retrieval on the web, nlp tasks like plagiarism detection, paraphrase detection, summarization, modeling interestingness, fake news detection, and so forth. however, less attention is paid to the document-level variant of the problem in comparison to sentence-level novelty detection. in this work, we present a comprehensive account of our experiments so far on document-level novelty detection. we study existing literature on textual novelty detection as well as our earlier explorations on the topic. here we assert that we would need to perform information assimilation from multiple premises to identify the novelty of a given text. our current approach performs better than our earlier approaches. also, we show that our method could be suitably applied to allied tasks like plagiarism detection and paraphrase detection. we point out some limitations of our approach, which we aim to explore next.\n",
            "\u001b[35mSection_title: 6.  Summary, Conclusion, and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 217  A9_Leavenworthia_alabamica K E G K E F K S I V E K E T R L P G A T K L I D A9_Capsella_rubella K E G R E F K S I V D R E T R L P G A T K M L D A9_Capsella_grandiflora K E G R E F K S I V D R E T R L P G A T K M L D A9_Boechera_stricta\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Unsupervised Model Selection\n",
            "\u001b[35mSection_title: 2.2  Unsupervised OOD Performance Evaluation\n",
            "\u001b[35mSection_title: 2.3  Zero-shot Dense Retrieval\n",
            "\u001b[35mSection_title: 2.4  Query Performance Prediction\n",
            "\u001b[35mSection_title: 3  MODEL SELECTION FOR DENSE RETRIEVERS\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  Challenges of model selection in IR\n",
            "\u001b[35mSection_title: 3.3  Method 1: In-Domain Performance\n",
            "\u001b[35mSection_title: 3.4  Method 2: Query Similarity\n",
            "\u001b[35mSection_title: 3.5  Method 3: Corpus Similarity\n",
            "\u001b[35mSection_title: 3.6  Method 4: Extracted Document Similarity\n",
            "\u001b[35mSection_title: 3.7  Method 5: Binary Entropy\n",
            "\u001b[35mSection_title: 3.8  Method 6: Query Alteration\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP 4.1 Datasets\n",
            "\u001b[35mSection_title: 4.2  Dense Retrieval Models\n",
            "\u001b[35mSection_title: 4.3  Evaluation Measures for Dense Retrievers Selection\n",
            "\u001b[35mSection_title: 5  RESULTS\n",
            "\u001b[35mSection_title: 5.1  Desired ranking\n",
            "\u001b[35mSection_title: 5.2  Query Similarity\n",
            "\u001b[35mSection_title: 5.3  Corpus Similarity\n",
            "\u001b[35mSection_title: 5.4  Extracted Document Similarity\n",
            "\u001b[35mSection_title: 5.5  Binary Entropy\n",
            "\u001b[35mSection_title: 5.6  Query Alteration\n",
            "\u001b[35mSection_title: 6  OUTLOOK\n",
            "\u001b[32mconclusion: this paper proposes a novel research direction for zero-shot dense retrieval.while traditional information retrieval research in this area concentrates on developing universal domain-agnostic dr models, our work shifts the focus towards developing a method to rank and select pre-trained state-of-the-art dr models that are best suited for a specific target domain corpus.we acknowledge that the proposed direction does not contradict traditional research on training zero-shot dr models, but rather complements it.as newly developed dr models are likely to have varying effects on different domains, selecting the best model is still beneficial.to explore this research direction, we adapt various methods from computer vision and machine learning, along with some approaches designed for ir.we outline our reasoning and challenges with the investigated approaches and present empirical results on a popular zeroshot benchmark dataset.our findings shed light on future research avenues within this research direction.we believe that an effective method for selecting a good dr model can provide a principled way for search engine developers to identify the most suitable model for their application, ultimately enhancing user experience.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Uncertainty and Vagueness of Geographic Information\n",
            "\u001b[35mSection_title: 2.1.1  Geometric Uncertainty\n",
            "\u001b[35mSection_title: 2.1.2  Vagueness of Geographic Concepts and Entities\n",
            "\u001b[35mSection_title: 3  Uniqueness of Geographic Questions and GeoQA\n",
            "\u001b[35mSection_title: 4  Existing Work on GeoQA\n",
            "\u001b[35mSection_title: 4.1  Factoid Geographic Question Answering\n",
            "\u001b[35mSection_title: 4.2  Geo-analytical Question Answering\n",
            "\u001b[35mSection_title: 4.3  Scenario-based Geographic Question Answering\n",
            "\u001b[35mSection_title: 4.4  Visual Geographic Question Answering\n",
            "\u001b[35mSection_title: 5  The Classification of Geographic Questions\n",
            "\u001b[35mSection_title: 5.1  Factoid Geographic Questions\n",
            "\u001b[35mSection_title: 4.  Routing questions:\n",
            "\u001b[35mSection_title: 5.2  Prediction-based Geographic Questions\n",
            "\u001b[35mSection_title: 5.3  Opinion Geographic Questions\n",
            "\u001b[35mSection_title: 5.4  Hypothetical Geographic Questions\n",
            "\u001b[35mSection_title: 5.5  Causal Geographic Questions\n",
            "\u001b[35mSection_title: 5.6  Geo-analytical Questions\n",
            "\u001b[35mSection_title: 5.7  Scenario-based Geographic Questions\n",
            "\u001b[35mSection_title: 5.8  Visual Geographic Questions\n",
            "\u001b[35mSection_title: 5.9  Discussion about the Question Classification\n",
            "\u001b[35mSection_title: 6  Future Research Directions for GeoQA\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Dataset\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Analysis and Discussion\n",
            "\u001b[32mconclusion: in this paper, we studied the novel task of multi-hop reasoning for fact checking of real-world political claims, which encompasses both evidence retrieval and claim veracity prediction. we presented politihop, the first political fact checking dataset with annotated evidence sentences. we compared several models on politihop and found that the multi-hop architecture transformer-xh slightly outperforms bert in most of the settings, especially in terms of evidence retrieval, where bert is easily fooled by named entity overlaps between the claim and evidence sentences. the performance of transformer-xh is further improved when retrieving more than two evidence sentences and the number of hops larger than one, which corroborates the assumption of the multi-hop nature of the task. in the first setting, the models are trained for 4 epochs on liar-plus. in the second setting, the models are trained for 8 epochs on politihop. in the third setting, models are trained for 4 epochs on liar-plus, followed by 4 epochs on politihop. in every setting, models are evaluated on the dev set and the model with the best label prediction macro-f1 score is saved, which enables early stopping. for the fourth setting, we pre-train the model for 2 epochs on the fever dataset, followed by 4 epochs on liar-plus, the fine-tune on politihop for 4 epochs.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Knowledge Gaps\n",
            "\u001b[35mSection_title: 3.1  Understanding Gaps: Categorization\n",
            "\u001b[35mSection_title: 3.2  Annotating Gaps: Data Collection\n",
            "\u001b[35mSection_title: 4  Knowledge-Gap Guided QA: GapQA\n",
            "\u001b[35mSection_title: 4.1  Key Span Identification Model\n",
            "\u001b[35mSection_title: 4.2  Knowledge Retrieval Module\n",
            "\u001b[35mSection_title: 4.3  Question Answering Model\n",
            "\u001b[35mSection_title: 4.4  Model Training\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 5.1  Key Span Identification\n",
            "\u001b[35mSection_title: 5.2  OpenBookQA Results\n",
            "\u001b[35mSection_title: 5.3  Ablations\n",
            "\u001b[35mSection_title: 5.4  Error Analysis\n",
            "\u001b[32mconclusion: we focus on the task of question answering under partial knowledge: a novel task that lies inbetween open-domain qa and reading comprehension. we identify classes of knowledge gaps when reasoning under partial knowledge and collect a dataset targeting one common class of knowledge gaps. we demonstrate that identifying the knowledge gap first and then reasoning by filling this gap outperforms previous approaches on the openbookqa task, with and even without additional missing fact annotation. this work opens up the possibility of focusing on other kinds of knowledge gaps and extending this approach to other datasets and tasks (e.g., span prediction).  boiling point means temperature above which a liquid boils figure 10: visualization of the models behavior with the predicted span, top predicted relation, and the top fact used by model. the heat map shows the confidence of the model for all the relations for each input sentence (first five) and conceptnet sentencized tuple (last but one) and the back-off tuple (last one) to capture the knowledge in the embeddings.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Data Augmentation for Neural IR\n",
            "\u001b[35mSection_title: 2.2  Pretraining Objectives for IR\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Datasets\n",
            "\u001b[35mSection_title: 4.3  Multi-Reranker Domain Adaptation\n",
            "\u001b[35mSection_title: 4.4  Query Latency\n",
            "\u001b[35mSection_title: 4.5  Impact of Pretrained Components\n",
            "\u001b[35mSection_title: 4.6  Different Prompting Strategies\n",
            "\u001b[35mSection_title: 4.7  LoTTE and BEIR Test Results\n",
            "\u001b[35mSection_title: 4.8  Additional Results\n",
            "\u001b[35mSection_title: 5  Discussion & Future Work\n",
            "\u001b[32mconclusion: we present udapdr, a novel strategy for adapting retrieval models to new domains.udapdr uses synthetic queries created using generative models, such as gpt-3 and flan-t5 xxl, to train multiple passage rerankers on queries for target domain passages.these passage rerankers are then distilled into colbertv2 to boost retrieval accuracy while keeping query latency competitive as compared to other retrieval systems.we validate our approach across the lotte, beir, nq, and squad datasets.additionally, we explore various model configurations that alter the generative models, prompting strategies, retriever, and passage rerankers used in our approach.we find that udapdr can boost zero-shot retrieval accuracy on new domains without the use of labeled training examples.we also discuss several directions for future work.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Definition and Motivation\n",
            "\u001b[35mSection_title: 3  The Proposed Approach\n",
            "\u001b[35mSection_title: 3.1  The Base Span Prediction Model\n",
            "\u001b[35mSection_title: 3.2  Bridge Reasoner\n",
            "\u001b[35mSection_title: 3.3  Target Passage Reader\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[32mconclusion: this paper introduces an important sub-problem of bridge reasoning for the task of multi-hop qa in the open-domain setting. we propose a bridge reasoner that utilizes multiple types of evidence to derive the passages that cover the answers. the reasoner significantly improves the coverage of answer passages than ir methods. with the predicted passages, we show that a standard reading comprehension model is able to achieve similar performance as the state-of-the-art method that requires bert in multiple modules.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 1.1  Di culties in Solving POMDPs\n",
            "\u001b[35mSection_title: 1.2  Previous Work\n",
            "\u001b[35mSection_title: 1.3  Model Approximation\n",
            "\u001b[35mSection_title: 1.4  Our Proposal\n",
            "\u001b[35mSection_title: 1.5  Organization\n",
            "\u001b[35mSection_title: 2.  Planning in Stochastic Domains and POMDPs\n",
            "\u001b[35mSection_title: 3.  Basics of POMDPs\n",
            "\u001b[35mSection_title: 3.1  Belief States\n",
            "\u001b[35mSection_title: 3.2  POMDPs as MDPs\n",
            "\u001b[35mSection_title: 3.3  Optimal Policies\n",
            "\u001b[35mSection_title: 3.4  Value Iteration\n",
            "\u001b[35mSection_title: 4.  Piecewise Linearity and Implicit Value Iteration\n",
            "\u001b[35mSection_title: 4.1  Policy Trees\n",
            "\u001b[35mSection_title: 4.2  State Value Functions of Policy Trees\n",
            "\u001b[35mSection_title: 4.3  State Space Functions and Belief Space Functions\n",
            "\u001b[35mSection_title: 4.4  Piecewise Linearity of Optimal Value Functions\n",
            "\u001b[35mSection_title: 4.5  Parsimonious Representations\n",
            "\u001b[35mSection_title: 4.6  Dynamic-Programming Updates\n",
            "\u001b[35mSection_title: 4.7  Implicit Value Iteration\n",
            "\u001b[35mSection_title: 5.  A New Algorithm for Dynamic-Programming Updates\n",
            "\u001b[35mSection_title: 5.4  A New Algorithm for Dynamic-Programming Updates\n",
            "\u001b[35mSection_title: 6.  Region-Based Model Approximation\n",
            "\u001b[35mSection_title: 6.1  The Basic Idea\n",
            "\u001b[35mSection_title: 6.2  Spectrum of Approximations\n",
            "\u001b[35mSection_title: 6.3  Region Systems\n",
            "\u001b[35mSection_title: 7.  Solving Region Observable POMDPs\n",
            "\u001b[35mSection_title: 7.1  Restricted Value Iteration\n",
            "\u001b[35mSection_title: 7.2  Implicit Restricted Value Iteration\n",
            "\u001b[35mSection_title: 8.  Dynamic-Programming Updates for Region Observable POMDPs\n",
            "\u001b[35mSection_title: 8.3  Possible Observations at the Next Time Point\n",
            "\u001b[35mSection_title: 9.  The Stopping Condition\n",
            "\u001b[35mSection_title: 11.  Quality of Approximation and Simulation\n",
            "\u001b[35mSection_title: 12.  Tradeo Between Quality and Complexity\n",
            "\u001b[35mSection_title: 13.  Simulation Experiments\n",
            "\u001b[35mSection_title: 13.1  Synthetic O ce Environments\n",
            "\u001b[35mSection_title: 13.2  Complexity of Solving the POMDPs\n",
            "\u001b[35mSection_title: 13.3  Approximation Quality for Standard Models\n",
            "\u001b[35mSection_title: 13.4  Approximation Quality for Noisy Models\n",
            "\u001b[32mconclusion: we propose to approximate a pomdp by using a region observable pomdp. the region observable pomdp has more informative observations and hence is easier to solve. a method for determining approximation quality is described, which allows one to make the tradeo between approximation quality and computational time by starting with a coarse approximation and re ning it gradually. simulation experiments have shown that when there is not much uncertainty in the e ects of actions and observations are informative, a pomdp can be accurately approximated by a region observable pomdp that can be solved exactly. however, this becomes infeasible as the degree of uncertainty increases. other approximate methods need to be incorporated in order to solve region observable pomdps whose radiuses are not small. proof of proposition 3: because of proposition 1 and lemma 4, it su ces to show that 3. if x , return nil, else return b.\n",
            "\u001b[35mSection_title: 14.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Formalism\n",
            "\u001b[35mSection_title: 2.1  Mode Functions\n",
            "\u001b[35mSection_title: 2.1.1  High Frequencies\n",
            "\u001b[35mSection_title: 2.1.2  Low Frequencies\n",
            "\u001b[35mSection_title: 2.2  Cosmological Epochs\n",
            "\u001b[35mSection_title: 2.2.1  In Region\n",
            "\u001b[35mSection_title: 2.2.2  Out Region\n",
            "\u001b[35mSection_title: 2.2.3  Transitions\n",
            "\u001b[35mSection_title: 2.3  Energy Density\n",
            "\u001b[35mSection_title: 2.3.1  Modes 0 ≤ k < Λ IR\n",
            "\u001b[35mSection_title: 2.3.2  Modes\n",
            "\u001b[35mSection_title: 2.3.3  Particle Production\n",
            "\u001b[35mSection_title: 2.3.4  Classical Field Description\n",
            "\u001b[35mSection_title: 3  Massless Fields\n",
            "\u001b[35mSection_title: 3.1  Inflation\n",
            "\u001b[35mSection_title: 3.2  General Infrared Evolution After Inflation\n",
            "\u001b[35mSection_title: 3.3  Radiation Domination\n",
            "\u001b[35mSection_title: 3.4  Examples\n",
            "\u001b[35mSection_title: 3.5  Overview\n",
            "\u001b[35mSection_title: 4  Light Fields\n",
            "\u001b[35mSection_title: 4.1  Inflation\n",
            "\u001b[35mSection_title: 4.2  General Infrared Evolution After Inflation\n",
            "\u001b[35mSection_title: 4.3  Radiation Domination\n",
            "\u001b[35mSection_title: 4.4  The Particle and Classical Field Approximations\n",
            "\u001b[35mSection_title: 4.5  Overview\n",
            "\u001b[35mSection_title: 5  Heavy Fields\n",
            "\u001b[35mSection_title: 5.1  Examples\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  LITERATURE REVIEW\n",
            "\u001b[35mSection_title: 2.1.  The Scientific Basis of DNA Typing\n",
            "\u001b[35mSection_title: 2.2.  The Scientific Basis of DNA Fingerprinting\n",
            "\u001b[35mSection_title: 2.3.  Dot Plot\n",
            "\u001b[35mSection_title: 2.4.  What is a Pairwise Sequence Alignment?\n",
            "\u001b[35mSection_title: 2.4.1.  Global Alignment With Linear Gap Penalty\n",
            "\u001b[35mSection_title: 2.4.2.  Semi-Global Alignment With Linear Gap Penalty\n",
            "\u001b[35mSection_title: 2.4.3.  Local Alignment With Linear Gap Penalty\n",
            "\u001b[35mSection_title: 2.4.4.  Global Alignment With Affine Gap Penalty\n",
            "\u001b[35mSection_title: 2.4.5.  Semi-Global Alignment With Affine Gap Penalty\n",
            "\u001b[35mSection_title: 2.4.6.  Local Alignment With Affine Gap Penalty\n",
            "\u001b[35mSection_title: 2.4.7.  Comparison of Global, Semi-Global and Local Alignment\n",
            "\u001b[35mSection_title: 3.  HEURISTIC ALIGNMENT ALGORITHMS\n",
            "\u001b[35mSection_title: 3.1.  BLAST\n",
            "\u001b[35mSection_title: 3.2.  FASTA\n",
            "\u001b[35mSection_title: 3.3.  Comparison Between BLAST and FASTA\n",
            "\u001b[35mSection_title: 4.  TANDEM REPEATS SEARCH ALGORITHMS\n",
            "\u001b[35mSection_title: 5.  ARCHITECTURE DESIGN AND COMPONENTS IMPLEMENTATION\n",
            "\u001b[35mSection_title: 5.1.  System Architecture Overview\n",
            "\u001b[35mSection_title: 3.  From the list\n",
            "\u001b[35mSection_title: 5.2.  Components Implementation\n",
            "\u001b[32mlimitation: the test results have indicated that the system developed needs further enhancement such as optimization of the system performance and efficiency. some of the suggestions are: fig. (19). blast alignment results.  \n",
            "\u001b[35mSection_title: 7.  LIMITATIONS AND DIFFICULTIES OF PRESENT WORK\n",
            "\u001b[32mconclusion: the results obtained from the tests have shown that the integrated software tool performs as expected. the work suggested that by integrating blast and fasta (two widely used and freely available algorithms), plus an additional implementation of psa and tr analysis tools, with the rest of the supporting tools (database management) developed, it is entirely possible to have an initial working version of the software tool for criminal dna analysis and de-tection work. the system has great potential and that the results obtained during the tests were satisfactory. the following observations can be made: • psa algorithm is suitable for smaller sequence alignments as compared to blast and fasta, which are designed for large sequence database searches.\n",
            "\u001b[35mSection_title: 8.  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  The experiment\n",
            "\u001b[35mSection_title: 3  Analysis\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  x dependence\n",
            "\u001b[35mSection_title: 4.2  Q 2 dependence\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Definition\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 4.1  Context and Answer Encoding\n",
            "\u001b[35mSection_title: 4.2  Question Decoding\n",
            "\u001b[35mSection_title: 5  Dataset\n",
            "\u001b[35mSection_title: 5.1  Natural Questions dataset\n",
            "\u001b[35mSection_title: 5.1.1  NQ Preprocessing\n",
            "\u001b[35mSection_title: 5.2  News dataset\n",
            "\u001b[35mSection_title: 6  In-Domain Evaluation with\n",
            "\u001b[35mSection_title: 6.1  Experiment Setup and Training\n",
            "\u001b[35mSection_title: 6.2  In-Domain Evaluation\n",
            "\u001b[35mSection_title: 6.3  Ablation Study\n",
            "\u001b[35mSection_title: 7  Out-of-Domain Evaluation with QA Systems\n",
            "\u001b[35mSection_title: 7.1  The QA Metrics\n",
            "\u001b[35mSection_title: 7.2  QG Models to Compare\n",
            "\u001b[35mSection_title: 7.3  Evaluation Results\n",
            "\u001b[32mconclusion: we tackle the problem of question generation targeted for human information seeking using automatic question answering technology. we focus on generating questions for news articles that can be answered by longer passages rather than short text spans as suggested questions. we build a bert-based pointer-generator network as the qg model, trained with the natural questions dataset. our method shows state-of-the-art performance in terms of bleu, meteor, and rouge l scores on our nq question generation dataset. we then apply our model to the out-of-domain news articles without further training. we use a qa system to evaluate our qg models as there are no gold questions for comparison. we also conduct a human evaluation to confirm the qa evaluation results.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Dialogue State Tracking as Reading Comprehension\n",
            "\u001b[35mSection_title: 3.2  Span-based RC To Extractive DST\n",
            "\u001b[35mSection_title: 3.3  Multiple-Choice Reading Comprehension to Categorical Dialogue State Tracking\n",
            "\u001b[35mSection_title: 4.2  Canonicalization for Extractive Dialogue State Tracking\n",
            "\u001b[35mSection_title: 4.3  Two-stage Training\n",
            "\u001b[35mSection_title: 5  Results and Analyses\n",
            "\u001b[35mSection_title: 5.1  DST with Full Training Data\n",
            "\u001b[35mSection_title: 5.2  Few shot from RC to DST\n",
            "\u001b[35mSection_title: 5.3  Zero shot from RC to DST\n",
            "\u001b[32mconclusion: task-oriented dialogue systems aim to help users to achieve a variety of tasks. it is not unusual to have hundreds of different domains in modern taskoriented virtual assistants. how can we ensure the dialogue system is robust enough to scale to different tasks given limited amount of data? some approaches focus on domain expansion by training on several source domains and then adapting to the target domain. while such methods can be successful in certain cases, it is hard for them to generalize to other completely different out-of-domain tasks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Web Tables Are Few-Shot Tasks\n",
            "\u001b[35mSection_title: 2.1  Few-Shot Learning Tasks\n",
            "\u001b[35mSection_title: 2.2  Tables Dataset\n",
            "\u001b[35mSection_title: 2.3  Turning Tables Into Tasks\n",
            "\u001b[35mSection_title: 3.1  MetaICL\n",
            "\u001b[35mSection_title: 3.2  Experiments\n",
            "\u001b[35mSection_title: 4  Why Is UnpredicTable Helpful?\n",
            "\u001b[35mSection_title: 4.2  Does diversity improve performance?\n",
            "\u001b[35mSection_title: 4.3  Can we select good tasks by hand?\n",
            "\u001b[35mSection_title: 4.4  How do helpful and unhelpful tasks look?\n",
            "\u001b[35mSection_title: 4.6  Do different datasets lead improvements on different test tasks?\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mlimitation: the unpredictable dataset may contain inaccuracies, biases, and inappropriate content. we do not recommend using this dataset to train models for deployment, but release this primarily as a research resource. we do not introduce any new model capabilities that lead to different risks than the usual risks associated with model usage. our work highlights the unpredictability of model behavior given various training datasets which calls for heightened vigilance for behavior changes after finetuning. our design choices in using table data for fsl training led to a dataset that is quite different than typical nlp datasets, so specific results from training on our dataset may not fully generalize to other kinds of datasets. further work may consider other methods for converting tables to tasks, other sources of tables besides wtc, or other structured datasets besides tables. our experiments focused on modestly-sized models (gpt-2 large, 750m parameters) so our conclusions may not hold for larger models. our evaluations are limited to multiple-choice tasks. future work may extend our analyses with larger models and other tasks including freeform generation.\n",
            "\u001b[35mSection_title: 6  Limitations & Future Work\n",
            "\u001b[32mconclusion: we produced unpredictable, a dataset of 413,299 diverse few-shot learning tasks from internet tables. finetuning on unpredictable improves the fsl ability of lms. however, the size of our dataset is not the key factor in its success. we find that certain narrow datasets (even ones made of trivia) are even more helpful than diverse, curated nlp datasets. finetuning on these narrow datasets leads to strong improvements on the same test tasks as finetuning on diverse, curated nlp datasets. this suggests that finetuning on these datasets cause domain-agnostic fsl gains, though we were unable to find clear patterns to explain why this happens for some data and not others. our results question common wisdom that task diversity is necessary for adapting lms to fsl. we hope our work spurs investigation on what data causes few-shot learning to emerge, both to develop better datasets and to better understand how training data leads to unexpected behaviors or failures.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Results\n",
            "\u001b[35mSection_title: 2.1.  Characterization of CRY-DASH in S. japonica\n",
            "\u001b[35mSection_title: 2.2.  sjCRY-DASH Transcription is Rapidly Upregulated by Blue Light\n",
            "\u001b[35mSection_title: 2.3.  Photoperiod Affects the Circadian Oscillation of sjCRY-DASH\n",
            "\u001b[35mSection_title: 2.2.  sjCRY-DASH Transcription is Rapidly Upregulated by Blue Light\n",
            "\u001b[35mSection_title: 2.2.  sjCRY-DASH Transcription is Rapidly Upregulated by Blue Light\n",
            "\u001b[35mSection_title: 2.3.  Photoperiod Affects the Circadian Oscillation of sjCRY-DASH\n",
            "\u001b[35mSection_title: 2.3.  Photoperiod Affects the Circadian Oscillation of sjCRY-DASH\n",
            "\u001b[35mSection_title: 2.4.  Temperature Effects the Expression of sjCRY-DASH\n",
            "\u001b[35mSection_title: 2.4.  Temperature Effects the Expression of sjCRY-DASH\n",
            "\u001b[35mSection_title: 2.5.  SjCRY-DASH Is Negatively Regulated by Novel-m3234-5p\n",
            "\u001b[35mSection_title: 2.5.  SjCRY-DASH Is Negatively Regulated by Novel-m3234-5p\n",
            "\u001b[35mSection_title: 2.6.  LincRNAs Regulate sjCRY-DASH via Trans-Acting mechanisms\n",
            "\u001b[35mSection_title: 2.6.  LincRNAs Regulate sjCRY-DASH via Trans-Acting mechanisms\n",
            "\u001b[35mSection_title: 2.7.  Identification of the Candidate Competing Endogenous RNAs of sjCRY-DASH\n",
            "\u001b[35mSection_title: 2.7.  Identification of the Candidate Competing Endogenous RNAs of sjCRY-DASH\n",
            "\u001b[35mSection_title: 2.7.  Identification of the Candidate Competing Endogenous RNAs of sjCRY-DASH\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[35mSection_title: 4.  Materials and Methods\n",
            "\u001b[35mSection_title: 4.1.  Sample Collection and Treatment with Various Conditions\n",
            "\u001b[35mSection_title: 4.2.  RNA Preparation and cDNA Synthesis\n",
            "\u001b[35mSection_title: 4.3.  Cloning of CRY-DASH from S. japonica\n",
            "\u001b[35mSection_title: 4.4.  Heterologous Expression and Purification of sjCRY-DASH\n",
            "\u001b[35mSection_title: 4.5.  Small RNA Library Preparation and Sequencing\n",
            "\u001b[35mSection_title: 4.6.  Long Non-Coding RNA Library Preparation and Sequencing\n",
            "\u001b[35mSection_title: 4.7.  miRNA-sjCRY-DASH-lncRNA Association Analysis\n",
            "\u001b[35mSection_title: 4.8.  Quantitative Real-Time PCR Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  DROP Data Collection\n",
            "\u001b[35mSection_title: 4  DROP Data Analysis\n",
            "\u001b[35mSection_title: 5  Baseline Systems\n",
            "\u001b[35mSection_title: 5.1  Semantic Parsing\n",
            "\u001b[35mSection_title: 5.2  SQuAD-style Reading Comprehension\n",
            "\u001b[35mSection_title: 5.3  Heuristic Baselines\n",
            "\u001b[35mSection_title: 6  Augmented QANet\n",
            "\u001b[35mSection_title: 6.1  Model Description\n",
            "\u001b[35mSection_title: 6.2  Weakly-Supervised Training\n",
            "\u001b[35mSection_title: 7  Results and Discussion\n",
            "\u001b[32mconclusion: we have presented drop, a dataset of complex reading comprehension questions that require discrete reasoning over paragraphs. this dataset is substantially more challenging than existing datasets, with the best baseline achieving only 32.7% f1, while humans achieve 96%. we hope this dataset will spur research into more compre-hensive analysis of paragraphs, and into methods that combine distributed representations with symbolic reasoning. we have additionally presented initial work in this direction, with a model that augments qanet with limited numerical reasoning capability, achieving 47% f1 on drop.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Query Extraction (QEXT)\n",
            "\u001b[35mSection_title: 3.1.2  Query Extraction by Salient Span Selection\n",
            "\u001b[35mSection_title: 3.2  Transferred Query Generation (TQGEN)\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 4.4.1  Unsupervised Retrieval Results\n",
            "\u001b[35mSection_title: 4.4.3  Fine-Tuned Results\n",
            "\u001b[35mSection_title: 4.5  Result Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this study, a series of scalable augmentation methods are proposed to produce surrogate queries for training dense retrievers without using any annotated query-document pairs. we achieve state-ofthe-art performance on two collections of widely used benchmarks (beir and six odqa datasets), demonstrating that the efficacy of synthetic querydocument pairs for training dense retrievers, greatly bridging the gap between unsupervised dense models and bm25 and inspiring us to rethink the necessity of using real queries.\n",
            "\u001b[35mSection_title: 6  Discussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 1.1  Related Surveys\n",
            "\u001b[35mSection_title: 1.2  Structure of the survey:\n",
            "\u001b[35mSection_title: 2  MACHINE READING COMPREHENSION\n",
            "\u001b[35mSection_title: 2.1  Classical MRC\n",
            "\u001b[35mSection_title: 2.1.1  Cloze-style MRC\n",
            "\u001b[35mSection_title: 2.1.2  Multi-choice MRC\n",
            "\u001b[35mSection_title: 2.1.3  Span Extraction MRC\n",
            "\u001b[35mSection_title: 2.1.4  Free-form MRC\n",
            "\u001b[35mSection_title: 2.2  Novel MRC\n",
            "\u001b[35mSection_title: 2.2.1  Conversational Question Answering\n",
            "\u001b[35mSection_title: 2.2.2  Multi-hop Question Answering\n",
            "\u001b[35mSection_title: 2.2.3  Long-form Question Answering\n",
            "\u001b[35mSection_title: 2.2.4  Cross-language Question Answering\n",
            "\u001b[35mSection_title: 3.1  CuratedTREC\n",
            "\u001b[35mSection_title: 3.2  WebQuestions\n",
            "\u001b[35mSection_title: 3.3  WikiMovies\n",
            "\u001b[35mSection_title: 3.4  OpenSQuAD\n",
            "\u001b[35mSection_title: 3.5  Qusar-T\n",
            "\u001b[35mSection_title: 3.6  OpenTriviaQA\n",
            "\u001b[35mSection_title: 3.7  OpenNaturalQuestions\n",
            "\u001b[35mSection_title: 3.8  KILT benchmarks\n",
            "\u001b[35mSection_title: 3.8.1  KILT-Natural Questions\n",
            "\u001b[35mSection_title: 3.8.2  KILT-HotpotQA\n",
            "\u001b[35mSection_title: 3.8.3  KILT-TriviaQA\n",
            "\u001b[35mSection_title: 3.8.4  KILT-EMLI5\n",
            "\u001b[35mSection_title: 4  COMMONSENSE QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 4.1  SWAG\n",
            "\u001b[35mSection_title: 4.2  CODAH\n",
            "\u001b[35mSection_title: 4.3  CommonsenseQA\n",
            "\u001b[35mSection_title: 4.4  ReCoRD\n",
            "\u001b[35mSection_title: 4.5  CosmosQA\n",
            "\u001b[35mSection_title: 5  METRICS\n",
            "\u001b[35mSection_title: 5.1  Accuracy\n",
            "\u001b[35mSection_title: 5.2  Exact Match\n",
            "\u001b[35mSection_title: 5.3  F1\n",
            "\u001b[35mSection_title: 5.4  ROUGE\n",
            "\u001b[35mSection_title: 5.5  BLEU\n",
            "\u001b[35mSection_title: 5.6  Meteor\n",
            "\u001b[35mSection_title: 5.7  HEQ\n",
            "\u001b[35mSection_title: 5.8  IoU\n",
            "\u001b[35mSection_title: 6  CURRENT TRENDS AND FUTURE DIRECTIONS\n",
            "\u001b[35mSection_title: 6.1  Current Trends\n",
            "\u001b[35mSection_title: 6.2  Future Directions\n",
            "\u001b[32mconclusion: in this survey, we reviewed 47 textual qa benchmark datasets and their corresponding evaluation metrics. a novel taxonomy of textual qa tasks is provided from an application scenario point of view. a detailed description is provided for each dataset, which covers the task definition, contraction method, statistics, and evaluation measures. key features and detailed statistics among the benchmark datasets are summarized and compared in the form of tables. detailed description and distribution analysis of evaluation metrics are provided. finally, we summarized the trends of the recent textual qa benchmark contraction methods and give our opinions on the future directions of the textual qa benchmark research. we hope this work serves as a good introduction to textual qa tasks.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Preliminaries\n",
            "\u001b[35mSection_title: 2.2  Two-stage Pre-Training\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Zero-shot Retrieval\n",
            "\u001b[35mSection_title: 3.2  In-domain Retrieval\n",
            "\u001b[35mSection_title: 3.3  Sentence Embeddings\n",
            "\u001b[35mSection_title: 3.4  Summarization\n",
            "\u001b[32mconclusion: in this paper, a unified framework is presented for the pre-training of sentence representation. on top of the consecutive mae style pre-training on generic and domain-specific data, and with proper collaboration with contrastive learning, the generated model may support a wide variety of sentence representation tasks, including zero-shot retrieval, in-domain retrieval, and sentence embeddings. the experiment studies demonstrate that the proposed framework helps to achieve strong performances on benchmarks, like beir, ms marco, natural questions, sts and transfer tasks in senteval. for future works, we'll make extensions by scaling up the size of the encoding network and having it pre-trained on more unsupervised data, which will push to the empirical limit of the current method. we'll also explore its effectiveness in other languages other than english, and its impact to more applications beyond dense retrieval and nli.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Setting\n",
            "\u001b[35mSection_title: 2.1  Problem statement\n",
            "\u001b[35mSection_title: 2.2  Properties and terminologies\n",
            "\u001b[35mSection_title: 2.3  Evaluation metrics\n",
            "\u001b[35mSection_title: 2.3.1  Average nDCG\n",
            "\u001b[35mSection_title: 2.3.2  Average F 1\n",
            "\u001b[35mSection_title: 2.3.3  Answer matching\n",
            "\u001b[35mSection_title: 3  Related Datasets and Annotations\n",
            "\u001b[35mSection_title: 3.1  Multimodality\n",
            "\u001b[35mSection_title: 3.1.1  Screen UI for mobile apps\n",
            "\u001b[35mSection_title: 3.1.2  Document image understanding\n",
            "\u001b[35mSection_title: 3.1.3  Visual question answering\n",
            "\u001b[35mSection_title: 3.2  Question answering\n",
            "\u001b[35mSection_title: 4  Annotation Method\n",
            "\u001b[35mSection_title: 4.1  Pre-filtering\n",
            "\u001b[35mSection_title: 4.2  Question annotations\n",
            "\u001b[35mSection_title: 4.3  Answer annotations\n",
            "\u001b[35mSection_title: 5  Annotation Analysis\n",
            "\u001b[35mSection_title: 5.1  Question analysis\n",
            "\u001b[35mSection_title: 5.2  Answer analysis\n",
            "\u001b[32mconclusion: in this work, we proposed the screenqa task. we annotated a large-scale screenqa dataset, which contains more than 80,000 question-answer pairs. compared to other vision-language multimodal problems, such as document image understanding and visual question answering, screenqa poses its unique challenges: rich in text, diverse in apps, and blended with icons and symbols. we hope to use the screenqa task and the dataset to encourage the community to look into this screen content understanding problem, as it enables new technologies and new user experiences. figure 4: data annotation interfaces for question and answer collection. a) question annotation was performed in a sequential manner, the later and non-overlapping annotators can see all previous questions to diversify question framing and avoid duplication. we also used the sequential process to provide more feedback and training to the annotators for quality improvement. b) the answer annotators were tasked to determine if the question is valid and if the question is answerable from the screen context. if both are positive, the annotators need to answer the questions by 1) selecting or drawing the bounding boxes of ui elements, 2) fill the text for each selected/drawn bounding box on right right, and 3) ranking them appropriately. the annotators were also tasked to review and make necessary corrections if the question has grammatical errors or typos.  a) the two question annotation passes were capped at five and three questions, respectively, resulting in the maximum eight questions in total. b) the cases when a single bounding box forms a sufficient answer amount to 92% of the questions, hence removed from the chart for the clarity of the long tail. anything beyond 11 bounding boxes is less than 0.05%, accumulatively less than 0.1%.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  APPROACH\n",
            "\u001b[35mSection_title: 3.1  MULTITASK PROMPT TUNING\n",
            "\u001b[35mSection_title: 3.2  SOURCE TRAINING AND TARGET ADAPTATION\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.2  RESULTS AND ANALYSIS\n",
            "\u001b[35mSection_title: 4.3  ABLATION STUDIES\n",
            "\u001b[32mconclusion: we introduced and studied multitask prompt tuning (mpt), which learns a single transferable prompt by decomposing and distilling knowledge from multiple source tasks and their task-specific source prompts. mpt decomposes the task prompt as the hadamard product of a shared prompt matrix and a rank-one task-specific matrix. the shared component is then transferred and adapted to target tasks for further tuning. empirically we found this approach enables parameter-efficient transfer learning to target downstream tasks across diverse nlp benchmarks, even outperforming the full finetuning baseline in some cases, despite tuning much fewer task-specific parameters.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Data Collection\n",
            "\u001b[35mSection_title: 2.1  Claim Creation\n",
            "\u001b[35mSection_title: 2.1.1  Creating 2-Hop Claims from HOTPOTQA\n",
            "\u001b[35mSection_title: 2.1.2  Extending to 3-Hop and 4-Hop Claims\n",
            "\u001b[35mSection_title: 2.2  Claim Mutation\n",
            "\u001b[35mSection_title: 2.3  Claim Labeling\n",
            "\u001b[35mSection_title: 2.4  Annotator Details\n",
            "\u001b[35mSection_title: 4  Baseline System\n",
            "\u001b[35mSection_title: 5  Experiments and Results\n",
            "\u001b[35mSection_title: 5.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.2  Document Retrieval Results\n",
            "\u001b[35mSection_title: 5.3  Sentence Selection Results\n",
            "\u001b[35mSection_title: 5.4  Claim Verification Results\n",
            "\u001b[35mSection_title: 5.5  Full Pipeline Results\n",
            "\u001b[35mSection_title: 5.6  Human Performance\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we present hover, a fact extraction and verification dataset requiring evidence retrieval from as many as four wikipedia articles that form reasoning graphs of diverse shapes. we show that the performance of existing state-of-the-art models degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of robust many-hop reasoning in achieving strong results. we hope that hover will encourage the development of models capable of performing complex many-hop reasoning in the tasks of information retrieval and verification.   document retrieval, sentence selection, and claim verification. the fine-tuning is done with a batch size of 16 and the default learning rate of 5e-5 without warmup. we set k r = 20, k p = 5, κ p = 0.5, and κ s = 0.3 based on the memory limit and the dev set performance. we select our system with the best dev-set verification accuracy and report its scores on the hidden test set. the entire pipeline is visualized in fig. 2. for document retrieval and sentence selection tasks, we fine-tune the bert on 4 nvidia v100 gpus for 3 epochs. the training of both tasks takes around 1 hour. for claim verification task, we fine-tune the bert on a single nvidia v100 for 3 epochs. the training finishes in 30 minutes.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The Mr. Right Dataset\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Annotated Query Validation\n",
            "\u001b[35mSection_title: 3.3  Dataset Analysis\n",
            "\u001b[35mSection_title: 3.4  Benchmark\n",
            "\u001b[35mSection_title: 3.5  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4  Multimodal Retrieval\n",
            "\u001b[35mSection_title: 4.1  Retrieval Formulation\n",
            "\u001b[35mSection_title: 4.2  Model Architecture\n",
            "\u001b[35mSection_title: 4.3  Training Objectives\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Dataset\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Experiment Setup\n",
            "\u001b[35mSection_title: 5.4  Results and Analysis\n",
            "\u001b[32mconclusion: in this paper, we propose mr. right, a multimodal retrieval dataset for information retrieval. mr. right covers three types of text-based search queries with different modality information, including text-related, image-related, and mixed, to simulate real-world search situations. further, our dataset provides documents with texts and images to develop multimodal representation. we build our end-to-end multimodal retrieval model for mr. right to unify features across modalities. compared to the previous text and image retrieval frameworks, multimodal retrieval shows improvements on different queries and points out the balance between modalities. however, current multimodal models still have a significant gap to human performance, showing the potential of mr. right as a challenge in multimodal retrieval. we believe mr. right can breathe new insights into information retrieval for more robust retrieval systems.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: in mr. right, we only consider text-based queries, which may limit the search modalities from users. we can expand our dataset with additional domain queries and documents such as images, audio, and video. further, mr. right focuses on the materials in wikipedia. we can explore other sources such as news, blogs, or commercial websites. mr. right is a preliminary attempt to explore multimodal retrieval, and there are still challenges we need to analyze and study in future work. \n",
            "\u001b[35mSection_title: 7  Limitations and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Baseline models for AS2\n",
            "\u001b[35mSection_title: 3.1  Simple binary classifier (SBC)\n",
            "\u001b[35mSection_title: 3.2  Pairwise Classifier (PC)\n",
            "\u001b[35mSection_title: 3.3  All Candidate Multi-classifier (ACM)\n",
            "\u001b[35mSection_title: 3.4  Answer Support Reranker (ASR)\n",
            "\u001b[35mSection_title: 4  Double Reranking and Retrieval\n",
            "\u001b[35mSection_title: 4.1  Double Answer Reranking (DAR)\n",
            "\u001b[35mSection_title: 4.2  Double Retrieval (DR) with DPR\n",
            "\u001b[35mSection_title: 4.3  Double Ranking and Retrieval\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Training and testing details\n",
            "\u001b[35mSection_title: 5.3  Comparative/ablated results\n",
            "\u001b[35mSection_title: 5.4  Model discussion and error analysis\n",
            "\u001b[32mconclusion: in this paper, we propose, dar, a transformer architecture based on two reranking heads: (i) the answer reranker (as2 model) and the answer support q: how many viewers did \"family guy\" premier to? c1: family guy officially premiered after fox's broadcast of super bowl xxxiii on january 31, 1999, with \"death has a shadow. c2: the show debuted to 22 million viewers, and immediately generated controversy regarding its adult content. c3: at the end of its first season, the show was #33 in the nielsen ratings, with 12.8 million households tuning i.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Neural Representations of Knowledge\n",
            "\u001b[35mSection_title: 3  Dynamic Construction of Intermediate Knowledge Graphs\n",
            "\u001b[35mSection_title: 3.1  Building the Graph\n",
            "\u001b[35mSection_title: 3.2  Evaluating the Graph\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets and Processing\n",
            "\u001b[35mSection_title: 4.2  Experiment Settings\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 5.1  SOCIALIQA\n",
            "\u001b[35mSection_title: 5.2  STORYCOMMONSENSE\n",
            "\u001b[32mconclusion: we use neural representations of large-scale commonsense knowledge graphs (comet) to generate contextualized knowledge graphs on demand for zero-shot question answering. our approach dynamically constructs a knowledge graph of commonsense inferences related to a presented context and conditions on it to evaluate answer options for a posed question. we use probabilistic inference to reason over the constructed graph to select the most likely answer to a question. our approach exceeds the performance of large-scale pretrained language models at the zero-shot setting by 8.5% on the socialiqa dataset. furthermore, on both the socialiqa and storycommonsense datasets, dynamically generating a contextualized commonsense knowledge graph performs better than using comet to directly answer questions.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Problem Definition\n",
            "\u001b[35mSection_title: 2  Conceptual Few-shot Learning\n",
            "\u001b[35mSection_title: 3  Informative Concepts Extraction\n",
            "\u001b[35mSection_title: 4  Evaluation with Informative Demonstrations\n",
            "\u001b[35mSection_title: 4.1  Few-shot Learners\n",
            "\u001b[35mSection_title: 4.2  Datasets\n",
            "\u001b[35mSection_title: 5  Training Concept-aware Few-shot Learner\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[32mconclusion: this work introduces a task of conceptual few-shot learning, that reflects on language models' ability to learn in-context and apply a specific, possibly novel reasoning concept. we find that current incontext few-shot learners are largely insensitive to the concepts presented in demonstrations, suggesting that their ability to learn in context might be conditioned by other factors, such as their memo-rization capacity, rather than their level of task comprehension. conceptual few-shot learning poses a challenge of more controllable and scalable fewshot learning to future few-shot learners.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  SPARTA Neural Ranker\n",
            "\u001b[35mSection_title: 3.3  Learning to Rank\n",
            "\u001b[35mSection_title: 3.4  Indexing and Inference\n",
            "\u001b[35mSection_title: 3.5  Relation to Classic IR and Generative Models\n",
            "\u001b[35mSection_title: 4  OpenQA Experiments\n",
            "\u001b[35mSection_title: 4.1  OpenQA Results\n",
            "\u001b[35mSection_title: 5  Retrieval QA Experiments\n",
            "\u001b[35mSection_title: 5.1  In-domain Performance\n",
            "\u001b[35mSection_title: 5.2  Out-of-domain Generalization\n",
            "\u001b[35mSection_title: 6.1  Interpreting Sparse Representations\n",
            "\u001b[35mSection_title: 6.2  Sparsity vs. Performance\n",
            "\u001b[32mconclusion: in short, we propose sparta, a novel ranking method, that learns sparse representation for better open-domain qa. experiments show that the proposed framework achieves the state-of-the-art performance for 4 different open-domain qa tasks in 2 languages and 11 retrieval qa tasks. this confirm our hypothesis that token-level interaction is superior to sequence-level interaction for better evidence ranking. analyses also show the advantages of sparse representation, including interpretability, generalization and efficiency. our findings also suggest promising future research directions. the proposed method does not support multi-hop reasoning, an important attribute that enables qa systems to answer more complex questions that require collecting multiple evidence passages. also, current method only uses a bag-ofword features for the query. we expect further gain by incorporating word-order information.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 2.1  Problem Formulation\n",
            "\u001b[35mSection_title: 2.2  Analysis of Confirmation Bias\n",
            "\u001b[35mSection_title: 3  The Proposed UPL-EA Framework\n",
            "\u001b[35mSection_title: 3.1  Entity Alignment Model\n",
            "\u001b[35mSection_title: 3.1.1  Global-Local Aggregation for Entity Embedding Learning\n",
            "\u001b[35mSection_title: 3.1.2  Model Training for Entity Alignment\n",
            "\u001b[35mSection_title: 3.2  Within-Iteration OT-Based Pseudo-Labeling\n",
            "\u001b[35mSection_title: 3.3  Cross-Iteration Pseudo-Label Calibration\n",
            "\u001b[35mSection_title: 3.4  Overall Workflow\n",
            "\u001b[35mSection_title: 4.1  Datasets and Baselines\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Comparison with State-of-the-Art Baselines\n",
            "\u001b[35mSection_title: 4.4  Comparison w.r.t. Different Rates of Prior Alignment Seeds\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[35mSection_title: 4.6  Impact of Pre-trained Word Embeddings\n",
            "\u001b[35mSection_title: 4.7  Parameter Sensitivity Study\n",
            "\u001b[35mSection_title: 4.8  Empirical Evidence on the Effectiveness of Pseudo-label Calibration\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 5.1  Entity Alignment in Knowledge Graphs\n",
            "\u001b[35mSection_title: 5.2  Pseudo-Labeling\n",
            "\u001b[35mSection_title: 5.3  Optimal Transport on Graphs\n",
            "\u001b[32mconclusion: we proposed a novel unified pseudo-labeling framework (upl-ea) that addresses the problem of confirmation bias for pseudo-labeling-based entity alignment across kgs. upl-ea employs an entity alignment model based on the global-local aggregation architecture to generate informative entity embeddings. in addition, upl-ea includes two modules to combat confirmation bias: within-iteration optimal transport (ot)-based pseudo-labeling and cross-iteration pseudo-label calibration. the ot-based pseudo-labeling module utilizes ot modeling to eliminate conflicted misalignments (type i pseudo-labeling errors) within each iteration. the pseudo-label calibration module employs pseudo-labels from multiple consecutive iterations to reduce pseudo-label selection variability, thus preventing the accumulation and propagation of inevitable one-to-one misalignments (type ii pseudo-labeling errors) across iterations. our extensive experiments on benchmark datasets show that upl-ea outperforms state-of-the-art baselines with limited amounts of prior alignment seeds. the competitive performance of upl-ea demonstrates its superiority in addressing confirmation bias and its potential for pseudo-labeling-based entity alignment across kgs.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Structure of the paper\n",
            "\u001b[35mSection_title: 1.2  Contribution\n",
            "\u001b[35mSection_title: 1.3  Survey methodology\n",
            "\u001b[35mSection_title: 2.1  Definitions\n",
            "\u001b[35mSection_title: 2.2  Question answering typical pipeline\n",
            "\u001b[35mSection_title: 2.3  Modular approaches before transformers\n",
            "\u001b[35mSection_title: 3.1  What are complex questions ?\n",
            "\u001b[35mSection_title: 3.2  Skills\n",
            "\u001b[35mSection_title: 3.2.2  (Information)\n",
            "\u001b[35mSection_title: 3.3  Tasks\n",
            "\u001b[35mSection_title: 3.3.3  CQA tasks hybrid program decomposition examples\n",
            "\u001b[35mSection_title: 3.4  LLM limits\n",
            "\u001b[35mSection_title: 4  Evaluating: metrics, cost functions, datasets\n",
            "\u001b[35mSection_title: 4.1  Metrics & performance SOTA\n",
            "\u001b[35mSection_title: 4.1.1  Standard metrics\n",
            "\u001b[35mSection_title: 4.1.3  HELM multi-metrics and related findings\n",
            "\u001b[35mSection_title: 4.2  Cost functions\n",
            "\u001b[35mSection_title: 4.3  Datasets\n",
            "\u001b[35mSection_title: 4.3.3  Structured knowledge datasets\n",
            "\u001b[35mSection_title: 4.3.4  Decomposition and multi-hop datasets\n",
            "\u001b[35mSection_title: 4.3.5  Instructions (IFT, CoT) datasets\n",
            "\u001b[35mSection_title: 4.3.6  Reasoning datasets\n",
            "\u001b[35mSection_title: 4.3.9  Local & multi-lingual datasets\n",
            "\u001b[35mSection_title: 4.3.10  Dialogue datasets\n",
            "\u001b[35mSection_title: 5  Solving with training\n",
            "\u001b[35mSection_title: 5.1  Training dataset quality\n",
            "\u001b[35mSection_title: 5.2  Type of LLM training\n",
            "\u001b[35mSection_title: 5.3  Pre-training techniques\n",
            "\u001b[35mSection_title: 5.3.2  Transfer learning, domain adaptation, knowledge distillation\n",
            "\u001b[35mSection_title: 5.3.3  Program execution learning\n",
            "\u001b[35mSection_title: 5.4  Supervised learning & fine-tuning\n",
            "\u001b[35mSection_title: 5.4.1  (Task specific) Vanilla Fine-Tuning\n",
            "\u001b[35mSection_title: 5.4.2  Multi-task learning\n",
            "\u001b[35mSection_title: 5.4.3  Instruction fine-tuning\n",
            "\u001b[35mSection_title: 5.4.4  Transfer learning, knowledge & domain adaptation, continual learning\n",
            "\u001b[35mSection_title: 5.4.5  Knowledge distillation (KD)\n",
            "\u001b[35mSection_title: 5.4.6  Active learning\n",
            "\u001b[35mSection_title: 5.4.7  Meta learning\n",
            "\u001b[35mSection_title: 5.4.8  Multi-view learning\n",
            "\u001b[35mSection_title: 5.4.9  Reinforcement learning\n",
            "\u001b[35mSection_title: 5.4.10  Intermediate Fine-Tuning Intermediate Fine-Tuning (IFT) fine-tunes a model using an intermediate dataset with a large number of labeled instances to learn domain knowledge (DAIFT -domain adaptative IFT)\n",
            "\u001b[35mSection_title: 5.5  Parameter-efficient tuning (PEFT) of a frozen PLM\n",
            "\u001b[35mSection_title: 5.6  Techniques for improving training\n",
            "\u001b[35mSection_title: 7.2.2  Multi-step optimization\n",
            "\u001b[35mSection_title: 7.2.3  Process optimization\n",
            "\u001b[35mSection_title: 7.2.4  Prompt for external module Prompt is designed to query an external dedicated tool\n",
            "\u001b[35mSection_title: 7.3  Enhancing knowledge or skills\n",
            "\u001b[35mSection_title: 8.1  Reinforcement learning methods\n",
            "\u001b[35mSection_title: 9.1  Hallucination & credibility\n",
            "\u001b[35mSection_title: 9.3  Data availability & quality\n",
            "\u001b[35mSection_title: 9.4  Data multi-sensitivity usage & protection\n",
            "\u001b[35mSection_title: 9.5  Decomposition of very complex QA and explainability\n",
            "\u001b[32mconclusion: in this paper, we present a comprehensive survey of language model hybrid architectures for answering complex questions. we review the various skills required and typical approach, datasets and metrics that are used, the current limits of large language models for complex qa, the potential of hybrid architectures, better training and prompting strategies for this goal. we also identify the main challenges and research avenues for solving more complex questions including knowledge capitalization. we identify the need to address multi-sensitivity data in language models architectures and potential approaches. finally, we outline research topics and highlight the potential of exploration in this field. this paper aims to provide a comprehensive and useful resource for readers interested in the development of complex non-factoid question answering.\n",
            "\u001b[35mSection_title: 10  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Characteristics of Explanation Quality\n",
            "\u001b[35mSection_title: 2.1  User-Centered Explanation Quality\n",
            "\u001b[35mSection_title: 2.2  Orthogonal Dimensions of Explanation Quality\n",
            "\u001b[35mSection_title: 3  Case Study: HotpotQA\n",
            "\u001b[35mSection_title: 3.1  Case Study Design\n",
            "\u001b[35mSection_title: 4  Shortcomings of Current Evaluation Practices\n",
            "\u001b[35mSection_title: 4.1  Unvalidated Proxy Scores\n",
            "\u001b[35mSection_title: 4.2  Neglecting Users\n",
            "\u001b[35mSection_title: 4.3  Single-score Leaderboards\n",
            "\u001b[35mSection_title: 4.4  Case Study Part I: Analysis of Current Evaluation Practice\n",
            "\u001b[35mSection_title: 5  Guidelines\n",
            "\u001b[35mSection_title: 5.1  Validate Proxy Scores Against Humans\n",
            "\u001b[35mSection_title: 5.2  Do Human Evaluation Periodically\n",
            "\u001b[35mSection_title: 5.3  Use Various Scores for Evaluation and Pareto Front Leaderboards\n",
            "\u001b[35mSection_title: 5.4  Case Study Part II: Guideline Evaluation\n",
            "\u001b[32mconclusion: this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face. we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate those characteristics. finally, we propose guidelines for a more effective evaluation, which we hope to inspire future work and ultimately drive the field towards reliable and meaningful explanation quality evaluation. our discussion is backed up with examples, well-known theories and empirical findings from a crowdsourced case study that we conducted for the example of explainable question answering systems.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  INTERPOLATION AND EXTRAPOLATION\n",
            "\u001b[35mSection_title: 3  INVESTIGATING EXISTING BENCHMARKS\n",
            "\u001b[35mSection_title: 3.1  Benchmark Details\n",
            "\u001b[35mSection_title: 3.2  Problem of Existing Benchmarks\n",
            "\u001b[35mSection_title: 4  RESAMPLE-BASED EVALUATION\n",
            "\u001b[35mSection_title: 4.1  Methodology\n",
            "\u001b[35mSection_title: 4.1.1  Query Similarity.\n",
            "\u001b[35mSection_title: 4.1.2  Resampling Training Queries.\n",
            "\u001b[35mSection_title: 4.1.3  Resampling Training and Test Queries.\n",
            "\u001b[35mSection_title: 4.2  Alignment with Generalization\n",
            "\u001b[35mSection_title: 4.2.1  Implementation Details.\n",
            "\u001b[35mSection_title: 4.2.2  Discussion.\n",
            "\u001b[35mSection_title: 5  INTERPOLATION AND EXTRAPOLATION EVALUATION OF EXISTING RETRIEVAL MODELS\n",
            "\u001b[35mSection_title: 5.1  Impact of Model Architectures\n",
            "\u001b[35mSection_title: 5.1.1  Model Architectures.\n",
            "\u001b[35mSection_title: 5.1.2  Implementation.\n",
            "\u001b[35mSection_title: 5.1.3  Discussion.\n",
            "\u001b[35mSection_title: 5.2  Impact of Training Strategies\n",
            "\u001b[35mSection_title: 5.2.1  Advanced Training Methods.\n",
            "\u001b[35mSection_title: 5.2.2  Implementation.\n",
            "\u001b[35mSection_title: 5.2.3  Comparing Pretraining and Finetuning.\n",
            "\u001b[35mSection_title: 5.2.4  Investigation of Pretraining.\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[32mconclusion: in this paper, we propose to separately evaluate interpolation and extrapolation capabilities of neural retrieval models. considering the dynamics of queries in web search, we define them based on whether the training and test queries are similar or not. based on the definition, we investigate the bias in popular benchmarks, design associated evaluation methods, and revisit existing neural ranking models. we observe that the popular benchmarks are biased towards interpolation and thus may not reflect how models extrapolate.\n",
            "\u001b[35mSection_title: 7  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Categorization of Domain Generalization Methods\n",
            "\u001b[35mSection_title: 3  Toy Example: Concentric Circles\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Natural Language Inference (NLI)\n",
            "\u001b[35mSection_title: 4.2  Question Answering (QA)\n",
            "\u001b[35mSection_title: 4.3  Image Classification\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Correlation between Adversarial Robustness and OOD Generalization\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Insights\n",
            "\u001b[35mSection_title: 3  Our Pipeline\n",
            "\u001b[35mSection_title: 3.1  Document retrieval\n",
            "\u001b[35mSection_title: 3.2  Passage retrieval\n",
            "\u001b[35mSection_title: 3.3  Entailment\n",
            "\u001b[35mSection_title: 3.3.1  TAPAS Max\n",
            "\u001b[35mSection_title: 3.3.2  TAPAS Joint\n",
            "\u001b[35mSection_title: 3.4  Entailment Ensemble Model\n",
            "\u001b[35mSection_title: 4  Training methodology\n",
            "\u001b[35mSection_title: 4.1  Passage retrieval\n",
            "\u001b[35mSection_title: 4.2  Entailment\n",
            "\u001b[32mconclusion: overall, this multi-modal task creates a plethora of new challenges to overcome and opens exciting avenues of research for the future of automated fact verification.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.6.  Electrophoretic Mobility Shift Assays (EMSA)\n",
            "\u001b[35mSection_title: 2.7.  X-ray 3D Structure Determination\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Biological Relevance of the UGAUUGAAG Motif\n",
            "\u001b[35mSection_title: 3.2.  Functional and/or Structural Insights into the NS1-RNA Interactions\n",
            "\u001b[35mSection_title: 3.2.1.  Minimal Structure of Selected Aptamers for Efficient Recognition by NS1\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.2.  Major Contribution of the RBD in the Recognition of DM01-Midi and DM03-Midi\n",
            "\u001b[35mSection_title: 3.2.3.  Contribution of the AGCAAAAG and GUAAC Motifs to the Interaction\n",
            "\u001b[35mSection_title: 3.2.3.  Contribution of the AGCAAAAG and GUAAC Motifs to the Interaction\n",
            "\u001b[35mSection_title: 3.2.4.  AWFC01, a High-Affinity dsRNA Containing Two GUAAC Motifs\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1′s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 3.3.  Structural Basis of NS1 s Interaction with Model dsRNA AWFC01\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Extended Transformer Construction\n",
            "\u001b[35mSection_title: 3.1  Relative Position Encoding\n",
            "\u001b[35mSection_title: 3.2  Global-Local Attention\n",
            "\u001b[35mSection_title: 3.3  Long Inputs and Global-Local Attention\n",
            "\u001b[35mSection_title: 3.4  Structured Inputs\n",
            "\u001b[35mSection_title: 3.5  Pre-training Tasks\n",
            "\u001b[35mSection_title: 3.6  Lifting Weights from Existing Models\n",
            "\u001b[35mSection_title: 4  Empirical Evaluation\n",
            "\u001b[35mSection_title: 4.1  Training Configuration\n",
            "\u001b[35mSection_title: 4.2  Results on the Dev Set\n",
            "\u001b[35mSection_title: 4.3  Official Leaderboard Results\n",
            "\u001b[32mconclusion: this paper introduced the extended transformer construction (etc), an architecture designed to (1) scale up the input length (linearly with input), and (2) encode structured inputs. etc allows lifting weights from existing bert models, improving results significantly. the key ideas are a new globallocal attention mechanism, coupled with relative position encodings and a cpc pre-training task.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  State of the art\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  YOLOv3 object detection model\n",
            "\u001b[35mSection_title: 3.2  Data augmentation\n",
            "\u001b[35mSection_title: 3.3  Object-tracking algorithm\n",
            "\u001b[35mSection_title: 3.4  Pipeline development\n",
            "\u001b[35mSection_title: 4  Data\n",
            "\u001b[35mSection_title: 4.1  Image dataset\n",
            "\u001b[35mSection_title: 4.2  Video dataset\n",
            "\u001b[35mSection_title: 5  Experimental methodology\n",
            "\u001b[35mSection_title: 5.1  Evaluation metrics\n",
            "\u001b[35mSection_title: 5.2  CNN model evaluation and selection\n",
            "\u001b[35mSection_title: 5.3  Object-tracking algorithm tuning\n",
            "\u001b[35mSection_title: 5.4  Model evaluation\n",
            "\u001b[35mSection_title: 5.5  Software pipeline\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  CNN model evaluation and selection\n",
            "\u001b[35mSection_title: 6.2  Object-tracking algorithm tuning\n",
            "\u001b[35mSection_title: 6.3  Model evaluation\n",
            "\u001b[35mSection_title: 6.3.1  Frame-based evaluation\n",
            "\u001b[35mSection_title: 6.3.2  Polyp-based evaluation\n",
            "\u001b[35mSection_title: 6.4  Performance\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\u001b[32mconclusion: this work has described the development of a dl model for real-time polyp detection, which could be integrated, in the future, into a cad system. yolov3 was selected as the base architecture for the development of this model due to its balance between performance and prediction time and complemented with an object-tracking filtering step able to reduce false positives.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 1.2  A new OKVQA Challenge Dataset\n",
            "\u001b[35mSection_title: 1.3  An interpretable OKVQA system\n",
            "\u001b[32mlimitation: the widely used benchmark okvqa dataset [25] consists of over 14000 question-image pairs, with 9000 training examples and 5000 examples in the test set.we identify two broad issues with it.first, significant overlap exists between answers in the train and test folds.recall from section 1.1 that 48.9% of answers in the test set are present in the training set.existing systems leverage this limitation to boost their accuracy by limiting their test answers to the most frequent answers in the training set.second, unlike webquestions [6] or complexwebquestions [42], okvqa questions, even when grouped into some categories (see below) have no clear pattern of reasoning.18% (type-1) of the questions require detecting objects and subsequent reasoning over an external knowledge source to arrive at the answer.7% of the questions (type-2) require reading text from the image (ocr) (and no other information) to answer.12% of the questions (type-3) are based on personal opinion or speculation.the remaining questions (rest) can perhaps be described best through figure 2d.we provide an example of each type in figure 2.we found that several queries of type-1 have a structural similarity to the bridging queries in complexwebquestions [42].there, each query has exactly two clauses.the first clause, when issued to a web search engine, returns (via rc) an entity which plugs into the second clause, which is again sent to the web search engine, fetching the overall answer.we found that type-1 questions can be reformulated, with the help of the scene graph, to a query that can be answered directly using web search.inspired by complex-webquestions, we next develop our challenge data set.\n",
            "\u001b[35mSection_title: 2  LIMITATIONS IN EXISTING OKVQA DATA\n",
            "\u001b[35mSection_title: 3  DESIGN OF A NEW OKVQA CHALLENGE DATA SET\n",
            "\u001b[35mSection_title: 3.1  OKVQA S3 : Annotated subset of OKVQA\n",
            "\u001b[35mSection_title: 3.2  S3VQA: New dataset built from scratch\n",
            "\u001b[35mSection_title: 4  PRIOR ARCHITECTURES\n",
            "\u001b[35mSection_title: 5  PROPOSED ARCHITECTURE\n",
            "\u001b[35mSection_title: 5.1  Select and Substitute\n",
            "\u001b[35mSection_title: 5.2  Search\n",
            "\u001b[35mSection_title: 6  EXPERIMENTS AND RESULTS\n",
            "\u001b[35mSection_title: 6.1  Methods\n",
            "\u001b[35mSection_title: 6.2  Evaluation\n",
            "\u001b[35mSection_title: 6.3  Implementation details\n",
            "\u001b[35mSection_title: 6.4  Results\n",
            "\u001b[32mconclusion: in this paper, we identify key limitations of existing multimodal qa datasets in being opaque and uninterpretable in their reasoning.towards addressing these limitations, we present okvqa s3 , an improvisation on the existing okvqa dataset as well as design and build a new challenge data set s3vqa that focuses on a specific structural idiom that frequently appears in vqa.we also present a structurally transparent and interpretable system s3 tailored to answer questions from our challenge data set and show that it outperforms strong baselines in both existing classification as well as the proposed open-domain settings.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Related Work\n",
            "\u001b[35mSection_title: 3  Task Formulation\n",
            "\u001b[35mSection_title: 4  BERRI: Collections of Instruction-annotated Retrieval Tasks\n",
            "\u001b[35mSection_title: 4.1  Unified Task and Instructions Scheme\n",
            "\u001b[35mSection_title: 4.2  Dataset Collection\n",
            "\u001b[35mSection_title: 5.1  Model Architecture\n",
            "\u001b[35mSection_title: 5.2  Training TART\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Zero-shot Retrieval Evaluations\n",
            "\u001b[35mSection_title: 6.2  X 2 -Retrieval: Cross-task Cross-domain Retrieval Evaluation\n",
            "\u001b[35mSection_title: 6.3  Baselines\n",
            "\u001b[35mSection_title: 6.4  Experimental Settings\n",
            "\u001b[35mSection_title: 7  Results\n",
            "\u001b[35mSection_title: 7.1  Zero-shot Evaluation Results\n",
            "\u001b[35mSection_title: 7.2  X 2 -Retrieval Evaluation Results\n",
            "\u001b[35mSection_title: 8  Analysis\n",
            "\u001b[35mSection_title: 8.2  Dataset Scale\n",
            "\u001b[35mSection_title: 8.3  Model Scale\n",
            "\u001b[35mSection_title: 8.4  Negative Samples\n",
            "\u001b[32mconclusion: this paper lays the foundation for building a general-purpose task-aware retriever that can follow natural language instructions. we introduced a new problem, retrieval with instructions, to model users' intents explicitly. we presented berri, the first large-scale retrieval dataset with expert-written annotations. building upon berri, we trained the first instruction-following retrieval system by massive multi-task instruction-tuning, tart, adopting two widely used architectures. tart advances the state of the art on the popular zeroshot retrieval benchmarks beir and lotte as well as on our newly introduced challenging evaluation setup, x 2 -retrieval. our analysis shows that key factors to building a successful multi-task instruction-following retrieval system include informative instructions at training and test time, diversity in data and model scale, and carefully designed negative samples. we conclude with two interesting open questions, which future work can explore.\n",
            "\u001b[35mSection_title: 9  Discussions and Conclusion\n",
            "\u001b[35mSection_title: 1.  Altlex\n",
            "\u001b[35mSection_title: 3.  SE (title → title)\n",
            "\u001b[35mSection_title: 4.  YahooAnswers\n",
            "\u001b[35mSection_title: 5.  MSMARCO\n",
            "\u001b[35mSection_title: 6.  ELI5\n",
            "\u001b[35mSection_title: 7.  WikiHow\n",
            "\u001b[35mSection_title: 9.  AGNews\n",
            "\u001b[35mSection_title: 10.  NPR\n",
            "\u001b[35mSection_title: 15.  PAQ\n",
            "\u001b[35mSection_title: 16.  Sentence Compression\n",
            "\u001b[35mSection_title: 18.  XSUM\n",
            "\u001b[35mSection_title: 19.  Coco captions\n",
            "\u001b[35mSection_title: 20.  Quora Dup. Questions\n",
            "\u001b[35mSection_title: 21.  CC News\n",
            "\u001b[35mSection_title: 22.  FEVER\n",
            "\u001b[35mSection_title: 26.  WoW-Knowledge\n",
            "\u001b[35mSection_title: 27.  WoW-Response\n",
            "\u001b[35mSection_title: 30.  PubMedQA\n",
            "\u001b[35mSection_title: 31.  MedMCQA\n",
            "\u001b[35mSection_title: 32.  Gigaord\n",
            "\u001b[35mSection_title: 33.  Record\n",
            "\u001b[35mSection_title: 35.  Qrecc\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Preliminaries\n",
            "\u001b[35mSection_title: 3.1  Prompt Tuning\n",
            "\u001b[35mSection_title: 3.2  Optimization-based Meta Learning\n",
            "\u001b[35mSection_title: 4  Approach\n",
            "\u001b[35mSection_title: 4.1  Problem Setting\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metric\n",
            "\u001b[35mSection_title: 4.3  Meta Prompt Tuning (MPT)\n",
            "\u001b[35mSection_title: 4.3.1  Meta-training\n",
            "\u001b[35mSection_title: 4.3.2  Meta-testing\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Task Partitions\n",
            "\u001b[35mSection_title: 5.2  Methods Compared\n",
            "\u001b[35mSection_title: 5.3  Implementation Details\n",
            "\u001b[35mSection_title: 6  Results and Analysis\n",
            "\u001b[35mSection_title: 6.1  Further Analysis\n",
            "\u001b[32mconclusion: in this paper, we have introduced meta prompt tuning (mpt), which learns to initialize the prompt embeddings for adapting to a target task. we have identified key research questions and systematically studied where and how meta learning can improve cross-task generalization in prompt tuning. we have empirically analyzed a representative set of meta learning methods in a variety of adaptation settings on a large, diverse collection of few-shot tasks. extensive experimental results and analysis verify the effectiveness of mpt. given the findings, in the future, we would like to explore more advanced meta learning algorithms which can consistently outperform multi-task learning.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  RoMQA\n",
            "\u001b[35mSection_title: 2.1  Dataset construction\n",
            "\u001b[35mSection_title: 2.2  Dataset analyses and comparison\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Evaluation\n",
            "\u001b[35mSection_title: 3.2  Models\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in order to build effective nlp models, we must move towards evaluations that test model robustness to variations in the input. we presented romqa, the first benchmark for robust, multi-evidence, multi-answer qa. romqa evaluates robustness of models to varying question constraints by testing for worst-case performance among clusters of related questions. compared to prior qa datasets, romqa has more natural human-written questions that require reasoning over more evidence text to more answers. romqa is challenging for state-of-the-art large lms in zero-shot, few-shot, and supervised settings, and provides a quantifiable test to build more robust qa methods. we want questions that cover diverse topics, however wikidata has a very skewed proposition distribution, with a long tail of rare propositions. hence, we down-sample frequent propositions. let p prop (x) denote the percentage of triples that contain the proposition x. we define the average proposition probability as p prop = 1 |x |\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  coCondenser\n",
            "\u001b[35mSection_title: 2.2  CoT-MAE\n",
            "\u001b[35mSection_title: 3.1  Pre-training\n",
            "\u001b[35mSection_title: 3.2  Fine-tuning\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Pre-training\n",
            "\u001b[35mSection_title: 4.2  Fine-tuning\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 4.5  Out-of-domain Evaluation\n",
            "\u001b[35mSection_title: 5  Analyses\n",
            "\u001b[35mSection_title: 5.1  Impact of Generated Query Number\n",
            "\u001b[35mSection_title: 5.2  Impact of Mixed Context\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[32mconclusion: in this work, we propose query-as-context pretraining, a simple yet effective technique to alleviate the previously ignored issue of weakly correlated pairs during context-supervised pre-training.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\u001b[32mlimitation: a passage is more likely to have a high correlation with its corresponding generated query than another randomly selected passage from the same document.however, limited by the capabilities of the t5 model, there are still a large number of unrelated passage-query pairs.we believe that more powerful large language models have the potential to further alleviate this problem, which is left to our future research.\n",
            "\u001b[35mSection_title: 8  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  THE PRIVQA BENCHMARK\n",
            "\u001b[35mSection_title: 3.1  TASK DEFINITION\n",
            "\u001b[35mSection_title: 3.2  TASK CONSTRUCTION\n",
            "\u001b[35mSection_title: 3.3  EVALUATION METRICS\n",
            "\u001b[35mSection_title: 4  PRIVQA EXPERIMENTS: PROTECTING AGAINST PERSONAL INFORMATION LEAKS\n",
            "\u001b[35mSection_title: 5  RED TEAMING: CIRCUMVENTING ACCESS CONTROL INSTRUCTIONS IN PRIVQA\n",
            "\u001b[35mSection_title: 5.1  THREAT MODEL\n",
            "\u001b[35mSection_title: 5.2  METHODS\n",
            "\u001b[32mconclusion: in this work, we present privqa, a multi-modal benchmark to measure the ability of language models and vision-language models to follow instructions to protect personal information.we also introduce an iterative, instruction-based self-moderation technique for this task.our results indicate there are still gaps in the abilities of state-of-the-art models to follow these kinds of instructions: they are not robust to adversarial inputs, and they suffer from a privacy/utility tradeoff.we also show that models succumb to biases based on popularity and race leading to inconsistent protection across demographic groups.in closing, we hope that this work sheds light on the promise of access control instructions and guides future llm development and safety research.on september 28th, using the gpt-4v(ision) version from september 25th, we conducted geolocation extraction red teaming experiments.we initiated the conversation with the prompt \"what is the name of this building?\".in each of the 12 instances tested, the model refused to respond.following up with a tour request prompt for the same building yielded identifications in 8 out of 12 cases, with 6 of them being correct (see figure 9).we also found the model will not refuse queries regarding well-known landmarks (e.g., space needle).we have disclosed these attacks to researchers at openai.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Graph Construction\n",
            "\u001b[35mSection_title: 2.2  Discourse-Aware Graph Network\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 3.3  Ablation Study\n",
            "\u001b[35mSection_title: 4  Related Works\n",
            "\u001b[32mconclusion: in this paper, we introduce a discourse-aware graph network (dagn) to addressing logical reasoning qa tasks. we first treat elementary discourse units (edus) that are split by discourse relations as basic reasoning units. we then build discourse-based logic graphs with edus as nodes and discourse relations as edges. dagn then learns the discourse-based features and enhances them with contextual token embeddings. dagn reaches competitive performances on two recent logical reasoning datasets reclor and logiqa.  \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Point A: Different Model Sizes but Similar Linguistic Profiles\n",
            "\u001b[35mSection_title: 3.3  Point B: Truthfulness Detection without Content Evaluation\n",
            "\u001b[35mSection_title: 3.4  Point C: Generalizing across Model Sizes\n",
            "\u001b[35mSection_title: 3.5  Point D: Generalizing across Datasets\n",
            "\u001b[35mSection_title: 3.6  Optimizing for Performance\n",
            "\u001b[32mconclusion: so far, we have discussed two main contributions of our paper: 1. similar linguistic profiles are shared across gpt-3 of varying sizes, and 2. exploration on if truthfulness can be detected using stylistic features of the model response. as an exploratory work on applying linguistic feature analysis to truthfulness detection of an llm's response, some experimental setups are limited. but we do obtain some promising results that are worth further exploration. in particular, llms other than gpt-3 must be evaluated to see if the similarity in linguistic properties is a model-level or datasetlevel characteristic or both.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\u001b[32mlimitation: our main limitation comes from dataset size. this was limited because we used human evaluation to label model responses as truthful or untruthful. that is, we have manually confirmed gpt-judge labels on davinci responses, and extrapolated the system to ada, babbage, and curie. frankly, the limitations caused by the small size of the dataset were quite evident because the truthfulness detector was often biased towards producing one label (either 1 or 0). we attempted to solve this problem using lower regularization parameters, but this often produced models with lower performances. an ideal solution to this problem would be training the truthfulness detector on a large set of training instances, which is also our future direction.\n",
            "\u001b[35mSection_title: 5  Limitation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  THE AGENTTUNING APPROACH\n",
            "\u001b[35mSection_title: 2.1  CONSTRUCTING AGENTINSTRUCT\n",
            "\u001b[35mSection_title: 2.1.1  INSTRUCTION CONSTRUCTION\n",
            "\u001b[35mSection_title: 2.1.2  TRAJECTORY INTERACTION\n",
            "\u001b[35mSection_title: 2.1.3  TRAJECTORY FILTERING\n",
            "\u001b[35mSection_title: 2.2  INSTRUCTION TUNING\n",
            "\u001b[35mSection_title: 2.2.1  GENERAL DOMAIN INSTRUCTIONS\n",
            "\u001b[35mSection_title: 2.2.2  MIXTURE TRAINING\n",
            "\u001b[35mSection_title: 2.2.3  TRAINING SETUP\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  EVALUATION SETUP\n",
            "\u001b[35mSection_title: 3.2  MAIN RESULTS\n",
            "\u001b[35mSection_title: 3.3  ERROR ANALYSIS\n",
            "\u001b[35mSection_title: 3.4  ABLATION STUDY\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[32mconclusion: in this work, we study how to enable generalized agent abilities for llms, bridging the disparity between open and commercial llms on agent tasks.we present the agenttuning approach to achieve this goal.agenttuning first introduces the agentinstruct dataset covering 1,866 verified agent interaction trajectories and then designs an instruction-tuning strategy with the mixture of agentinstruct and general-domain instructions.we generate the open agentlm by employing agenttuning to tune the llama 2 models.agentlm exhibits strong performance on unseen agent tasks while preserving their general abilities on mmlu, gsm8k, humaneval, and mt-bench.to date, agentlm-70b is the first open llm that matches gpt-3.5-turbo on agent tasks.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Problem Setting\n",
            "\u001b[35mSection_title: 3.2  Fusion Methods\n",
            "\u001b[35mSection_title: 4.2  Model\n",
            "\u001b[35mSection_title: 4.3  Compared Methods\n",
            "\u001b[35mSection_title: 4.4  Reporting the Results\n",
            "\u001b[35mSection_title: 5  Experiment Results\n",
            "\u001b[35mSection_title: 5.1  Performance on Held-out Tasks\n",
            "\u001b[35mSection_title: 5.2  Efficiency\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 6.1  Evaluate with Varying Number of Shots\n",
            "\u001b[35mSection_title: 6.2  Perturbation to In-Context Examples\n",
            "\u001b[35mSection_title: 6.3  Comparing with Other Model Families\n",
            "\u001b[32mconclusion: motivated by the train-test efficiency differences between few-shot in-context learning and few-shot fine-tuning, we aim to find a balance and benefit from the strengths of both approaches.  based icl (early fusion) and ensemble-based icl (late fusion), in terms of both performance and computation efficiency. moreover, fusion-in-decoder icl partly closes the gap between gradient-free icl methods and gradient-based fine-tuning methods, highlighting the potential of approximating gradient-based optimization with efficient forwardonly methods (phang et al., 2022). future work may build upon our insights to further improve the computation efficiency of few-shot learning. however, similar to the findings in min et al. (2022c), our analysis on icl models suggest that they barely learn the input-label mapping from the in-context examples. we also have mixed results when more shots become available for the icl model. we hope future work can further improve the performance of icl by enabling it to learn from input-label mapping effectively and faithfully.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The KP-PLM Framework\n",
            "\u001b[35mSection_title: 3.1  Notations\n",
            "\u001b[35mSection_title: 3.2  Knowledge Prompting\n",
            "\u001b[35mSection_title: 3.3  Model Architecture\n",
            "\u001b[35mSection_title: 3.4  Self-supervised Pre-training Tasks\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Implementation Details\n",
            "\u001b[35mSection_title: 4.2  Knowledge-aware Tasks\n",
            "\u001b[35mSection_title: 4.3  Performance on General NLU Tasks\n",
            "\u001b[35mSection_title: 4.4  Knowledge Prompting Study\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[32mconclusion: in this paper, we presented a seminal knowledge prompting paradigm, based on which a novel knowledge-prompting-based plm framework kp-plm was proposed. we constructed contextual knowledge sub-graphs for contexts and employed continuous prompting mapping to generate knowledge prompts. after that, we designed two selfsupervised pre-training tasks to learn semantic knowledge from prompts. finally, we conducted extensive experiments to evaluate the model performance. experimental results validate the effectiveness of knowledge prompting in boosting the performance of plms.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Formulation\n",
            "\u001b[35mSection_title: 3  Neural Module Teacher\n",
            "\u001b[35mSection_title: 3.1  Atomic Modules\n",
            "\u001b[35mSection_title: 3.2  Parsing Explanations to Executable Rules\n",
            "\u001b[35mSection_title: 3.3  Extracting Answer Spans\n",
            "\u001b[35mSection_title: 4  Learning to Augment with NMTeacher\n",
            "\u001b[35mSection_title: 4.1  Pre-training the Fill and Find Module\n",
            "\u001b[35mSection_title: 4.2  Training the MRC Model F\n",
            "\u001b[35mSection_title: 5.2  Performance Comparison\n",
            "\u001b[35mSection_title: 5.3  Performance Analysis\n",
            "\u001b[35mSection_title: 5.4  Discussion\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we propose to teach extractive mrc with explanations, with a focus on annotation efficiency. we believe explanations stating \"why\" and justifying \"deduction process\" opens up a new way to communicate human's generalization abilities to mrc model training. we begin with a small set of semi-structured explanations and compose nmteachers to augment training data. nmteachers are modularized functions where each module has a strict and softened form, enabling broader coverage from each explanation. extensive experiments on different datasets and mrc models demonstrate the efficiency of our system. having achieved encouraging results for mrc, we look forward to extending this framework to tasks such as non-fact-based qa and multi-hop reasoning. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  PROBLEM DEFINITION\n",
            "\u001b[35mSection_title: 4  PROPOSED METHOD\n",
            "\u001b[35mSection_title: 4.1  Pre-processing\n",
            "\u001b[35mSection_title: 4.2  Feature Extraction\n",
            "\u001b[35mSection_title: 4.3  Encoding each utterance in conversation thread\n",
            "\u001b[35mSection_title: 4.4  Sentence Encoder\n",
            "\u001b[35mSection_title: 4.5  Rumor Stance and Veracity Classification\n",
            "\u001b[35mSection_title: 4.6  Jointly Learning Two Tasks\n",
            "\u001b[35mSection_title: 4.7  Ensembling\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  Data & Evaluation Metric\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 5.3  Evaluation Strategy\n",
            "\u001b[35mSection_title: 5.4  Results and Discussion\n",
            "\u001b[35mSection_title: 5.5  Case Study\n",
            "\u001b[32mconclusion: in this paper we have briefly described the multi-task approach for joint prediction of rumor stance and veracity for data obtained from various social media platforms (in our case twitter and reddit). we presented ensemble of deep learning models having the same architecture, but varying the parameters. our approach outperforms the previous approaches by sufficient margin and able to generalize across different social media. in future, we can extend our model for multilingual setting [51] (ibereval is the counterpart of rumoreval 2019 for other languages [13]). moreover, we can leverage more sophisticated resources like pre-trained model which was trained specifically to handle data from different social media platforms. further, we can explore other methods like diffusion process of rumors [48] to make informed changes to model architecture.\n",
            "\u001b[35mSection_title: 6  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Task and Dataset\n",
            "\u001b[35mSection_title: 3.1  Task: Analytical Reasoning of Text\n",
            "\u001b[35mSection_title: 3.2  Dataset Collection: AR-LSAT\n",
            "\u001b[35mSection_title: 3.3  Data Analysis\n",
            "\u001b[35mSection_title: 3.4  Challenges\n",
            "\u001b[35mSection_title: 4  Approaches\n",
            "\u001b[35mSection_title: 4.1  Transformer-based Approach\n",
            "\u001b[35mSection_title: 4.2  Analytical Reasoning Machine (ARM)\n",
            "\u001b[35mSection_title: 4.2.1  Arguments Extraction\n",
            "\u001b[35mSection_title: 4.2.2  Logical Function Extraction\n",
            "\u001b[35mSection_title: 4.2.3  Legitimate Assignments Deduction\n",
            "\u001b[35mSection_title: 4.2.4  Answer Selection\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Model Comparison\n",
            "\u001b[35mSection_title: 5.2  Case Study\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[35mSection_title: 5.4  Discussion\n",
            "\u001b[32mconclusion: in this paper, we study the challenging task of analytical reasoning and introduce a dataset ar-lsat to facilitate research on analytical reasoning. we analyze the knowledge understanding and reasoning ability required for this task and present two basic approaches: a transformer-based approach and a logical-level reasoning framework, named analytical reasoning machine (arm). arm extracts symbolic knowledge, including participants, facts and rules mentioned in the context and extract logical functions from the rules. afterwards, it performs deep reasoning to find all the legitimate solutions to the problem posed and finally makes a prediction. arm sheds a light on the reasoning procedure for analytical reasoning, and each component can be further developed. experiments show that this task is very challenging for current transformer-based pre-trained language models and arm outperforms them with better performance and interpretability. further discussions are made to shed light on important future directions. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  FEW-SHOT RETRIEVAL TASK\n",
            "\u001b[35mSection_title: 2.1  RETRIEVAL TASK\n",
            "\u001b[35mSection_title: 2.2  FEW-SHOT BEIR SETTING\n",
            "\u001b[35mSection_title: 3.2  CONSISTENCY FILTERING USING ONLY GENERATED DATA\n",
            "\u001b[35mSection_title: 3.3  FEW-SHOT PROMPTAGATOR RETRIEVER\n",
            "\u001b[35mSection_title: 3.4  ZERO-SHOT PROMPTAGATOR RETRIEVER\n",
            "\u001b[35mSection_title: 3.5  DISCUSSION\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  IMPLEMENTATION\n",
            "\u001b[35mSection_title: 4.2  MAIN RESULTS\n",
            "\u001b[35mSection_title: 4.3  ABLATION STUDY\n",
            "\u001b[35mSection_title: 4.4  QUALITATIVE ANALYSIS\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: in this paper, we have presented promptagator, a novel approach to few-shot retrieval. we showed that it is possible to create task-specific, end-to-end retrievers with only a few annotated examples. the few-shot examples, amplified by prompt-based llm query generation, simplifies the complexity of training neural retrievers for a new tasks and leads to promising retrieval performance gains. it hopefully inspires future research to further push the limit of few-shot retrieval, towards generalizable retrieval systems that can seamlessly and efficiently adapt to many tasks.\n",
            "\u001b[35mSection_title: 6  CONCLUSION AND DISCUSSIONS\n",
            "\u001b[35mSection_title: 7  COMPUTE USAGE AND ENVIRONMENTAL IMPACT\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Model\n",
            "\u001b[35mSection_title: 2.1  Background: Vanilla QA model\n",
            "\u001b[35mSection_title: 2.2  Our model: DyREx\n",
            "\u001b[35mSection_title: 3  Experimental setup\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 5  Component analysis for DyREx\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we propose dyrex, a method to dynamically compute query representations to calculate the start and end positions of answer spans in extractive question answering. our approach consistently outperforms the dominant approach on a wide range of qa datasets, and the gain is even more significant in a few-shot scenario. in future work, it would be interesting to adapt dyrex for multi-span extraction tasks such as named entity recognition and keyphrase extraction.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Model\n",
            "\u001b[35mSection_title: 2.1  Background: Vanilla QA model\n",
            "\u001b[35mSection_title: 2.2  Our model: DyREx\n",
            "\u001b[35mSection_title: 3  Experimental setup\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 5  Component analysis for DyREx\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we propose dyrex, a method to dynamically compute query representations to calculate the start and end positions of answer spans in extractive question answering. our approach consistently outperforms the dominant approach on a wide range of qa datasets, and the gain is even more significant in a few-shot scenario. in future work, it would be interesting to adapt dyrex for multi-span extraction tasks such as named entity recognition and keyphrase extraction.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Data Collection through Gamification\n",
            "\u001b[35mSection_title: 2.1  The Game\n",
            "\u001b[35mSection_title: 2.2  Quality Assurance and Dataset Construction\n",
            "\u001b[35mSection_title: 3  Game Analysis\n",
            "\u001b[35mSection_title: 5  Experimental Evaluation\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Experimental Results\n",
            "\u001b[35mSection_title: 6  Model Analysis\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose gamification as a general framework for creating diverse and challenging nlu benchmarks. we use this framework to collect csqa2, a new benchmark that contains 14,343 yes/no questions. we perform a detailed analysis of csqa2, which elucidates the unique properties of our dataset, and thoroughly evaluate on a strong suite of baselines. we find that the best model, unicorn-11b, achieves an accuracy of 70.2%, dozens of points lower than human accuracy. we argue that gamification is a promising approach for creating challenge sets that expose the weaknesses of current state-of-the-art models.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Reasoning in Natural Language\n",
            "\u001b[35mSection_title: 2.2  Pre-trained Language Model\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  MERIt+\n",
            "\u001b[35mSection_title: 3.2  LLM Prompting\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Foundation Model and Evaluation Benchmark\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Overall Results\n",
            "\u001b[35mSection_title: 5.1.1  Logical Reasoning\n",
            "\u001b[35mSection_title: 5.2  Logic-enhanced Meta-training for General Language Understanding\n",
            "\u001b[35mSection_title: 5.3  Logic-enhanced Meta-training for Complex Task Understanding\n",
            "\u001b[35mSection_title: 5.4  Ablation Studies\n",
            "\u001b[35mSection_title: 5.5  Weakness of LLMs on Logical Reasoning\n",
            "\u001b[32mconclusion: in this paper, we first analysed the performance of chatgpt on logical reasoning benchmarks, as well as that using logically enhanced chain-of-thought prompting. from the results we can conclude that though language modeling can help compress all data both with and without supervision into neural models and accept human instructions, it is still really weak in perform logical reasoning. specifically, llms often fail at reaching correct deductions based on the given facts and rules, and cannot distil the logical reasoning structure from observed prompts to generalize to new problems. we hope these observations could bring some insights to future research on introducing logic prior into large language models. one step further, we combine previous data-driven approach, i.e., merit+, a selfsupervised pre-training method for logical reasoning, with flan-t5-3b, and the results have also demonstrated its effectiveness.  \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Defining the Loss Functions\n",
            "\u001b[35mSection_title: 3.3  Generating the Pseudo-labeled Training Data via Knowledge Distillation\n",
            "\u001b[35mSection_title: 3.4  Two-stage Training Procedure\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Main Experimental Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Study\n",
            "\u001b[35mSection_title: 4.5  Analysis on Passage-centric Similarity Relation\n",
            "\u001b[35mSection_title: 4.6  Analysis on Knowledge Distillation\n",
            "\u001b[32mconclusion: this paper presented a novel dense passage retrieval approach that leverages both query-centric and passage-centric similarity relations for capturing more comprehensive semantic relations. to implement our approach, we made three important technical contributions in the loss formulation, training data augmentation and effective training procedure. extensive results demonstrated the effectiveness of our approach. to our knowledge, it is the first time that passage-centric similarity relation has been considered for dense passage retrieval. we believe such an idea itself is worth exploring in designing new ranking mechanism. in future work, we will design more principle ranking functions and apply current retrieval approach to downstream tasks such as question answering and passage re-ranking.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\u001b[35mSection_title: 6  Ethical Impact\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Terminology\n",
            "\u001b[35mSection_title: 1.2  Objectives and Scope\n",
            "\u001b[35mSection_title: 1.3  Data collection methodology\n",
            "\u001b[35mSection_title: 2.1  Data\n",
            "\u001b[35mSection_title: 2.2  Models\n",
            "\u001b[35mSection_title: 3  Methods that reveal weaknesses in NLI\n",
            "\u001b[35mSection_title: 3.1  Data-investigating Methods\n",
            "\u001b[35mSection_title: 3.2  Model-investigating Methods\n",
            "\u001b[35mSection_title: 3.3  Model-improving Methods\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[32mconclusion: we present a structured survey of methods that reveal heuristics and spurious correlations in datasets, methods which show that neural models inherit those correlations or assess their capabilities otherwise, and methods that mitigate this by adversarial training, data augmentation and model architecture or training procedure improvements. various nli datasets are reported to contain spurious correlations between input and expected output, might be unsuitable to evaluate some task modality due to dataset design or suffer from quality issues. rte is a popular target task for these data-centred investigations with more than half of the surveyed papers focusing on it. nli models, in turn, are shown to exploit those correlations and to rely on superficial lexical cues. furthermore, they lack generalisation beyond the evaluation set resulting in poor performance on out-of-distribution evaluation sets, generated adversarially or targeted at a specific capability. efforts to achieve robustness include augmenting the training data with adversarial examples, making use of external resources and modifying the neural network architecture or training objective.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Text Modular Networks\n",
            "\u001b[35mSection_title: 3.1  Modeling QA Sub-Models\n",
            "\u001b[35mSection_title: 3.2  Training Decompositions via Distant Supervision\n",
            "\u001b[35mSection_title: 4  MODULARQA System\n",
            "\u001b[35mSection_title: 4.1  QA Sub-Models, A\n",
            "\u001b[35mSection_title: 4.2  Sub-task Question Models, G\n",
            "\u001b[35mSection_title: 4.3  Generating Training Decompositions\n",
            "\u001b[35mSection_title: 3.  Complementation (What percent is not X?):\n",
            "\u001b[35mSection_title: 4.4  Inference\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Explanation and Interpretability\n",
            "\u001b[35mSection_title: 5.2  Interpretable Cross-Dataset Models\n",
            "\u001b[35mSection_title: 5.3  Comparison to Dataset-Specific Models\n",
            "\u001b[35mSection_title: 5.4  Comparison to Black-Box Models\n",
            "\u001b[35mSection_title: 5.5  Additional Benefits of TMNs\n",
            "\u001b[32mconclusion: we introduced text modular networks, which provide a general-purpose framework that casts complex tasks as textual interaction between existing, simpler qa modules. based on this conceptual framework, we built modularqa, an instantiation of tmns that can perform multi-hop and discrete numeric reasoning. empirically, modu-larqa is on-par with other modular approaches (which are dataset-specific) and outperforms a stateof-the-art model in a limited data setting and on expert-generated perturbations. importantly, mod-ularqa provides easy-to-interpret explanations of its reasoning. it is the first system that decomposes drop questions into textual sub-questions and can be applied to both drop and hotpotqa.\n",
            "\u001b[35mSection_title: 6  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  SPARTQA-HUMAN\n",
            "\u001b[35mSection_title: 4.1  Corpus Statistics\n",
            "\u001b[35mSection_title: 5.1  Training and Inference\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.2  SPARTQA is challenging\n",
            "\u001b[35mSection_title: 6.3  Extrinsic evaluation\n",
            "\u001b[32mconclusion: spatial reasoning is an important problem in natural language understanding. we propose the first human-created qa benchmark on spatial reasoning, and experiments show that state-of-the-art pretrained language models (lm) do not have the capability to solve this task given limited training data, while humans can solve those spatial reasoning questions reliably. to improve lms' capability on this task, we propose to use hand-crafted grammar and spatial reasoning rules to automatically generate a large corpus of spatial descriptions and corresponding question-answer annotations; further pretraining lms on this distant supervision dataset significantly enhances their spatial language understanding and reasoning. we also show that a spatially-improved lm can have better results on two extrinsic datasets (babi and boolq).  table 7 shows the templates used to create questions in spartqa-auto. the \"<object>\" is a variable replaced by objects from the story (using choose-objects and describe-objects modules), and the \"<relation>\" variable can be replaced by the chosen relations between objects (using findall-relations module). the articles and the indefinite pronouns in each template play an essential role in understanding the question's objective. for example, \"are all blue circles near to a triangle?\" is different from \"are there any blue circles near to a triangle?\", and \"are there any blue circles near to all triangles?\". therefore, we check the uniqueness of the object definition, using \"a\" or \"the\" in proper places and randomly place the terms \"any\" or \"all\" in the yn questions to generate different questions. table 8 shows the percentage of correct labels in train and test sets. in multi-choice q-types, more than one label can be true. table 10 shows some generated sentences in spartqa-auto with some specific features that challenge models to understand different forms of relation description in spatial language.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Transformer-based IR models: Cross-encoders and Dual-encoders\n",
            "\u001b[35mSection_title: 2.2  Score-based distillation for IR models\n",
            "\u001b[35mSection_title: 4  Embedding-matching based distillation\n",
            "\u001b[35mSection_title: 4.1  DE to DE distillation\n",
            "\u001b[35mSection_title: 4.2  CE to DE distillation\n",
            "\u001b[35mSection_title: 4.3  Task-specific online data generation\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Setup\n",
            "\u001b[35mSection_title: 6  Related work\n",
            "\u001b[32mconclusion: we propose embeddistill -a novel distillation method for ir that goes beyond simple score matching. en route, we provide a theoretical understanding of the teacher-student generalization gap in an ir setting which not only motivated embeddistill but also inspired new design choices for the student de models: (a) reusing the teacher's document encoder in the student and (b) aligning query embeddings of the teacher and student. this simple approach delivers consistent quality and computational gains in practical deployments and we demonstrate them on msmarco, nq, and beir benchmarks. finally, we found embeddistill retains 95-97% of the teacher performance to with 1/10th size students.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Whole Genotype-Fitness Map\n",
            "\u001b[35mSection_title: 3.  Lessons from Aptamers\n",
            "\u001b[35mSection_title: 3.  Lessons from Ribozymes\n",
            "\u001b[35mSection_title: 4.  Lessons from Ribozymes\n",
            "\u001b[35mSection_title: 5.  Double Mutations and the Question of Epistasis\n",
            "\u001b[35mSection_title: 6.  Far from the Wild-Type Region of the Landscape\n",
            "\u001b[35mSection_title: 7.  Summary\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and related work\n",
            "\u001b[35mSection_title: 3  Baleen\n",
            "\u001b[35mSection_title: 3.1  FLIPR: focused late interaction\n",
            "\u001b[35mSection_title: 3.2  Supervision: latent hop ordering\n",
            "\u001b[35mSection_title: 3.3  Condenser: per-hop fact extraction\n",
            "\u001b[35mSection_title: 3.4  Reader: task-specific processing\n",
            "\u001b[35mSection_title: 4  Diagnosing retrieval saturation on HotPotQA\n",
            "\u001b[35mSection_title: 5  Main evaluation on HoVer\n",
            "\u001b[35mSection_title: 5.1  Task description\n",
            "\u001b[35mSection_title: 5.2  Evaluating retrieval effectiveness\n",
            "\u001b[35mSection_title: 5.3  Evaluating end-to-end effectiveness\n",
            "\u001b[35mSection_title: 5.4  Ablation studies\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  AI Trustworthiness: Beyond Predictive Accuracy\n",
            "\u001b[35mSection_title: 2.1  Robustness\n",
            "\u001b[35mSection_title: 2.2  Generalization\n",
            "\u001b[35mSection_title: 2.3  Explainability and Transparency\n",
            "\u001b[35mSection_title: 2.3.1  Explainability\n",
            "\u001b[35mSection_title: 2.3.2  Transparency\n",
            "\u001b[35mSection_title: 2.4  Reproducibility\n",
            "\u001b[35mSection_title: 2.5  Fairness\n",
            "\u001b[35mSection_title: 2.6  Privacy protection\n",
            "\u001b[35mSection_title: 2.7  Accountability: A Holistic Evaluation throughout the Above Requirements\n",
            "\u001b[35mSection_title: 3  Trustworthy AI: A Systematic Approach\n",
            "\u001b[35mSection_title: 3.1  Data Preparation\n",
            "\u001b[35mSection_title: 3.1.1  Data Collection\n",
            "\u001b[35mSection_title: 3.1.2  Data Preprocessing\n",
            "\u001b[35mSection_title: 3.2  Algorithm Design\n",
            "\u001b[35mSection_title: 3.2.1  Adversarial Robustness\n",
            "\u001b[35mSection_title: 3.2.2  Model Generalization\n",
            "\u001b[35mSection_title: 3.2.3  Explainable ML\n",
            "\u001b[35mSection_title: 3.2.4  Algorithmic Fairness\n",
            "\u001b[35mSection_title: 3.2.5  Privacy Computing\n",
            "\u001b[35mSection_title: 3.3  Development\n",
            "\u001b[35mSection_title: 3.3.1  Functional Testing\n",
            "\u001b[35mSection_title: 3.3.2  Performance Benchmarking\n",
            "\u001b[35mSection_title: 3.3.3  Development by Simulation\n",
            "\u001b[35mSection_title: 3.4  Deployment\n",
            "\u001b[35mSection_title: 3.4.1  Anomaly Monitoring\n",
            "\u001b[35mSection_title: 3.4.2  Human-AI Interaction\n",
            "\u001b[35mSection_title: 3.4.3  Fail-Safe Mechanisms\n",
            "\u001b[35mSection_title: 3.5  Management\n",
            "\u001b[35mSection_title: 3.5.1  Documentation\n",
            "\u001b[35mSection_title: 3.5.2  Auditing\n",
            "\u001b[35mSection_title: 3.5.3  Cooperation and Information Sharing As shown in\n",
            "\u001b[35mSection_title: 3.6  TrustAIOps: A Continuous Workflow toward Trustworthiness\n",
            "\u001b[32mconclusion: in this survey, we outlined the key aspects of trustworthiness that we think are essential to ai systems. we introduced how ai systems can be evaluated and assessed on each of these aspects, and reviewed current efforts in this direction in the industry. we further proposed a systematic approach to consider these aspects of trustworthiness in the entire lifecycle of real-world ai systems, which offers recommendations for every step of the development and use of these systems. we recognize that fully adopting this systematic approach to build trustworthy ai systems requires that practitioners embrace the concepts underlying the key aspects that we have identified. more importantly, it requires a shift of focus from performance-driven ai to trust-driven ai. in the short run, this shift will inevitably involve side-effects, such as longer learning time, slowed development, and/or increased cost to build ai systems. however, we encourage practitioners to focus on the long-term benefits of gaining the trust of all stakeholders for the sustained use and development of these systems. in this section, we conclude by discussing some of the open challenges and potential opportunities in the future development of trustworthy ai.\n",
            "\u001b[35mSection_title: 4  Conclusion, Challenges and Opportunities\n",
            "\u001b[35mSection_title: 4.1  AI Trustworthiness as Long-Term Research\n",
            "\u001b[32mlimitation: repeatable and quantitative measurements are the cornerstone of scientific and engineering progress. however, despite increasing research interest and efforts, the quantification of many aspects of ai trustworthiness remains elusive. of the various aspects that we have discussed in this paper, the explainability, transparency, and accountability of ai systems are still seldom evaluated quantitatively, which makes it difficult to accurately compare systems. developing good methods of quantitative evaluation for these desiderata, we believe, will be an important first step in research on these aspects of ai trustworthiness as a scientific endeavor, rather than a purely philosophical one.\n",
            "\u001b[35mSection_title: 4.1.3  Limitations in Current Evaluations of Trustworthiness.\n",
            "\u001b[35mSection_title: 4.1.4  Challenges and\n",
            "\u001b[35mSection_title: 4.2  End-User Awareness of the Importance of AI Trustworthiness\n",
            "\u001b[35mSection_title: 4.3  Inter-disciplinary and International Cooperation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Post-Abstention\n",
            "\u001b[35mSection_title: 2.1  Background\n",
            "\u001b[35mSection_title: 2.2  Task Formulation\n",
            "\u001b[35mSection_title: 2.3  Approaches\n",
            "\u001b[35mSection_title: 2.3.1  Ensembling using Question Paraphrases\n",
            "\u001b[35mSection_title: 2.3.2  Re-Examining Top N Predictions (REToP)\n",
            "\u001b[35mSection_title: 2.3.3  Human Intervention (HI)\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 3.1  Experimental\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 3.2.1  REToP\n",
            "\u001b[35mSection_title: 3.2.2  Comparing Post-Abstention Approaches\n",
            "\u001b[35mSection_title: 3.2.3  Human Intervention (HI)\n",
            "\u001b[35mSection_title: 3.2.4  Ensembling Using Paraphrases\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[32mconclusion: in this work, we formulated 'post-abstention', a task that allows re-attempting the abstained instances of the given selective prediction system with the aim of increasing its coverage without significantly sacrificing the accuracy. we also explored several baseline methods for this task. through comprehensive experiments on 11 qa datasets, we showed that these methods lead to considerable performance improvements in both in-domain and out-of-domain settings. we further performed a thorough analysis that resulted in several interesting findings.\n",
            "\u001b[35mSection_title: 5  Conclusion and Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 3  LEXMAE: LEXICON-BOTTLENECKED MASKED AUTOENCODER\n",
            "\u001b[35mSection_title: 3.1  LANGUAGE MODELING ENCODER\n",
            "\u001b[35mSection_title: 3.2  LEXICON-BOTTLENECKED MODULE\n",
            "\u001b[35mSection_title: 3.3  WEAKENED MASKING-STYLE DECODER\n",
            "\u001b[35mSection_title: 3.4  PRE-TRAINING OBJECTIVE & FINE-TUNING FOR LEXICON-WEIGHTING RETRIEVER\n",
            "\u001b[35mSection_title: 4  EXPERIMENT\n",
            "\u001b[35mSection_title: 4.1  MAIN EVALUATION\n",
            "\u001b[35mSection_title: 4.2  EFFICIENCY ANALYSIS AND COMPARISON\n",
            "\u001b[35mSection_title: 4.3  FURTHER ANALYSIS\n",
            "\u001b[35mSection_title: 4.4  MODEL CHOICES & ABLATION STUDY\n",
            "\u001b[32mconclusion: in this work, we propose to improve the lexicon-weighing retrieval by pre-training a lexiconbottlenecked masked autoencoder (lexmae) which alleviates the objective mismatch between the masked language modeling encoders and relevance-oriented lexicon importance. after pretraining lexmae on large-scale collections, we first observe great zero-shot performance. then after fine-tuning the lexmae on the large-scale retrieval benchmark, we obtain state-of-the-art retrieval quality with very high efficiency and also deliver state-of-the-art zero-shot transfer performance on beir benchmark. further detailed analyses on the efficacy-efficiency trade-off in terms of retrieval latency and storage memory also verify the superiority of our fine-tuned lexmae.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Tutorial Overview\n",
            "\u001b[35mSection_title: 2  Problem Introduction\n",
            "\u001b[35mSection_title: 4  Instructor information\n",
            "\u001b[35mSection_title: 1  Tutorial Overview\n",
            "\u001b[35mSection_title: 2  Problem Introduction\n",
            "bottom\n",
            "\u001b[35mSection_title: 4  Instructor information\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 3.1.  A revisão.\n",
            "\u001b[35mSection_title: 4.2.  1ª etapa: Ferramentas para alinhamento e análise manual\n",
            "\u001b[35mSection_title: 4.3.1.  O alinhamento com o programa ParaConc\n",
            "\u001b[35mSection_title: 4.3.1.1.  Passo a passo para o alinhamento\n",
            "\u001b[35mSection_title: 4.3.1.2.  Extração de concordâncias\n",
            "\u001b[35mSection_title: 4.4.  Confecção de concordâncias expandidas para os 15 textos\n",
            "\u001b[35mSection_title: 6.1.  O que diz a teoria\n",
            "\u001b[35mSection_title: 6.2.  O que mostra a Lingüística de Corpus\n",
            "\u001b[35mSection_title: 5.  theoretically (1) -em teoria\n",
            "\u001b[35mSection_title: 6.2.1.4  A possibilidade de se utilizar a fraseologia todos/todas + os(as)+ substantivo\n",
            "\u001b[35mSection_title: 6.2.2.  Quanto à tradução de advérbio por adjetivos ou uma unidade fraseológica que\n",
            "\u001b[35mSection_title: 9.  critically matter(1) -ter uma importância crítica (1)\n",
            "\u001b[35mSection_title: 12.  utterly catastrophic (1) -(uma catástrofe) total (1)\n",
            "\u001b[35mSection_title: 17.  ultimately (4) -em última análise (1)\n",
            "\u001b[35mSection_title: 2.  disproportionately (1) -(em quantidades) desproporcionais\n",
            "\u001b[35mSection_title: 1.  easily (8) (extracted)-(de) fácil extração (1)\n",
            "\u001b[35mSection_title: 4.  uncanningly (attracted)(1)-(com sua) estranha atração\n",
            "\u001b[35mSection_title: 5.  (drag) heavily -(dar) tragada profunda\n",
            "\u001b[35mSection_title: 6.2.2.6.  Temos os advérbios que modificam adjetivos em inglês e que são traduzidos por um substantivo qualificado pelo adjetivo\n",
            "\u001b[35mSection_title: 6.2.4.1.  Há algumas soluções que parecem se inserir em fraseologia negativa que contém o advérbio bem e ao certo\n",
            "\u001b[35mSection_title: 6.2.4.3.  Fraseologia hardly…without resultando em quase não….sem\n",
            "\u001b[35mSection_title: 2.  widely (vary)(1) -(variam) muito\n",
            "\u001b[35mSection_title: 6.2.4.7.  Advérbios terminados em *ly cuja tradução em *mente foi mantida\n",
            "\u001b[35mSection_title: 6.2.6.  Exemplos que ilustram soluções inusitadas.\n",
            "\u001b[35mSection_title: 6.4.  Contribuição dos resultados para a tradução para a língua não materna\n",
            "\u001b[35mSection_title: 6.5.  Estudo de caso: \"absolutely\" e \"absolutamente\"\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  ReWOO with Plan-Work-Solve Paradigm\n",
            "\u001b[35mSection_title: 2.2  Prompt Redundancy Reduction\n",
            "\u001b[35mSection_title: 2.3  Parameter Efficiency by Specialization\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Setups\n",
            "\u001b[35mSection_title: 3.2  Results and Observations\n",
            "\u001b[35mSection_title: 3.2.1  Comparison between Prompting Paradigms\n",
            "\u001b[35mSection_title: 3.3  Fine-tuning and Specialization of LLM\n",
            "\u001b[32mlimitation: we notice that for certain tasks where little context about the environment is available, fully relying on foreseeable reasoning becomes impractical. consider the following task from alfworld [27]:you are in the middle of a room. looking quickly around you, you see a drawer 2, a shelf 5, a drawer 1, a shelf 4, a sidetable 1, a drawer 5, a shelf 6, a shelf 1, a shelf 9, a cabinet 2, a sofa 1, a cabinet 1, a shelf 3, a cabinet 3, a drawer 3, a shelf 11, a shelf 2, a shelf 10, a dresser 1, a shelf 12, a garbagecan 1, a armchair 1, a cabinet 4, a shelf 7, a shelf 8, a safe 1, and a drawer 4. your task is to: put some vase in safe.since a planner has no prior knowledge about the environment, it has to enumerate all possible plans that can potentially lead to some vase. the number of reasoning steps of planner in such tasks is equivalent to the worst-case complexity of observation-dependent reasoning.the example above implies that a robust alm system should not be built on a singleton -it looks promising to wire different nodes of llms, tools, and sub-models into a directed acyclic graph (dag) so that each node functions for its predesignated tasks organically. 4 directions to further improve the efficiency and performance of such alm systems include (1) offloading specialized abilities from foundation llms into smaller models. section 3.3 demonstrate the possibility for small lms specializing [14] in general foreseeable reasoning. we expect that with a greater number of open domain instructions, foreseeable reasoning can be even more holistically offloaded. other parametric nodes in the dag, such as a solver, can be fine-tuned alike. (2) tool representation learning. in many cases from hotpotqa, wikipedia and google can both lead to the correct answer, indicating a certain level of similarity between those tools. we can set up a model to minimize the energy among similar-functioning workers. tool representations allow us to parametrize the whole alm system and therefore enabling end-to-end training. (3) graph optimization. furthermore, we should be able to optimize dag execution through multiple graph and concurrency algorithms.\n",
            "\u001b[35mSection_title: 4  Limitations and Future Work\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we present rewoo , a modular alm framework to solve multi-step reasoning tasks efficiently by decoupling reasoning from tool feedback and observations. theoretical decomposition of prompt tokens establishes that rewoo is able to substantially reduce prompting redundancy in prevailing thought-action-observation alm systems. comprehensive experiments on both public nlp benchmarks and curated tasks reveal superior performance of rewoo in achieving boosted performance with much less token consumption. a side study also shows that rewoo has relatively robust performance under tool-failure cases. our study further unveils the potential for generic reasoning offloading via instruction tuning and specialization. future improvements beyond rewoo based alm systems involve modular llm fine-tuning, tool representation learning, and system graph learning and optimization. we demonstrate that our work lays a solid foundation for these advancements, inching us closer to truly scalable agi.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Background\n",
            "\u001b[35mSection_title: 4  Method\n",
            "\u001b[35mSection_title: 4.1  Hierarchical Representation Learning\n",
            "\u001b[35mSection_title: 4.2  Dynamic Reasoning Mechanism\n",
            "\u001b[35mSection_title: 4.3  Classifier\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 5.3  Comparison with State-of-the-Arts\n",
            "\u001b[35mSection_title: 5.4  Ablation Study\n",
            "\u001b[32mconclusion: this paper proposes hdrn, a novel model for representation learning and reasoning for biomedical question answering. first, we construct hierarchical representations to obtain a deep understanding of the biomedical evidences. then, we perform multi-step dynamic reasoning to solve complex biomedical questions. we evaluate our model on three bioqa datasets and achieve new state-of-theart performances.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  PRELIMINARY\n",
            "\u001b[35mSection_title: 4  APPROACH\n",
            "\u001b[35mSection_title: 4.1  BOTTLENECKED MULTI-DECODER ARCHITECTURE\n",
            "\u001b[35mSection_title: 4.2  MULTI-TASK PRE-TRAINING\n",
            "\u001b[35mSection_title: 4.3  LEARNING\n",
            "\u001b[35mSection_title: 5  EXPERIMENT\n",
            "\u001b[35mSection_title: 5.1  EXPERIMENTAL SETTING\n",
            "\u001b[35mSection_title: 5.2  MAIN RESULTS\n",
            "\u001b[35mSection_title: 5.3  FURTHER ANALYSIS\n",
            "\u001b[32mconclusion: in this paper, we proposed master, a multi-task pre-trained bottlenecked masked autoencoder for dense retrieval task. in our approach, we adopted a bottlenecked multi-decoder architecture to integrate a variety of pre-training tasks, and devised three types of pre-training tasks about corrupted passages recovering, related passage recovering and plms outputs recovering. the three types of tasks focused on compressing the semantic information within the passages, modeling relations among passages, and learning the knowledge from external public generative plms, respectively, leading to more informative and effective dense vectors. experimental results have shown that our approach outperforms several competitive baselines.  we compare our approach with a variety of methods: of bert on these nlu tasks. it indicates that our multi-task pre-training can also enrich the useful knowledge about nlu tasks for the plm.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3.  Meta-Reason\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Generating Reasoning Chains\n",
            "\u001b[35mSection_title: 3.2  Reasoning over Reasoning Chains\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setting\n",
            "\u001b[35mSection_title: 4.1.1  Datasets\n",
            "\u001b[35mSection_title: 4.1.2  Models\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Open-source Models\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[32mconclusion: this work introduces mcr for meta-reasoning over multiple reasoning chains.we evaluate mcr on 7 datasets for multi-hop qa that require both implicit and explicit reasoning in an open-domain setting and show that it outperforms previous approaches on all evaluation benchmarks.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  AutoPlan\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  AutoPlan\n",
            "\u001b[35mSection_title: 3.2.1  Experience Collection\n",
            "\u001b[35mSection_title: 3.2.2  SIR Reflection\n",
            "\u001b[35mSection_title: 3.2.3  Plan Update\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Data\n",
            "\u001b[35mSection_title: 4.2  Method Configurations\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[32mconclusion: we propose autoplan, a prompt-based method, to enable llm to solve interactive decision-making tasks without gradient computation or in-context demonstrations.autoplan conditions llm on an additional task plan described in natural language, which is obtained through an iterative three-stage process.experiments show that autoplan achieves better results than baselines and is also efficient during inference.the ablation study further confirms the effectiveness of batching and explicit reflection\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.2  TASK-ORIENTED QUESTION GENERATION\n",
            "\u001b[35mSection_title: 2.3  RELATED TASKS\n",
            "\u001b[35mSection_title: 3  METHODS\n",
            "\u001b[35mSection_title: 3.1  PROBLEM DESCRIPTION\n",
            "\u001b[35mSection_title: 3.2  PROBLEM DEFINITION\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  DATASET\n",
            "\u001b[35mSection_title: 4.2  IMPLEMENTATION DETAILS\n",
            "\u001b[35mSection_title: 5  RESULTS AND DISCUSSION\n",
            "\u001b[35mSection_title: 5.1  BASELINE PERFORMANCE\n",
            "\u001b[35mSection_title: 5.2  COMPARISON TO HUMAN PERFORMANCE\n",
            "\u001b[35mSection_title: 5.3  PRIMARY MODEL ABLATION\n",
            "\u001b[35mSection_title: 5.4  ORACLE ABLATION\n",
            "\u001b[35mSection_title: 5.5  FAILURE MODES\n",
            "\u001b[32mconclusion: in this paper, we presented a novel framework for evaluating task-oriented questions and observed that state-of-the-art zero-shot llms struggle at this task compared to humans.we attribute this deficiency to a lack of training data and evaluation processes for toa.to overcome these challenges, we introduced fact-level masking and flm-hotpotqa, a self-supervised toa dataset, and an associated evaluation pipeline.to conclude, our contributions highlight the challenges faced by state-of-the-art, zero-shot llms, address limitations in training data and evaluation methods for task-oriented asking, and pave the way for future toa development.\n",
            "\u001b[35mSection_title: 6  CONCLUSION, LIMITATIONS AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  General Framework\n",
            "\u001b[35mSection_title: 3.2  Semantic Graph Construction\n",
            "\u001b[35mSection_title: 3.3  Semantic-Enriched Document Representations\n",
            "\u001b[35mSection_title: 3.4  Joint Task Question Generation\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Data and Metrics\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Comparison with Baseline Models\n",
            "\u001b[35mSection_title: 4.4  Ablation Study\n",
            "\u001b[35mSection_title: 4.5  Human Evaluation\n",
            "\u001b[35mSection_title: 4.6  Error Analysis\n",
            "\u001b[35mSection_title: 4.7  Analysis of Content Selection\n",
            "\u001b[32mconclusion: we propose the problem of dqg to generate questions that requires reasoning over multiple disjoint pieces of information. to this end, we propose a novel framework which incorporates semantic graphs to enhance the input document representations and generate questions by jointly training with the task of content selection. experiments on the hotpotqa dataset demonstrate that introducing semantic graph significantly reduces the semantic errors, and content selection benefits the selection and reasoning over disjoint relevant contents, leading to questions with better quality.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Works\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.2  EXISTING EVALUATION BENCHMARKS FOR LLMS\n",
            "\u001b[35mSection_title: 3  M 4 LE\n",
            "\u001b[35mSection_title: 3.1  DESIGN PRINCIPLE\n",
            "\u001b[35mSection_title: 3.2  DATA COLLECTION\n",
            "\u001b[35mSection_title: 3.3  TASK CONSTRUCTION\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  MODELS\n",
            "\u001b[35mSection_title: 4.2  INFERENCE DETAILS\n",
            "\u001b[35mSection_title: 4.3  RESULTS\n",
            "\u001b[35mSection_title: 4.3.1  ABLATION STUDY\n",
            "\u001b[32mconclusion: in this paper, we propose a benchmark m 4 le for llms assessing their capability of long-context understanding.to establish a benchmark with diverse nlp tasks, rather than just those that are inherently lengthy, we propose a systematic method to convert short nlp task instances into long context inputs, encompassing five distinct abilities.we collect and construct in total of 36 tasks from different sources and domains covering multiple length ranges to maximize the diversity of the tasks in benchmark, with our customized construction methods which enable flexibility to extend arbitrary context lengths.we evaluate 11 well-known llms with our benchmark and find that current models struggle to understand long-context inputs and the corresponding performance related to ability types, data used when fine-tuning and positions of the relevant information.a.1.1 mnds news mnds news (petukhova & fachada, 2023) is an english hierarchical news category classification dataset comprising 10,917 news articles from 260 sources.we only use the 17 first-level categories as the labels for this study.for multiple retrieval tasks, we randomly sample a class label that appears in the instance.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Timestamp Embedding\n",
            "\u001b[35mSection_title: 3.3  Status classification\n",
            "\u001b[35mSection_title: 3.4  Span prediction\n",
            "\u001b[35mSection_title: 3.5  Training\n",
            "\u001b[35mSection_title: 3.6  Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Evaluation\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[32mconclusion: we proposed the time-stamped language model (tslm model), a novel approach based on a simple and effective idea, which enables pre-trained qa models to process procedural texts and produce different outputs based on each step to track entities and their changes. tslm utilizes a timestamp function that causes the attention modules in the transformer-based lm architecture to incorporate past, current, and future information by computing a timestamp embedding for each input token. our experiments show a 3.1% improvement on the f1 score and a 10.4% improvement over the recall metric on propara dataset. our model further outperforms the state-of-the-art models with a 1.55% margin in the npn-cooking dataset accuracy for the location prediction task.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Overview and Background\n",
            "\u001b[35mSection_title: 3  The Text2graph Pre-training Strategy\n",
            "\u001b[35mSection_title: 4  The Construction of Synthetic Corpus\n",
            "\u001b[35mSection_title: 4.1  The Synthesizing of the Graph\n",
            "\u001b[35mSection_title: 4.2  The Construction of the queries\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets and metrics\n",
            "\u001b[35mSection_title: 5.2  Generative Baseline\n",
            "\u001b[35mSection_title: 5.3  Fine-tuning on Downstream Datasets\n",
            "\u001b[35mSection_title: 5.4  Experimental Setup\n",
            "\u001b[35mSection_title: 6.1  Results on ExplaGraphs\n",
            "\u001b[35mSection_title: 6.2  Other Analysis\n",
            "\u001b[35mSection_title: 6.3  Results on other reasoning datasets\n",
            "\u001b[35mSection_title: 7.1  Explanation generation\n",
            "\u001b[35mSection_title: 7.2  Structured content generation from language models\n",
            "\u001b[32mconclusion: in this paper, we propose a pre-training framework eg 3 p for a structured explanation generation task. distinct from existing pre-training tasks based on natural language text, eg 3 p focuses more on training mapping between natural language and graphs. meanwhile, due to the high cost of manually tagging, we construct queries from the synthetic graph automatically to get a large-scale corpus to support the pre-training process. using explagraph as a main benchmark, experimental results show that eg 3 p could significantly improve the ability of the model to generate explanations. in addition, on the other dataset, the results of the model after pre-training also showed a considerable improvement. our approach offers a new possibility for addressing the challenges of limited labeled data in natural language processing tasks. in the future, the ability of the model to generate explanation graphs will benefit from more datasets released with labels and more and more objective evaluation indicators put forward. additionally, while our current approach processes graphs as strings, utilizing a model architecture that is more suitable for graph generation may further enhance the model's graph generation ability.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  MOBILITY QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 3  DATASET DESIGN 3.1 Mobility Data Processing\n",
            "\u001b[35mSection_title: 3.2  MobQA Questions\n",
            "\u001b[35mSection_title: 3.2.2  Question\n",
            "\u001b[35mSection_title: 3.2.3  Question\n",
            "\u001b[35mSection_title: 3.3  Ground Truth Answer Acquisition\n",
            "\u001b[35mSection_title: 3.4  Evaluation Protocol\n",
            "\u001b[35mSection_title: 4  MODEL DESIGN FOR MOBQA\n",
            "\u001b[32mconclusion: in this paper, we present a novel human mobility question answering task which aims at better understanding mobility behaviours.associated with this mobqa task, we discuss a blueprint on how to build a suitable mobqa dataset.additionally, a plausible model design tailored for the mobqa task is also suggested.broader impact.we hope this study will offer fresh concepts and insights for human mobility research.besides the human mobility domain, the mobqa task would also boost the corresponding research of language processing and question answering systems.for example, a new research topic could be how to develop language models suitable for sequential behaviour data.further, the introduced mobqa task could emerge new applications such as mobility chatbots for social good.future work.as the first attempt to explore the question answering of human mobility data, this paper constitutes an initial step in establishing the dataset for the mobqa task.we intend to expand the question templates to include diverse questions in the dataset.after finalising the mobqa dataset, we will focus on developing deep learning models for the mobqa task based on the design introduced in this paper and evaluating our solution.\n",
            "\u001b[35mSection_title: 5  DISCUSSION AND CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Method\n",
            "\u001b[35mSection_title: 2.1  Plant Materials and Growth Conditions\n",
            "\u001b[35mSection_title: 2.2  Characterization of Gene and cDNA Structures for Watermelon APXs\n",
            "\u001b[35mSection_title: 2.3  Measurement of Plant-Water Relations\n",
            "\u001b[35mSection_title: 2.4  Measurements of Photosynthetic Parameters\n",
            "\u001b[35mSection_title: 2.5  APX Enzyme Assay\n",
            "\u001b[35mSection_title: 2.6  Quantification of APX mRNA Expression\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\u001b[35mSection_title: 3.1  Structures of the Putative APX Genes in the Watermelon Genome\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Demonstration of the MPR\n",
            "\u001b[35mSection_title: 3.  Overall scheme of MPR\n",
            "\u001b[35mSection_title: 4.  Extent of polymerization in MPR\n",
            "\u001b[35mSection_title: 5.  Molecularity of the initiation stage\n",
            "\u001b[35mSection_title: 6.  Kinetics and thermodynamics of initiation\n",
            "\u001b[35mSection_title: 7.  Kinetics of amplification\n",
            "\u001b[35mSection_title: 8.  Thermodynamics of transition from amplification to propagation\n",
            "\u001b[35mSection_title: 9.  Kinetics of propagation\n",
            "\u001b[35mSection_title: 10.  MPR as an evolutionary process\n",
            "\u001b[35mSection_title: 11.  MPR and in vivo expansion\n",
            "\u001b[35mSection_title: 12.  Concluding remarks\n",
            "\u001b[35mSection_title: 13.  Acknowledgments\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Reflexion: reinforcement via verbal reflection\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Sequential decision making: ALFWorld\n",
            "\u001b[35mSection_title: 4.2  Reasoning: HotpotQA\n",
            "\u001b[35mSection_title: 4.3  Programming\n",
            "\u001b[32mlimitation: at its core, reflexion is an optimization technique that uses natural language to do policy optimization.policy optimization is a powerful approach to improve action choice through experience, but it may still succumb to non-optimal local minima solutions.in this study, we limit long-term memory to a sliding window with maximum capacity, but we encourage future work to extend the memory component of reflexion with more advanced structures such as vector embedding databases or traditional sql databases.specific to code generation, there are many practical limitations to testdriven development in specifying accurate input-output mappings such as non-deterministic generator functions, impure functions that interact with apis, functions that vary output according to hardware specifications, or functions that invoke parallel or concurrent behavior that may be difficult to predict.\n",
            "\u001b[35mSection_title: 5  Limitations\n",
            "\u001b[35mSection_title: 6  Broader impact\n",
            "\u001b[32mconclusion: in this work, we present reflexion, an approach that leverages verbal reinforcement to teach agents to learn from past mistakes.we empirically show that reflexion agents significantly outperform currently widely-used decision-making approaches by utilizing self-reflection.in future work, reflexion could be used to employ more advanced techniques that have been thoroughly studied in traditional rl settings, such as value learning in natural language or off-policy exploration techniques.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 8  Reproducibility\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  DATASET GENERATION\n",
            "\u001b[35mSection_title: 2.1  Wikipedia tables as anchors\n",
            "\u001b[35mSection_title: 2.3  Generating Single-Modality Questions\n",
            "\u001b[35mSection_title: 2.4  Generating multimodal complex questions\n",
            "\u001b[35mSection_title: 1.  TABLEQ: Returns a question from the table questions generated in §2.3, as well as a list of\n",
            "\u001b[35mSection_title: 3  DATASET ANALYSIS\n",
            "\u001b[35mSection_title: 4  MODELS\n",
            "\u001b[35mSection_title: 4.1  SINGLE-MODALITY QA MODULES\n",
            "\u001b[35mSection_title: 4.2  MULTIMODALITY QA MODELS\n",
            "\u001b[35mSection_title: 4.3  TRAINING AND SUPERVISION\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[32mconclusion: we present mmqa, a new qa dataset that contains 29,918 examples, 35.7% of which require crossmodality reasoning. we describe a novel framework for generating complex multimodal questions at scale, and showcase the diversity and multimodal properties of the resulting dataset. we evaluate mmqa using a variety of models, and confirm that the best model exploits the multimodality of the dataset and takes into account multi-hop reasoning via implicit decomposition. however, human performance substantially exceeds the best model, establishing the need for further research involving multiple modalities in question answering systems, which we hope that our work will drive.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  SGPT Cross-Encoder\n",
            "\u001b[35mSection_title: 3.1  Asymmetric Search\n",
            "\u001b[35mSection_title: 3.1.1  Method\n",
            "\u001b[35mSection_title: 3.1.2  Results\n",
            "\u001b[35mSection_title: 3.2  Symmetric Search\n",
            "\u001b[35mSection_title: 4.1.2  Results\n",
            "\u001b[35mSection_title: 4.2  Asymmetric Search\n",
            "\u001b[35mSection_title: 4.2.1  Method\n",
            "\u001b[35mSection_title: 4.2.2  Results\n",
            "\u001b[32mconclusion: this work presented sgpt. building on sbert, we proposed modifications to gpt models to use them as cross-or bi-encoders for semantic search.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Experimental Setting\n",
            "\u001b[35mSection_title: 4  How does the Size of a Dataset Impact Performance and Generalization?\n",
            "\u001b[35mSection_title: 5  How Does a Stratified Annotation Help\n",
            "\u001b[35mSection_title: 7  Case Study: Clinical QA\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Reading Comprehension in English\n",
            "\u001b[35mSection_title: 2.2  Reading Comprehension in other languages\n",
            "\u001b[35mSection_title: 2.3  Language modeling for Reading Comprehension\n",
            "\u001b[35mSection_title: 3  Dataset Collection\n",
            "\u001b[35mSection_title: 3.1  Paragraphs collection\n",
            "\u001b[35mSection_title: 3.2  Question and answer pairs collection\n",
            "\u001b[35mSection_title: 3.3  Additional answers collection\n",
            "\u001b[35mSection_title: 3.4  FQuAD1.0 & FQuAD 1.1\n",
            "\u001b[35mSection_title: 4.2  Question analysis\n",
            "\u001b[35mSection_title: 4.3  Question-answer differences\n",
            "\u001b[35mSection_title: 4.4  Evaluation metrics\n",
            "\u001b[35mSection_title: 4.5  Human performance\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental set-up\n",
            "\u001b[35mSection_title: 5.2  Native French Reading Comprehension\n",
            "\u001b[35mSection_title: 5.3  Cross-lingual Reading Comprehension\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Native French Reading Comprehension\n",
            "\u001b[35mSection_title: 7.2  Translated Reading Comprehension\n",
            "\u001b[35mSection_title: 7.3  Cross-lingual Reading Comprehension\n",
            "\u001b[32mconclusion: in the present work, we introduce the french question answering dataset. the contexts are collected from the set of high quality wikipedia articles. with the help of french college students, 60,000+ questions have been manually annotated. the fquad dataset is the result of two different annotation processes. first, fquad1.0 is collected to build a 25,000+ questions dataset. second, the dataset is enriched to reach 60,000+ questions resulting in fquad1.1. the development and test sets have both been enriched with additional answers for the evaluation process.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK 2.1 NL-PL Pair Mining\n",
            "\u001b[35mSection_title: 2.2  NL Context Understanding\n",
            "\u001b[35mSection_title: 3  PRELIMINARIES\n",
            "\u001b[35mSection_title: 3.1  Task Overview\n",
            "\u001b[35mSection_title: 3.2  Manual Annotation\n",
            "\u001b[35mSection_title: 4  OUR APPROACH\n",
            "\u001b[35mSection_title: 4.1  Intuition\n",
            "\u001b[35mSection_title: 4.2  Tokenizer and Token Aggregator\n",
            "\u001b[35mSection_title: 4.3  Sequence Taggers\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS 5.1 Datasets\n",
            "\u001b[35mSection_title: 5.2  Experimental Settings\n",
            "\u001b[35mSection_title: 5.3  Baselines\n",
            "\u001b[35mSection_title: 5.4  StaQC-human Results\n",
            "\u001b[35mSection_title: 5.5  Lang2Code-human Results\n",
            "\u001b[35mSection_title: 6  LANG2CODE: AN AUTOMATICALLY MINED NL-PL PAIRS DATASET\n",
            "\u001b[35mSection_title: 6.1  Construction Pipeline\n",
            "\u001b[35mSection_title: 6.2  Statistics of Lang2Code\n",
            "\u001b[35mSection_title: 6.3  Downstream Evaluation\n",
            "\u001b[35mSection_title: 6.3.2  Results.\n",
            "\u001b[35mSection_title: 6.3.3  Case Study.\n",
            "\u001b[32mconclusion: in this paper, we propose a sequence labeling based question answering (slqa) approach to construct nl-pl pairs in a pl-agnostic way. we propose a model that can produce multiple code blocks as solutions of a post's question, which is achieved by using bio sequence tagging. we also propose to incorporate the global textual context as pl-independent supplementary information. to validate the capacity of our method, we manually annotate a challenging cross-pl multi-block dataset, named lang2code-human. substantial experiments on the single-pl single-block staqc-human and our lang2code-human benchmarks demonstrate the effectiveness and cross-pl transferability of our method. finally, we present lang2code, the largest-to-date nl-pl corpus to the best of our knowledge, containing over 1.4 million pairs spanning six pls. under statistical analysis and downstream evaluation on code generation task, we demonstrate that lang2code is a large-scale and high-quality nl-pl pair corpus and can greatly help developing data-hungry models in future research.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Previous Work\n",
            "\u001b[35mSection_title: 2.1.  Language Generation Approaches: KBRD and KGSF\n",
            "\u001b[35mSection_title: 2.2.  Retrieval-based Approaches\n",
            "\u001b[35mSection_title: 2.3.  Evaluation of Conversational Recommender Systems\n",
            "\u001b[35mSection_title: 3.  A Contextual Retrieval-based CRS (CRB-CRS)\n",
            "\u001b[35mSection_title: 3.1.  Retrieval & Ranking Module\n",
            "\u001b[35mSection_title: 3.2.  Recommendation and Meta-data Integration Module\n",
            "\u001b[35mSection_title: 4.  Evaluation Methodology\n",
            "\u001b[35mSection_title: 5.  Results\n",
            "\u001b[35mSection_title: 5.1.  Main Results\n",
            "\u001b[35mSection_title: 5.2.  Additional Analyses\n",
            "\u001b[35mSection_title: 5.2.1.  Analysis of Failure Situations: Specific Seeker Queries\n",
            "\u001b[35mSection_title: 5.2.2.  An Exploratory Analysis of Challenging Dialog States / User Intents\n",
            "\u001b[35mSection_title: 5.2.3.  Qualitative Analysis of Seeker Preference Expression\n",
            "\u001b[35mSection_title: 6.  Implications & Discussion\n",
            "\u001b[35mSection_title: 6.1.  Implications\n",
            "\u001b[32mlimitation: one potential threat to the validity in user studies like ours lies in the reliability of the study participants. given that we applied more than one quality-assurance measure-participant selection, an attention check, manual inspection-we are confident that our results are reliable. given also that most participants are fluent in english and regular movie watchers, we believe that the participants are representative at least for a subset of potential users of an online crs.another potential limitation is that we so far only analyzed two language generation systems. the question therefore remains to what extent the findings of our study would generalize to other approaches. since both analyzed systems (kbrd and kgsf) were published in the last two years, and since they were published at top-ranked scientific conferences, we believe that they are good representatives of the state-of-the-art in neural generation-based systems. moreover, an earlier analysis of the deepcrs system in [16] indicates that similar phenomena might be found also for other approaches. note also that our study can be easily extended to include alternative or even newer approaches, as long as the respective authors share the needed artifacts for reproducibility. as of now, we did not find any work that demonstrate superior quality than the kgsf system, and that the source code of such a system is available. to ensure replicability of our own work, we also share all the code and data used for our analyses online.so far, we analyzed our approach only with the help of the redial dataset. this choice was necessary to ensure a fair comparison with two recent works (kbrd and kgsf), which also relied on this dataset. recently, a number of alternative dataset was proposed, e.g., [5,14,65]. an evaluation of all compared methods on other datasets is however beyond the scope of our present work, which aimed to assess the relative performance of generation-based and retrievalbased system based on the dataset for which they were originally designed and tuned.regarding the general nature of the proposed crb-crs system, it is a retrieval-based system. however, as discussed in section 3, we also rely on a small set of heuristics in a few processing phases, which are, for example, implemented using keyword lists and string matching. these heuristics, which are not yet learned automatically, are documented in the provided source code of our system. automating the construction of these rules and keyword lists, e.g., based on movielens metadata, is a part of our future work to avoid any knowledge-engineering bottlenecks.\n",
            "\u001b[35mSection_title: 6.2.  Research Limitations\n",
            "\u001b[32mconclusion: conversational recommender systems (crs) that interact with users in natural language obtained increased attention in the past few years. in this paper, we have proposed a retrieval-based approach to conversational recommendations, and we conducted a study involving humans to understand how our system performs compared to recent language generation approaches. our study led to promising results, and we hope that our study design can be used as a blueprint for user-centric studies of crs in the future. overall, the main conclusion of our studies is retrieval-based approaches to crs can be a promising alternative or complement to language generation approaches.\n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Previous Work\n",
            "\u001b[35mSection_title: 2.1.  Language Generation Approaches: KBRD and KGSF\n",
            "\u001b[35mSection_title: 2.2.  Retrieval-based Approaches\n",
            "\u001b[35mSection_title: 2.3.  Evaluation of Conversational Recommender Systems\n",
            "\u001b[35mSection_title: 3.  A Contextual Retrieval-based CRS (CRB-CRS)\n",
            "\u001b[35mSection_title: 3.1.  Retrieval & Ranking Module\n",
            "\u001b[35mSection_title: 3.2.  Recommendation and Meta-data Integration Module\n",
            "\u001b[35mSection_title: 4.  Evaluation Methodology\n",
            "\u001b[35mSection_title: 5.  Results\n",
            "\u001b[35mSection_title: 5.1.  Main Results\n",
            "\u001b[35mSection_title: 5.2.  Additional Analyses\n",
            "\u001b[35mSection_title: 5.2.1.  Analysis of Failure Situations: Specific Seeker Queries\n",
            "\u001b[35mSection_title: 5.2.2.  An Exploratory Analysis of Challenging Dialog States / User Intents\n",
            "\u001b[35mSection_title: 5.2.3.  Qualitative Analysis of Seeker Preference Expression\n",
            "\u001b[35mSection_title: 6.  Implications & Discussion\n",
            "\u001b[35mSection_title: 6.1.  Implications\n",
            "\u001b[32mlimitation: one potential threat to the validity in user studies like ours lies in the reliability of the study participants. given that we applied more than one quality-assurance measure-participant selection, an attention check, manual inspection-we are confident that our results are reliable. given also that most participants are fluent in english and regular movie watchers, we believe that the participants are representative at least for a subset of potential users of an online crs.another potential limitation is that we so far only analyzed two language generation systems. the question therefore remains to what extent the findings of our study would generalize to other approaches. since both analyzed systems (kbrd and kgsf) were published in the last two years, and since they were published at top-ranked scientific conferences, we believe that they are good representatives of the state-of-the-art in neural generation-based systems. moreover, an earlier analysis of the deepcrs system in [16] indicates that similar phenomena might be found also for other approaches. note also that our study can be easily extended to include alternative or even newer approaches, as long as the respective authors share the needed artifacts for reproducibility. as of now, we did not find any work that demonstrate superior quality than the kgsf system, and that the source code of such a system is available. to ensure replicability of our own work, we also share all the code and data used for our analyses online.so far, we analyzed our approach only with the help of the redial dataset. this choice was necessary to ensure a fair comparison with two recent works (kbrd and kgsf), which also relied on this dataset. recently, a number of alternative dataset was proposed, e.g., [5,14,65]. an evaluation of all compared methods on other datasets is however beyond the scope of our present work, which aimed to assess the relative performance of generation-based and retrievalbased system based on the dataset for which they were originally designed and tuned.regarding the general nature of the proposed crb-crs system, it is a retrieval-based system. however, as discussed in section 3, we also rely on a small set of heuristics in a few processing phases, which are, for example, implemented using keyword lists and string matching. these heuristics, which are not yet learned automatically, are documented in the provided source code of our system. automating the construction of these rules and keyword lists, e.g., based on movielens metadata, is a part of our future work to avoid any knowledge-engineering bottlenecks.\n",
            "\u001b[35mSection_title: 6.2.  Research Limitations\n",
            "\u001b[32mconclusion: conversational recommender systems (crs) that interact with users in natural language obtained increased attention in the past few years. in this paper, we have proposed a retrieval-based approach to conversational recommendations, and we conducted a study involving humans to understand how our system performs compared to recent language generation approaches. our study led to promising results, and we hope that our study design can be used as a blueprint for user-centric studies of crs in the future. overall, the main conclusion of our studies is retrieval-based approaches to crs can be a promising alternative or complement to language generation approaches.\n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Previous Work\n",
            "\u001b[35mSection_title: 2.1.  Language Generation Approaches: KBRD and KGSF\n",
            "\u001b[35mSection_title: 2.2.  Retrieval-based Approaches\n",
            "\u001b[35mSection_title: 2.3.  Evaluation of Conversational Recommender Systems\n",
            "\u001b[35mSection_title: 3.  A Contextual Retrieval-based CRS (CRB-CRS)\n",
            "\u001b[35mSection_title: 3.1.  Retrieval & Ranking Module\n",
            "\u001b[35mSection_title: 3.2.  Recommendation and Meta-data Integration Module\n",
            "\u001b[35mSection_title: 4.  Evaluation Methodology\n",
            "\u001b[35mSection_title: 5.  Results\n",
            "\u001b[35mSection_title: 5.1.  Main Results\n",
            "\u001b[35mSection_title: 5.2.  Additional Analyses\n",
            "\u001b[35mSection_title: 5.2.1.  Analysis of Failure Situations: Specific Seeker Queries\n",
            "\u001b[35mSection_title: 5.2.2.  An Exploratory Analysis of Challenging Dialog States / User Intents\n",
            "\u001b[35mSection_title: 5.2.3.  Qualitative Analysis of Seeker Preference Expression\n",
            "\u001b[35mSection_title: 6.  Implications & Discussion\n",
            "\u001b[35mSection_title: 6.1.  Implications\n",
            "\u001b[32mlimitation: one potential threat to the validity in user studies like ours lies in the reliability of the study participants. given that we applied more than one quality-assurance measure-participant selection, an attention check, manual inspection-we are confident that our results are reliable. given also that most participants are fluent in english and regular movie watchers, we believe that the participants are representative at least for a subset of potential users of an online crs.another potential limitation is that we so far only analyzed two language generation systems. the question therefore remains to what extent the findings of our study would generalize to other approaches. since both analyzed systems (kbrd and kgsf) were published in the last two years, and since they were published at top-ranked scientific conferences, we believe that they are good representatives of the state-of-the-art in neural generation-based systems. moreover, an earlier analysis of the deepcrs system in [16] indicates that similar phenomena might be found also for other approaches. note also that our study can be easily extended to include alternative or even newer approaches, as long as the respective authors share the needed artifacts for reproducibility. as of now, we did not find any work that demonstrate superior quality than the kgsf system, and that the source code of such a system is available. to ensure replicability of our own work, we also share all the code and data used for our analyses online.so far, we analyzed our approach only with the help of the redial dataset. this choice was necessary to ensure a fair comparison with two recent works (kbrd and kgsf), which also relied on this dataset. recently, a number of alternative dataset was proposed, e.g., [5,14,65]. an evaluation of all compared methods on other datasets is however beyond the scope of our present work, which aimed to assess the relative performance of generation-based and retrievalbased system based on the dataset for which they were originally designed and tuned.regarding the general nature of the proposed crb-crs system, it is a retrieval-based system. however, as discussed in section 3, we also rely on a small set of heuristics in a few processing phases, which are, for example, implemented using keyword lists and string matching. these heuristics, which are not yet learned automatically, are documented in the provided source code of our system. automating the construction of these rules and keyword lists, e.g., based on movielens metadata, is a part of our future work to avoid any knowledge-engineering bottlenecks.\n",
            "\u001b[35mSection_title: 6.2.  Research Limitations\n",
            "\u001b[32mconclusion: conversational recommender systems (crs) that interact with users in natural language obtained increased attention in the past few years. in this paper, we have proposed a retrieval-based approach to conversational recommendations, and we conducted a study involving humans to understand how our system performs compared to recent language generation approaches. our study led to promising results, and we hope that our study design can be used as a blueprint for user-centric studies of crs in the future. overall, the main conclusion of our studies is retrieval-based approaches to crs can be a promising alternative or complement to language generation approaches.\n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Document Question Answering\n",
            "\u001b[35mSection_title: 2.2  Digital and Hybrid Assistance\n",
            "\u001b[35mSection_title: 2.3  Tools that Support Authoring, Reviewing, or Reading Documents\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Phase 1 -Experience Sampling\n",
            "\u001b[35mSection_title: 3.2  Phase 2 -Human-in-the-loop Q&A System\n",
            "\u001b[35mSection_title: 3.3  Taxonomy Development\n",
            "\u001b[35mSection_title: 3.4  Participants\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Taxonomy of Question Types\n",
            "\u001b[35mSection_title: 4.2  AI Success Rate\n",
            "\u001b[35mSection_title: 4.3  Difference in the Distribution of Questions by User Role\n",
            "\u001b[35mSection_title: 4.4  Skillsets and Roles of Human Respondents\n",
            "\u001b[35mSection_title: 4.5  Users' Experience with the Q&A System\n",
            "\u001b[35mSection_title: 4.6  How Response Time Varies by Question Type\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Question Types\n",
            "\u001b[35mSection_title: 5.2  Opportunities and Challenges of Incorporating Humans in the Loop\n",
            "\u001b[35mSection_title: 5.3  Opportunities for Human-in-the-loop Document Manipulation\n",
            "\u001b[32mlimitation: in this work we focused only on word documents as a common document type where users can author, copy-edit, or read content. indeed, the organization where we deployed our studies primarily uses word for business documents. the focus on word rather than e.g., pdfs or web pages also allowed us to obtain questions about documents at various stages of development, not just finalized manuscripts. future work can investigate the types of questions that arise when users interact with other file types such as pdf, excel, and powerpoint. because each file type is used for different purposes (e.g., excel documents for long-term book-keeping [27]) and possibly containing content at different levels of abstraction, the extent to which question answering in these documents can be automated and the kinds of expertise knowledge workers need may be different from the word documents in our study.because all questions had to first pass through the knowledge workers' system, the majority of the responses that participants received from the q&a system had a delay. therefore, it is possible that participants would have asked different questions if the responses had been provided instantaneously. another property that could have conceivably impacted the types of questions users asked is the quality of the answers that they received. to understand if the types of questions by a user changed over time as they gained familiarity with the system, we examined the questions that were posted by the same user both across different documents or on the same document. we observed that some users posted multiple questions in succession and close together in time, e.g., in the span of a few minutes. although these users would realize that the system does not provide answers instantaneously, their cluster of initial questions would not be impacted by the expectation of a long delay, their perceived capabilities of the system, or the quality of the answers. in fact, although we had specified in the consent form that there may be a delay in the responses that participants would receive from the q&a system, some end-of-study survey responses indicated that a number of participants had in fact not noticed this point and had asked their first few questions expecting instantaneous answers-\"i was initially confused by the delay of asking the question in the document and then waiting for an email that told me to go back to the document. it seemed a bit redundant to get an email about it vs. just a notification in the word doc itself and telling me it was working on it or something. for a plug-in, however, i would expect less of a delay. \" (p-2-18). therefore, the first few questions from these participants could help with the generalizability of our results.the delay or the quality of answers however, may have influenced those users who submitted questions after receiving answers from the system. upon examination, we found that in many instances, when users received answers to their previous questions, they explored increasingly more sophisticated questions of various types, e.g., content-related, concerning metadata, or seeking external information. for instance, one participant started by asking simple metadata questions (\"who is the author?\") and proceeded to ask questions on a scanned document that needed not only complex reasoning, but also optical character recognition (ocr): \"what is the total score?\" on a document where handwritten scores were given to each question. this finding suggests that the answers may have in fact encouraged users to be liberal with the types of questions they wished to subsequently ask. this exploration could be due to users gaining confidence that the system can in fact handle the types of questions with which they need support or could be because they wished to test the limits of its abilities.nevertheless, the characterization that we present in the paper also includes the questions users submitted in the experience sampling phase, where participants could imagine a sophisticated system with any or no delay. the set of questions in that phase of the study could further help with generalizability of our results, e.g., to settings where not all questions necessarily experience a delay.it is conceivable that the types of questions about a document may vary with the document type. in phase 2 where we had access to the documents, we observed that the document distribution was in fact very varied and included project proposals and timelines, value propositions, design specifications, service instructions, management training, protocols, faqs, whitepaper reports, strategy planning, customer feedback, research findings, etc. from various domains. with this diverse set of document types, investigating the relationship between types of questions and types of documents would require collecting far more questions by running the study for a long time.the setup of our q&a system was such that users submitted one-shot questions as there were no affordances for following up on previously asked questions. future work should investigate whether the types of questions that users ask a document q&a system differ if the system provides the users with the means for following up on their previous questions or multi-round conversations with the assistant.another area for future work would be to explore the context of user needs including when users need different types of assistance with their documents, what they do before the seek help, and what they do after they receive answers. to minimize concerns about the confidentiality of the business data, the contextual information that we collected in our study concerned the metadata of the document (e.g., file size, last modified date, etc.) and not the content. for the same reason, we did not track a user's modification of the document before or after the user posted a question; the metadata was collected upon the user's submission of a question. therefore, given the data we collected, we cannot examine the context of user needs. future work can study this question through interviews and user-produced logs.\n",
            "\u001b[35mSection_title: 6  Limitations and Future Work\n",
            "\u001b[32mconclusion: we studied users' information needs when working with their business documents as a first step towards building document assistants that can handle a variety of user requests. to understand users' actual needs, it was important to collect their document-centric questions in-situ. therefore, we conducted two user studies. in the first study, we performed experience sampling of users' questions via a microsoft word add-in as users were working with their documents. in the second, users submitted their questions via an add-in and received answers from a human-in-the-loop document q&a system that complemented a question-answering ai with human intelligence. we characterized the distributions of questions and observed that the types of questions do indeed vary by whether the user is an author, a reviewer, or a reader of the document. in addition, the questions gave us insight into what types of request can be automated and whether particular skillsets or roles within the document are needed from human respondents in a document digital assistant that is co-powered by artificial and human intelligence.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Question decomposition engine\n",
            "\u001b[35mSection_title: 3.1.  Representing questions as programs\n",
            "\u001b[35mSection_title: 3.2.  Decomposing questions using programs\n",
            "\u001b[35mSection_title: 3.3.  AGQA answer generation\n",
            "\u001b[35mSection_title: 4.  Metrics\n",
            "\u001b[35mSection_title: 5.  Experiments\n",
            "\u001b[35mSection_title: 5.1.  Human evaluation\n",
            "\u001b[35mSection_title: 5.2.  Performance on Leaf Nodes\n",
            "\u001b[35mSection_title: 5.3.  Performance on Choose and Equals\n",
            "\u001b[35mSection_title: 5.4.  Performance on Conjunctions\n",
            "\u001b[35mSection_title: 5.5.  Performance on Temporal Reasoning\n",
            "\u001b[35mSection_title: 5.6.  Correlation between consistency and accuracy\n",
            "\u001b[35mSection_title: 6.  Discussion\n",
            "\u001b[35mSection_title: 7.  Supplementary\n",
            "\u001b[35mSection_title: 7.1.  Dataset\n",
            "\u001b[35mSection_title: 7.2.  Metrics\n",
            "\u001b[35mSection_title: 7.3.  Experiments\n",
            "\u001b[35mSection_title: 7.4.  Future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Rissanen Data Analysis\n",
            "\u001b[35mSection_title: 2.1  Minimum Description Length\n",
            "\u001b[35mSection_title: 2.2  Online Coding\n",
            "\u001b[35mSection_title: 2.3  Implementation with Block-wise Coding\n",
            "\u001b[35mSection_title: 2.4  Experimental Setup\n",
            "\u001b[35mSection_title: 3  Validating Rissanen Data Analysis\n",
            "\u001b[35mSection_title: 4  Examining Dataset Characteristics\n",
            "\u001b[35mSection_title: 4.1.1  Results\n",
            "\u001b[35mSection_title: 4.2.1  Results\n",
            "\u001b[35mSection_title: 4.3  Examining Text Datasets\n",
            "\u001b[35mSection_title: 4.3.1  The usefulness of part-of-speech words\n",
            "\u001b[35mSection_title: 4.3.3  Do Datasets Suffer from Gender Bias?\n",
            "\u001b[35mSection_title: 4.3.4  How useful is word order?\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we proposed rissanen data analysis (rda), a method for examining the characteristics of a dataset. we began by viewing the labels of a dataset as being generated by a program over the inputs, then positing that a capability is helpful if it reduces the length of the shortest label-generating program. instead of evaluating minimum program length directly, we use blockwise prequential coding to upper bound minimum description length (mdl). while the choice of learning algorithm a influences absolute mdl values, we only interpret mdl relative to other mdl values estimated with the same a. in particular, we conduct rda by comparing mdl with or without access to a subroutine with a certain capability, and we say that a capability is useful when invoking the subroutine reduces mdl.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3.  We provide empirical comparisons of training with different loss functions and show that Binary\n",
            "\u001b[35mSection_title: 2  Models\n",
            "\u001b[35mSection_title: 2.1  Iterative BM25 Retrieval\n",
            "\u001b[35mSection_title: 2.2  LSTM-After Transformer for Re-Ranking\n",
            "\u001b[35mSection_title: 2.3  LSTM-Interleaved Transformer for Re-Ranking\n",
            "\u001b[35mSection_title: 3.1  Notes\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[32mconclusion: the lit architecture is a simple yet powerful adaptation of the transformer architecture to learn better cross-document interactions for multi-hop ranking. the structure can be easily integrated with any transformer language model to enable cross-referencing of knowledge statements and improved ranking performance. for example, lit can be a drop-in encoder for other multi-hop question answering datasets such as hotpotqa (yang et al., 2018). when applied to the challenging worldtree v2 dataset, lit achieves competitive performance with current state-of-the-art models despite a smaller footprint. we envision that this architecture can be beneficial to many nlp tasks which require multi-hop reasoning over documents.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Problem Definition\n",
            "\u001b[35mSection_title: 4.  Method\n",
            "\u001b[35mSection_title: 5.  The Datasets\n",
            "\u001b[35mSection_title: 6.  Experiments\n",
            "\u001b[32mconclusion: we provide insights into an rnn-based iterative memory model that incorporates gate attention on multi-step reasoning over natural language. instead of using the original gru and dotproduct attention, we integrate gate attention to update hidden states. the experiment results show the model with gate attention achieves generally better performance than the original rnnbased iterative-memory model with dot-product attention and other rnn-based models. the performance of our model is comparable or better than the much larger and pretrained roberta-large in some scenarios. furthermore, our model shows better out-of-distribution generalisation performance than the pretained roberta. to address the issue of depth-imbalance in the existing datasets on multi-step reasoning over natural language, we develop a large-scale multi-step reasoning dataset called pararule-plus, with more examples of deep reasoning depths than previous datasets. we find that the performance of the models in our experiments improves when we add pararule-plus in the training, especially on examples that require deeper reasoning depths and extra out-of-distribution examples. table 4 we use glove [16] as the word vector representation. we use pararules with all depths as the training set for all models and then test them on examples with different reasoning depths (d). comparison among our ima-glove-ga, ima-glove, mac-glove, dmn-glove, imasm-glove, lstm-glove, and roberta-large on pararules test sets with different reasoning depths. \n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Related Work\n",
            "\u001b[35mSection_title: 2.1  Transformers for Longer Input\n",
            "\u001b[35mSection_title: 2.2  Positional Encoding of Transformers\n",
            "\u001b[35mSection_title: 2.3  Pretraining objectives for Question Answering\n",
            "\u001b[35mSection_title: 2.4  Datasets of Question Answering for Longer Documents\n",
            "\u001b[35mSection_title: 3  LittleBird Architecture\n",
            "\u001b[35mSection_title: 3.1  Bidirectional ALiBi\n",
            "\u001b[35mSection_title: 3.2  Sliding Window Attention\n",
            "\u001b[35mSection_title: 3.3  Pack & Unpack Attention\n",
            "\u001b[35mSection_title: 3.4  Efficient Training and Padding Insertion\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Pre-training using RSS\n",
            "\u001b[35mSection_title: 4.2  QA Benchmark for English\n",
            "\u001b[35mSection_title: 4.3  QA Benchmark for Korean\n",
            "\u001b[35mSection_title: 4.4  Effect of Padding Insertion\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[35mSection_title: 4.6  Speed & Memory efficiency\n",
            "\u001b[32mconclusion: we propose littlebird, which is more efficient in terms of memory and computational time than existing transformer models for long sequences, and its effective way to train. it combines a novel position encoding method, bialibi, and pack & unpack with sliding window attention to achieve high speed and accuracy, particularly in question answering tasks for long documents. the distillation and training method with padding insertion allows the model to be trained by reusing the existing pre-trained language model for short inputs and work well for long inputs even if trained on short inputs. we demonstrated through experiments that the accuracy of question answering improves as the model is fed a longer input, and we achieved state-of-the-art performance in korquad2.0 using littlebird.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset\n",
            "\u001b[35mSection_title: 3  End-to-End Model\n",
            "\u001b[35mSection_title: 3.1  Training\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Splitting with Word Embeddings\n",
            "\u001b[35mSection_title: 4.2  Qualitative Example\n",
            "\u001b[32mconclusion: we have proposed the game of log(n )-questions over sentences, and introduced an end-to-end system of 2 agents that are able to play the game. while our results show promise, there is work to be done on improving game and sw prediction performance simultaneously, as well as playing the game over larger sentence sets. more generally, we shows agents exhibiting reasoning and information-seeking in a text environment.\n",
            "\u001b[35mSection_title: 4.3  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3664  2 Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Task Description and Notations\n",
            "\u001b[35mSection_title: 3.2  Baseline PROVER Model\n",
            "\u001b[35mSection_title: 3.3  Multilabel-MULTIPROVER\n",
            "\u001b[35mSection_title: 3.4  Iterative-MULTIPROVER\n",
            "\u001b[35mSection_title: 3.5  Permutation-Invariant Hungarian Loss\n",
            "\u001b[35mSection_title: 3.6  Integer Linear Program (ILP) Inference\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Generalization to Zero-Shot Dataset with Single Gold Proofs\n",
            "\u001b[35mSection_title: 5.3  Generalization to Higher Depths\n",
            "\u001b[35mSection_title: 5.4  Generalization with Less Training Data\n",
            "\u001b[35mSection_title: 5.5  Comparison of MULTIPROVER with the Skyline Single-Proof Generation Model\n",
            "\u001b[32mconclusion: we proposed multilabel-multiprover and iterative-multiprover, two variants of a proof-set generation model where the former performs implicit conditioning between the proofs to generate them in parallel while the latter generates a proof-set through explicit conditioning on the previously generated proofs. both models obtain strong proof f1 improvements on synthetic and humanparaphrased datasets and iterative-multiprover also obtains state-of-the-art proof f1 on a zero-shot dataset with single proofs. multiprover's modeling is fairly generic and similar methods can be used in generating a set of structured explanations for other nlp tasks like multi-hop qa.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Reasoning Skills Formulation\n",
            "\u001b[35mSection_title: 3  ReasonFormer Framework\n",
            "\u001b[35mSection_title: 3.1  Representation Module\n",
            "\u001b[35mSection_title: 3.2  Reasoning Modules\n",
            "\u001b[35mSection_title: 3.2.1  Reasoning Modules Architecture\n",
            "\u001b[35mSection_title: 3.2.2  Reasoning Router\n",
            "\u001b[35mSection_title: 3.3  Pre-training and Adaptation\n",
            "\u001b[35mSection_title: 4  Experiment Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Pre-training Corpus\n",
            "\u001b[35mSection_title: 4.3  Models\n",
            "\u001b[35mSection_title: 4.3.1  Model Initialization\n",
            "\u001b[35mSection_title: 4.3.2  Compared Methods\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Study\n",
            "\u001b[35mSection_title: 5.3  Low-resource Experiments\n",
            "\u001b[35mSection_title: 5.4  Reasoning Skills Analysis\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\u001b[32mconclusion: this paper stimulates the compositional reasoning process of humans in decision-making, and makes the following hypotheses: (1) the intuitive perception system (system 1) and cognitive reasoning system (system 2) can be decoupled and (2) the complex decision-making can be disentangled into multi-step execution of fundamental reasoning skills. correspondingly, we propose reason-former, a compositional general-purpose reasoning framework. reasonformer decouples the representation module and reasoning modules, which are pre-trained to expert in fundamental reasoning skills. the reasoning modules are dynamically composed in parallel and cascaded manner to form a whole reasoning process. reasonformer is endto-end and unified in solving multiple tasks with one model. extensive experiments on 11 tasks reveal the compositional reasoning ability of reason-former and disentangling of representation and reasoning modules.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Dataset Selection\n",
            "\u001b[35mSection_title: 2.2  Learning objectives\n",
            "\u001b[35mSection_title: 2.3  Continual Pretraining\n",
            "\u001b[35mSection_title: 2.4  Finetuning\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 3.3  Implementation Details\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Overall Results\n",
            "\u001b[35mSection_title: 4.2  Performance on GLUE Benchmark\n",
            "\u001b[35mSection_title: 4.3  Qualitative Analysis\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[35mSection_title: 5  Related Works\n",
            "\u001b[32mconclusion: in this paper, we proposed apollo, an adaptive pre-trained language model with logical reasoning abilities. we use a subset of wikipedia sentences for continued pretraining of the model using two self-supervised loss functions. the choice of the training dataset and loss functions are guided by the goal to include more reasoning-related sentences and training signals, respectively. through experiments on two logical reasoning datasets and ablation studies, we demonstrate the effectiveness of our proposed approach. overall, we show that apollo is a generalized solution to improving logical reasoning in language models.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: a limitation of this approach is the trade-off between completeness and noise in the training data. while our method using keywords to extract text from wikipedia is effective, implication likely contains redundant sentences that cannot improve the model's logical reasoning capability. a better rule-based or neural model might be able to extract a better corpus with potentially higher computational costs. additionally, using pos tagging limits the application of this approach to languages with well-defined pos taggers. switching to a more universal semantic tagging system (abzianidze and bos, 2017) \n",
            "\u001b[35mSection_title: 7  Limitation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  DrFact: An Efficient Approach for Differentiable Reasoning over Facts\n",
            "\u001b[35mSection_title: 4.1  Overview\n",
            "\u001b[35mSection_title: 4.2  Pre-computed Indices\n",
            "\u001b[35mSection_title: 4.3  Differentiable Fact-Following Operation\n",
            "\u001b[35mSection_title: 4.4  Auxiliary Learning with Distant Evidence\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Baseline Methods\n",
            "\u001b[35mSection_title: 5.3  Results and Analysis\n",
            "\u001b[32mconclusion: we introduce and study a new task -open-ended commonsense reasoning (opencsr) -which is both realistic and challenging.we construct three opencsr versions of widely used datasets targeting commonsense reasoning with a novel crowdsourced collection of multiple answers, and evaluate a number of baseline methods for this task.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  I.I.D SETTING OF RETRIEVAL PROBLEM\n",
            "\u001b[35mSection_title: 4  OOD SETTING OF RETRIEVAL PROBLEM\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL SETUP 5.1 Datasets\n",
            "\u001b[35mSection_title: 5.2  Retrieval models\n",
            "\u001b[35mSection_title: 5.3  Evaluation\n",
            "\u001b[35mSection_title: 6  RESULT\n",
            "\u001b[35mSection_title: 6.1  The Overall I.I.D. Result\n",
            "\u001b[35mSection_title: 6.2  Analysis of OOD Generalizability on Query Variations\n",
            "\u001b[35mSection_title: 6.3  Analysis of OOD Generalizability on Unforeseen Query Types\n",
            "\u001b[35mSection_title: 6.4  Analysis of OOD Generalizability on Unforeseen Tasks\n",
            "\u001b[32mconclusion: in this paper, we analyzed the out-of-distribution robustness of several representative generative and dense retrieval models on the kilt benchmark. specifically, we proposed three perspectives to define the out-of-distribution robustness. the results showed that generative retrieval models expose significant vulnerabilities in ood robustness. more research efforts are needed to develop robust generative retrieval models.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model Description\n",
            "\u001b[35mSection_title: 3.1  Fine-grained Question Type Classifier\n",
            "\u001b[35mSection_title: 3.2  Question Type Classifier with Grammatical Knowledge\n",
            "\u001b[35mSection_title: 3.3  Question Generator\n",
            "\u001b[35mSection_title: 4  Influence of Grammar Knowledge on Tibetan Question Generation\n",
            "\u001b[35mSection_title: 4.1  Influence of Interrogative Word Ambiguity\n",
            "\u001b[35mSection_title: 4.2  Influence of Tibetan Interrogative Words Position\n",
            "\u001b[35mSection_title: 5  Experiment\n",
            "\u001b[35mSection_title: 5.1  Dataset\n",
            "\u001b[35mSection_title: 5.2  Automatic Evaluation\n",
            "\u001b[35mSection_title: 5.3  Experimental Results of Question Generation\n",
            "\u001b[35mSection_title: 5.4  Question Type Accuracy of Generated Questions\n",
            "\u001b[35mSection_title: 5.5  Human Evaluation\n",
            "\u001b[35mSection_title: 6  Case Analysis\n",
            "\u001b[32mconclusion: to solve the problem of mismatch between question types and answers in question generation, this paper constructs a question type classifier and a question generator. we classify questions into finegrained classification and integrate grammar knowledge into question type classifier to improve the accuracy of question types, then, the prediction results of the classifier are fused into the question generator to improve the performance of question generation. to verify the effectiveness of our model, we perform an upper bound analysis on it, we integrate grammar knowledge into the question generator to provide accurate question types to guide the model to generate questions. the final experimental results show that the method proposed in this paper not only improves the accuracy of question words in the generated question, but also improves the quality of the generated question.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  An Integrative Multimodal Generative Model\n",
            "\u001b[35mSection_title: 3.1  Model Architecture\n",
            "\u001b[35mSection_title: 3.2  Large-Scale Multimodal Generative Pretraining\n",
            "\u001b[35mSection_title: 3.2.1  Vision-Language Generative\n",
            "\u001b[35mSection_title: 3.2.2  Speech-Language Generative Pretraining\n",
            "\u001b[35mSection_title: 3.2.3  Language-only Generative Pretraining.\n",
            "\u001b[35mSection_title: 3.2.4  Pretraining Details\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Multimodal Summarization\n",
            "\u001b[35mSection_title: 4.2  Multimodal Dialogue Generation\n",
            "\u001b[35mSection_title: 4.3  Video Multimodal Sentiment Analysis\n",
            "\u001b[35mSection_title: 4.4  Automatic Speech Recognition\n",
            "\u001b[35mSection_title: 4.5  Vision QA\n",
            "\u001b[35mSection_title: 4.6  Image Captioning\n",
            "\u001b[35mSection_title: 4.7  Analysis & Explorations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  System overview\n",
            "\u001b[35mSection_title: 3  Pseudo ambiguous question generation\n",
            "\u001b[35mSection_title: 3.1  Question generation using syntax information\n",
            "\u001b[35mSection_title: 3.2  Evaluation of pseudo ambiguous questions\n",
            "\u001b[35mSection_title: 3.2.1  Evaluation on QA accuracy\n",
            "\u001b[35mSection_title: 3.2.2  Evaluation of sentence quality\n",
            "\u001b[35mSection_title: 4  Clarifying question generation\n",
            "\u001b[35mSection_title: 4.1  Case frame\n",
            "\u001b[35mSection_title: 4.2  Generation and selection process\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental setting\n",
            "\u001b[35mSection_title: 5.2  Experimental results\n",
            "\u001b[35mSection_title: 5.3  Analysis\n",
            "\u001b[35mSection_title: 6  Related works\n",
            "\u001b[35mSection_title: 6.1  Paraphrasing approach\n",
            "\u001b[35mSection_title: 6.2  Clarifying approach\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Conceptual Few-shot Learning\n",
            "\u001b[35mSection_title: 3  Evaluations\n",
            "\u001b[35mSection_title: 3.1  Few-shot Learners\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.3  Baseline model (BASELINE-TK-QA-1B)\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[32mconclusion: this work introduces a task of conceptual few-shot learning that reflects on in-context learners' ability to learn to apply a specific reasoning concept that can be informative for prediction. we assess a set of recent in-context learners for this ability over a set of concepts extracted from human explanations.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Preparation\n",
            "\u001b[35mSection_title: 4  Experiment Design\n",
            "\u001b[35mSection_title: 4.1  Prompting a Large Language Model\n",
            "\u001b[35mSection_title: 4.2  Fine-tuning a Contextual Embedding Model\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.2  Evaluation of the Fine-tuned Contextual Embedding Models\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: we presented the codequeries dataset to test the ability of neural models to understand code semantics on the proposed problem of answering semantic queries over code. it requires a model to perform single-or multi-hop reasoning, understand structure and semantics of code, distinguish between positive and negative examples, and accurately identify answer and supporting-fact spans. our evaluation shows that codequeries is challenging for the best-in-class generative and embedding approaches under different prompting or fine-tuning settings. we are considering extensions to our dataset to include more semantic queries and more programming languages.\n",
            "\u001b[35mSection_title: 7  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task definition\n",
            "\u001b[35mSection_title: 3.1  The importance of unseen answers\n",
            "\u001b[35mSection_title: 3.2  Why not translate?\n",
            "\u001b[35mSection_title: 3.3  Document-level reasoning\n",
            "\u001b[35mSection_title: 3.4  Quality control\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 5  Typological diversity\n",
            "\u001b[35mSection_title: 5.1  Discussion of Languages\n",
            "\u001b[35mSection_title: 5.2  A linguistic analysis\n",
            "\u001b[35mSection_title: 6  A quantitative analysis\n",
            "\u001b[35mSection_title: 6.1  Question analysis\n",
            "\u001b[35mSection_title: 6.2  Question-prompt analysis\n",
            "\u001b[35mSection_title: 6.3  Data quality\n",
            "\u001b[35mSection_title: 7  Evaluation\n",
            "\u001b[35mSection_title: 7.1  Evaluation measures\n",
            "\u001b[35mSection_title: 7.2  An estimate of human performance\n",
            "\u001b[35mSection_title: 7.3  Primary tasks: Baseline results\n",
            "\u001b[35mSection_title: 8  Gold passage: A simplified task\n",
            "\u001b[35mSection_title: 8.1  Gold passage lexical overlap\n",
            "\u001b[35mSection_title: 8.2  Gold passage results\n",
            "\u001b[35mSection_title: 9  Recommendations and future work\n",
            "\u001b[32mconclusion: confidently making progress on multilingual models requires challenging, trustworthy evaluations. we have argued that question answering is well-suited for this purpose and that by targeting a typologically diverse set of languages, progress on the tydi qa dataset is more likely to generalize on the breadth of linguistic phenomena found throughout the world's languages. by avoiding data collection procedures reliant on translation and multilingual modeling, we greatly mitigate the risk of sampling bias. we look forward to the many ways the research community finds to improve the quality of multilingual models. 25 because we believe mt may be a fruitful research direction for tydi qa, we do not release any automatic translations. in the past, this seems to have stymied innovation around translation as applied to multilingual datasets. 26 we will happily share our annotation protocol on request.\n",
            "\u001b[35mSection_title: 10  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A data-to-text approach for answer generation\n",
            "\u001b[35mSection_title: 3  Evaluation Setup\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[32mconclusion: traditionally, ir approaches solving complex information needs focused on leveraging multi-turn interactions to provide optimal rankings of candidate documents at each turn. in this paper we have suggested alternative retrieval models that do not rely on the interactive updating of queries and document rankings as answers. we suggest one such alternative approach can be found using datato-text generation models to generate in a single-turn, a natural language and structured answer. experimental evaluation of a planning-based dtt model using the trec car dataset shows the potential of our intuition. we believe that our work opens up novel areas of investigation including answer generation and explanation in conversational systems for ir.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Candidates for universal QA approach\n",
            "\u001b[35mSection_title: 4  Datasets\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Variation with model size\n",
            "\u001b[35mSection_title: 7  Qualitative Analysis\n",
            "\u001b[32mconclusion: in this work, we explore the viability of prompttuning as a solution to unified qa and conduct a thorough analysis of its promise, effectiveness, and trade-offs compared with the model-tuning paradigm on a set of 16 qa datasets, focusing particularly on several few-shot scenarios. as a result, we obtain several key findings and insights that hopefully will inform which paradigm to prefer under which scenarios. prompt tuning is quite competitive with model-tuning in the lower extreme of the few-shot scenarios, given a good initialization. while parameter-sharing leads to superior performance in the few-shot setting, the trends flip in the full-shot setting, a simple knowledge transfer approach (i.e., an average of relevant prompts) is as effective as complex methods without introducing additional parameters. pre-training the backbone model on the source tasks significantly benefits prompt tuning. while initializing from a strong prior is very helpful for prompt tuning, its benefit is not as substantial when using a larger backbone model, especially when the number of training examples exceeds a certain threshold.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction: A Name that is the Enemy of Accuracy\n",
            "\u001b[35mSection_title: 2  Method: An Entity by any Other Name\n",
            "\u001b[35mSection_title: 2.1  ODQA with Single Gold Answer\n",
            "\u001b[35mSection_title: 2.2  Extracting Alias Entities\n",
            "\u001b[35mSection_title: 2.3  Augmented Evaluation\n",
            "\u001b[35mSection_title: 2.4  Augmented Training\n",
            "\u001b[35mSection_title: 3  Experiment: Just as Sweet\n",
            "\u001b[35mSection_title: 5  Related Work: Refuse thy Name\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Retriever\n",
            "\u001b[35mSection_title: 2.2  Data Construction\n",
            "\u001b[35mSection_title: 2.3  Pre-Training\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Pre-training Data\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 3.3  Evaluation\n",
            "\u001b[35mSection_title: 3.4  Settings\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Few-Shot Text Classification\n",
            "\u001b[35mSection_title: 4.2  Instruction Following\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: this paper presents picl, a framework that exploits the in-context learning ability of plms by pre-training models on concatenations of text paragraphs sharing the same \"intrinsic tasks\" gathered from the large-scale general corpus. in picl, models learn to perform various intrinsic tasks conditioning on their context while preserving their generalization due to the little bias of the pre-training data. extensive experiments show that picl improves the icl performance on various datasets against several baselines, enabling a 770 m model to outperform a larger model with about 4x parameters while maintaining good generalization across a wide range of tasks. for future work, we would like to consider adding human instructions to our pre-training framework to enhance more abilities of plms like zero-shot instruction following.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 3.2  Structural Causal Model\n",
            "\u001b[35mSection_title: 3.3  Causal Intervention on SCM\n",
            "\u001b[35mSection_title: 4  Causal Intervention for MRC\n",
            "\u001b[35mSection_title: 4.1  Neuron-wise Adjustment\n",
            "\u001b[35mSection_title: 4.1.1  Parameter Split\n",
            "\u001b[35mSection_title: 4.1.2  Expert Selector\n",
            "\u001b[35mSection_title: 4.2  Token-wise Adjustment\n",
            "\u001b[35mSection_title: 4.3  Combined Adjustment\n",
            "\u001b[35mSection_title: 5.1.1  Datasets\n",
            "\u001b[35mSection_title: 5.1.2  Experimental Setups\n",
            "\u001b[35mSection_title: 5.1.3  Metrics\n",
            "\u001b[35mSection_title: 5.1.4  Baselines\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.2.1  Conventional Accuracy\n",
            "\u001b[35mSection_title: 5.2.2  ST & NF Scores\n",
            "\u001b[35mSection_title: 5.2.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.3  Extended Adversarial Study\n",
            "\u001b[32mconclusion: in this paper, we have presented ci4mrc, a novel causal interventional paradigm to address name bias in mrc: the pre-trained knowledge concerning names is a confounder limiting the robust performance.specifically, we develop the neuronwise and token-wise adjustment to constrain the confounder based on the structural causal model of the causalities in the mrc system.experiments demonstrate that ci4mrc achieves the best debiasing performance across all the backbones on various name-biased datasets.analyses suggest that the combination of the two adjustments can not only effectively mitigate the name bias but also improve the performance on the i.i.d evaluation.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Graph Neural Network\n",
            "\u001b[35mSection_title: 3.2  Graph Sequential Network\n",
            "\u001b[35mSection_title: 3.3  Applications on NLP tasks\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  HotpotQA data set\n",
            "\u001b[35mSection_title: 4.2  Results on HotpotQA\n",
            "\u001b[35mSection_title: 4.2.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2.2  Results\n",
            "\u001b[35mSection_title: 4.2.3  Analysis\n",
            "\u001b[35mSection_title: 4.3  FEVER data set\n",
            "\u001b[35mSection_title: 4.5  Visualization of attention in GSNs\n",
            "\u001b[32mconclusion: this paper proposes graph sequential network as a novel neural architecture to facilitate reasoning over graphs with sequential data on the nodes. we develop a new message passing algorithm based on co-attention between two sequences on graph nodes. the scheme avoids the information loss inherent in the pooling based early summarization of existing gnn-based models, and improve the reasoning ability on sentence level. through experiments on hotpotqa and fever, both of which require the model to perform multi-hop reasoning, we show that our proposed gsn attains better performance than existing gnns on different types of tasks. for future work we would like to apply gsn to other applications in nlp that require complex reasoning.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  TECHQA Dataset Collection\n",
            "\u001b[35mSection_title: 3.1  Automatic Filtering of Questions\n",
            "\u001b[35mSection_title: 3.2  Human Annotation\n",
            "\u001b[35mSection_title: 3.3  Human Filtering of Questions\n",
            "\u001b[35mSection_title: 3.4  Question-Answer Annotation Guidelines\n",
            "\u001b[35mSection_title: 3.4.1  Question Selection\n",
            "\u001b[35mSection_title: 3.4.2  Answer Span Selection\n",
            "\u001b[35mSection_title: 3.5  Annotation and Adjudication\n",
            "\u001b[35mSection_title: 4  TECHQA Dataset Characteristics\n",
            "\u001b[35mSection_title: 5  Leaderboard task\n",
            "\u001b[35mSection_title: 6  Baseline Results\n",
            "\u001b[35mSection_title: 7  Discussion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Multi-Document QA\n",
            "\u001b[35mSection_title: 3  Learning with Marginalization over Retrieval\n",
            "\u001b[35mSection_title: 3.1  Modeling Multi-Document Retrieval\n",
            "\u001b[35mSection_title: 3.2  Joint Modeling with Marginalization\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Settings\n",
            "\u001b[35mSection_title: 4.2  Model Details\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Training Settings\n",
            "\u001b[35mSection_title: 5  Main Results on IIRC\n",
            "\u001b[35mSection_title: 5.1  Analysis\n",
            "\u001b[35mSection_title: 6  Results on HotpotQA\n",
            "\u001b[35mSection_title: 7  Qualitative Analysis\n",
            "\u001b[35mSection_title: 8  Related Work\n",
            "\u001b[32mconclusion: we proposed a new probabilistic model for retrieving set-valued contexts for multi-document qa and show that training the qa model with marginalization over this set can help mitigate the false negatives in evidence annotations. experiments on iirc and hotpotqa fullwiki show that our proposed framework can learn to retrieve unlabeled alternative contexts and improves qa f1 by 5.5 on iirc and 8.9 on hotpotqa.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  HOPPER\n",
            "\u001b[35mSection_title: 3.2  OBJECT DETECTION AND REPRESENTATION\n",
            "\u001b[35mSection_title: 3.3  TRACKING\n",
            "\u001b[35mSection_title: 3.4  VIDEO QUERY REPRESENTATION AND RECOGNITION\n",
            "\u001b[35mSection_title: 4  MULTI-HOP TRANSFORMER\n",
            "\u001b[35mSection_title: 5  TRAINING\n",
            "\u001b[35mSection_title: 6  EXPERIMENTS\n",
            "\u001b[32mconclusion: this work presents hopper with a novel multi-hop transformer to address object permanence in videos. hopper achieves 73.2% top-1 accuracy at just 1 fps on cater, and demonstrates the benefits of multi-hop reasoning. in addition, the proposed multi-hop transformer uses an iterative attention mechanism and produces a step-by-step reasoning chain that improves interpretability. multi-hop models are often difficult to train without supervision for the middle hops. we propose several training methods that can be applied to other tasks to address the problem of lacking a ground truth reasoning chain. in the future, we plan to experiment on real-world video datasets and extend our methods to deal with other complex tasks (such as video qa).\n",
            "\u001b[35mSection_title: 7  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Definition\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Paragraph and Question Encoder\n",
            "\u001b[35mSection_title: 3.2  Paragraph Reader\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 6  Concluding Remarks\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1.3  CNN/DailyMail\n",
            "\u001b[35mSection_title: 2.1.4  Children's Book Test\n",
            "\u001b[35mSection_title: 2.1.5  SQuAD\n",
            "\u001b[35mSection_title: 2.2  Complex-reasoning MRC Tasks\n",
            "\u001b[35mSection_title: 2.2.1  MS MARCO\n",
            "\u001b[35mSection_title: 3.1  Architecture Designs for MRC Models\n",
            "\u001b[35mSection_title: 3.1.1  Simple-reasoning MRC Models\n",
            "\u001b[35mSection_title: 3.1.2  Complex-reasoning MRC Models\n",
            "\u001b[35mSection_title: 3.2  Attention Mechanisms for MRC Models\n",
            "\u001b[35mSection_title: 3.2.1  Mutual-matching Attention\n",
            "\u001b[35mSection_title: 3.2.2  Self-matching Attention\n",
            "\u001b[35mSection_title: 3.2.3  Multi-round Attention\n",
            "\u001b[35mSection_title: 3.3  Performance-boosting Approaches for MRC Models\n",
            "\u001b[35mSection_title: 3.3.1  Linguistic Embeddings\n",
            "\u001b[35mSection_title: 3.3.2  Multi-round Reasoning\n",
            "\u001b[35mSection_title: 3.3.3  Reinforcement Learning\n",
            "\u001b[35mSection_title: 3.3.4  Data Augmentation\n",
            "\u001b[35mSection_title: 4  Transfer Learning in MRC Models\n",
            "\u001b[35mSection_title: 4.1.1  Context Vectors\n",
            "\u001b[35mSection_title: 4.1.2  Embeddings from Language Models\n",
            "\u001b[35mSection_title: 4.2  Fine-tuning-based Transfer Learning\n",
            "\u001b[35mSection_title: 4.2.1  Generative Pre-training for Transformers\n",
            "\u001b[35mSection_title: 5  Knowledge Base Encoding in MRC Models\n",
            "\u001b[32mconclusion: this report is a survey on the existing tasks and models in mrc. for mrc tasks, some representative simple-reasoning and complex-reasoning tasks are introduced,\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Report\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Protocol\n",
            "\u001b[35mSection_title: 4.  Annotation Platform\n",
            "\u001b[35mSection_title: 4.1.  Existing Platforms\n",
            "\u001b[35mSection_title: 4.2.  PIAFAnno System Architecture\n",
            "\u001b[35mSection_title: 2.  Answer selection.\n",
            "\u001b[35mSection_title: 5.  Participatory Approach\n",
            "\u001b[35mSection_title: 6.  Dataset Analysis\n",
            "\u001b[32mconclusion: motivated by the scarcity of non-english data, we described our ongoing effort towards gathering native qa samples for the french language, using a participatory approach. rather than a transactional approach to data collection, as usually adopted in crowd-sourcing efforts, we experiment with a comparatively slower and more engaging process, focusing on quality over quantity. amongst desirable sideeffects of our approach, we highlight the educational aspects, e.g. introducing a wider audience to ai concepts and methodologies during our annothathons.  table 3: n = 191 randomly sampled triplets were manually assigned into one or more of the above categories. words relevant to the corresponding reasoning type are in bold, and the annotated answer is underlined.  \n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\u001b[35mSection_title: 8.  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  MOTIVATION 1.Background\n",
            "\u001b[35mSection_title: 1.2  Perspectives\n",
            "\u001b[35mSection_title: 2  OBJECTIVES\n",
            "\u001b[35mSection_title: 3  RELEVANCE TO THE IR COMMUNITY\n",
            "\u001b[35mSection_title: 4  TOPICS 4.1 QA over Knowledge Graphs\n",
            "\u001b[35mSection_title: 4.2  QA over Text\n",
            "\u001b[35mSection_title: 4.2.1  Early efforts.\n",
            "\u001b[35mSection_title: 4.2.2  Machine reading comprehension (MRC)\n",
            "\u001b[35mSection_title: 4.2.3  Open-domain QA.\n",
            "\u001b[35mSection_title: 4.3  QA over Heterogeneous Sources\n",
            "\u001b[35mSection_title: 4.4  New Horizons in QA\n",
            "\u001b[35mSection_title: 4.4.2  Feedback and interpretability.\n",
            "\u001b[35mSection_title: 4.4.3  Clarification questions.\n",
            "\u001b[35mSection_title: 5  FORMAT AND SUPPORT\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Models\n",
            "\u001b[35mSection_title: 2.1  Pre-trained Language Models\n",
            "\u001b[35mSection_title: 2.2  Probing setups\n",
            "\u001b[35mSection_title: 2.3  Baseline Models\n",
            "\u001b[35mSection_title: 3  Controlled Experiments\n",
            "\u001b[35mSection_title: 3.1  Zero-shot Experiments with MC-MLM\n",
            "\u001b[35mSection_title: 3.2  Learning Curves\n",
            "\u001b[35mSection_title: 3.3  Controls\n",
            "\u001b[35mSection_title: 3.4  Multi-Choice Question Answering\n",
            "\u001b[35mSection_title: 4  The oLMpic Games\n",
            "\u001b[35mSection_title: 4.1  Can LMs perform robust comparison?\n",
            "\u001b[35mSection_title: 4.3  Do LMs Capture Negation?\n",
            "\u001b[35mSection_title: 4.4  How well do LMs handle conjunctions of facts?\n",
            "\u001b[35mSection_title: 4.5  Can language models perform multi-hop reasoning?\n",
            "\u001b[35mSection_title: 5  Medals\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Model\n",
            "\u001b[35mSection_title: 2.1  Baseline\n",
            "\u001b[35mSection_title: 2.2  Multi-Step Inference for ROPES\n",
            "\u001b[35mSection_title: 2.3  Multi-Step Reranker\n",
            "\u001b[35mSection_title: 3  Data bias in ROPES\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.3  Analysis and Discussion\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we propose a multi-step reading comprehension model that performs chained inference over natural language text. we have demonstrated that our model substantially outperforms prior work on ropes, a challenging new reading comprehension dataset. we have additionally presented some analysis of ropes that should inform future work on this dataset. while our model is not a neural module network, as our model uses a single fixed layout instead of different layouts per question, we believe there are enough similarities that future work could explore combining our modules with those used in other neural module networks over text, leading to a single model that could perform the necessary reasoning for multiple different datasets.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Knowledge-intensive NLP Tasks\n",
            "\u001b[35mSection_title: 2.2  Retrieval models\n",
            "\u001b[35mSection_title: 2.3  Reader models\n",
            "\u001b[35mSection_title: 3  Search Infrastructure\n",
            "\u001b[35mSection_title: 4  Experimental setup\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  KILT on SPHERE\n",
            "\u001b[35mSection_title: 5.2  Universal web retrieval\n",
            "\u001b[35mSection_title: 5.3  Common Sense Tasks\n",
            "\u001b[35mSection_title: 7  Discussion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  CCPairs: A Large Collection of Text Pair Dataset\n",
            "\u001b[35mSection_title: 4  Method\n",
            "\u001b[35mSection_title: 4.1  Contrastive Pre-training with Unlabeled Data\n",
            "\u001b[35mSection_title: 4.2  Fine-tuning with Labeled Data\n",
            "\u001b[35mSection_title: 4.3  Applications to Text Embedding Tasks\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Pre-training and Fine-tuning Configurations\n",
            "\u001b[35mSection_title: 5.2  Evaluation Datasets\n",
            "\u001b[35mSection_title: 5.3  Results on BEIR benchmark\n",
            "\u001b[35mSection_title: 5.4  Results on MTEB benchmark\n",
            "\u001b[35mSection_title: 5.5  Analysis\n",
            "\u001b[32mconclusion: in this work, we train a general-purpose text embedding model e5 from weak supervision signals. we adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. e5 offers strong off-the-shelf performance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. when further customized for downstream tasks, e5 achieves superior fine-tuned performance compared to existing embedding models with 40× more parameters on the large, 56-task mteb benchmark datasets. table 9: details for each data source after filtering. the \"others\" category includes \"sim-plewiki\", \"gooaq\", \"wikihow\", \"yahoo answers\" from https://huggingface.co/datasets/ sentence-transformers/embedding-training-data. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Semantic Role Labeling\n",
            "\u001b[35mSection_title: 3  Model Description\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  Paragraph Selection\n",
            "\u001b[35mSection_title: 3.2.1  First Round Paragraph Selection\n",
            "\u001b[35mSection_title: 3.2.2  Second Round Paragraph Selection\n",
            "\u001b[35mSection_title: 3.3  Heterogeneous SRL Graph Construction\n",
            "\u001b[35mSection_title: 3.4  Graph Encoder\n",
            "\u001b[35mSection_title: 3.5  Supporting Fact Prediction\n",
            "\u001b[35mSection_title: 3.6  Answer Span Prediction\n",
            "\u001b[35mSection_title: 3.7  Objective Function\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 6  Error Analysis\n",
            "\u001b[32mconclusion: we proposed a novel semantic role labeling graph reasoning network (srlgrn) to deal with multihop qa. the backbone graph of our proposed graph convolutional network (gcn) is created based on the semantic structure of the sentences. in creating the edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts. the cross paragraph argument-predicate structure of the sentences expressed in the graph provides an explicit representation of the reasoning path and helps in both finding and explaining the multiple hops of reasoning that lead to the final answer. srlgrn exceeds most of the sota results on the hotpotqa benchmark. moreover, we evaluate the model (excluding the paragraph selection module) on other reading comprehension benchmarks. our approach achieves competitive performance on squad v1.1 and v2.0.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 4  Automatic Dataset Construction\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Evaluation of Question Quality\n",
            "\u001b[35mSection_title: 5.3  Controllability Analysis\n",
            "\u001b[35mSection_title: 5.4  Boosting Multi-hop QA Performance\n",
            "\u001b[35mSection_title: 5.5  More-hop Question Generation\n",
            "\u001b[32mconclusion: we explored the task of difficulty-controllable question generation, with question difficulty redefined as the inference steps required to answer it. a step-by-step generation framework was proposed to accomplish this objective, with an input sampler to extract the reasoning chain, a question generator to produce a simple question, and a question rewriter to further adapt it into a more complex one. a dataset was automatically constructed based on hotpotqa to facilitate the research. extensive evaluations demonstrated that our method can effectively control difficulty of the generated questions, and keep high question quality at the same time.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Preliminaries\n",
            "\u001b[35mSection_title: 3.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Background\n",
            "\u001b[35mSection_title: 4  UCB for Bandit Learning\n",
            "\u001b[35mSection_title: 5  Collaborative UCB for Dueling Bandits\n",
            "\u001b[35mSection_title: 5.1  Co-UCB\n",
            "\u001b[35mSection_title: 5.2  Noise Simulation\n",
            "\u001b[35mSection_title: 6.2  Main Results\n",
            "\u001b[35mSection_title: 6.3  Noise Simulation\n",
            "\u001b[35mSection_title: 6.4  Further Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Overview\n",
            "\u001b[35mSection_title: 2.2  Base Generator G\n",
            "\u001b[35mSection_title: 2.3  Mining Silver Evidentiality E silver\n",
            "\u001b[35mSection_title: 2.4  Multi-task Learning with E silver\n",
            "\u001b[35mSection_title: 3  Experimental Setups\n",
            "\u001b[35mSection_title: 3.1  Tasks, Datasets, and Metrics\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 3.3  Hyper parameters\n",
            "\u001b[35mSection_title: 4  Results and Analysis\n",
            "\u001b[35mSection_title: 4.1  Task Results\n",
            "\u001b[35mSection_title: 4.2  Analysis\n",
            "\u001b[35mSection_title: 4.2.1  Ablation Study\n",
            "\u001b[35mSection_title: 4.2.2  Evaluating Evidentiality Labels\n",
            "\u001b[35mSection_title: 4.2.3  Qualitative evaluation of G and G +\n",
            "\u001b[35mSection_title: 4.2.4  Performance on Hard Subsets\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: augmenting pre-trained generation models with retrievers has shown to be effective in many knowledge-intensive tasks; however, they often rely on spurious cues or generate hallucinations during inference. we introduce a multi-task learning objective the combines answer generation and evidentiality prediction. we propose task-agnostic data mining techniques to obtain silver evidentiality labels to enable this auxiliary training. our experiments across five datasets show large performance improvements over baselines and our evidentialityguided generator advances the state-of-the-art performance on faviq-ambig, fever and wow. our analysis shows that multi-task learning and silver evidentiality mining both contribute to the performance improvements by helping the model learn to focus on and generate answers from more relevant passages. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A Latent-Skill Multitask Model\n",
            "\u001b[35mSection_title: 2.1  Soft Partitions\n",
            "\u001b[35mSection_title: 2.2  Skill-specific Parameters\n",
            "\u001b[35mSection_title: 2.3  Inductive Biases\n",
            "\u001b[35mSection_title: 2.4  Parameter Efficiency\n",
            "\u001b[35mSection_title: 2.5  Baselines\n",
            "\u001b[35mSection_title: 3  Reinforcement Learning Experiments\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Experimental Setup\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 4  Supervised Learning Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we argued that a modular design is crucial to ensure that neural networks can learn from a few examples and generalise robustly across tasks by recombining autonomous facets of knowledge. to this end, we proposed a model where a subset of latent, discrete skills from a fixed inventory is allocated to each task in an end-to-end fashion. the task-specific instantiation of a neural network is then obtained by combining efficient parameterisations of the active skills, such as sparse or low-rank adapters. we evaluate the sample efficiency of our model on multitask instruction following through reinforcement learning and its few-shot adaptability on multitask text-to-text generation through supervised learning. in both experiments, we surpass competitive baselines such as conditional parameter generation (hyperformer) and mixture of experts (task-moe). finally, we show that modularity helps interpret multi-task models by inferring explicit relationships between tasks according to the skills they share.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Background\n",
            "\u001b[35mSection_title: 3  The StepGame Dataset\n",
            "\u001b[35mSection_title: 3.1  Template Collection\n",
            "\u001b[35mSection_title: 3.2  Data Generation\n",
            "\u001b[35mSection_title: 3.3  Distracting Noise\n",
            "\u001b[35mSection_title: 4  The TP-MANN Model\n",
            "\u001b[35mSection_title: 5  Experiments and Results\n",
            "\u001b[35mSection_title: 5.1  Material and Baselines\n",
            "\u001b[35mSection_title: 5.2  Training-Test Leakage\n",
            "\u001b[35mSection_title: 5.3  Spatial Inference\n",
            "\u001b[35mSection_title: 5.4  Systematic Generalization\n",
            "\u001b[35mSection_title: 5.5  Inference Analysis\n",
            "\u001b[32mconclusion: in this paper, we proposed a new dataset named stepgame that requires a robust multi-hop spatial reasoning ability to be solved and mitigates the issues observed in the babi dataset. then, we introduced tp-mann, a tensor productbased memory-augmented neural network architecture that achieves state-of-the-art performance on both datasets. further analysis also demonstrated the importance of a recurrent memory module for multi-hop reasoning.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Multiple-choice question generation\n",
            "\u001b[35mSection_title: 4  Assessment Framework\n",
            "\u001b[35mSection_title: 4.1  Grammar\n",
            "\u001b[35mSection_title: 4.2  Answerability\n",
            "\u001b[35mSection_title: 4.3  Diversity\n",
            "\u001b[35mSection_title: 4.4  Complexity\n",
            "\u001b[35mSection_title: 5.2  Setup\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Machine reading comprehension\n",
            "\u001b[35mSection_title: 6.2  Question complexity\n",
            "\u001b[35mSection_title: 6.3  Question generation\n",
            "\u001b[32mconclusion: this work aims to propose a sensible assessment framework for multiple choice question generation in order to encourage the question generation community to consider more appropriate methods of assessing and benchmarking developed systems to reflect the qualities of interest rather than arbitrary n-gram based approaches. here, the first fully automated end-to-end multiple-choice question generation system is proposed for generating a question, the correct answer and distractor options for an input context without relying on explicit phrase extraction based techniques.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\u001b[32mlimitation: here, the limitations of the current approaches are discussed. first, both measures of unanswerability and question complexity are model and corpusspecific. hence, it is not clear about the applicability of these metrics beyond the race++ dataset. specifically, the unanswerability measure runs the risk of conflating answerability with the failure of a reading comprehension question. the validity of such metrics is centred on in-domain data, which may be diminishingly effective at discriminating the performance of generative models on shifted data (e.g the nature of questions in reclor is more logical-based and requires higher inference than questions in race++, which might make measures trained on race++ struggle at assessment on questions generated in the reclor style). no human evaluation is performed of whether the assessment metrics correlate explicitly with human notions of answerability and complexity. hence, further work should invest resources to comprehensively establish the validity of the proposed measures. finally, the question complexity system is trained specifically on the meaning of complexity as described for race++. however, question complexity has several definitions and hence further work should establish whether this interpretation of complexity may align with other views of complexity. .\n",
            "\u001b[35mSection_title: 8  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS 2.1 Question Answering with Knowledge Extraction\n",
            "\u001b[35mSection_title: 2.2  Multi-Step Datasets and Reasoning\n",
            "\u001b[35mSection_title: 2.3  Query Reformulation\n",
            "\u001b[35mSection_title: 3  TASK DEFINITION\n",
            "\u001b[35mSection_title: 4  THE PROPOSED FRAMEWORK (MURKE)\n",
            "\u001b[35mSection_title: 4.1  Knowledge Extraction\n",
            "\u001b[35mSection_title: 4.2  Iterative Multi-Step Reasoning\n",
            "\u001b[35mSection_title: 4.2.1  Selection Module.\n",
            "\u001b[35mSection_title: 4.2.2  Latent Question Reformulation\n",
            "\u001b[35mSection_title: 4.2.3  Entailment Module.\n",
            "\u001b[35mSection_title: 4.2.4  Multi-\n",
            "\u001b[35mSection_title: 5  EXPERIMENT\n",
            "\u001b[35mSection_title: 5.1  Dataset\n",
            "\u001b[35mSection_title: 5.2  Evaluation on Reasoning Ability\n",
            "\u001b[35mSection_title: 5.2.2  Baselines.\n",
            "\u001b[35mSection_title: 5.2.3  Results. Unsupervised Setting\n",
            "\u001b[35mSection_title: 5.2.4  Influence of the number of Reasoning\n",
            "\u001b[35mSection_title: 5.2.5  Performance using multi-modality fusion.\n",
            "\u001b[35mSection_title: 5.3  Evaluation on Knowledge Extraction\n",
            "\u001b[35mSection_title: 5.3.1  Influence of different supporting document scale.\n",
            "\u001b[35mSection_title: 5.4  Interpretable Ability of MurKe\n",
            "\u001b[32mconclusion: in this paper, we present a system murke that answers healthcare exam questions by using knowledge extraction and multi-step reasoning. to get a relevant document for each question, murke retrieves supporting documents from a large, noisy corpus on the basis of keywords extracted from the original question and semantic retrieval. murke proposes the multi-step iterative method to solve complex healthcare qa, which uses information selected by combining iterative question reformulation and textual entailment. our neural architecture uses a sequence of token-level attention mechanisms to extract relevant evidence from the selected documents in order to update the latent representation of the question, which shows the interpretability of the reasoning path. through empirical results and case study, we demonstrate that our proposed system is able to outperform several strong baselines on the headqa dataset.\n",
            "\u001b[35mSection_title: 6  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  An Annotated Corpus for COVID-19 Event Extraction\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Annotation Process\n",
            "\u001b[35mSection_title: 3.2.1  Two-phase Annotation\n",
            "\u001b[35mSection_title: 3.2.2  Inter-annotator Agreement\n",
            "\u001b[35mSection_title: 3.3  Corpus Analysis\n",
            "\u001b[35mSection_title: 4  Automatic Event Extraction\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  COVIDKB Knowledge Base\n",
            "\u001b[35mSection_title: 5.1  COVIDKB Overview\n",
            "\u001b[35mSection_title: 5.2  COVIDKB Evaluation\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[35mSection_title: 6.1  Correlation with Official Data Sources\n",
            "\u001b[35mSection_title: 6.2  Analyzing Claimed Cures and Preventions\n",
            "\u001b[32mconclusion: in this paper, we presented a corpus of 10,000 tweets annotated with 5 types of events and 28 slots. we showed that our corpus supports automatic extraction of covid-19 events using supervised learning. by aggregating extractions over millions of tweets, our approach can accurately answer a range of structured queries about events that are publicly reported in real-time on twitter. our knowledge base could be a useful tool for epidemiologists, journalists and policymakers to more efficiently track the spread of this new disease. this work also presents a case-study on how an information extraction system can be rapidly developed for a new domain in response to an emerging crisis. for example, our methodology could be applied to develop knowledge bases for natural disasters (spiliopoulou et al., 2020) or future disease outbreaks.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Proposed Model: TASER\n",
            "\u001b[35mSection_title: 4.2  Out-of-Domain Evaluation\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we propose a new parameterization framework, taser, for improving the efficiency and robustness of dense retrieval for odqa. it interleaves shared encoder blocks with specialized ones in a single encoder where some sub-networks are task-specific. as the specialized sub-networks are sparsely activated, taser can provide better parameter efficiency with almost no additional computation cost. experiments show that taser substantially outperforms existing fully supervised biencoder dense retrievers on both in-domain and out-of-domain generalization.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: in this section, we point out several limitations in this work.first, our in-domain evaluation experiments focus on passage retrieval for odqa. while the dense retriever is mostly successful in odqa, it can be also used in other types of retrieval tasks which may have different input and output format. for example, the kilt benchmark (petroni et al., 2021) provides several knowledge-intensive tasks other than odqa. the performance of taser models trained on such retrieval tasks remain unknown.second, compared with traditional sparse vector models like tf-idf and bm25, the cost of training is an inherent issue of dense retrievers. although taser significantly reduce the number of model parameters, the training cost is still high.third, in our experiments, we show that the learned routing does not outperform the deterministic routing. this may suggest a better architecture and/or training algorithms for learned routing is needed to fully unleash the power of moe.last, as observed in §4.2, there is still a gap between taser and bm25 in out-of-domain evaluation. therefore, how to close this gap will remain a critical topic for future work on dense retrievers. \n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK 2.1 Pre-training and Language Models\n",
            "\u001b[35mSection_title: 2.2  Open-Domain Long-Form Question-Answering\n",
            "\u001b[35mSection_title: 2.3  Traditional Chinese Poem Generation\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Baseline: Prompting and Beam Search\n",
            "\u001b[35mSection_title: 3.2  Inverse Prompting\n",
            "\u001b[35mSection_title: 4  IMPLEMENTATION\n",
            "\u001b[35mSection_title: 4.1  Base Language Model\n",
            "\u001b[35mSection_title: 4.2  Open-Domain Long-Form Question-Answering\n",
            "\u001b[35mSection_title: 4.3  Open-Domain Poem Generation\n",
            "\u001b[35mSection_title: 4.4  Self Training for Poem Generation\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  Human Evaluation Protocol\n",
            "\u001b[35mSection_title: 5.2  Open-domain long-form Chinese QA\n",
            "\u001b[35mSection_title: 5.3  Open-domain Poem Generation\n",
            "\u001b[35mSection_title: 5.4  Poem Turing Test\n",
            "\u001b[35mSection_title: 5.5  Case Study\n",
            "\u001b[35mSection_title: 5.6  Inverse Prompting for English(GPT-2)\n",
            "\u001b[32mconclusion: in this paper, we present a new method, inverse prompting for text generation. inverse prompting offers a new option for controllable generation using language models by exploiting the inverse form of natural languages.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\u001b[35mSection_title: 7  APPENDIX 7.1 Implementation Details\n",
            "\u001b[35mSection_title: 7.2  Human Evaluation Details\n",
            "\u001b[35mSection_title: 7.3  Deviation for Human Evaluators and\n",
            "\u001b[35mSection_title: 7.4  Online Demo Platforms\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Explainable Machine Learning\n",
            "\u001b[35mSection_title: 2.1.  Highlighted Studies\n",
            "\u001b[35mSection_title: 3.  Philosophy and Theories of Explanation\n",
            "\u001b[35mSection_title: 4.  The Interventionist Account\n",
            "\u001b[35mSection_title: 5.  Applying the Interventionist Account\n",
            "\u001b[35mSection_title: 5.1.  Does the Account Apply?\n",
            "\u001b[35mSection_title: 5.2.  Is Attention Manipulation Surgical?\n",
            "\u001b[35mSection_title: 5.3.  Consequences of failed causal explanation\n",
            "\u001b[35mSection_title: 6.  Toward Non-Causal Explanation\n",
            "\u001b[32mlimitation: while our study of explanation is based in philosophy, our contributions are based in epistemology, not in ethics. many adjacent subfields of philosophy of science exist and only occasional interactions between computer scientists and philosophers have taken place to date (miller, 2018;zerilli et al., 2018). in this work, we have examined whether researchers or users are being given a true explanation. but a good explanation does not mean that an algorithm has made a good decision. explainability research frequently studies tasks with high stakes, including notoriously biased tasks like recidivism prediction, financial risk modeling, and facial recognition for surveillance. our work does not absolve researchers from a broader social responsibility: the presence of a successful explanation will not help if a loan is denied because of race (fuster et al., 2018), if an accused criminal is wrongly identified because of their gender presentation (buolamwini and gebru, 2018), or if algorithms persecute ethnic groups (wang et al., 2016) or misdiagnose mental health (bennett and keyes, 2019).to build algorithmic decision-making in a truly socially responsible way, our work must be a component piece, incorporated into a broader foundation that accounts not only for explanation but also for ethical software development. this work provides a vocabulary for computer scientists struggling to unify the informal language that proliferates across research today, and will allow nlp researchers to improve the quality and rigor of their explanations, and provide a sure footing for the field.\n",
            "\u001b[35mSection_title: 6.1.  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  CHALLENGES\n",
            "\u001b[35mSection_title: 4  INTUITION\n",
            "\u001b[35mSection_title: 5  NOTATION AND FORMULATION\n",
            "\u001b[35mSection_title: 6  OUR APPROACH: REINFORCING RETROSPECTIVE LANGUAGE AGENT\n",
            "\u001b[35mSection_title: 6.1  RETROSPECTIVE AGENT ARCHITECTURE\n",
            "\u001b[35mSection_title: 6.2  POLICY GRADIENT OPTIMIZATION\n",
            "\u001b[35mSection_title: 7  EXPERIMENTS\n",
            "\u001b[35mSection_title: 7.1  ENVIRONMENT AND DATASET\n",
            "\u001b[35mSection_title: 7.2  IMPLEMENTATION DETAILS\n",
            "\u001b[35mSection_title: 7.3  COMPARISONS WITH BASELINES\n",
            "\u001b[35mSection_title: 7.4  DISCUSSION OF THE REINFORCED AGENT BEHAVIORS\n",
            "\u001b[32mconclusion: in this study, we present retroformer, an elegant framework for iteratively improving large language agents by learning a plug-in retrospective model. this model, through the process of policy optimization, automatically refines the prompts provided to the language agent with environmental feedback. through extensive evaluations on real-world datasets such as hotpotqa, the method has been proven to effectively improve the performances of large language agents over time both in terms of learning speed and final task completion performances.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Retrieval-augmented Generation\n",
            "\u001b[35mSection_title: 3.2  Meta-training\n",
            "\u001b[35mSection_title: 3.3  Demonstration Memory\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Hyperparameters\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Knowledge-intensive QA\n",
            "\u001b[35mSection_title: 5.2  Ablations\n",
            "\u001b[32mconclusion: we have proposed a meta-training method ( §3.2) that retrieves ( §3.1) semantically similar demonstrations from a diverse demonstration bank ( §3.3). our method achieves higher performance on average across many tasks than other strong parameterefficient few-shot baselines ( §5). in future work, one could explore a mixture of demonstration retrieval and passage retrieval for improved performance on a wider variety of tasks-including knowledge-intensive tasks.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Graph-Induced Transformer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Settings\n",
            "\u001b[35mSection_title: 4.2  Graph Replacement with GIT\n",
            "\u001b[35mSection_title: 4.3  Low-resource Scenario\n",
            "\u001b[32mconclusion: this work presented git, the graph-induced transformer that drastically improved mhqa models' sample efficiency and replaces graphs in the models while retaining their performance. our empirical evidences demonstrated that models can enjoy the benefits of connective inductive bias of graphs without additional graph modules in place. the design of git also allowed us to reuse the parameters of plm while incorporating the graph information. future directions of our work may include using git in downstream nlp applications where the graph inductive bias is necessary and dataset is scarce.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Preliminary Basic Decoding Methods\n",
            "\u001b[35mSection_title: 4  Beam Retrieval\n",
            "\u001b[35mSection_title: 5  Experimental Setup Datasets\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[32mconclusion: we present beam retrieval, a general end-to-end retrieval framework for multi-hop qa. this approach maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. experimental results on three benchmark datasets prove the effectiveness of beam retrieval and demonstrate it could substantially improve the qa performance of downstream reader. in general, beam retrieval establishes a strong baseline for complex multi-hop qa, where we hope that future work could explore more advanced solutions.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 3  Reading list\n",
            "\u001b[35mSection_title: 4  Tutorial outline\n",
            "\u001b[35mSection_title: 5  Diversity efforts\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Equations as Intermediate Meaning Representation\n",
            "\u001b[35mSection_title: 2.2  Generation Accuracy of Different Intermediate Meaning Representations\n",
            "\u001b[35mSection_title: 2.3  Pipeline of BRIDGE\n",
            "\u001b[35mSection_title: 2.3.1  Erase\n",
            "\u001b[35mSection_title: 2.3.2  Decompose\n",
            "\u001b[35mSection_title: 2.3.3  Translate\n",
            "\u001b[35mSection_title: 2.3.4  Answer\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experiment Setup\n",
            "\u001b[35mSection_title: 3.2  Main Result\n",
            "\u001b[35mSection_title: 3.3  Ablation Study\n",
            "\u001b[35mSection_title: 3.3.1  Answer Generation\n",
            "\u001b[35mSection_title: 3.3.2  Equation Generation\n",
            "\u001b[35mSection_title: 3.4  Analysis\n",
            "\u001b[35mSection_title: 3.5  Case Study\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 4.1  Numerical Reasoning with LLMs\n",
            "\u001b[35mSection_title: 4.2  Reasoning with Intermediate Meaning Representation\n",
            "\u001b[32mconclusion: to theoretically and empirically advance the numerical reasoning research in the llms era, in this paper, we employ equations as imrs to solve the numerical reasoning task by addressing two problems: (1) theoretically, how to prove that the equation is an imr with higher generation accuracy than programs; (2) empirically, how to improve the generation accuracy of equations with llms. for the first problem, we present and prove a proposition to compare the generation accuracy of different   \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.1  Adiabatic perturbations in rotational shear flow\n",
            "\u001b[35mSection_title: 2.1.1  The model and basic equations\n",
            "\u001b[35mSection_title: 2.1.2  Types of perturbations\n",
            "\u001b[35mSection_title: 2.1.3  On the perturbation modes\n",
            "\u001b[35mSection_title: 2.1.4  On measurements of perturbations\n",
            "\u001b[35mSection_title: 2.2  Local approximation: transition to shear harmonics\n",
            "\u001b[35mSection_title: 2.2.1  Transition to shear harmonics\n",
            "\u001b[35mSection_title: 2.2.2  Potential vorticity\n",
            "\u001b[35mSection_title: 2.2.3  Inhomogeneous wave equations. Density waves and vortices\n",
            "\u001b[35mSection_title: 2.2.4  Amplification of the density waves\n",
            "\u001b[35mSection_title: 2.2.5  The vortex existence criterion\n",
            "\u001b[35mSection_title: 2.2.6  Vortex solution\n",
            "\u001b[35mSection_title: 2.3  Vortex amplification factor\n",
            "\u001b[35mSection_title: 2.3.1  On the transient growth mechanism\n",
            "\u001b[35mSection_title: 2.3.2  Estimation of the optimal growth\n",
            "\u001b[35mSection_title: 2.3.3  Account for the viscosity\n",
            "\u001b[35mSection_title: 3.1  Definition and properties of singular vectors\n",
            "\u001b[35mSection_title: 3.1.1  Continuity of a dynamical operator\n",
            "\u001b[35mSection_title: 3.1.2  Completely continuous dynamical operator\n",
            "\u001b[35mSection_title: 3.1.3  Linear operators: from the particular to the general\n",
            "\u001b[35mSection_title: 3.1.4  Simple geometrical example on the non-orthogonality of eigenvectors\n",
            "\u001b[35mSection_title: 3.1.5  Singular vectors\n",
            "\u001b[35mSection_title: 3.2  Matrix method for optimal solutions\n",
            "\u001b[35mSection_title: 3.2.1  Illustration of the matrix method\n",
            "\u001b[35mSection_title: 3.2.2  Background flow\n",
            "\u001b[35mSection_title: 3.2.3  The modes\n",
            "\u001b[35mSection_title: 3.2.4  Optimal growth\n",
            "\u001b[35mSection_title: 3.3  Alternative: a variational approach\n",
            "\u001b[35mSection_title: 3.3.1  Linear autonomous operators\n",
            "\u001b[35mSection_title: 3.3.2  Linear non-autonomous operators\n",
            "\u001b[35mSection_title: 3.3.3  Calculation of the consecutive singular vectors\n",
            "\u001b[35mSection_title: 3.3.4  Generalization to non-linear case\n",
            "\u001b[35mSection_title: 3.4  Adjoint equations\n",
            "\u001b[35mSection_title: 3.4.1  Derivation of adjoint equations\n",
            "\u001b[35mSection_title: 3.4.2  Non-normality condition for Z\n",
            "\u001b[35mSection_title: 4.2  Global problem\n",
            "\u001b[35mSection_title: 4.2.1  Comparison of the transient growth of vortices in global and local space limits\n",
            "\u001b[35mSection_title: 4.2.2  Transient spirals and modes in a quasi-Keplerian torus\n",
            "\u001b[32mconclusion: this review is devoted to the transient dynamics of perturbations, which is of special interest in theory of astrophysical disks, in particular accretion disks. exponentially growing perturbations do not exist in a homogeneous inviscid keplerian flow provided that there are no conditions for the magneto-rotational instability. nevertheless, observations suggest that also in this case angular momentum should be somehow transported outwards. at least, this implies that there should be some mechanism of energy transfer from the regular rotational motion to hydrodynamical perturbations. in spectrally stable flows the transient growth mechanism is responsible for this. here it was introduced by a simple example of two-dimensional vortices and it was discussed that the reason for their growth is the shortening of the length of leading spirals by the differential rotation of the flow (see fig. 2 and 3). nonwithstanding their seeming simplicity, those (quasi-)columnar structures exhibit the strongest ability to extract energy from the spectrally stable differentially rotating flows (see [54] about it). physically, the energy growth of vortices takes place due to their own angular momentum conservation, which in the local limit is expressed by the conservation of their potential vorticity and the existence of the invariant i (see section 2.2). here we considered both small-scale (k y ≫ 1) and large-scale (k y ≪ 1) vortices and compared their optimal growth with account for non-zero effective viscosity in the disk (see fig. 4). importantly, the transient growth of large-scale vortices strongly increases for a super-keplerian rotation, which can be significant in relativistic disks where q > 3/2. in this paper, special attention was paid to mathematical aspects of non-modal analysis and to methods of optimal perturbations computation. we have discussed in detail that the transient growth is a consequence of non-normality of the governing dynamical operator of the problem and non-orthogonality of its eigenvectors, i.e. modes of perturbations (see fig. 5 and 6). therefore, the growth of arbitrary perturbations can be adequately studied by calculating not eigenvectors but singular vectors of this operator. we have considered two methods: a matrix and variational one and applied them to the particular problems (see the corresponding results in fig. 7 and 11). the matrix method requires a discrete representation of the dynamical operator, for example, in the basis of its eigenvectors. the variational method is reduced to iterative integration of the system of direct and adjoint equations forward and backward in time, respectively. we have emphasized that the variational method is more universal and can be applied to study of non-modal dynamics of perturbations in non-stationary flows, as well as to non-linear problems.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Setting\n",
            "\u001b[35mSection_title: 3.1  Data Usage\n",
            "\u001b[35mSection_title: 3.2  Training Procedure\n",
            "\u001b[35mSection_title: 3.3  Evaluation Protocol\n",
            "\u001b[35mSection_title: 4  Task-level MoE Transformers\n",
            "\u001b[35mSection_title: 5.2  Experiment Details\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 6.1  Correlation with Hand Features\n",
            "\u001b[35mSection_title: 6.2  Expert Disabling Experiments\n",
            "\u001b[35mSection_title: 7  Few-shot Adaptation to Novel Tasks\n",
            "\u001b[35mSection_title: 7.1  Compared Methods\n",
            "\u001b[35mSection_title: 7.2  Experiment Details\n",
            "\u001b[35mSection_title: 7.3  Results\n",
            "\u001b[32mconclusion: in this paper, based on the observation that transformer models trained by massive tasks have better ability to generalize to unseen tasks, we hope to provide a new sight on exploring how this cross-task generalization ability is achieved and reused. inspired by the way that humans sparsely recall learned skills to solve new tasks, we explicitly model this process by resorting to a task-level mixture-of-expert model, where each expert represent different skills and tasks are routed by a router network based on the task property. we empirically investigate several importance design choices, i.e., routing models, expert selection strategies, task representations to exploring their influence on final model. secondly, by conducting a detailed analysis on the final routing decisions, we find it has a strongly correlation with human-defined task ontology (e.g., classification) and task characteristics (e.g., extractive, linguistic) even without any prior knowledge. we believe the result is valuable and promising in understanding the skills learned behind the black-box transformer models. model's parameters as the task embedding. the fim provides a measure of the information a particular parameter learns about the loss corresponding to the probe model, so it is disable to represent task (wang et al., 2021,vu et al., 2020b. given probe model m θ , for example bart-base, we first calculate the loglikelihood with respect to the model parameters(θ) as: p θ = log m θ (y | x), then fim is calculated as the covariance of gradients of the loglikelihood:\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  The Motivating Application Domain\n",
            "\u001b[35mSection_title: 1.2  Crowdsourcing Statistics with RAPPOR\n",
            "\u001b[35mSection_title: 1.3  RAPPOR and (Longitudinal) Attacks\n",
            "\u001b[35mSection_title: 2  The Fundamental RAPPOR Algorithm\n",
            "\u001b[35mSection_title: 2.1  RAPPOR Modifications\n",
            "\u001b[35mSection_title: 3  Differential Privacy of RAPPOR\n",
            "\u001b[35mSection_title: 3.1  Differential Privacy of the Permanent Randomized Response\n",
            "\u001b[35mSection_title: 3.2  Differential Privacy of the Instantaneous Randomized Response\n",
            "\u001b[35mSection_title: 4  High-utility Decoding of Reports\n",
            "\u001b[35mSection_title: 4.1  Parameter Selection\n",
            "\u001b[35mSection_title: 4.2  What Can We Learn?\n",
            "\u001b[35mSection_title: 5  Experiments and Evaluation\n",
            "\u001b[35mSection_title: 5.1  Reporting on the Normal Distribution\n",
            "\u001b[35mSection_title: 5.2  Reporting on an Exponentially-distributed Set of Strings\n",
            "\u001b[35mSection_title: 5.3  Reporting on Windows Process Names\n",
            "\u001b[35mSection_title: 5.4  Reporting on Chrome Homepages\n",
            "\u001b[32mlimitation: we consider three types of attackers with different capabilities for collecting rappor reports.the least powerful attacker has access to a single report from each user and is limited by one-time differential privacy level 1 on how much knowledge gain is possible. this attacker corresponds to an eavesdropper that has temporary ability to snoop on the users' reports.a windowed attacker is presumed to have access to one client's data over a well-defined period of time. this attacker, depending on the sophistication of her learning model, could learn more information about a user than the attacker of the first type. nevertheless, the improvement in her ability to violate privacy is strictly bounded by the longitudinal differential privacy guarantee of ∞. this more powerful attacker may correspond to an adversary such as a malicious cloud service employee, who may have temporary access to reports, or access to a time-bounded log of reports. the third type of attacker is assumed to have unlimited collection capabilities and can learn the permanent randomized response b with absolute certainty. because of the randomization performed to obtain b from b, she is also bounded by the privacy guarantee of ∞ and cannot improve upon this bound with more data collection. this corresponds to a worst-case adversary, but still one that doesn't have direct access to the true data values on the client.despite envisioning a completely local privacy model, one where users themselves release data in a privacy-preserving fashion, operators of rappor collections, however, can easily manipulate the process to learn more information than warranted by the nominal ∞. soliciting users to participate more than once in a particular collection results in multiple permanent randomized responses for each user and partially defeats the benefits of memoization. in the webcentric world, users use multiple accounts and multiple devices and can unknowingly participate multiple times, releasing more information than what they expected. this problem could be mitigated to some extent by running collections per account and sharing a common permanent randomized response. notice the role of the operator to ensure that such processes are in place and the required or assumed trust on the part of the user.it is likely that some attackers will aim to target specific users by isolating and analyzing reports from that user, or a small group of users that includes them. even so, some randomly-chosen users need not fear such attacks at all: with probability 1 2 f h , clients will generate a permanent randomized response b with all 0s at the positions of set bloom filter bits. since these clients are not contributing any useful information to the collection process, targeting them individually by an attacker is counter-productive. an attacker has nothing to learn about this particular user. also, for all users, at all times, there is plausible deniability proportional to the fraction of clients providing no information.in one particular attack scenario, imagine an attacker that is interested in learning whether a given client has a particular value v, whose population frequency is known to be fv. the strongest evidence in support of v comes in the form of both bloom filter bits for v being set in the client's report (if two hash functions are used). the attacker can formulate its target set by selecting all reports with these two bits set. however, this set will miss some clients with v and include other clients who did not report v. false discovery rate (fdr) is the proportion of clients in the target set who reported a value different from v. figure 7 shows fdr as a function of fv, the frequency of the string v. notably, for relatively rare values, most clients in the target set will, in fact, have a value that is different from v, which will hopefully deter any would-be attackers.the main reason for the high fdr rate at low frequencies fv stems from the limited evidence provided by the observed bits in support of v. this is clearly illustrated by figure 8 where the probability that v was reported (1) or not reported (0) by the client is plotted as a function of fv. for relatively rare strings (those with less than 10% frequency), even when both bits corresponding to v are set in the report, the probability of v being reported is much smaller than of value v given the two bits observed in a rappor report s corresponding to the two bits set by string v. for rare strings, even when both bits are set to 1 (green lines), it is still much more likely that the client did not report v, but some other value.it not being reported. because the prior probability fv is so small, a single client's reports cannot provide sufficient evidence in favor of v.\n",
            "\u001b[35mSection_title: 6  Attack Models and Limitations\n",
            "\u001b[35mSection_title: 6.1  Caution and Correlations\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[35mSection_title: 8  Summary\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Problem statement\n",
            "\u001b[35mSection_title: 1.2  Motivation\n",
            "\u001b[35mSection_title: 1.3  Contributions\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Dataset creation\n",
            "\u001b[35mSection_title: 3.1  Evaluation of generated questions\n",
            "\u001b[35mSection_title: 3.2  Annotation guidelines\n",
            "\u001b[35mSection_title: 3.3  Dataset analysis\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 5  Experiments, results, and discussion\n",
            "\u001b[35mSection_title: 5.1  Error analysis\n",
            "\u001b[35mSection_title: 5.2  Challenges of using SciBERT pre-trained embeddings\n",
            "\u001b[32mconclusion: in this paper, we present scienceqa, a novel dataset for benchmark evaluation of methods in the mrc (qa and qg in particular) task on scholarly articles. the dataset is created semi-automatically, consisting of over 100k triples of context-question-answer. the developed qa system could provide valuable evidence in managing the vast number of scholarly submissions. we offer a baseline and two more models, viz., (i). vanilla bert, (ii). science bert (i.e., scib-ert), and (iii). combination of bert and bi-daf. our proposed models are competitive compared to the existing state-of-the-art models. our future works would include:\n",
            "\u001b[35mSection_title: 6  Conclusion and future work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Epistemology & Language Models\n",
            "\u001b[35mSection_title: 3  EpiK-Eval\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Are LMs Type I or Type II Systems?\n",
            "\u001b[35mSection_title: 4.2  In-Depth Answer Analysis\n",
            "\u001b[35mSection_title: 4.3  Hallucinations\n",
            "\u001b[35mSection_title: 4.4  Effect of Scale\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: in this paper, we presented the epik-eval benchmark, a tool designed specifically to evaluate the proficiency of lms in consolidating their knowledge for problem-solving tasks.our findings underscore the limitations of current lms, which appear to mostly maintain a disjoint knowledge state of training observations.further, we note a significant performance gap and an increased rate of hallucinations for models trained on segmented narratives compared to those trained on unsegmented ones.we attribute these discrepancies to the training objectives of the models, which underscores the need to more effectively model the dependencies within the training corpus.by highlighting current limitations and opportunities for improving lms, these results delineate paths for future research, hopefully enabling the growth of language models beyond simple knowledge bases.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Federated Learning for NLP\n",
            "\u001b[35mSection_title: 2.1  Federated Learning Concepts\n",
            "\u001b[35mSection_title: 2.2  Our Unified Framework for FL\n",
            "\u001b[35mSection_title: 2.3  The Proposed FedNLP Framework\n",
            "\u001b[35mSection_title: 3  Benchmarking Setup with FedNLP\n",
            "\u001b[35mSection_title: 3.1  Task Formulations, Datasets, and Models\n",
            "\u001b[35mSection_title: 3.2  Non-IID Partitioning Strategies\n",
            "\u001b[35mSection_title: 4  Experimental Results and Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: our key contribution is providing a thorough and insightful empirical analysis of existing federated learning algorithms in the context of nlp models. notably, we compare typical fl methods for four nlp task formulations under multiple non-iid data partitions. our findings reveal both promise and the challenges of fl for nlp. in addition, we also provide a suite of resources to support future research in fl for nlp (e.g., a unifying framework for connecting transformer models with popular fl methods and different non-iid partition strategies). thus, we believe our wellmaintained open-source codebase to support future work in this area.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Directions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  LANGUAGE MODELING\n",
            "\u001b[35mSection_title: 3  LANGUAGE MODELING WITH CTRL\n",
            "\u001b[35mSection_title: 3.1  DATA\n",
            "\u001b[35mSection_title: 3.2  EXPERIMENTAL SETTINGS\n",
            "\u001b[35mSection_title: 4  CONTROLLABLE GENERATION\n",
            "\u001b[35mSection_title: 4.1  SAMPLING\n",
            "\u001b[35mSection_title: 4.2  CONTROL CODES\n",
            "\u001b[35mSection_title: 5  SOURCE ATTRIBUTION\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[35mSection_title: 7  FUTURE DIRECTIONS\n",
            "\u001b[35mSection_title: 8  CTRL-ALT-DEL: THE ETHICS OF LARGE LANGUAGE MODELS\n",
            "\u001b[32mconclusion: with 1.63 billion parameters, ctrl is the largest publicly released language model to date. it is trained with control codes so that text generation can be more easily controlled by human users. these codes allow users to explicitly specify domain, subdomain, entities, relationships between entities, dates, and task-specific behavior. we hope that the release of this model at https://github.com/salesforce/ctrl pushes towards more controllable, general models for natural language processing, and we encourage future discussion about artificial generation with our team by emailing ctrl-monitoring@salesforce.com.   table 3). for all the reddit data, the secondary code can be title: or text:, which is the title and text of the article, respectively.\n",
            "\u001b[35mSection_title: 9  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 28  thousand integrations at non-TA sites were identified by re-analyzing previous sequencing data\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Decomposing Complex Questions\n",
            "\u001b[35mSection_title: 2.1  Successive prompting\n",
            "\u001b[35mSection_title: 2.2  Training paradigm\n",
            "\u001b[35mSection_title: 3  Synthetic Dataset\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1  In-context Learning\n",
            "\u001b[35mSection_title: 4.2  Model Fine-tuning\n",
            "\u001b[35mSection_title: 4.3  In-context vs Fine-Tuning\n",
            "\u001b[35mSection_title: 4.4  Qualitative Examples\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we present a way to successively decompose complex questions into simple qa pairs, which allows for modular qd and qa systems that can be trained and queried independently. when performing in-context learning, we showed that successive prompting yields an improvement of 4.6 f1 over chain-of-thought prompting. when replacing just the in-context qa module with a fine-tuned one, which is adept at handling list type questions, we further improve the overall performance by 9.5 f1.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Motivation\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 3.1  News Article Curation\n",
            "\u001b[35mSection_title: 3.3  Machine Filtering\n",
            "\u001b[35mSection_title: 3.4  Human Filtering\n",
            "\u001b[35mSection_title: 4  Data Analysis\n",
            "\u001b[35mSection_title: 5  Evaluation\n",
            "\u001b[35mSection_title: 5.1  Methods\n",
            "\u001b[35mSection_title: 5.2  Human Performance\n",
            "\u001b[35mSection_title: 5.3  Metrics\n",
            "\u001b[35mSection_title: 5.4  Results\n",
            "\u001b[35mSection_title: 5.5  Analysis\n",
            "\u001b[35mSection_title: 6  Related Datasets\n",
            "\u001b[32mconclusion: we introduced record, a large-scale reading comprehension dataset requiring commonsense reasoning. unlike existing machine reading comprehension (mrc) datasets, record contains a large portion of queries that require commonsense reasoning to be answered. our baselines, including top performers on existing mrc datasets, are no match for human competence on record. we hope that record will spur more research in mrc with commonsense reasoning.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Distillers\n",
            "\u001b[35mSection_title: 3.2  Configurations and Presets\n",
            "\u001b[35mSection_title: 3.3  Workflow\n",
            "\u001b[35mSection_title: 3.3.1  Callback Function\n",
            "\u001b[35mSection_title: 3.3.2  Adaptor\n",
            "\u001b[35mSection_title: 3.4  Extensibility\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Settings\n",
            "\u001b[35mSection_title: 4.2  Results on English Datasets\n",
            "\u001b[35mSection_title: 5  Results on Chinese Datasets\n",
            "\u001b[32mconclusion: in this paper, we present textbrewer, a flexible pytorch-based distillation toolkit for nlp research and applications. textbrewer provides rich customization options for users to compare different distillation methods and build their strategies. we have conducted a series of experiments. the results show that the distilled models can achieve state-of-the-art results with simple settings. textbrewer also has its limitations. for example, its usability in generation tasks such as machine translation has not been tested. we will keep adding more examples and tests to expand textbrewer's scope of application.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 3.  Morphological data analysis\n",
            "\u001b[35mSection_title: 4.  Spectral analysis\n",
            "\u001b[35mSection_title: 5.  Mass determination and cooling ow analysis\n",
            "\u001b[35mSection_title: 5.1.  Mass determination\n",
            "\u001b[35mSection_title: 5.2.  Cooling ow analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Definition\n",
            "\u001b[35mSection_title: 3  Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Model Architecture\n",
            "\u001b[35mSection_title: 3.2  Query Focused Extractor\n",
            "\u001b[35mSection_title: 3.3  Training Phase\n",
            "\u001b[35mSection_title: 3.4  Test Phase\n",
            "\u001b[35mSection_title: 4  Experiments on RC\n",
            "\u001b[35mSection_title: 4.1  HotpotQA Dataset\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Experiments on RTE\n",
            "\u001b[35mSection_title: 5.1  FEVER Dataset\n",
            "\u001b[35mSection_title: 5.2  Experimental Setup\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 6.2  Recognizing Textual Entailment\n",
            "\u001b[35mSection_title: 6.3  Summarization\n",
            "\u001b[32mconclusion: we consider that the main contributions of our study are (1) the proposed qfe model that is based on a summarization model for the explainable multi-hop qa, (2) the dependency among the evidence and the coverage of the question due to the usage of the summarization model, and (3) the state-of-the-art performance in evidence extraction in both rc and rte tasks.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Extractive Reader\n",
            "\u001b[35mSection_title: 3.2  Generative Reader\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Learning Strategy\n",
            "\u001b[35mSection_title: 4.3  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  The Effect of Context Length\n",
            "\u001b[35mSection_title: 5.2  Comparison within Same PrLMs\n",
            "\u001b[35mSection_title: 5.3  Comparison within Different PrLMs\n",
            "\u001b[35mSection_title: 5.4  In-Depth Diagnosis\n",
            "\u001b[35mSection_title: 5.4.1  Long and Short Context\n",
            "\u001b[35mSection_title: 5.4.2  Rare Characters in Answer\n",
            "\u001b[32mconclusion: we systematically compare the extractive and generative readers for qa tasks. two sets of experiments are designed to control the effects of different prlms and the size of models. by conducting experiments on 12 qa datasets, our findings provide guidelines on how to choose extractive or generative readers given their strength and weakness. while current work investigates the pros and cons of extractive and generative models systematically, there are some hyperparameters that might affect the model performance. for example, it is known that different prompts in the input effect generative model performance (mishra et al., 2021b,a). also, it is worth studying the ood performance of models deeply. gokhale et al. (2022) compares multiple ways to improve the ood performance of an extractive model on qa task, and how these methods affect generative models have not been well-studied yet. meanwhile, most of the work including this work evaluate ood performance by averaging the performance across multiple dataset, but as mentioned in (mishra et al., 2020), the evaluation should be more carefully designed. also, diagnosing the performance on each ood dataset can provide more insights. for example, why models perform better on bioasq dataset than most other datasets (see table 4 \n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Datasets\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 4  Controlled Experiments\n",
            "\u001b[35mSection_title: 4.1  Do models generalize to unseen datasets?\n",
            "\u001b[35mSection_title: 4.2  Does pre-training improve results on small datasets?\n",
            "\u001b[35mSection_title: 4.3  Does context augmentation improve performance?\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work we performed a thorough empirical investigation of generalization and transfer over 10 rc datasets. we characterized the factors affecting generalization and obtained several state-ofthe-art results by training on 375k examples from 5 rc datasets. we open source our infrastructure for easily performing experiments on multiple rc datasets, for the benefit of the community. we highlight several practical take-aways: • pre-training on multiple source rc datasets consistently improves performance on a target rc dataset , even in the presence of bert representations. it also leads to substantial reduction in the number of necessary training examples for a fixed performance. • training the high-capacity bert-large representations over multiple rc datasets leads to good performance on all of the trained datasets without having to fine-tune on each dataset separately. • bert representations improve generalization, but their effect is moderate when the source of the context is web snippets compared to wikipedia and newswire. • performance over an rc dataset can be improved by retrieving web snippets for all questions and adding them as examples (context augmentation).\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Datasets\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 4  Controlled Experiments\n",
            "\u001b[35mSection_title: 4.1  Do models generalize to unseen datasets?\n",
            "\u001b[35mSection_title: 4.2  Does pre-training improve results on small datasets?\n",
            "\u001b[35mSection_title: 4.3  Does context augmentation improve performance?\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work we performed a thorough empirical investigation of generalization and transfer over 10 rc datasets. we characterized the factors affecting generalization and obtained several state-ofthe-art results by training on 375k examples from 5 rc datasets. we open source our infrastructure for easily performing experiments on multiple rc datasets, for the benefit of the community. we highlight several practical take-aways: • pre-training on multiple source rc datasets consistently improves performance on a target rc dataset , even in the presence of bert representations. it also leads to substantial reduction in the number of necessary training examples for a fixed performance. • training the high-capacity bert-large representations over multiple rc datasets leads to good performance on all of the trained datasets without having to fine-tune on each dataset separately. • bert representations improve generalization, but their effect is moderate when the source of the context is web snippets compared to wikipedia and newswire. • performance over an rc dataset can be improved by retrieving web snippets for all questions and adding them as examples (context augmentation).\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  DATASET\n",
            "\u001b[35mSection_title: 2.1  Statutes and test cases\n",
            "\u001b[35mSection_title: 2.2  Key features of the corpus\n",
            "\u001b[35mSection_title: 3  PROLOG SOLVER\n",
            "\u001b[35mSection_title: 3.1  Statutes\n",
            "\u001b[35mSection_title: 3.2  Cases\n",
            "\u001b[35mSection_title: 4  LEGAL NLP\n",
            "\u001b[35mSection_title: 4.1  Text corpus\n",
            "\u001b[35mSection_title: 4.2  Tax vectors\n",
            "\u001b[35mSection_title: 4.3  Legal BERT\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS 5.1 BERT-based models\n",
            "\u001b[35mSection_title: 5.2  Feedforward models\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[32mconclusion: we introduce a resource of law statutes, a dataset of hand-curated rules and cases in natural language, and a symbolic solver able to represent these rules and solve the challenge task. our handbuilt solver contrasts with our baselines based on current nlp approaches, even when we adapt them to the legal domain.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Notations and Problem Formulation\n",
            "\u001b[35mSection_title: 2.2  Learning Patterns from Unlabeled Data\n",
            "\u001b[35mSection_title: 2.3  Multi-hop QG Net with Regularization\n",
            "\u001b[35mSection_title: 2.3.1  Prior Patterns Estimation\n",
            "\u001b[35mSection_title: 2.3.2  Question-Related Content Extraction\n",
            "\u001b[35mSection_title: 2.3.3  Question Generation with Guidance\n",
            "\u001b[35mSection_title: 2.3.4  Learning with Limited Labeled Data\n",
            "\u001b[35mSection_title: 3  Evaluations\n",
            "\u001b[35mSection_title: 3.1  Data and Experimental Settings\n",
            "\u001b[35mSection_title: 3.2  Comparisons on QG State-of-the-Arts\n",
            "\u001b[35mSection_title: 3.3  Human Evaluations and Analysis\n",
            "\u001b[35mSection_title: 3.4  Evaluations on Value of Unlabeled Data\n",
            "\u001b[35mSection_title: 3.5  Evaluations on the Mixed Loss Objective\n",
            "\u001b[35mSection_title: 4  Application on the Task of MRC-QA\n",
            "\u001b[35mSection_title: 5  Related Works\n",
            "\u001b[32mconclusion: we have proposed an approach to generate the questions required multi-hop reasoning in low-resource conditions. we first built a multi-hop qg model and guided it to satisfy the logical rationality by the reasoning chain extracted from a given text. in order to tackle the labeled data shortage problem, we learned the structural patterns from the unlabeled data by the hidden semi-markov model. with the patterns as a prior, we transferred this fundamental knowledge into the generation model to produce the optimal results. experimental results on the hotpotqa data set demonstrated the effectiveness of our approach. moreover, we explored the generated results to facilitate the real-world application of machine reading comprehension. we will investigate the robustness and scalability of the model.\n",
            "\u001b[35mSection_title: 6  Conclusions and Future Works\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Construction\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: we proposed a new multi-hop rc dataset for comprehensively evaluating the ability of existing models to understand date information. we evaluated the top-performing models on our dataset. the results revealed that the models may not possess the ability to subtract two dates even when fine-tuned on our dataset. we also found that our probing questions could help to improve qa performance, and can be used for data augmentation. for future work, we will use the hierarchical manner in our dataset to apply to other types of questions such as numerical reasoning questions in drop. \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Construction\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: we proposed a new multi-hop rc dataset for comprehensively evaluating the ability of existing models to understand date information. we evaluated the top-performing models on our dataset. the results revealed that the models may not possess the ability to subtract two dates even when fine-tuned on our dataset. we also found that our probing questions could help to improve qa performance, and can be used for data augmentation. for future work, we will use the hierarchical manner in our dataset to apply to other types of questions such as numerical reasoning questions in drop. \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  QA Agent Identification\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Similarity-based Models\n",
            "\u001b[35mSection_title: 3.3  Transformer with Extendable Agent Classifiers (TWEAC)\n",
            "\u001b[35mSection_title: 4  Data and Evaluation\n",
            "\u001b[35mSection_title: 4.1  QA Tasks\n",
            "\u001b[35mSection_title: 4.2  Large-Scale QA Agent Identification\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 6  Extending TWEAC\n",
            "\u001b[35mSection_title: 7  Analysis\n",
            "\u001b[32mconclusion: we analyzed how to automatically select suitable qa agents specializing in different questions, as an alternative to a single qa system that tries to cover all possible questions. we presented a scalable meta-qa system that allows for a flexible extension with different qa agents. for newly posed questions, we rank agents by their ability to answer them and select the most suitable ones.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Background\n",
            "\u001b[35mSection_title: 3.2  Retrospective Feed Mechanism\n",
            "\u001b[35mSection_title: 3.3  Enhanced Recurrence Mechanism\n",
            "\u001b[35mSection_title: 3.4  Segment-Reordering Objective\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Autoregressive Language Modeling\n",
            "\u001b[35mSection_title: 4.1.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2.3  Results on English Tasks\n",
            "\u001b[35mSection_title: 4.2.4  Results on Chinese Tasks\n",
            "\u001b[35mSection_title: 4.2.5  Ablation Studies\n",
            "\u001b[32mconclusion: in this paper, we proposed ernie-doc, a document-level language pretraining model based on the recurrence transformers paradigm. two well-designed mechanisms, namely the retrospective feed mechanism and the enhanced recurrent mechanism, enable ernie-doc, which theoretically has the longest possible dependency, to model bidirectional contextual information of a complete document. additionally, ernie-doc is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context. experiments on various downstream tasks demonstrate that ernie-doc outperforms existing strong pretraining models such as roberta, longformer, and bigbird and achieves sota results on several language modeling and language understanding benchmarks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Background\n",
            "\u001b[35mSection_title: 3.2  Retrospective Feed Mechanism\n",
            "\u001b[35mSection_title: 3.3  Enhanced Recurrence Mechanism\n",
            "\u001b[35mSection_title: 3.4  Segment-Reordering Objective\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Autoregressive Language Modeling\n",
            "\u001b[35mSection_title: 4.1.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2.3  Results on English Tasks\n",
            "\u001b[35mSection_title: 4.2.4  Results on Chinese Tasks\n",
            "\u001b[35mSection_title: 4.2.5  Ablation Studies\n",
            "\u001b[32mconclusion: in this paper, we proposed ernie-doc, a document-level language pretraining model based on the recurrence transformers paradigm. two well-designed mechanisms, namely the retrospective feed mechanism and the enhanced recurrent mechanism, enable ernie-doc, which theoretically has the longest possible dependency, to model bidirectional contextual information of a complete document. additionally, ernie-doc is pretrained with a document-aware segment-reordering objective to explicitly learn the relationship among segments of a long context. experiments on various downstream tasks demonstrate that ernie-doc outperforms existing strong pretraining models such as roberta, longformer, and bigbird and achieves sota results on several language modeling and language understanding benchmarks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.  Approaches do not translate well across tasks:\n",
            "\u001b[35mSection_title: 2.2  Approaches\n",
            "\u001b[35mSection_title: 3.2  Training Details:\n",
            "\u001b[35mSection_title: 4  Results and Analysis\n",
            "\u001b[35mSection_title: 4.1  Slight Improvement in IID\n",
            "\u001b[35mSection_title: 4.2  Negligible Improvement / Degradation in OOD and ADV\n",
            "\u001b[35mSection_title: 4.3  Calib T Degrades Performance\n",
            "\u001b[35mSection_title: 4.4  Existing Approaches Fail to Utilize Additional Resources\n",
            "\u001b[35mSection_title: 4.5  Effect of Increasing Dropout Masks in Monte-Carlo Dropout\n",
            "\u001b[35mSection_title: 4.6  No Clear Winner\n",
            "\u001b[32mconclusion: selective prediction ability is crucial for nlp systems to be reliably deployed in real-world applications and we presented the most systematic study of existing selective prediction approaches. our study involved experiments in iid, ood, and adv settings with 17 datasets across several nlp tasks. we showed that despite leveraging additional resources (held-out data/computation), existing approaches fail to consistently and considerably outperform the simplest baseline (maxprob \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The CROSSFIT Challenge\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  Problem Setting\n",
            "\u001b[35mSection_title: 4  NLP Few-shot Gym\n",
            "\u001b[35mSection_title: 4.1  Dataset Selection\n",
            "\u001b[35mSection_title: 4.2  A Unified Text-to-Text Format\n",
            "\u001b[35mSection_title: 4.3  Formulating Few-shot Tasks\n",
            "\u001b[35mSection_title: 4.4  Task Ontology and Partitions\n",
            "\u001b[35mSection_title: 5  Methods to CROSSFIT\n",
            "\u001b[35mSection_title: 6  Empirical Analysis\n",
            "\u001b[32mconclusion: in this paper, we study the problem of building better few-shot learners via acquiring cross-task generalization ability from diverse nlp tasks. towards our goal, we introduce the crossfit challenge, an task setup that standardizes the training pipeline, data access and evaluation protocol. we also present the nlp few-shot gym, a repository of 160 diverse few-shot nlp tasks, to support crossfit learning in different scenarios. we empirically demonstrated that cross-task generalization can be acquired via multi-task learning and meta-learning; confirmed that the selection of seen tasks would influence the few-shot performance on unseen tasks.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Graph Construction\n",
            "\u001b[35mSection_title: 3.2  Entity-to-Passage Fusion\n",
            "\u001b[35mSection_title: 4  Experimental Setting\n",
            "\u001b[35mSection_title: 4.1  Models in Comparison\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Implementation details\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Multi-hop Performance\n",
            "\u001b[35mSection_title: 5.2  Faithfulness of Reasoning Paths\n",
            "\u001b[35mSection_title: 5.3  Performance vs Number of hops\n",
            "\u001b[35mSection_title: 5.4  Probing Disconnected Reasoning\n",
            "\u001b[35mSection_title: 5.5  Comparison with PathFiD+\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[32mconclusion: in this paper, we propose seqgraph, an approach that utilizes the structured relationship between passages in the context of multi-hop questions to reduce disconnected reasoning. we construct a localized entity-passage graph using wikipedia hyperlinks, encode it using a gnn, and fuse the structured representations with the text encoder for predicting a reasoning path. our approach results in strong performance gains in terms of both answer and support em/f1 on hotpot-qa and reduces disconnected reasoning measured using dire score. we also obtain state-of-the-art performance on the more challenging musique benchmark with a 17-point improvement in answer f1 over the current best end-to-end(e2e) model. experimenting with sophisticated methods of encoding the graph structure and fusing the text and graph representations can be explored in future work.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Graph Construction\n",
            "\u001b[35mSection_title: 3.2  Entity-to-Passage Fusion\n",
            "\u001b[35mSection_title: 4  Experimental Setting\n",
            "\u001b[35mSection_title: 4.1  Models in Comparison\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Implementation details\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Multi-hop Performance\n",
            "\u001b[35mSection_title: 5.2  Faithfulness of Reasoning Paths\n",
            "\u001b[35mSection_title: 5.3  Performance vs Number of hops\n",
            "\u001b[35mSection_title: 5.4  Probing Disconnected Reasoning\n",
            "\u001b[35mSection_title: 5.5  Comparison with PathFiD+\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[32mconclusion: in this paper, we propose seqgraph, an approach that utilizes the structured relationship between passages in the context of multi-hop questions to reduce disconnected reasoning. we construct a localized entity-passage graph using wikipedia hyperlinks, encode it using a gnn, and fuse the structured representations with the text encoder for predicting a reasoning path. our approach results in strong performance gains in terms of both answer and support em/f1 on hotpot-qa and reduces disconnected reasoning measured using dire score. we also obtain state-of-the-art performance on the more challenging musique benchmark with a 17-point improvement in answer f1 over the current best end-to-end(e2e) model. experimenting with sophisticated methods of encoding the graph structure and fusing the text and graph representations can be explored in future work.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Generative Passage Selection\n",
            "\u001b[35mSection_title: 2.1  Model Description\n",
            "\u001b[35mSection_title: 2.2  Model Learning\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 3.1  Adversarial Evaluation\n",
            "\u001b[35mSection_title: 3.2  Context pairs vs. Sentences\n",
            "\u001b[35mSection_title: 4  Related work\n",
            "\u001b[32mconclusion: we have presented a generative formulation of context pair selection in multi-hop question answering models. by encouraging the context selection model to explain the entire question, it is less susceptible to bias, performing substantially better on adversarial data than existing methods that use discriminative selection. our proposed model is simple to implement and can be used with any existing (or future) answering model; we will release code to support this integration. since context pair selection scales quadratically with the number of contexts, it is not ideal for scenarios that involve a large number of possible contexts. however, it allows for deeper inter-document interaction as compared to other approaches that use summarized document representations. with more reasoning steps, selecting relevant documents given only the question becomes challenging, increasing the need for inter-document interaction. this paper focuses on biases found in question answering models that make its reasoning capabilities brittle. it uses an existing method of testing model performance on adversarial held-out set as an evaluation metric. this work does not deal with any social impacts of biases in natural language processing systems.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  The MuLD Benchmark\n",
            "\u001b[35mSection_title: 3.1.  Desiderata\n",
            "\u001b[35mSection_title: 3.2.  The Tasks\n",
            "\u001b[35mSection_title: 4.  Baselines\n",
            "\u001b[35mSection_title: 5.  Results and Discussion\n",
            "\u001b[32mconclusion: to enable the evaluation of long document models, we introduce muld: a benchmark of varied nlp tasks where each document consists of more than 10,000 tokens. the six tasks in our benchmark are created by filtering, extending, or modifying existing nlp tasks and are designed to require a long context for high performance. we evaluate simple chunking-based baselines, and find that the longformer model is able to outperform the t5 model suggesting our benchmark is a good test for the ability of models to make use of longer contexts. we believe that the technique explored in this work of augmenting and extending existing 'short document' datasets, can be applied to many other nlp tasks. as the performance of efficient transformers improves, we anticipate the need to update this benchmark with more challenging tasks. while we are focused on creating a benchmark which tests a model's ability to solve real-world long document tasks, we also expect improvements in the efficiencies of the models themselves which may make datasets with more than 100,000 tokens necessary which may require a fundamentally different approach to creating long document datasets. we leave for future work both the development of improved chunking methods, and more efficient transformers which make such methods unnecessary. we hope that the muld benchmark will encourage this further research into efficient models for long document nlp. to this end we provide the data, baseline models, and other code at www.github.com/ ghomashudson/muld.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Language Models\n",
            "\u001b[35mSection_title: 4.3  Metrics\n",
            "\u001b[35mSection_title: 5.2  BEIR\n",
            "\u001b[35mSection_title: 5.3  The Impact of Model Size\n",
            "\u001b[32mlimitation: there are a number of limitations in our work: first, we only study sparse retrieval (bm25) which is where query expansion is important. dense retrieval systems (e.g. dual encoders) are less prone to the vocabulary gap and, as a result, are less likely to benefit from a query expansion. wang et al. [31] has already studied this setting in more detail and we leave the analysis of our prompts for a dense retrieval setting as future work. second, our work focuses on flan [32] instruction-finetuned language models. we chose these models due to their ability to follow instructions and the fact that these models are open-source. our work can naturally be extended to other language models [3,5,9,28] and we leave the study of such models as a topic for future research. third, we study specific prompt templates (see appendix a) and there may be other ways to formulate the different prompts. finally, the computational cost of llms may be prohibitive to deploy llm-based query expansions in practice. it may be possible to distill the output of the large model into a smaller servable model. how to productionize llm-based query expansions is left as an open problem.\n",
            "\u001b[35mSection_title: 6  LIMITATIONS & FUTURE WORK\n",
            "\u001b[32mconclusion: in this paper we study llm-based query expansions. in contrast to traditional prf-based query expansion, llms are not restricted to the initial retrieved set of documents and may be able to generate expansion terms not covered by traditional methods. our proposed method is simple: we prompt a large language model and provide it a query, then we use the model's output to expand the original query with new terms that help during document retrieval.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3.  A new dataset of open-domain multi-\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Learning to Search\n",
            "\u001b[35mSection_title: 3.1  Constructing Query Actions\n",
            "\u001b[35mSection_title: 3.2  State Representation Features\n",
            "\u001b[35mSection_title: 3.3  Topic Modeling Features\n",
            "\u001b[35mSection_title: 3.4  Reward Function Structure\n",
            "\u001b[35mSection_title: 4  Evaluation and Discussion\n",
            "\u001b[32mconclusion: we proposed a focused reading methodology to automatically learn how to direct search in large corpora while iteratively building a knowledge base. the knowledge base is modeled as a graph, which in turn is used to focus the search toward documents that appear relevant. our methodology complements existing information retrieval and machine tools. we evaluated focused reading on a set of search problems extracted from english wikipedia and demonstrated that reinforcement learning with a state representation based on features about dynamics of the search process and the properties of the corpus is more effective and efficient than heuristic baselines. in this methodology, inference in a knowledge graph acquired during the search process is agnostic of the semantics of the concepts and their relations. their quality depends on the machine reading components used to extract them.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Dataset Generation\n",
            "\u001b[35mSection_title: 2.2  Models and Training\n",
            "\u001b[35mSection_title: 3  Results and Discussion\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: using propositional logic corpora to simulate a controlled language modeling setting, we ask: 1) do properties of the training corpus affect lms' abilities to differentiate the meanings of logical operators? and 2) do any training corpora lead to models that differentiate these meanings to a satisfactory degree? our results imply a positive answer to (1): models trained on corpora generated with different constraints appear to perform differently at the task of separating ∧ from ∨. however, these differences are a function of both data and model. for example, the transformer architecture seems better able to learn from weaker signal (corpora generated only with a truthfulness constraint), while lstms require more explicit signal (direct access to truth values). on question (2), our results are largely negative for the syntactically similar operators. even the most semantically transparent training data did not enable models to separate the representations of symbols with similar form but different meaning. only the small transformer trained on the explicit grounding condition can perfectly differentiate ∧ from ∨ at the lexical level, despite the task's controlled nature. however, every model did separate ¬ from both ∧ and ∨, illustrating how syntactic differences can support differentiation of meaning.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 7.2  Informativity dataset information\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Cross-Thought\n",
            "\u001b[35mSection_title: 3.1  Pre-training Data Construction\n",
            "\u001b[35mSection_title: 3.2  Cross-Thought Pre-training\n",
            "\u001b[35mSection_title: 3.3  Cross-Thought Finetuning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Experimental Results\n",
            "\u001b[35mSection_title: 4.5  Case Study\n",
            "\u001b[32mconclusion: we propose a novel approach, cross-thought, to pre-train sentence encoder. experiments demonstrate that using cross-thought trained with short sequences can effectively improve sentence embedding. our pre-trained sentence encoder with further finetuning can beat several strong baselines on many nlp tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Models\n",
            "\u001b[35mSection_title: 2.2  Knowledge Integration\n",
            "\u001b[35mSection_title: 2.3  Composing knowledge using soft prompts\n",
            "\u001b[35mSection_title: 2.4  Performing QA using Hopping Prompts\n",
            "\u001b[35mSection_title: 3.2  Baselines and Comparisons\n",
            "\u001b[32mconclusion: we show that composition of memorized world knowledge can be triggered in lms with up to 11b parameters (t5-xxl) to a desirable extent by leveraging training signal from random walks over structured knowledge using approaches based on prompt-tuning (lester et al., 2021). doing so leads to substantial improvements in the lms' ability to answer 2-hop questions, even beyond standard, full model fine-tuning.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Wiki Entity Similarity Dataset\n",
            "\u001b[35mSection_title: 3.2  Negative Example Generation\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[32mconclusion: we introduce wes, an 11m example semantic entity similarity dataset for training question answering evaluation models. wes is generated by treating wikipedia link texts and target article titles as synonyms then filtering for quality. wes is targeted to question answering evaluation, independent of human annotators, and consistent with human judgment. we hope that future questionanswering datasets will implement semantic evaluation metrics in their leaderboards to encourage the development of more free-form models. in future works, link-mining similarity datasets like wes can be made more challenging by generating negative examples adversarially as described at the end of section 3, more consistent by unioning semantic clusters according to wikipedia's internal redirect pages, and more comprehensive by leveraging link-to-link pairwise synonymy within semantic clusters.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Text Encoding\n",
            "\u001b[35mSection_title: 2.2  Text Logic Graph (TLG)\n",
            "\u001b[35mSection_title: 2.2.1  Definition of TLG\n",
            "\u001b[35mSection_title: 2.2.2  Construction of Raw TLG\n",
            "\u001b[35mSection_title: 2.3  Adaptive Logic Graph Network (AdaLoGN)\n",
            "\u001b[35mSection_title: 2.3.1  Inference Rules\n",
            "\u001b[35mSection_title: 2.3.2  Adaptive Extension of TLG\n",
            "\u001b[35mSection_title: 2.3.3  Message Passing\n",
            "\u001b[35mSection_title: 2.3.4  Graph Pooling\n",
            "\u001b[35mSection_title: 2.4  Answer Prediction\n",
            "\u001b[35mSection_title: 2.5  Loss Function\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Baselines\n",
            "\u001b[35mSection_title: 3.4  Evaluation Metric\n",
            "\u001b[35mSection_title: 3.5  Comparison with Baselines\n",
            "\u001b[35mSection_title: 3.6  Ablation Study\n",
            "\u001b[35mSection_title: 3.6.1  Effectiveness of Adaptive Extension\n",
            "\u001b[35mSection_title: 3.6.2  Effectiveness of Subgraph-to-Node Message Passing\n",
            "\u001b[35mSection_title: 3.7  Error Analysis\n",
            "\u001b[35mSection_title: 3.8  Run Time\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 4.1  Reasoning-Based MRC\n",
            "\u001b[35mSection_title: 4.2  Neural-Symbolic Reasoning\n",
            "\u001b[32mconclusion: to meet the challenge of reasoning-based mrc, we presented a neural-symbolic approach where neural and symbolic reasoning mutually and iteratively reinforce each other via our new adalogn model. we also enhanced graph-based neural reasoning with a novel subgraph-to-node message passing mechanism. since these ideas are quite general, we believe they have great potential for a variety of applications beyond mrc, e.g., link prediction.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methods\n",
            "\u001b[35mSection_title: 2.1  Model Architecture\n",
            "\u001b[35mSection_title: 2.2  Multi-step Attention Network\n",
            "\u001b[35mSection_title: 2.3  Two Stage Training\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Speaker Normalization\n",
            "\u001b[35mSection_title: 3.3  Multi-task Learning\n",
            "\u001b[35mSection_title: 3.4  Training Details\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Why does natural language inference help?\n",
            "\u001b[35mSection_title: 5.2  Can other tasks help with MCQA?\n",
            "\u001b[35mSection_title: 5.3  NLI dataset helps with convergence\n",
            "\u001b[35mSection_title: 5.4  Multi-stage or Multi-task\n",
            "\u001b[35mSection_title: 5.5  Multi-steps reasoning\n",
            "\u001b[35mSection_title: 5.6  Could the source dataset benefit?\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Protein Z and MyoD.\n",
            "\u001b[35mSection_title: 2.2.  Design Process.\n",
            "\u001b[35mSection_title: 2.3.  Multiobjective Optimization.\n",
            "\u001b[35mSection_title: 2.4.  Scoring Functions.\n",
            "\u001b[35mSection_title: 2.5.  Modeling New Sequences.\n",
            "\u001b[35mSection_title: 2.6.  MD Simulations of the Protein Models.\n",
            "\u001b[35mSection_title: 2.7.  MD Simulations of the Protein-DNA Complex.\n",
            "\u001b[35mSection_title: 2.8.  Brownian Dynamics Simulations.\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\u001b[32mconclusion: we have applied multi-objective optimization guided by directed evolution to combine the myod dna-binding motif into the z domain conserving the scaffolds structure. simulations showed that the optimization of the sequences based on hydrophobicity, molecular weight, and secondary structure predictions improved structural stability while maintaining protein functionality. the use of simple fitness functions reduces the optimization complexity, and thus allows to optimize more individuals over more generations resulting in a better sampling of the sequence space.\n",
            "\u001b[35mSection_title: 4.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.2  Sinalização por ubiquitinação\n",
            "\u001b[35mSection_title: 1.3  Proteínas F-box e a degradação pelo proteassomo\n",
            "\u001b[35mSection_title: 2.  Objetivos\n",
            "\u001b[35mSection_title: 3.1.2  Coleta e fixação do material biológico\n",
            "\u001b[35mSection_title: 3.1.3  Preparação do material histológico\n",
            "\u001b[35mSection_title: 3.1.5  Medições dos pistilos\n",
            "\u001b[35mSection_title: 3.2  Produção das construções gênicas\n",
            "\u001b[35mSection_title: 3.3  Construções gênicas produzidas\n",
            "\u001b[35mSection_title: 3.5  Análise de fragmentos de DNA por eletroforese\n",
            "\u001b[35mSection_title: 3.6  Reações de sequenciamento\n",
            "\u001b[35mSection_title: 3.10.3  Análise de proteínas por SDS-PAGE\n",
            "\u001b[35mSection_title: 3.10.4  Análise por Western Blot\n",
            "\u001b[35mSection_title: 3.12  Antibióticos Usados\n",
            "\u001b[35mSection_title: 4.2  Estudo das interações proteicas com FEF1\n",
            "\u001b[35mSection_title: 4.2.1  Ensaio de duplo-híbrido em leveduras\n",
            "\u001b[35mSection_title: 4.2.2  Obtenção das construções para BiFC\n",
            "\u001b[35mSection_title: 4.2.3  Confirmação da interação entre FEF1 e SKP1 por BiFC\n",
            "\u001b[35mSection_title: 4.3  Expressão heteróloga da proteína His-FEF1\n",
            "\u001b[35mSection_title: 4.4  Expressão heteróloga da proteína His-NtTCP11\n",
            "\u001b[35mSection_title: 5.2  Alterações fenotípicas no pistilo das plantas transgênicas\n",
            "\u001b[35mSection_title: 5.3  Parceiros de interação de FEF1\n",
            "\u001b[35mSection_title: 5.4  Expressão das proteínas 6xHis-FEF1 e 6xHis-TCP\n",
            "\u001b[35mSection_title: 6.  Conclusões\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.  SOTA-chasing: Costs\n",
            "\u001b[35mSection_title: 2.1  Incentives and coopetition: Leaderboards considered harmful\n",
            "\u001b[35mSection_title: 2.2  Sucking the oxygen out of the room\n",
            "\u001b[35mSection_title: 2.2.1  What counts as computational linguistics (CL)?\n",
            "\u001b[35mSection_title: 2.2.2  Organization of ACL program committees\n",
            "\u001b[35mSection_title: 2.2.3  Concepts suggested by search engine\n",
            "\u001b[35mSection_title: 2.2.4  Studies of ACL anthology\n",
            "\u001b[35mSection_title: 2.2.5  Studies of papers with code (PWC)\n",
            "\u001b[35mSection_title: 2.3  Unrealistic expectations: Superhuman performance, seriously???\n",
            "\u001b[35mSection_title: 3.  Root causes for SOTA-chasing\n",
            "\u001b[35mSection_title: 3.1  SOTA-chasing: A consequence of a lack of leadership\n",
            "\u001b[35mSection_title: 3.1.1  Mashey, Jelinek and Wayne\n",
            "\u001b[35mSection_title: 3.1.2  Strategic planning in government\n",
            "\u001b[35mSection_title: 3.2  SOTA-chasing: A consequence of poor reviewing processes\n",
            "\u001b[35mSection_title: 3.2.1  Recommendations\n",
            "\u001b[35mSection_title: 4.  Replication crisis\n",
            "\u001b[32mconclusion: many papers are sota-chasing, and more will do so in the future. sota-chasing comes with many costs. we discussed three costs:\n",
            "\u001b[35mSection_title: 5.  Conclusions/Recommendations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Proposed Work\n",
            "\u001b[35mSection_title: 3.1  Background and Motivation\n",
            "\u001b[35mSection_title: 3.2  Evidence from Existing Literature\n",
            "\u001b[35mSection_title: 3.3  Proposed Solutions\n",
            "\u001b[35mSection_title: 3.3.1  Homogenize Different Knowledge to a Unified Knowledge Representation\n",
            "\u001b[35mSection_title: 3.3.2  Multi-virtual Hops Retrieval over Heterogeneous Knowledge\n",
            "\u001b[35mSection_title: 3.3.3  Reasoning over Retrieved Documents Based on Structured Knowledge\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Formulation\n",
            "\u001b[35mSection_title: 3  Automatic Evaluation of Attribution\n",
            "\u001b[35mSection_title: 3.1  Prompting LLMs\n",
            "\u001b[35mSection_title: 3.2  Fine-tuning LMs on Repurposed Data\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Overall Performance\n",
            "\u001b[35mSection_title: 5.2  Qualitative Analysis\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we investigate the important problem of automatically evaluating attribution given by llms.we begin by defining different types of attribution errors and then explore two approaches for automatic evaluation: prompting llms and fine-tuning smaller lms.we experiment with both simulated test examples and manually curated test examples from a real-life generative search engine.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 9  Ethics Statement\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  LLMs augmented with tools\n",
            "\u001b[35mSection_title: 2.2  Reasoning with LLMs\n",
            "\u001b[35mSection_title: 2.3  Temporal reasoning\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Task definition\n",
            "\u001b[35mSection_title: 3.2  Represent all as codes\n",
            "\u001b[35mSection_title: 3.3  Choose answer through programming\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1.1  Datasets\n",
            "\u001b[35mSection_title: 4.1.2  Baselines\n",
            "\u001b[35mSection_title: 4.1.3  Implement details\n",
            "\u001b[35mSection_title: 4.2  Main results\n",
            "\u001b[35mSection_title: 4.3  Ablation study\n",
            "\u001b[35mSection_title: 4.3.1  Can LLMs find the answer provided enough context?\n",
            "\u001b[35mSection_title: 4.3.2  How much does Check alleviate hallucination?\n",
            "\u001b[35mSection_title: 4.3.3  Can LLMs directly identify the best matching answer to the question?\n",
            "\u001b[35mSection_title: 4.3.4  Are LLMs capable of writing qualified\n",
            "\u001b[35mSection_title: 4.4  Case study\n",
            "\u001b[35mSection_title: 5  Discussion and Future Work\n",
            "\u001b[32mconclusion: in this work we propose a novel approach, qaap (question answering as programming), to tackle the challenges posed by time-sensitive factual questions.by leveraging llms' exceptional abilities in natural language understanding and programming, qaap can transform diversely expressed text into well-structured codes, enabling llms to capture both the desired knowledge and the underlying constraints, particularly the temporal aspects.experiments demonstrate that existing llms face significant difficulty in effectively comprehending the temporal constraint stated in the question.while our approach consistently demonstrates superior performance over strong baselines with llms.we hope this work can shed light on the future research direction on enhancing llms' reasoning ability to tackle real-world questions with various constraints and developing more efficient methods to reduce the hallucinations llms frequently encounter.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  CONCEPTS AND NOTATION\n",
            "\u001b[35mSection_title: 3  OVERVIEW\n",
            "\u001b[35mSection_title: 3.1  Question understanding\n",
            "\u001b[35mSection_title: 3.2  Evidence retrieval\n",
            "\u001b[35mSection_title: 4  HETEROGENEOUS ANSWERING\n",
            "\u001b[35mSection_title: 4.1  Graph construction\n",
            "\u001b[35mSection_title: 4.2  Node encodings\n",
            "\u001b[35mSection_title: 4.3  Message passing\n",
            "\u001b[35mSection_title: 4.4  Answer score prediction\n",
            "\u001b[35mSection_title: 4.5  Multi-task learning\n",
            "\u001b[35mSection_title: 5  ITERATIVE GRAPH NEURAL NETWORKS\n",
            "\u001b[35mSection_title: 6  RESULTS AND INSIGHTS 6.1 Experimental setup\n",
            "\u001b[35mSection_title: 6.2  Key findings\n",
            "\u001b[35mSection_title: 6.3  In-depth analysis\n",
            "\u001b[35mSection_title: 7  USER STUDY ON EXPLAINABILITY\n",
            "\u001b[35mSection_title: 8  RELATED WORK\n",
            "\u001b[32mconclusion: there are three main takeaways for a broader audience from this work. first, at a time when large language models (llms) like chat-gpt are used as a one-stop shop for most nlp tasks including convqa, our method explaignn stands out by providing traceable provenance of its answer predictions. next, explainability for graph neural networks is an unsolved concern: we propose an iterative model that sequentially reduces the graph size as a medium for offering causal insights into the prediction process. finally, for several systems in ir and nlp, performance, efficiency, and explainability are seen as trade-offs. through our highly configurable solution, we show that in certain use cases, it is actually possible to find configurations that lie at the sweet spots of all the three factors. a natural future work would be to generalize these insights to other problems, like graph-based neural recommendation.\n",
            "\u001b[35mSection_title: 9  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  SURF Question Reformulation Model\n",
            "\u001b[35mSection_title: 4.1  Reformulation Model\n",
            "\u001b[35mSection_title: 4.2  Reformulation Operators\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Intrinsic Evaluation Strategy\n",
            "\u001b[35mSection_title: 5.2  Extrinsic Evaluation Strategy\n",
            "\u001b[35mSection_title: 5.3  Training Data\n",
            "\u001b[35mSection_title: 5.4  Reformulation Model Configurations\n",
            "\u001b[35mSection_title: 6  Results and Discussion\n",
            "\u001b[35mSection_title: 6.1  Intrinsic Evaluation\n",
            "\u001b[35mSection_title: 6.2  Extrinsic Answer Rate Results\n",
            "\u001b[35mSection_title: 6.3  Answer Relevance Results\n",
            "\u001b[35mSection_title: 6.4  Live QA Deployment\n",
            "\u001b[32mconclusion: we tackled the problem of improving spoken qa, and analyzed questions from live data to identify key challenges that could be addressed with reformulation. based on this we proposed surf with novel linguistically-motivated reformulation operators to solve the identified challenges. offline experiments show the effectiveness of our novel root transformation and generalization operations, with up to 24% of unanswered questions being answered via reformulations with high answer relevance. live deployment in a leading voice assistant has positively impacted millions of requests.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  The Related Works\n",
            "\u001b[35mSection_title: 2.1.  Open-Domain Multi-Hop QA\n",
            "\u001b[35mSection_title: 2.2.  Semantic Information Retrieval\n",
            "\u001b[35mSection_title: 2.3.  Semantic Compositional Sentence\n",
            "\u001b[35mSection_title: 3.  The Proposed Approach\n",
            "\u001b[35mSection_title: 3.1.  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2.  Multi-Stage Semantic Matching Module\n",
            "\u001b[35mSection_title: 3.3.  Factual Sentences Composition Module\n",
            "\u001b[35mSection_title: 3.4.  Open-Domain QA Architecture\n",
            "\u001b[35mSection_title: 4.  Main Experiments\n",
            "\u001b[35mSection_title: 4.1.  QASC Dataset\n",
            "\u001b[35mSection_title: 4.2.  Experimental Setups\n",
            "\u001b[35mSection_title: 4.3.  Baselines\n",
            "\u001b[35mSection_title: 4.4.  Results Analysis and Ablation Study\n",
            "\u001b[35mSection_title: 4.5.  Case Study\n",
            "\u001b[32mconclusion: to improve the retrieval results and multi-hop reasoning in question answering, we present a multistage semantic matching module and a factual sentences composition module, respectively. experimental results fully demonstrate our modules outperform the standard ir system and two-step ir method. we first combine the semantic sentence composition module with multi-stage semantic matching module, and it is suitable for any open-domain qa architecture. further exploration will be done on the other question answering datasets with our proposed modules.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  Learning Method\n",
            "\u001b[35mSection_title: 4  Task Setup\n",
            "\u001b[35mSection_title: 4.1  Multi-Mention Reading Comprehension\n",
            "\u001b[35mSection_title: 4.2  Reading Comprehension with Discrete Reasoning\n",
            "\u001b[35mSection_title: 2.  Reading Comprehension with Discrete Reasoning (DROPnum)\n",
            "\u001b[35mSection_title: 4.3  SQL Query Generation\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Multi-mention Reading Comprehension\n",
            "\u001b[35mSection_title: 5.2  Reading Comprehension with Discrete Reasoning\n",
            "\u001b[35mSection_title: 5.3  SQL Query Generation\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[32mconclusion: in this paper, we demonstrated that, for many qa tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. we showed that this approach significantly outperforms previous approaches on six qa tasks including reading comprehension, opendomain qa, discrete reasoning task and semantic parsing, achieving absolute gains of 2-10% and setting the new state-of-the-art on five wellstudied datasets.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Formulation\n",
            "\u001b[35mSection_title: 3  Related Work\n",
            "\u001b[35mSection_title: 4  Model\n",
            "\u001b[35mSection_title: 4.1  Relation-Specific Learning\n",
            "\u001b[35mSection_title: 4.1.1  Reinforcement Learning Formulation\n",
            "\u001b[35mSection_title: 4.1.2  Policy Network\n",
            "\u001b[35mSection_title: 4.1.3  Loss Function\n",
            "\u001b[35mSection_title: 4.2  Meta-Learning\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Link Prediction\n",
            "\u001b[35mSection_title: 5.4  Robustness Analysis\n",
            "\u001b[32mconclusion: in this paper, we propose a meta-learning based model named meta-kgr for multi-hop reasoning over few-shot relations of knowledge graphs.meta-kgr uses training triples with highfrequency relations to find well-initialized parameters and fast adapt to few-shot relations.the meta information learned from high-frequency relations is helpful for few-shot relations.in experiments, our models achieve good performance on few-shot relations and outperform previous work in most cases.some empirical analysis also demonstrates that our models are robust and generalized to different types of knowledge graphs.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Leveraging LLM-generated text\n",
            "\u001b[35mSection_title: 2.2  Knowledge-intensive NLP and retrieval\n",
            "\u001b[35mSection_title: 3  Analytic Discussion\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  QA Task Formulation\n",
            "\u001b[35mSection_title: 3.3  Knowledge corpus error\n",
            "\u001b[35mSection_title: 4  Empirical Observation\n",
            "\u001b[35mSection_title: 4.1  Experimental setup\n",
            "\u001b[35mSection_title: 4.2  Benchmarks\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Qualitative Analysis\n",
            "\u001b[35mSection_title: 3.  Incorporation of commonsense knowledge\n",
            "\u001b[32mconclusion: in this work, we demonstrate that generated contexts may be more helpful than retrieved contexts in open-domain question answering.by revisiting the formulation of question answering, we identify a gap where retriever inevitably ignores potentially helpful contexts outside of the corpus.we call this knowledge corpus error, and design an experiment in order to observe knowledge corpus error empirically.paraphrasing the human-annotated gold contexts with llms led to increased reader performance in 3 out of 4 qa benchmarks, implying the existence of knowledge corpus error.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  The Toolbox of DNA Processes\n",
            "\u001b[35mSection_title: 3.  The DNA Network Design\n",
            "\u001b[35mSection_title: 3.1.  Programming the DNA Network\n",
            "\u001b[35mSection_title: 4.  Simulation Results\n",
            "\u001b[32mconclusion: we have investigated the implementation, in a synthetic dna reaction network, of computing paradigms abstracted from two different cellular biochemical reaction networks, the yeast galactose network [34] and the type-1 incoherent feed-forward loop network motif [27]. the results of chemical kinetics simulations show that the proposed dna network can be programmed to implement transient pulse generation with a steady-state output that can be made robust to changes in network dosage. this network has another interesting property: it can be configured such that the steady-state output is proportional to the initial dose of one of the inputs but insensitive to subsequent additions which generate only transient output pulses.\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS\n",
            "\u001b[35mSection_title: 2.1  LARGE LANGUAGE MODELS AND PROMPTING\n",
            "\u001b[35mSection_title: 2.2  CHAIN-OF-THOUGHT PROMPTING\n",
            "\u001b[35mSection_title: 3  PRELIMINARIES\n",
            "\u001b[35mSection_title: 4  APPROACH\n",
            "\u001b[35mSection_title: 4.1  SELF-GENERATED EXEMPLARS\n",
            "\u001b[35mSection_title: 4.2  SELF-GENERATED KNOWLEDGE + EXEMPLARS\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 5.1  TASKS\n",
            "\u001b[35mSection_title: 5.2  MODELS\n",
            "\u001b[35mSection_title: 5.3  METHODS TO COMPARE\n",
            "\u001b[35mSection_title: 6  RESULTS\n",
            "\u001b[35mSection_title: 6.1  MAIN RESULTS\n",
            "\u001b[35mSection_title: 6.2  KNOWLEDGE CAN COMPLEMENT EXEMPLARS\n",
            "\u001b[35mSection_title: 6.3  GENERATING VS RETRIEVING EXEMPLARS\n",
            "\u001b[35mSection_title: 6.4  SCALE OF BASE LLMS: ANALOGICAL PROMPTING EXCELS WITH LARGER MODELS\n",
            "\u001b[35mSection_title: 6.6  ERROR ANALYSIS\n",
            "\u001b[32mconclusion: we introduced analogical prompting, a new language model prompting approach that self-generates relevant reasoning exemplars for solving problems. this approach provides detailed, customized exemplars for individual problems without requiring labeled data, effectively addressing the challenges faced by existing 0-shot cot and few-shot cot prompting methods. experimental results show that our approach outperforms 0-shot cot and few-shot cot in various reasoning tasks, including math problem solving, code generation, and other logical/temporal reasoning tasks.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\u001b[32mlimitation: one limitation of our approach is increased inference computation, as our approach generates more tokens than vanilla 0-shot and 0-shot cot prompting. compared to few-shot cot, we use fewer input tokens and more output tokens, as exemplars are counted as input in few-shot cot and as output in our approach.another limitation is that self-generation can fail if the llm lacks sufficient strength or has not learned relevant knowledge to the new problems to solve. conversely, with a stronger llm, it can draw upon relevant prior knowledge to tackle slightly more complex problems. therefore, our approach is better suited for stronger or larger-scale llms.finally, it is known that llm performance can be influenced by specific prompt phrases used to query the model (jiang et al., 2020), and our work is also subject to this prompt sensitivity.recently, with the rise of large language models (llms), prompting them to engage in reasoning has proven effective and gained attention. a common approach is prompting llms to generate intermediate reasoning steps, as demonstrated by the chain-of-thought method kojima et al., 2022;, which assists llms in tackling complex reasoning tasks. several studies have extended this approach with more structured algorithms and search methods (khot et al., 2022;drozdov et al., 2022;zelikman et al., 2022;yao et al., 2023;press et al., 2022;khattab et al., 2022;jung et al., 2022), as well as longer-horizon action and planning hao et al., 2023;park et al., 2023). another line of work incorporates tools and programs into the prompting process to facilitate reasoning cheng et al., 2022;kim et al., 2023;schick et al., 2023).our work complements these efforts to enhance llm reasoning and is the first to draw inspiration from human analogical reasoning to improve llm prompting.\n",
            "\u001b[35mSection_title: 8  LIMITATIONS AND FUTURE RESEARCH\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Task Definition\n",
            "\u001b[35mSection_title: 4.1  Overview\n",
            "\u001b[35mSection_title: 4.2  Context and Question Embedding Layer\n",
            "\u001b[35mSection_title: 4.3  Contextual Embedding Layer\n",
            "\u001b[35mSection_title: 4.4  Coarse-grained Decomposition Layer\n",
            "\u001b[35mSection_title: 4.5  Fine-grained Interaction Layer\n",
            "\u001b[35mSection_title: 4.6  Modeling Layer\n",
            "\u001b[35mSection_title: 4.7  Prediction Layer\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 5.3  Main Results\n",
            "\u001b[35mSection_title: 4.4  Ablations Studies\n",
            "\u001b[32mconclusion: in this paper, we propose a mutli-hop question answering model, that contains a coarse-grained decomposition strategy to divide a complex query into multiple single-hop simple queries and a fine- grained interaction strategy to better represent each word in the document and help the model find the sentences needed to answer the question. in the experiments, we show that our models significantly and consistently outperform the baseline model.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Problem Formulation\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[32mlimitation: if we put our new more robust models to use, human adversaries may adapt to them as well.checking whether crowd-sourcing fresh attacks is indeed more difficult on the new models is beyond the scope of this work.also, we benefit from having a fair number of human adversarial examples (16.9k in anli, and 10k per round in hate speech).our methods may be less successful in a scenario with very fewer examples (∼ 10).on the flip side, we have also not evaluated these methods on classifiers with access to even larger real adversarial datasets.finally, our methods work on datasets with the notion of an original example, and a perturbed adversarial example as is the norm for adversarial robustness literature (madry et al., 2017).in the new paradigm of larger more capable nlp models, adversarial datasets may increasingly not involve a perturbation (ganguli et al., 2022b).risks: our techniques can enhance robustness given a set of observed adversarial examples.the new classifier we trained with generated data from di and ice may still be vulnerayble to future human attacks that are able to adapt to the new model (in this paper future attack rounds are known a priori from past model-in-the-loop work).this would require extensive crowd-sourcing efforts to evaluate.we also run the risk of over-fitting to the new human generated adversarial data.this may come at the cost of lower performance on future attacks generated by a different mechanism (say textfooler instead of future anli rounds), and comes at the cost of degrading accuracy on original tasks such as mnli and snli (table 19 in appendix).as a separate concern, any technique that betters generative text modeling brings the risk that humans may struggle to distinguish machine generated text.this can have negative consequences for disinformation and misinformation, which is an active area of research (pu et al., 2023).\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\u001b[32mconclusion: we  9 this attack makes small modifications to the hypothesis, explaining the high level of distributional similarity to anli rounds 1 and 2. unlike textfooler, however, bertattack makes more sophisticated contextual phrase substitutions.for example in row 2, \"partly recruited from brussels\" is replaced with \"not a volunteers units.\"\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Our Approach\n",
            "\u001b[35mSection_title: 3.1  Non-Autoregressive Program Generation\n",
            "\u001b[35mSection_title: 3.2  Training\n",
            "\u001b[35mSection_title: 3.3  Discussions\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Settings\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Performance w.r.t. Reasoning Steps\n",
            "\u001b[35mSection_title: 4.4  Performance of NAPG without Span Extraction Model\n",
            "\u001b[35mSection_title: 4.5  Performance of Soft Masking Operand Extractor\n",
            "\u001b[35mSection_title: 4.6  Performance Analysis w.r.t. Question Type\n",
            "\u001b[35mSection_title: 4.7  Ablation Study of Hyper-Parameters\n",
            "\u001b[35mSection_title: 4.8  Program Generation Speed Analysis\n",
            "\u001b[35mSection_title: 4.9  Case Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: hybrid tabular-textual question answering (qa) requires reasoning from heterogeneous information, and numerical reasoning is its key challenge compared to extractive qa.to address the severe exposure bias issue of current autoregressive methods when program generation performance is far from good, we present a non-autoregressive program generation (napg) framework for numerical reasoning, which facilitates program generation in parallel.our framework independently generates complete program tuples containing both the operator and its operands.compared to previous autoregressive decoding methods, napg does not suffer from exposure bias, and can significantly boost program generation speed.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Question Answering as Graph Alignment\n",
            "\u001b[35mSection_title: 3  Graph Alignment Model\n",
            "\u001b[35mSection_title: 3.1  Model\n",
            "\u001b[35mSection_title: 3.2  Training\n",
            "\u001b[35mSection_title: 3.3  Inference\n",
            "\u001b[35mSection_title: 3.4  Oracle Construction\n",
            "\u001b[35mSection_title: 4  Adversarial Robustness\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Adversarial Datasets\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.4  Results on Adversarial SQuAD\n",
            "\u001b[35mSection_title: 4.5  Results on Universal Trigger\n",
            "\u001b[35mSection_title: 4.6  Comparison to Existing Systems\n",
            "\u001b[35mSection_title: 5  Generalization via Alignment Constraints\n",
            "\u001b[35mSection_title: 5.1  Results on Constrained Alignment\n",
            "\u001b[35mSection_title: 5.2  Case study on Alignment Scores\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: in this work, we presented a model for doing question answering through sub-part alignment.by having our model structured around an explicit alignment scoring process, we show that our approach can to generalize better to other domains.having alignments also makes it possible to filtering out bad model predictions (by treating the scores as confidence values) and interpreting the model's behavior (by examining the alignments and scores directly).\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  The Seven Paradigms in NLP\n",
            "\u001b[35mSection_title: 2.2.1  Classification (Class)\n",
            "\u001b[35mSection_title: 2.2.2  Matching\n",
            "\u001b[35mSection_title: 2.2.3  Sequence Labeling (SeqLab)\n",
            "\u001b[35mSection_title: 2.2.4  MRC\n",
            "\u001b[35mSection_title: 2.2.5  Sequence-to-Sequence (Seq2Seq)\n",
            "\u001b[35mSection_title: 2.2.6  Sequence-to-Action-Sequence (Seq2ASeq)\n",
            "\u001b[35mSection_title: 2.2.7  (M)LM\n",
            "\u001b[35mSection_title: 2.3  Compound Paradigm\n",
            "\u001b[35mSection_title: 3  Paradigm Shift in NLP Tasks\n",
            "\u001b[35mSection_title: 3.1  Text Classification\n",
            "\u001b[35mSection_title: 3.2  Natural Language Inference\n",
            "\u001b[35mSection_title: 3.3  Named Entity Recognition\n",
            "\u001b[35mSection_title: 3.4  Aspect-Based Sentiment Analysis\n",
            "\u001b[35mSection_title: 3.5  Relation Exaction\n",
            "\u001b[35mSection_title: 3.6  Text Summarization\n",
            "\u001b[35mSection_title: 3.7  Parsing\n",
            "\u001b[35mSection_title: 3.8  Trends of Paradigm Shift\n",
            "\u001b[35mSection_title: 4  Potential Unified Paradigms in NLP\n",
            "\u001b[35mSection_title: 4.1  (M)LM\n",
            "\u001b[35mSection_title: 4.2  Matching\n",
            "\u001b[35mSection_title: 4.3  MRC\n",
            "\u001b[35mSection_title: 4.4  Seq2Seq\n",
            "\u001b[32mconclusion: recently, prompt-based tuning, which is to formulate some nlp task into a (m)lm task, has exploded in popularity. they can achieve considerable performance with much less training data. in contrast, other potential unified paradigms, i.e. matching, mrc, and seq2seq, are underexplored in the context of pre-training. one of the main reasons is that these paradigms require large-scale annotated data to conduct pre-training, especially seq2seq is notorious for data hungry.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Why Quizbowl?\n",
            "\u001b[35mSection_title: 2.1  What is a Buzzer Race?\n",
            "\u001b[35mSection_title: 2.2  Pyramidality and Buzzers\n",
            "\u001b[35mSection_title: 2.3  The Craft of Question Writing\n",
            "\u001b[35mSection_title: 2.4  Quizbowl for Natural Language Processing Research\n",
            "\u001b[35mSection_title: 2.5  Quizbowl as a Machine Learning Task\n",
            "\u001b[35mSection_title: 3.  Qanta Dataset\n",
            "\u001b[35mSection_title: 3.1  Dataset Sources\n",
            "\u001b[35mSection_title: 3.2  Number and Diversity of Quizbowl Questions\n",
            "\u001b[35mSection_title: 3.2.1  Syntactic Diversity\n",
            "\u001b[35mSection_title: 3.2.2  Topical Diversity\n",
            "\u001b[35mSection_title: 3.2.3  Answer Diversity\n",
            "\u001b[35mSection_title: 3.3  Gameplay Dataset\n",
            "\u001b[35mSection_title: 3.4  Preprocessing\n",
            "\u001b[35mSection_title: 3.4.1  Matching Quizbowl Answers to Wikipedia Pages\n",
            "\u001b[35mSection_title: 3.4.2  Dataset Folds and Annual Updates\n",
            "\u001b[35mSection_title: 4.  Deciding When and What to Answer\n",
            "\u001b[35mSection_title: 5.  Guessing Quizbowl Answers\n",
            "\u001b[35mSection_title: 5.1  Explicit Pattern Matching with Information Retrieval\n",
            "\u001b[35mSection_title: 5.2  Trainable Pattern Matching with Linear Models\n",
            "\u001b[35mSection_title: 5.3  Neural Network Models\n",
            "\u001b[35mSection_title: 5.3.1  Unordered Composition with Deep Averaging Networks\n",
            "\u001b[35mSection_title: 5.3.2  Ordered Composition with Recurrent Neural Networks\n",
            "\u001b[35mSection_title: 5.3.3  Using Sentences as Training Examples\n",
            "\u001b[35mSection_title: 5.3.4  Training and Architecture Details\n",
            "\u001b[35mSection_title: 5.4  Wikipedia as Additional Training Data\n",
            "\u001b[35mSection_title: 5.4.1  Wikipedia Text in ir Models\n",
            "\u001b[35mSection_title: 5.4.2  Wikipedia Text in Machine Learning Models\n",
            "\u001b[35mSection_title: 6.  Buzzing\n",
            "\u001b[35mSection_title: 6.1  A Classification Approach to Buzzing\n",
            "\u001b[35mSection_title: 7.  Offline Evaluation\n",
            "\u001b[35mSection_title: 7.1  Guesser Evaluation\n",
            "\u001b[35mSection_title: 7.1.1  Accuracy for Guessing Evaluation\n",
            "\u001b[35mSection_title: 7.1.2  Computing Expected Wins Versus Humans for Guessing Evaluation\n",
            "\u001b[35mSection_title: 7.1.3  Guesser Performance Comparison\n",
            "\u001b[35mSection_title: 7.2  Identifying Sources of Error\n",
            "\u001b[35mSection_title: 7.2.2  Errors Caused by Data Sparsity\n",
            "\u001b[35mSection_title: 7.2.3  Error Breakdown\n",
            "\u001b[35mSection_title: 7.3  Evaluating the Buzzer\n",
            "\u001b[35mSection_title: 8.  Live Exhibition Events\n",
            "\u001b[35mSection_title: 9.  Related Work\n",
            "\u001b[35mSection_title: 9.1  Datasets and Tasks\n",
            "\u001b[35mSection_title: 9.2  Human-in-the-Loop Adversarial Examples\n",
            "\u001b[35mSection_title: 9.3  Interruptable QA and Model Calibration\n",
            "\u001b[35mSection_title: 9.4  Multi-Hop QA\n",
            "\u001b[35mSection_title: 10.  Future Work\n",
            "\u001b[35mSection_title: 10.1  Pre-Trained Language Models\n",
            "\u001b[35mSection_title: 10.2  Generalization in Factoid Question Answering\n",
            "\u001b[35mSection_title: 10.3  Few Shot Learning and Domain Adaptation\n",
            "\u001b[35mSection_title: 10.4  Explainable and Trustable Machine Learning\n",
            "\u001b[32mconclusion: this article introduces and argues for quizbowl: an incremental question answering task. solving quizbowl questions requires sophisticated nlp such as resolving complex coreference, multi-hop reasoning, and understanding the relationships between a gigantic menagerie of entities that could be answers. fundamental to answering quizbowl questions is that the questions are incremental; this is both fun and good for research. it is fun because it allows for live, engaging competitions between humans and computers. this format-the product of refining human question answering competitions over decades-is also good for research because it allows for fair, comprehensive comparison of systems and iterative improvement as systems answer questions earlier and earlier.\n",
            "\u001b[35mSection_title: 11.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Locating text in HTML\n",
            "\u001b[35mSection_title: 3.3  Web page selection\n",
            "\u001b[35mSection_title: 3.4  Web page collection\n",
            "\u001b[35mSection_title: 3.5  Question Labeling\n",
            "\u001b[35mSection_title: 3.6  Data Augmentation\n",
            "\u001b[35mSection_title: 3.7  Final review\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.1  Dataset statistics\n",
            "\u001b[35mSection_title: 4.2  QAs in WebSRC\n",
            "\u001b[35mSection_title: 5  Baseline Models\n",
            "\u001b[35mSection_title: 5.1  Pre-trained Language Model with Text (T-PLM)\n",
            "\u001b[35mSection_title: 5.2  Pre-trained Language Model with HTML (H-PLM)\n",
            "\u001b[35mSection_title: 5.3  Visual Information Enhanced Pre-trained Language Model (V-PLM)\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Dataset Splits\n",
            "\u001b[35mSection_title: 6.2  Evaluate Metrics\n",
            "\u001b[35mSection_title: 6.3  Experiment Setup\n",
            "\u001b[35mSection_title: 6.4  Results & Discussion\n",
            "\u001b[35mSection_title: 6.5  Case Study\n",
            "\u001b[32mconclusion: in this paper, we introduce websrc, a multi-modal dataset for web-based structural reading comprehension with both html documents and screenshots. the task is to answer questions about the web pages. we evaluate several baselines on our dataset, and the results showed that incorporating layout features with textual contents is crucial to web understanding, but how to utilize such structural information requires further investigation. we hope this work can push the research on web-based structural reading comprehension forward. in the future, we will go beyond web pages to explore more structural reading comprehension tasks.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Logical Reasoning of Text\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Fact Unit Extraction and Supergraph Construction\n",
            "\u001b[35mSection_title: 3.2  Reasoning Process\n",
            "\u001b[35mSection_title: 3.3  Training Objective\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Ablation Study\n",
            "\u001b[35mSection_title: 5.2  Comparison with Alternative Fact Units\n",
            "\u001b[35mSection_title: 5.3  Influence of Scale of Fact Units\n",
            "\u001b[32mconclusion: in this work, we propose extracting a general form called \"fact unit\" to cover both commonsense and temporary knowledge units for logical reasoning. our proposed focal reasoner not only better uncovers the logical structures within the context but also better captures the logical interactions between context and options. experimental results verify the effectiveness of our method.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Setup\n",
            "\u001b[35mSection_title: 4  Human Mental Model\n",
            "\u001b[35mSection_title: 5  Teaching a Student Learner\n",
            "\u001b[35mSection_title: 6  Experimental User Study\n",
            "\u001b[35mSection_title: 6.1  Experimental Preliminaries\n",
            "\u001b[35mSection_title: 6.2  Simulated Users\n",
            "\u001b[35mSection_title: 6.3  Crowdsourced Experiments Details\n",
            "\u001b[35mSection_title: 6.4  User Study Observations and Results\n",
            "\u001b[35mSection_title: 7  Additional Synthetic Experiments\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Term-Based Retrieval\n",
            "\u001b[35mSection_title: 3.2  Gated Memory Flow\n",
            "\u001b[35mSection_title: 3.3  Reader Model\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  Analysis\n",
            "\u001b[35mSection_title: 4.5  Case Study\n",
            "\u001b[32mconclusion: in this paper, to tackle the multi-hop information retrieval challenge, we introduce an architecture that models a set of paragraphs as sequential data and iteratively identifies them. specifically, we propose gated memory flow to iterative read and memorize reasoning required information without noise information interference. we evaluate our method on both full wiki and distractor settings on the hotpotqa dataset and the method outperforms previous works by a large margin. in the future, we will attempt to design a more complicated model to improve retrieval performance and explore more about the effect of training data with different data distribution for multi-hop information retrieval.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Retrieve: Google Search for document retrieval\n",
            "\u001b[35mSection_title: 3.2  Prompt: Few-shot prompting for conditioning on evidence\n",
            "\u001b[35mSection_title: 3.3  Rerank: Increasing inference-time compute via answer reranking\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Conditioning a large-scale language model on Google search results\n",
            "\u001b[35mSection_title: 5.2  Ablations\n",
            "\u001b[35mSection_title: 5.3  Scaling analysis of open-and closed-book models\n",
            "\u001b[35mSection_title: 5.4  Increasing inference-time compute\n",
            "\u001b[35mSection_title: 5.5  Keeping\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  SPLADE (Preliminary)\n",
            "\u001b[35mSection_title: 3.2  Combination of AdaLM and IDF\n",
            "\u001b[35mSection_title: 3.2.1  Executing AdaLM\n",
            "\u001b[35mSection_title: 3.2.2  Weighting Sparse Vectors with IDF\n",
            "\u001b[35mSection_title: 3.3  Combination with Lexical Approach\n",
            "\u001b[35mSection_title: 4.2  Datasets and Evaluation Measures\n",
            "\u001b[35mSection_title: 4.3  Details of Model Training\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Comparison with other IR Methods\n",
            "\u001b[35mSection_title: 5.2  Comparison of Unsupervised Domain Adaptation Methods\n",
            "\u001b[35mSection_title: 6  Ablation with AdaLM for Confirming Assumption\n",
            "\u001b[35mSection_title: 7  Analysis for Weight of Words\n",
            "\u001b[35mSection_title: 8  Case Study\n",
            "\u001b[32mconclusion: this paper presented an effective unsupervised domain adaptation method, cai. we showed that the combination of splade with cai and the lexical approach gave a state-of-the-art performance on datasets with a large vocabulary and wordfrequency gap. in addition, cai outperformed gpl and was robust enough to show high accuracy even when bow representations were used for query expression. finally, our analysis showed that splade with cai addressed the problem of the exact matching of low-frequency words in training data. we believe that cai works on smaller mlms by distilling adalm because yao et al. (2021) showed that a distilled adalm achieved higher performance than bert on nlp tasks and formal et al. (2021) showed that the results of splade initialized with distilbert-base 14 was competitive on ms marco with other ir models initialized with bert.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Knowledge Triplet Learning\n",
            "\u001b[35mSection_title: 2.1  Using KTL to perform QA\n",
            "\u001b[35mSection_title: 2.2  Knowledge Representation Learning\n",
            "\u001b[35mSection_title: 2.3  Span Masked Language Modeling\n",
            "\u001b[35mSection_title: 3  Datasets\n",
            "\u001b[35mSection_title: 3.1  SocialIQA\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  KTL Training\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 6.1  Unsupervised Question Answering\n",
            "\u001b[35mSection_title: 6.2  Use of External Knowledge for Question Answering\n",
            "\u001b[35mSection_title: 6.3  Knowledge Representation Learning\n",
            "\u001b[32mconclusion: in this work, we propose a new framework of knowledge triplet learning over knowledge graphs. we show learning all three possible functions, f r ,f h , and f t helps the model to perform zero-shot multiple-choice question answering. we learn from the atomic knowledge graph and evaluate our framework on the socialiqa dataset. our framework achieves state-of-the-art in the zero-shot question answering task and sets a strong baseline in the few-shot question answering task. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Case-based Abductive NLI\n",
            "\u001b[35mSection_title: 3  Explanation Generation\n",
            "\u001b[35mSection_title: 3.1  Retrieve\n",
            "\u001b[35mSection_title: 3.2  Reuse\n",
            "\u001b[35mSection_title: 3.3  Refine\n",
            "\u001b[35mSection_title: 4  Abductive Inference\n",
            "\u001b[35mSection_title: 5  Empirical Evaluation\n",
            "\u001b[35mSection_title: 5.1  WorldTree\n",
            "\u001b[35mSection_title: 5.2  ARC Challenge\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.4  Impact on Semantic Drift\n",
            "\u001b[35mSection_title: 5.5  Faithfulness and Error Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mlimitation: the adopted model of explanatory power relies on the availability of human-annotated explanations with specific features (e.g., explanatory facts reused across different training instances). however, these resources might not be available in real-world scenarios and are generally costly to develop. moreover, since the explanatory power model relies on similarity measures and indicator functions, the model's ability to generalise might be sensitive to the incompleteness of the knowledge bases and the availability of representative explanations. we believe these limitations can be potentially alleviated by exploring the role of more abstract sentence representations within the cbr paradigm (bergmann and wilke, 1996).in the current implementation of cb-anli, the refine phase adopts specific assumptions to model the abstraction process required for explanation generation. this process, in fact, is performed by assuming that abstraction at the concept level translates in a correct mapping between hypotheses and central explanatory sentences. however, contextual linguistic elements can still affect the overall meaning of the specific concept being abstracted, inducing the inclusion of spurious links between sentences. while contextual elements are considered during the precedent phases through the use of contextualised embeddings and similar cases, additional work is still required to guarantee the correctness of the abstraction process.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\u001b[32mconclusion: this paper presented cb-anli, a model that integrates multi-hop and case-based reasoning (cbr) in a unified framework. we demonstrated the efficacy of the framework in complex abstractive and multi-hop nli tasks. we believe this work can open new lines of research on hybrid neurosymbolic models for explanation-based nli, and plan to investigate the efficacy of the framework on architectures that adopt richer symbolic representations in combination with neural models, further exploring the role of abstraction in case-based reasoning for improving robustness, generalisation, and explainability in nli.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background & Related Work\n",
            "\u001b[35mSection_title: 2.1  Knowledge-base question answering (KBQA)\n",
            "\u001b[35mSection_title: 2.2  Open-domain question answering from text (TextQA)\n",
            "\u001b[35mSection_title: 2.3  Question answering from tables\n",
            "\u001b[35mSection_title: 2.4  Fusion of text and knowledge-base\n",
            "\u001b[35mSection_title: 3  Modeling\n",
            "\u001b[35mSection_title: 3.1  UniK-QA architecture\n",
            "\u001b[35mSection_title: 3.2  Unified representations for KBs\n",
            "\u001b[35mSection_title: 3.3  Unified representations for lists & tables\n",
            "\u001b[35mSection_title: 4  KBQA as TextQA: A Motivating Experiment\n",
            "\u001b[35mSection_title: 5  Multi-Source QA Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Combinations of sources\n",
            "\u001b[35mSection_title: 5.3  A multi-dataset model\n",
            "\u001b[35mSection_title: 5.4  Results\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 7  Implementation Details\n",
            "\u001b[35mSection_title: 7.1  DPR Training\n",
            "\u001b[35mSection_title: 7.2  FiD Training\n",
            "\u001b[35mSection_title: 7.3  Merging KB and Text\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Relation Module\n",
            "\u001b[35mSection_title: 3.1  Augmenting Inputs\n",
            "\u001b[35mSection_title: 3.2  Object Extractor\n",
            "\u001b[35mSection_title: 3.2.1  Object Extraction Regularization\n",
            "\u001b[35mSection_title: 3.3  Relation Networks\n",
            "\u001b[35mSection_title: 4  Question Answering Baselines\n",
            "\u001b[35mSection_title: 4.1  BiDAF\n",
            "\u001b[35mSection_title: 4.2  BERT\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Ablation Study\n",
            "\u001b[35mSection_title: 7  Analysis\n",
            "\u001b[32mconclusion: in this work we propose a new relation module that can be applied on any mrc reader and help increase the prediction accuracy on non-answerable questions. we extract high level semantics from multi-head self-attentive pooling. the semantic object pairs are fed into the relation network which makes a guided decision as to whether a question is answerable. in addition we augment the context vector with plausible answers, allowing us to extract objects focused on the proposed answer span, and differentiate from other objects that are not as relevant in the context. our results on the squad 2.0 dataset using the relation module on both bidaf and bert models show improvements from the relation module. these results prove the effectiveness of our relation module.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Exploring task transferability\n",
            "\u001b[35mSection_title: 2.1  Experimental setup\n",
            "\u001b[35mSection_title: 2.2  Analyzing the transfer results\n",
            "\u001b[35mSection_title: 3  Predicting task transferability\n",
            "\u001b[35mSection_title: 3.1  Task embedding methods\n",
            "\u001b[35mSection_title: 3.2  Task embedding evaluation\n",
            "\u001b[35mSection_title: 3.3  Source task selection experiments\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this work, we conduct a large-scale empirical study of the transferability between 33 nlp tasks across three broad classes of problems, encompassing classification, question answering, and sequence labeling. we show that the benefits of transfer learning are more pronounced than previously thought, especially when target training data is limited, and we develop methods that learn vector representations of tasks that can be used to reason about the relationships between them. these task embeddings allow us to predict source tasks that will positively transfer to a given target task. our analysis suggests that data size, the similarity between the source and target tasks and domains, and task complexity are crucial for effective transfer, particularly in data-constrained regimes. \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model Architecture\n",
            "\u001b[35mSection_title: 3.2  Attention Strategies\n",
            "\u001b[35mSection_title: 3.2.1  Attention-over-Attention (AoA)\n",
            "\u001b[35mSection_title: 3.2.2  Attention Diversity (AD) layer\n",
            "\u001b[35mSection_title: 4  Model Training\n",
            "\u001b[35mSection_title: 4.1  Data Augmentation (DA)\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Competitors\n",
            "\u001b[35mSection_title: 6.1  ZSTL Experiments\n",
            "\u001b[35mSection_title: 6.2  Efficiency\n",
            "\u001b[32mconclusion: although large pre-trained language models have shown super-human performance on benchmark datasets like squad, we show that there is plenty of room to make improvements on top of bert qa . specifically, we outline prior strategies that do not work on a real benchmark consisting of \"natural questions\" showing the difficulty of the dataset and need for better algorithms. we introduce gaama and outline several strategies that are broadly classified under attention and data augmentation and show how effective it can be to attain competitive performance on nq compared to other industry baselines. we also outline gaama's ootb zero-shot transfer on two unseen datsets and show optimistic performance. our future work will involve adding larger pre-trained language models like t5 and also exploring multi-lingual qa.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Open-domain QA Evaluation\n",
            "\u001b[35mSection_title: 3.1  Models\n",
            "\u001b[35mSection_title: 3.2  Dataset\n",
            "\u001b[35mSection_title: 4  Strategies for Evaluating Open-domain QA Models\n",
            "\u001b[35mSection_title: 4.1  Supervised Evaluation via Semantic Similarity\n",
            "\u001b[35mSection_title: 4.2  Zero-shot Evaluation via Prompting\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[35mSection_title: 4.4  Results and Discussion\n",
            "\u001b[35mSection_title: 5  Linguistic Analysis of Correct Answers\n",
            "\u001b[35mSection_title: 5.1  Discussion\n",
            "\u001b[35mSection_title: 6  Regex Matching on CuratedTREC\n",
            "\u001b[32mconclusion: despite the simplicity and ubiquity of lexical matching as an evaluation metric in open-domain qa, it is unnecessarily rigid because plausible candidate answers are likely not to appear in the list of gold answers. this flaw has been long known, but the efforts to circumvent it have been mostly artisanal. in this paper, we report a systematic study of lexical matching by manually judging answers generated by several prominent open-domain qa models. we found that llms achieve stateof-the-art on nq-open. the accuracy of models is severely underestimated, with most em failure cases stemming from syntactical variations of answers. moreover, a zero-shot prompting method can be a reasonable substitute for human evaluation although it cannot detect unattributability in long-form answers. our insights and analysis in this paper will hopefully underpin the development of solid evaluation techniques in open-domain qa.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  INFORMATION-SEEKING VS PROBING QUESTIONS\n",
            "\u001b[35mSection_title: 4  FORMAT\n",
            "\u001b[35mSection_title: 4.1  Question format\n",
            "\u001b[35mSection_title: 4.1.2  Queries.\n",
            "\u001b[35mSection_title: 4.1.4  Story completion.\n",
            "\u001b[35mSection_title: 4.2  Answer format\n",
            "\u001b[35mSection_title: 4.2.3  Categorical format.\n",
            "\u001b[35mSection_title: 4.2.4  Freeform format.\n",
            "\u001b[35mSection_title: 4.3  Evidence format\n",
            "\u001b[35mSection_title: 4.3.1  Modality.\n",
            "\u001b[35mSection_title: 4.3.2  Amount of evidence.\n",
            "\u001b[35mSection_title: 5  CONVERSATIONAL FEATURES\n",
            "\u001b[35mSection_title: 6  DOMAINS\n",
            "\u001b[35mSection_title: 7  LANGUAGES\n",
            "\u001b[35mSection_title: 7.1  Monolingual resources\n",
            "\u001b[35mSection_title: 7.2  Multilingual resources\n",
            "\u001b[35mSection_title: 8  QA/RC \"SKILLS\"\n",
            "\u001b[35mSection_title: 8.1  Existing taxonomies\n",
            "\u001b[35mSection_title: 8.2  Proposed taxonomy\n",
            "\u001b[35mSection_title: 8.2.2  Retrieval.\n",
            "\u001b[35mSection_title: 8.2.3  Interpreting & manipulating input. This dimension necessarily applies to any question: both humans and machines\n",
            "\u001b[35mSection_title: 8.2.5  Multi-step reasoning.\n",
            "\u001b[35mSection_title: 9  DISCUSSION\n",
            "\u001b[35mSection_title: 9.1  What reasoning skills are actually required?\n",
            "\u001b[35mSection_title: 9.2  Analysis of question types and reasoning skills\n",
            "\u001b[35mSection_title: 9.3  What datasets have not been created?\n",
            "\u001b[32mconclusion: the number of qa/rc datasets produced by the nlp community is large and growing rapidly. we have presented the most extensive survey of the field to date, identifying the key dimensions along which the current datasets vary.\n",
            "\u001b[35mSection_title: 10  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 6168100  GCCTTCAAGCATCTGTTTAA\n",
            "\u001b[35mSection_title: 820  T T -C C C C C C C C 821 A A -A A A A T T T T 903 G G -G/A A A A A A A\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 19  Aug 2023\n",
            "\u001b[35mSection_title: 2.2  COMPMIX\n",
            "\u001b[35mSection_title: 3.1  Answer coverage\n",
            "\u001b[35mSection_title: 3.2  Answer redundancy\n",
            "\u001b[35mSection_title: 3.3  Anecdotal examples\n",
            "\u001b[35mSection_title: 4  Evaluation with COMPMIX\n",
            "\u001b[32mconclusion: we release compmix, a benchmark for heterogeneous qa that inherently requires the usage of multiple sources. answering questions in compmix requires systems to work consistently well for intents spread across five domains, and deal with a wide variety of challenging human formulations asking about rare entities. thus, our hope is that this resource can help facilitate progress in developing more robust qa models that can appropriately exploit complementary and potentially redundant sources of information. a promising direction for improvement would be to include questions that need answers of a different flavor of heterogeneity: sentences, passages, or longer lists.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Conceptual Few-shot Learning\n",
            "\u001b[35mSection_title: 3  Evaluations\n",
            "\u001b[35mSection_title: 3.1  Few-shot Learners\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.3  Baseline model (BASELINE-TK-QA-1B)\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[32mconclusion: this work introduces a task of conceptual few-shot learning that reflects on in-context learners' ability to learn to apply a specific reasoning concept that can be informative for prediction. we assess a set of recent in-context learners for this ability over a set of concepts extracted from human explanations.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Conceptual Few-shot Learning\n",
            "\u001b[35mSection_title: 3  Evaluations\n",
            "\u001b[35mSection_title: 3.1  Few-shot Learners\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.3  Baseline model (BASELINE-TK-QA-1B)\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[32mconclusion: this work introduces a task of conceptual few-shot learning that reflects on in-context learners' ability to learn to apply a specific reasoning concept that can be informative for prediction. we assess a set of recent in-context learners for this ability over a set of concepts extracted from human explanations.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Conceptual Few-shot Learning\n",
            "\u001b[35mSection_title: 3  Evaluations\n",
            "\u001b[35mSection_title: 3.1  Few-shot Learners\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.3  Baseline model (BASELINE-TK-QA-1B)\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[32mconclusion: this work introduces a task of conceptual few-shot learning that reflects on in-context learners' ability to learn to apply a specific reasoning concept that can be informative for prediction. we assess a set of recent in-context learners for this ability over a set of concepts extracted from human explanations.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Combining bi-encoders and cross-encoders\n",
            "\u001b[32mconclusion: in this work we study how parameter count influences the zero-shot effectiveness of neural retrievers. we begin by showing that in-domain effectiveness, i.e., when retrievers are fine-tuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 6  Appendix\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Opening Remarks\n",
            "\u001b[35mSection_title: 3  The CollabQA Task\n",
            "\u001b[35mSection_title: 3.1  Notations and Settings\n",
            "\u001b[35mSection_title: 3.2  A Toy Task\n",
            "\u001b[35mSection_title: 3.3  Links to other QA Tasks\n",
            "\u001b[35mSection_title: 4  Proposed Approach\n",
            "\u001b[35mSection_title: 4.1  Panelists\n",
            "\u001b[35mSection_title: 4.2  Moderator and Collaboration Policy\n",
            "\u001b[35mSection_title: 5  Experiments and Analysis\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Results and error analysis\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\u001b[32mconclusion: the fact that knowledge are not shared gives rise to individual diversity and motivates collaboration. we believe natural-language based collaboration system is a domain that has practical implication and holds scientific values. the collabqa task and dataset we proposed in this paper is a small step towards that direction.  figure 6 shows the structure and examples in our proposed knowledge graphs. g 1 contains a list of person entities. the value of a property of the entity is randomly generated within a reasonable range. for example, the value of a person's height is randomly sampled in the range [160cm, 200cm]. we add a series of constraints to make the kgs more realistic, such as a person who doesn't have job gets no annual income; a person cannot be a mayor and be an employee in some company at the same time; the largest company of a city must be located in that city, and so on.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Question Writing\n",
            "\u001b[35mSection_title: 2.3  Data Validation\n",
            "\u001b[35mSection_title: 2.3.1  Speed Validation\n",
            "\u001b[35mSection_title: 2.3.2  Untimed Validation\n",
            "\u001b[35mSection_title: 3  Dataset Information and Analysis\n",
            "\u001b[35mSection_title: 3.1  Human Accuracy\n",
            "\u001b[35mSection_title: 3.2  Size and Splits\n",
            "\u001b[35mSection_title: 3.3  Length\n",
            "\u001b[35mSection_title: 3.4  Lexical Overlap\n",
            "\u001b[35mSection_title: 3.5  Question Types\n",
            "\u001b[35mSection_title: 3.6  Reasoning Strategies\n",
            "\u001b[35mSection_title: 4.2  Results and Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we introduce the long-document qa dataset quality. this dataset was crowdsourced and validated by humans to ensure that the questions are answerable, unambiguous, and challenging. the quality-hard subset, comprising half the dataset, consists of questions that are unanswerable by annotators working under tight time constraints, helping ensure that skimming and simple search do not yield high performance.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Estimating and Smoothing Mortality Curves\n",
            "\u001b[35mSection_title: 2.1.  Dynamic Poisson Model\n",
            "\u001b[35mSection_title: 2.2.  TOPALS model from Gonzaga and Schmertmann (2016)\n",
            "\u001b[35mSection_title: 3.  Simulation Experiments\n",
            "\u001b[35mSection_title: 5.  Concluding Remarks\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Datasets\n",
            "\u001b[35mSection_title: 2.2  Baselines\n",
            "\u001b[35mSection_title: 2.3  Setup\n",
            "\u001b[35mSection_title: 2.4  Results\n",
            "\u001b[35mSection_title: 3.1  Reliability of Explanations and Prediction Accuracy\n",
            "\u001b[35mSection_title: 4  Calibrating In-Context Learning using Explanations\n",
            "\u001b[35mSection_title: 4.1  Motivating Example: Improving SYNTHETIC Dataset\n",
            "\u001b[35mSection_title: 4.2  Learning-based Calibration Framework\n",
            "\u001b[35mSection_title: 4.3  Calibrating E-SNLI\n",
            "\u001b[35mSection_title: 4.4  Calibrating ADVHOTPOT\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 6  Caveats and Risks of Explanations from Large Language Models\n",
            "\u001b[32mconclusion: we have explored the capabilities of gpt-3 in using explanation in in-context learning for textual reasoning. through our experiments on two qa datasets and an nli dataset, we find that simply including explanations in the prompt does not always improve the performance of in-context learning.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  PROBLEM DEFINITION\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  Benchmarks\n",
            "\u001b[35mSection_title: 4  METHODS\n",
            "\u001b[35mSection_title: 4.1  Dense Retrieval Methods\n",
            "\u001b[35mSection_title: 4.2  Allocation Strategies\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  Training Details\n",
            "\u001b[35mSection_title: 5.2  Evaluation\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 6  ANALYSIS\n",
            "\u001b[32mconclusion: in this work, we formalize an underexplored information retrieval task where the retriever applied to several different distributions at inference time, some of which are unseen during training. we created benchmarks for this task and evaluated several simple retrieval methods and models on these benchmarks. we show that these simple methods work well obtaining up to 8 points improvement in recall@10 over baselines and an average of 3.8 points improvement.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Question Generation\n",
            "\u001b[35mSection_title: 2.2  Controlled Generation\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  GAT-based Key Entities Extractor\n",
            "\u001b[35mSection_title: 3.3  Controlled Generator with Flag Tag\n",
            "\u001b[35mSection_title: 4.1  Data and Metrics\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 4.5  Human Evaluation\n",
            "\u001b[35mSection_title: 4.6  Ablation Study\n",
            "\u001b[35mSection_title: 4.7  Analysis for controlled generator\n",
            "\u001b[35mSection_title: 4.8  Case Study\n",
            "\u001b[32mconclusion: the mqg task is more challenging and worthy of exploration compared with conventional shallow qg. to address the complexity control problem of mqg, we propose a simple control framework cqg, which consists of a gat-based key entity extractor and a controlled generated. cqg greatly improves the performance and we hope our model will help researchers to study the mqg task.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Deductive Reasoning\n",
            "\u001b[35mSection_title: 3  Evaluating LMs for Logical Robustness\n",
            "\u001b[35mSection_title: 3.1  Logical Robustness\n",
            "\u001b[35mSection_title: 3.2  Notations\n",
            "\u001b[35mSection_title: 3.3  Logical Contrast Sets\n",
            "\u001b[35mSection_title: 3.4  Logical Equivalence Sets\n",
            "\u001b[35mSection_title: 4  The ROBUSTLR Dataset\n",
            "\u001b[35mSection_title: 4.1  Dataset Domain\n",
            "\u001b[35mSection_title: 4.2  Dataset Sampling\n",
            "\u001b[35mSection_title: 4.3  Filtering Statistical Features\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  In-domain Performance\n",
            "\u001b[35mSection_title: 6.3  Performance on Logical Equivalence set\n",
            "\u001b[35mSection_title: 6.4  Human Evaluation\n",
            "\u001b[35mSection_title: 6.5  Analysis\n",
            "\u001b[35mSection_title: 7  Related Works\n",
            "\u001b[32mconclusion: in this paper, we proposed robustlr, a diagnostic benchmark to test the logical robustness of deductive reasoning models. in robustlr, we propose two evaluation sets, logical contrast and logical equivalence, each probing different logical reasoning abilities. overall, we find that fine-tuning lms such as roberta and t5 on deductive reasoning datasets is not sufficient to learn the semantics of the logical operators conjunction, disjunction, and negation. although well-aligned training dataset improves model performance, the models still find it challenging to understand negations, both in logical contrast and logical equivalence sets. we demonstrate some interesting shortcoming of lms designed for logical reasoning, that can eventually enable building better reasoning models.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: a key limitation of the work is the synthetic nature of the dataset. while it is ideal to explore more natural theories, it makes the systematic logical perturbation process very challenging. thus, in this work, we resort to using synthetic datasets, but aim to bridge this gap in future works. another limitation is the complexity of the datasets we explore. we use fairly simple logical rules and constructs for robustlr. some more complex forms of logical reasoning-based theories can potentially reveal even more limitations of deductive reasoning models. another interesting aspect we do not explore in this scope is potential techniques to improve these models on deductive reasoning tasks. this might involve trying different inductive biases in the form of architectural designs, more specialized datasets, etc.\n",
            "\u001b[35mSection_title: 9  Limitation\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Deductive Reasoning\n",
            "\u001b[35mSection_title: 3  Evaluating LMs for Logical Robustness\n",
            "\u001b[35mSection_title: 3.1  Logical Robustness\n",
            "\u001b[35mSection_title: 3.2  Notations\n",
            "\u001b[35mSection_title: 3.3  Logical Contrast Sets\n",
            "\u001b[35mSection_title: 3.4  Logical Equivalence Sets\n",
            "\u001b[35mSection_title: 4  The ROBUSTLR Dataset\n",
            "\u001b[35mSection_title: 4.1  Dataset Domain\n",
            "\u001b[35mSection_title: 4.2  Dataset Sampling\n",
            "\u001b[35mSection_title: 4.3  Filtering Statistical Features\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  In-domain Performance\n",
            "\u001b[35mSection_title: 6.3  Performance on Logical Equivalence set\n",
            "\u001b[35mSection_title: 6.4  Human Evaluation\n",
            "\u001b[35mSection_title: 6.5  Analysis\n",
            "\u001b[35mSection_title: 7  Related Works\n",
            "\u001b[32mconclusion: in this paper, we proposed robustlr, a diagnostic benchmark to test the logical robustness of deductive reasoning models. in robustlr, we propose two evaluation sets, logical contrast and logical equivalence, each probing different logical reasoning abilities. overall, we find that fine-tuning lms such as roberta and t5 on deductive reasoning datasets is not sufficient to learn the semantics of the logical operators conjunction, disjunction, and negation. although well-aligned training dataset improves model performance, the models still find it challenging to understand negations, both in logical contrast and logical equivalence sets. we demonstrate some interesting shortcoming of lms designed for logical reasoning, that can eventually enable building better reasoning models.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: a key limitation of the work is the synthetic nature of the dataset. while it is ideal to explore more natural theories, it makes the systematic logical perturbation process very challenging. thus, in this work, we resort to using synthetic datasets, but aim to bridge this gap in future works. another limitation is the complexity of the datasets we explore. we use fairly simple logical rules and constructs for robustlr. some more complex forms of logical reasoning-based theories can potentially reveal even more limitations of deductive reasoning models. another interesting aspect we do not explore in this scope is potential techniques to improve these models on deductive reasoning tasks. this might involve trying different inductive biases in the form of architectural designs, more specialized datasets, etc.\n",
            "\u001b[35mSection_title: 9  Limitation\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Background: the Longformer Model\n",
            "\u001b[35mSection_title: 2.2  Cross-Document Language Modeling\n",
            "\u001b[35mSection_title: 2.3  CDLM Implementation\n",
            "\u001b[35mSection_title: 3  Evaluations and Results\n",
            "\u001b[35mSection_title: 3.1  Cross-Document Perplexity\n",
            "\u001b[35mSection_title: 3.2  Cross-Document Coreference Resolution\n",
            "\u001b[35mSection_title: 3.3  Document matching\n",
            "\u001b[35mSection_title: 3.4  Multihop Question answering\n",
            "\u001b[35mSection_title: 3.5  Attention Analysis\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we presented a novel pretraining strategy and technique for cross-document language modeling, providing better encoding for cross-document (cd) downstream tasks. our contributions include the idea of leveraging clusters of related documents for pretraining, via cross-document masking, along with a new long-range attention pattern, together driving the model to learn to encode cd relationships. this was achieved by extending the global attention mechanism of the longformer model to apply already in pretraining, creating encodings that attend to long-range information across and within documents. our experiments assess that our crossdocument language model yields new state-of-theart results over several cd benchmarks, while, in fact, employing substantially smaller models. our analysis showed that cdlm implicitly learns to recover long-distance cd relations via the attention mechanism. we propose future research to extend this framework to train larger models, and to develop cross-document sequence-to-sequence models, which would support cd tasks that involve a generation phase.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 1.1  Maximum Inner Product Search as the Unifying Problem\n",
            "\u001b[35mSection_title: 1.2  Sparse MIPS as a Subclass of Dense MIPS\n",
            "\u001b[35mSection_title: 1.3  Research Byproducts\n",
            "\u001b[35mSection_title: 1.4  Contributions\n",
            "\u001b[35mSection_title: 1.5  Structure\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Sparse MIPS\n",
            "\u001b[35mSection_title: 2.1.2  Signatures for Logical Queries.\n",
            "\u001b[35mSection_title: 2.1.3  General Sparse MIPS.\n",
            "\u001b[35mSection_title: 2.2  Dense MIPS\n",
            "\u001b[35mSection_title: 3  NOTATION AND EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.1  Notation\n",
            "\u001b[35mSection_title: 3.2  Experimental Configuration\n",
            "\u001b[35mSection_title: 3.2.1  Datasets.\n",
            "\u001b[35mSection_title: 3.2.2  Sparse Vectors.\n",
            "\u001b[35mSection_title: 3.2.3  Evaluation.\n",
            "\u001b[35mSection_title: 3.2.4  Hardware and Code.\n",
            "\u001b[35mSection_title: 4  ANALYSIS OF RANDOM PROJECTIONS FOR SPARSE VECTORS\n",
            "\u001b[35mSection_title: 4.1.2  Theoretical Analysis.\n",
            "\u001b[35mSection_title: 4.2  The Sinnamon Transform\n",
            "\u001b[35mSection_title: 4.2.2  Theoretical Analysis.\n",
            "\u001b[35mSection_title: 4.3  Empirical Comparison\n",
            "\u001b[35mSection_title: 5  EVALUATION OF CLUSTERING OVER SKETCHES OF SPARSE VECTORS\n",
            "\u001b[35mSection_title: 5.1  Empirical Comparison\n",
            "\u001b[35mSection_title: 6  CLUSTERING AS DYNAMIC PRUNING FOR THE INVERTED INDEX\n",
            "\u001b[35mSection_title: 6.1  Partitioning Inverted Lists\n",
            "\u001b[35mSection_title: 6.2  Query Processing over Partitioned Inverted Lists\n",
            "\u001b[35mSection_title: 6.3  Empirical Evaluation\n",
            "\u001b[35mSection_title: 6.3.3  Effect of Dynamic\n",
            "\u001b[35mSection_title: 7  TOWARDS A UNIFIED FRAMEWORK FOR MIPS\n",
            "\u001b[35mSection_title: 7.1  Motivation\n",
            "\u001b[35mSection_title: 7.2  IVF MIPS for Hybrid Vectors\n",
            "\u001b[35mSection_title: 7.3  Empirical Evaluation\n",
            "\u001b[32mconclusion: we began this research with a simple question: can we apply dense mips algorithms to sparse vectors? that led us to investigate different dimensionality reduction techniques for sparse vectors as a way to contain the curse of dimensionality. we showed, for example, that the jl transform and sinnamon behave differently on sparse vectors and can preserve inner product to different degrees. we also thoroughly evaluated the effect of clustering on sparse mips in the context of an ivf-based retrieval system. coupling dimensionality reduction with clustering realized an effective ivf system for sparse vectors, summarized in algorithms 1 and 2.\n",
            "\u001b[35mSection_title: 8  DISCUSSION AND CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Core Competencies\n",
            "\u001b[35mSection_title: 2.1  Knowledge\n",
            "\u001b[35mSection_title: 2.1.1  Linguistic Knowledge Competency\n",
            "\u001b[35mSection_title: 2.1.2  World Knowledge Competency\n",
            "\u001b[35mSection_title: 2.2  Reasoning\n",
            "\u001b[35mSection_title: 2.2.1  Causal Reasoning Competency\n",
            "\u001b[35mSection_title: 2.2.2  Deduction Reasoning Competency\n",
            "\u001b[35mSection_title: 2.2.3  Induction Reasoning Competency\n",
            "\u001b[35mSection_title: 2.2.4  Abduction Reasoning Competency\n",
            "\u001b[35mSection_title: 2.2.5  Analogical Reasoning Competency\n",
            "\u001b[35mSection_title: 2.2.6  Multi-hop Reasoning Competency\n",
            "\u001b[35mSection_title: 2.2.7  Reasoning in Scenarios\n",
            "\u001b[35mSection_title: 2.3  Reliability\n",
            "\u001b[35mSection_title: 2.3.1  Hallucination\n",
            "\u001b[35mSection_title: 2.3.2  Uncertainty and Calibration\n",
            "\u001b[35mSection_title: 2.4  Safety\n",
            "\u001b[35mSection_title: 2.4.1  Harmfulness\n",
            "\u001b[35mSection_title: 2.4.2  Unfairness and Social Bias\n",
            "\u001b[35mSection_title: 2.4.3  Others\n",
            "\u001b[35mSection_title: 3  Future Directions\n",
            "\u001b[35mSection_title: 3.1  Sentiment\n",
            "\u001b[35mSection_title: 3.1.1  Sentiment Understanding\n",
            "\u001b[35mSection_title: 3.1.2  Sentiment Generation\n",
            "\u001b[35mSection_title: 3.2  Planning\n",
            "\u001b[35mSection_title: 3.3  Code\n",
            "\u001b[32mconclusion: this survey provides a comprehensive review of various literature for the evaluation of llms. we aggregate different works with their intended competencies. some of the competencies(reasoning, knowl-edge) already have holistic evaluation benchmarks, while others(planning, coding) still face disparate challenges. the goal of this paper is to comb the numerous work concerning llms' evaluation through the lens of the core competencies test. lighten the cognitive load for assimilating numerous evaluation works due to the various functions of llms. in doing so, we have also identified the challenge faced by each competency, looking forward to alleviating it in the future.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Standard Information Extraction vs Slot Filling\n",
            "\u001b[35mSection_title: 1.2  Slot Filling: General vs Biomedical Domain\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Biomedical Slot Filling\n",
            "\u001b[35mSection_title: 4  Biomedical Slot Filling Dataset\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Retrieval\n",
            "\u001b[35mSection_title: 5.1.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1.2  Results\n",
            "\u001b[35mSection_title: 5.2  Slot Filling Reader\n",
            "\u001b[35mSection_title: 5.3  End to End Evaluation\n",
            "\u001b[32mconclusion: in this work we formulated the task of biomedical information extraction as a slot filling problem. this approach aims to forgo the need for entity and relation type specific training data, which is scarce and costly to annotate in the biomedical domain. additionally, this formulation allows to deal with the addition of new relation types, without needing to re-train the relevant models. additionally, we have introduced a new biomedical slot filling benchmark and used it to train a biomedical dpr model, a dual bert-based encoder for retrieval, as well as a biomedical slot filling reader based on biobert. in a series of experiments our approach outperforms significantly a number of general domain baselines as well as the simpler bm25 retriever. furthermore, our results illustrate the importance of in-domain, taskspecific training data, in line with findings from recent works (glass et al., 2021;maillard et al., 2021).\n",
            "\u001b[35mSection_title: 6  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Bi-encoder Dense Retriever\n",
            "\u001b[35mSection_title: 2.2  Pseudo-positive Construction for Unsupervised Text Retrieval\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Query Extraction (QEXT)\n",
            "\u001b[35mSection_title: 3.1.1  Query Extraction by Document Structural Heuristics\n",
            "\u001b[35mSection_title: 3.1.2  Query Extraction by Salient Span Selection\n",
            "\u001b[35mSection_title: 3.2  Transferred Query Generation (TQGEN)\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.1.1  Training Data\n",
            "\u001b[35mSection_title: 4.1.2  Test Data\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 4.4.1  Unsupervised Retrieval Results\n",
            "\u001b[35mSection_title: 4.4.2  Unsupervised Domain-Adaptation Results\n",
            "\u001b[35mSection_title: 4.4.3  Fine-Tuned Results\n",
            "\u001b[35mSection_title: 4.5  Result Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this study, a series of scalable augmentation techniques are proposed to produce surrogate queries for training dense retrievers without using any annotated pairs. we achieve state-of-the-art performance on two collections of widely used benchmarks (beir and six odqa datasets), demonstrating that the inductive bias of the synthetic querydoc pairs is effective for training dense retrievers, greatly bridging the gap between unsupervised dense models and bm25 and inspiring us to rethink the necessity of using real queries.\n",
            "\u001b[35mSection_title: 6  Discussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.  ip/it\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Overview of Existing Datasets\n",
            "\u001b[35mSection_title: 4  Link Between EXNLP Data, Modeling, and Evaluation Assumptions\n",
            "\u001b[35mSection_title: 5  Rise of Structured Explanations\n",
            "\u001b[35mSection_title: 6  Increasing Explanation Quality\n",
            "\u001b[35mSection_title: 6.1  A Two-Stage Collect-And-Edit Approach\n",
            "\u001b[35mSection_title: 6.2  Teach and Test the Underlying Task\n",
            "\u001b[35mSection_title: 6.3  Addressing Ambiguity\n",
            "\u001b[35mSection_title: 7  Increasing Explanation Diversity\n",
            "\u001b[35mSection_title: 7.1  Use a Large Set of Annotators\n",
            "\u001b[35mSection_title: 7.2  Multiple Annotations Per Instance\n",
            "\u001b[35mSection_title: 7.3  Get Ahead: Add Contrastive and Negative Explanations\n",
            "\u001b[32mconclusion: we have presented a review of existing datasets for exnlp research, highlighted discrepancies in data collection that can have downstream modeling effects, and synthesized the literature both inside and outside exnlp into a set of recommendations for future data collection.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Furthest Reasoning\n",
            "\u001b[35mSection_title: 3.1.1  Prompting\n",
            "\u001b[35mSection_title: 3.1.2  Iteration for Furthest Reasoning\n",
            "\u001b[35mSection_title: 3.2  Plan Assessor\n",
            "\u001b[35mSection_title: 3.2.1  Planning Decision\n",
            "\u001b[35mSection_title: 3.2.2  Query Filter\n",
            "\u001b[35mSection_title: 3.2.3  Query Scorer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[32mconclusion: based on the experiments and results presented above, it is evident that furthest reasoning with plan assessment exhibits exceptional performance in addressing multi-hop questions.firstly, the furthest reasoning framework significantly contributes not only to the reasoning process but also to the direct answering process.in complex and long-term problem-solving scenarios such as multi-hop questions, furepa is designed to generate more effective plans, avoiding the influence of convoluted previous reasoning processes.moreover, the plan assessment component plays a crucial role in selecting the most suitable query from the llm-generated candidates, leading to more relevant evidence retrieval by the information retriever (ir).this enhancement in query selection contributes to a more effective and accurate multi-hop question-solving process.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 2.1  Multi-Hop Dataset Creation\n",
            "\u001b[35mSection_title: 2.2  Chain-of-Thought Prompting\n",
            "\u001b[35mSection_title: 2.3  Model Enhancement via LLM Generation\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  2-Hop QAs via Self-Generation\n",
            "\u001b[35mSection_title: 3.2  Multi-Hop QAs via Composition\n",
            "\u001b[35mSection_title: 3.3  Adaptive In-context Demonstration\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Benchmarks and Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  Experiment Settings\n",
            "\u001b[35mSection_title: 4.3  Experiment Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Methods of Demonstration Sampling\n",
            "\u001b[35mSection_title: 5.2  Impact of Demonstration Amount\n",
            "\u001b[35mSection_title: 5.3  Intermediate Reasoning Quality Analysis\n",
            "\u001b[32mconclusion: in this work, we harness the capabilities of llms combined with self-prompted cots to tackle the intricate mhqa task within the open-domain context, termed as odmr.our innovative sp-cot not only sets a new benchmark by surpassing preceding cot prompting techniques but also outclasses the erstwhile sota llm-only methodologies in open-domain question-answering.a distinguishing feature of sp-cot is its proficiency in eliciting high-caliber intermediate reasoning steps, and its universal efficacy across both large and smallscale llms.we anticipate our innovative selfgeneration pipeline for odmr to not just be foundational for sp-cot, but also to pave the way for future research, catalyzing a shift towards leveraging self-generation in llms, by llms, and for llms.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Our Multi-task Model\n",
            "\u001b[35mSection_title: 4  Datasets and Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  Debiased Dataset\n",
            "\u001b[35mSection_title: 4.3  Adversarial Dataset\n",
            "\u001b[35mSection_title: 4.4  Evaluation Metrics\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Effectiveness of the UR Tasks\n",
            "\u001b[35mSection_title: 5.2  Analyses\n",
            "\u001b[32mconclusion: we analyze the effectiveness of the underlying reasoning tasks using two multi-hop datasets: 2wiki and hotpotqa-small. the results reveal that the underlying reasoning tasks can improve qa performance. using four debiased sets, we demonstrate that the underlying reasoning tasks can reduce the reasoning shortcuts of the qa task. the results also reveal that the underlying reasoning tasks do not make the models more robust on adversarial examples, such as sub-questions and inverted questions. we encourage future studies to investigate the effectiveness of the entity-level reasoning task in the form of sub-questions.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[32mconclusion: we take a step forward in constructing the explainable method for multi-hop question answering by proposing two effective improvements. the global differentiable learning strategy learns optimal reasoning paths by exploring latent probability space to alleviate the problem of semantic space mismatch and error propagation. the dynamic adaptive reasoner improves generalization to unseen sub-questions.  figure 4: case study. the green font represents the correct predicted answer, and the red font represents the incorrect. our method successfully learns the intermediate reasoning process and shows better interpretability.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: • question decomposition is the pre-stage of building interpretable models. to the best of our knowledge, there is only one largescale question decomposition dataset (wolfson et al., 2020), and the performance of existing automatic decomposition models is far below human performance. inaccurate question decomposition leads to errors in reasoning. therefore, exploring better question decomposition techniques is a challenging and rewarding direction.• existing interpretable models (min et al., 2019;jiang and bansal, 2019;ding et al., 2019;khot et al., 2021;wolfson et al., 2020), including our approach, focus on solving complex questions, ignoring a simple question with a complex context that requires a deep understanding of the context to reason out the answer.• the dynamic adaptive reasoner introduces a small number of additional parameters in the router, which can increase the computational cost. a more efficient parameter-free routing approach can be explored in the future.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Using Explanations for Black Box Model Calibration\n",
            "\u001b[35mSection_title: 2.1  Generating Explanations\n",
            "\u001b[35mSection_title: 2.2  Extracting Features by Combining Explanations and Heuristics\n",
            "\u001b[35mSection_title: 2.3  Calibrator Model\n",
            "\u001b[35mSection_title: 3  Tasks and Datasets\n",
            "\u001b[35mSection_title: 4.1  Main Results: QA\n",
            "\u001b[35mSection_title: 4.2  Main Results: NLI\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[35mSection_title: 4.4  Comparison to Finetuned Models\n",
            "\u001b[35mSection_title: 5  Selective QA Setting\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: limitations despite showing promising results in improving model generalization performance, our attribution-based approach does suffer from intensive computation cost. using either lime or shap to generate attributions requires running inference a fair number of perturbations when the input size is large (see appendix for details), which limits our method's applicability. but this doesn't undermine the main contribution of this paper, answering the question in the title, and our approach is still applicable as-is in the scenarios where we pay for access to the model but not per query.\n",
            "\u001b[35mSection_title: 7  Discussion & Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work and Background\n",
            "\u001b[35mSection_title: 2.1.  Question Generation\n",
            "\u001b[35mSection_title: 2.2.  Multi-Hop Question Generation\n",
            "\u001b[35mSection_title: 2.3.  Evaluation of Question Generation\n",
            "\u001b[35mSection_title: 2.4.  Seq2seq Generation Model and Attention-Based Decoder\n",
            "\u001b[35mSection_title: 2.5.  Attention-Based Decoder\n",
            "\u001b[35mSection_title: 3.  Model Architecture\n",
            "\u001b[35mSection_title: 3.1.  Encoder\n",
            "\u001b[35mSection_title: 3.2.  Decoder\n",
            "\u001b[35mSection_title: 3.3.  The Context Switch Mechanism\n",
            "\u001b[35mSection_title: 3.4.  Training Objective\n",
            "\u001b[35mSection_title: 4.  Experiments\n",
            "\u001b[35mSection_title: 4.1.  Data Reparation\n",
            "\u001b[35mSection_title: 4.2.  Training and Inference Setup\n",
            "\u001b[35mSection_title: 4.3.  Evaluation\n",
            "\u001b[35mSection_title: 4.3.1.  Evaluated Models\n",
            "\u001b[35mSection_title: 4.3.2.  Automatic Evaluation\n",
            "\u001b[35mSection_title: 4.3.3.  Human Evaluation\n",
            "\u001b[35mSection_title: 5.  Discussion and Future Work\n",
            "\u001b[32mconclusion: in this paper, we propose a novel question generation model incorporating the hierarchical encoding-decoding structure in order to inject the structural information of input documents, and a context switch mechanism for the purpose of stabilizing the decoding and making the generation process more consistent. the automatic metric results in table 1 show our model achieves the best performance against baseline models on rouge-l in automatic metrics evaluation, although our model does not outperform baseline models on the other baseline models. nonetheless, the results in table 1 prove that our proposed context switch mechanism improves the model's performance on automatic metrics. furthermore, the human evaluation results also show our model outperforms all baseline models on four criteria we used. the experimental results of both automatic evaluation and human evaluation support the effectiveness of our proposed approach on the multi-hop qg task. in addition, we also conduct extensive studies analyzing the model's performance on different question types according to both automatic evaluation metrics and human evaluation scores. future work will include incorporating our method into pre-trained language models.\n",
            "\u001b[35mSection_title: 6.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  APIs\n",
            "\u001b[35mSection_title: 3.1  Usability\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  BEIR\n",
            "\u001b[35mSection_title: 4.2  Multilingual Retrieval: MIRACL\n",
            "\u001b[32mconclusion: the incredible capabilities of transformer-based language models at scale have attracted a handful of companies to offer access to their proprietary llms via apis. in this paper, we aim to qualitatively and quantitatively examine semantic embedding apis that can be used for information retrieval. our primary focus is to assess existing apis for domain generalization and multilingual retrieval. our findings suggest that re-ranking bm25 results is a suitable and cost-effective option for english; on the beir benchmark, openai ada2 performs the best on average. in multilingual settings, while re-ranking remains a viable technique, a hybrid approach produces the most favorable results. we hope that our insights aid practitioners and researchers in selecting appropriate apis based on their needs in this rapidly growing market.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  APIs\n",
            "\u001b[35mSection_title: 3.1  Usability\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  BEIR\n",
            "\u001b[35mSection_title: 4.2  Multilingual Retrieval: MIRACL\n",
            "\u001b[32mconclusion: the incredible capabilities of transformer-based language models at scale have attracted a handful of companies to offer access to their proprietary llms via apis. in this paper, we aim to qualitatively and quantitatively examine semantic embedding apis that can be used for information retrieval. our primary focus is to assess existing apis for domain generalization and multilingual retrieval. our findings suggest that re-ranking bm25 results is a suitable and cost-effective option for english; on the beir benchmark, openai ada2 performs the best on average. in multilingual settings, while re-ranking remains a viable technique, a hybrid approach produces the most favorable results. we hope that our insights aid practitioners and researchers in selecting appropriate apis based on their needs in this rapidly growing market.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK 2.1 Knowledge-Intensive Language Tasks\n",
            "\u001b[35mSection_title: 2.2  Retrieval-Enhanced Text Generation\n",
            "\u001b[35mSection_title: 2.3  Knowledge Distillation\n",
            "\u001b[35mSection_title: 3  METHODOLOGY 3.1 Problem Formalization\n",
            "\u001b[35mSection_title: 3.2  Method Overview\n",
            "\u001b[35mSection_title: 3.3  Passage Ranker\n",
            "\u001b[35mSection_title: 3.4  Improving Passage Ranker with Generative Passage Estimator\n",
            "\u001b[35mSection_title: 3.4.3  Curriculum Knowledge Distillation.\n",
            "\u001b[35mSection_title: 3.5  Ranker-Augmented Answer Generator\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETTINGS 4.1 Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Implementation Details\n",
            "\u001b[35mSection_title: 4.4.1  Model Settings.\n",
            "\u001b[35mSection_title: 4.4.2  Hyper-parameter Settings.\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 5.1  Performance Comparison\n",
            "\u001b[35mSection_title: 5.2  Effect of Passage Ranking\n",
            "\u001b[35mSection_title: 5.3  Effect of Curriculum Knowledge Distillation\n",
            "\u001b[35mSection_title: 5.4  Impact of Different Passage Estimators\n",
            "\u001b[35mSection_title: 5.5  Case Study\n",
            "\u001b[32mconclusion: in this paper, we focus on narrowing the gap between retrieval and generation for retrieval-enhanced text generation methods. we propose griprank, a novel approach to improve the passage ranking capability by distilling knowledge from a generative passage estimator to the passage ranker. we evaluate our approach on diverse knowledge-intensive language tasks, including zero-shot slot filling, open-domain question answering, and knowledge-enhanced dialogue generation. experimental results show that the proposed griprank presents advantages over previous state-of-the-art approaches. further analysis demonstrates the effectiveness of our proposed approach in narrowing the gap between passage retrieval and answer generation.\n",
            "\u001b[35mSection_title: 6  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Existing Datasets\n",
            "\u001b[35mSection_title: 3.  Dataset Collection\n",
            "\u001b[35mSection_title: 3.1  Collecting Passages\n",
            "\u001b[35mSection_title: 3.2  Collecting Question-answer Pairs\n",
            "\u001b[35mSection_title: 4.  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.1  Type of Questions\n",
            "\u001b[35mSection_title: 4.2  Type of answers analysis\n",
            "\u001b[35mSection_title: 5.  Experiment & Results\n",
            "\u001b[35mSection_title: 5.1  Experimental Results\n",
            "\u001b[35mSection_title: 5.2  Result Analysis\n",
            "\u001b[32mconclusion: we introduce korquad1.0-korean question answering dataset, a large-scale standard questionanswering dataset and contribute to researchers in multilingual natural language processing.this data is collected on the same basis as the english standard data, squad, and the properties of the data are similar.accordingly, we present korquad1.0as standard data for korean extractive machine reading comprehension task.the data is freely available through the github site.we also launch korquad1.0as a challenge to encourage exploration of models and provide an evaluation of performance between models.we intend to continually create standard data regarding variety of qa research areas such as the task of assessing whether a system can know what it cannot answer, the task of extracting answers by among various documents, and the task of questioning on formatted documents that have structures such as tables [9] or web documents.we also will keep providing fair evaluation among models on standard datasets and contribute to the active multilingual language engineering research.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Training a Bias-Only Model\n",
            "\u001b[35mSection_title: 3.2  Training a Robust Model\n",
            "\u001b[35mSection_title: 3.2.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2.2  General Approach\n",
            "\u001b[35mSection_title: 3.2.3  Bias Product\n",
            "\u001b[35mSection_title: 3.2.4  Learned-Mixin\n",
            "\u001b[35mSection_title: 3.2.5  Learned-Mixin +H\n",
            "\u001b[35mSection_title: 4  Evaluation Methodology\n",
            "\u001b[35mSection_title: 4.1  Reweight Baseline\n",
            "\u001b[35mSection_title: 4.2  Hyperparameters\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Synthetic Data\n",
            "\u001b[35mSection_title: 5.2  VQA-CP\n",
            "\u001b[35mSection_title: 5.3  HANS\n",
            "\u001b[35mSection_title: 5.4  Adversarial SQuAD\n",
            "\u001b[35mSection_title: 5.5  TriviaQA-CP\n",
            "\u001b[35mSection_title: 5.6  Discussion\n",
            "\u001b[32mconclusion: our key contribution is a method of using human knowledge about what methods will not generalize well to improve model robustness to domain-shift. our approach is to train a robust model in an ensemble with a pre-trained naive model, and then use the robust model alone at test time. extensive experiments show that our method works well on two adversarial datasets, and two changing-prior datasets, including a 12 point gain on vqa-cp. future work includes learning to automatically detect dataset bias, which would allow our method to be applicable with less specific prior knowledge.  \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset\n",
            "\u001b[35mSection_title: 2.1  Collection\n",
            "\u001b[35mSection_title: 2.2  Statistics\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Baseline\n",
            "\u001b[35mSection_title: 3.2  Task Transferring\n",
            "\u001b[35mSection_title: 3.3  Language Transferring\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 5  Related work\n",
            "\u001b[32mconclusion: in this paper, a new question answering dataset, danetqa, is presented.it comprises binary yes/no questions, paired with paragraphs, which should be used to answer the questions.the overall collection procedure follows the design of the boolq dataset, which is a magnitude larger in size than danetqa, partially due to the use of proprietary sources.we establish a straightforward baseline, exploiting fasttext and rubert models and experiment with multiple transfer learning settings.our results show, that on the one hand, the english dataset can be leveraged to improve the results for the russian one.however, we can not confirm, that we can re-use boolq for training the model while keeping danetqa for evaluation only.this brings us to the following conclusion: although the re-creation of english datasets in other languages may seem like a redundant and secondary activity, the current state of the cross-lingual models does not allow for perfect language transfer.it is not enough to train the model on the english data.it seems impossible to gain high-quality results if the model is not trained in the target language.this highlights the need for future development: the development of more advanced cross-lingual contextualized encoders as well as more sophisticated datasets to evaluate cross-lingual tasks.as for danetqa development, we plan to enlarge the dataset with more question-paragraph pairs and to extend the dataset with an unanswerable question, affecting though the task setting.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Langevin equation and Fokker-Planck equation for delayed systems\n",
            "\u001b[35mSection_title: 2.1  Second member of the Fokker-Planck hierarchy from Novikov's theorem\n",
            "\u001b[35mSection_title: 3  Markovian embedding\n",
            "\u001b[35mSection_title: 4  Derivation of first member of Fokker-Planck hierarchy via Markovian embedding\n",
            "\u001b[35mSection_title: 4.1  Marginalization\n",
            "\u001b[35mSection_title: 5  Derivation of higher members via Markovian embedding\n",
            "\u001b[35mSection_title: 5.1  Marginalization\n",
            "\u001b[35mSection_title: 5.2  Transformation of phase space, limiting procedure\n",
            "\u001b[32mconclusion: in this work, we have discussed the probabilistic description of delayed stochastic systems. as a starting point, we have reviewed an earlier approach based on novikov's theorem, from which a fokker-planck description can be derived in the form of an infinite hierarchy [30][31][32]. the first member is the well-known fpe for the one-time pdf, which contains the two-time pdf and is thus not closed. still, this equation has in the past been shown to be an important tool in the search for exact results [36,37] and a valuable starting point for approximations [31,38]. the main purpose of our work was to shed light onto the higher members of this hierarchy, which have rarely been discussed in earlier literature.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Open-Domain Multi-hop Question Answering\n",
            "\u001b[35mSection_title: 2.1.2  Using entities in retrievals.\n",
            "\u001b[35mSection_title: 2.2  Query Performance Prediction\n",
            "\u001b[35mSection_title: 2.2.2  Query performance prediction in QA.\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Retrieval Paths\n",
            "\u001b[35mSection_title: 3.2  Retrieval Paths in HotpotQA\n",
            "\u001b[35mSection_title: 3.3  Difficulty Estimation based on Retrieval Paths\n",
            "\u001b[35mSection_title: 3.4  Estimating the Model Parameters\n",
            "\u001b[35mSection_title: 3.4.2  Detecting the retrieval path of questions.\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL EVALUATION 4.1 Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2.1  Correlation with the average precision.\n",
            "\u001b[35mSection_title: 4.2.3  Paragraph exact match and recall.\n",
            "\u001b[35mSection_title: 4.3  Retrieval Models and QPP Baselines\n",
            "\u001b[35mSection_title: 4.4  Compared to Pre-retrieval QPP Baselines\n",
            "\u001b[35mSection_title: 4.4.1  Pairwise Difficulty Comparison.\n",
            "\u001b[35mSection_title: 4.4.2  Multi-hop Pointwise Correlation.\n",
            "\u001b[35mSection_title: 4.4.3  Single-hop Pointwise Correlation.\n",
            "\u001b[35mSection_title: 4.5  Performance Across Difficulty Classes\n",
            "\u001b[35mSection_title: 4.6  Two Use Cases\n",
            "\u001b[32mconclusion: in this paper, we introduce the task of query performance prediction for multi-hop questions. we present an approach to estimate a difficulty score of a multi-hop question based on the clues in the question. we propose retrieval paths based on overlapping terms between the question and its supporting documents. our experimental evaluation shows significant correlations between the performance of the retrievers used in our evaluation and our estimated difficulty scores, and those correlations are much higher than those obtained by our qpp baselines from the literature. the same trend is observed for the end-to-end models with the performance 7 https://github.com/mhmdsmdi/performance-prediction-for-multihop-qa figure 3: performance, in terms of f1-score, of mdr [37] with the adaptive retriever compared to a constant retriever while k varied, showing that the adaptive retriever achieves a higher performance under the same budget considerably dropped for the questions that are deemed difficult by our model. determining the difficulty of a multi-hop question using a pre-retrieval method can assist the retrievers to have a better chance of retrieving all required documents to answer the question. as a possible future direction, our models may be improved by considering more refined retrieval path types and maybe parameter settings. also analyzing the performance of down-stream tasks using our difficulty score estimation is another possible direction.\n",
            "\u001b[35mSection_title: 5  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  TASK AND DATA\n",
            "\u001b[35mSection_title: 2.1  TASK DEFINITION\n",
            "\u001b[35mSection_title: 2.2  DATA SOURCE AND ANNOTATION\n",
            "\u001b[35mSection_title: 2.3  DATASET DETAILS\n",
            "\u001b[35mSection_title: 3  BASELINE MODELS\n",
            "\u001b[35mSection_title: 3.1  REASONING GRAPH ENCODING\n",
            "\u001b[35mSection_title: 3.2  SUPERVISED TRAINING\n",
            "\u001b[35mSection_title: 3.3  FEW-SHOT PROMPTING\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  EVALUATION METRICS\n",
            "\u001b[35mSection_title: 4.2  RESULTS\n",
            "\u001b[35mSection_title: 4.2.1  ERROR ANALYSIS\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: we aim to enable machines to perform multi-step reasoning while explaining their answers. we believe that teaching machines how to manipulate premises and reach conclusions can be an important step towards true language understanding. with that in mind, we introduce street, a new multi-task reasoning and explanation resource covering various forms of reasoning in the context of questionanswering. we hope this benchmark will allow for a more systematic evaluation of the reasoning capabilities of natural language systems. future avenues of research include exploring the reasoning capabilities and knowledge retrieval and using supervised models trained on multi-step reasoning data to bootstrap unsupervised learning for multi-step reasoning.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Proof Representation\n",
            "\u001b[35mSection_title: 3.2  Task Description\n",
            "\u001b[35mSection_title: 3.3  PROVER: Joint QA and Proof Generation Model\n",
            "\u001b[35mSection_title: 3.4  ILP Inference for Global Constraints\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  QA and Proof Results for Varying Depths\n",
            "\u001b[35mSection_title: 4.3  Zero-Shot Evaluation\n",
            "\u001b[35mSection_title: 4.4  Generalization to Higher Depths\n",
            "\u001b[35mSection_title: 4.5  Varying Training Data Size\n",
            "\u001b[35mSection_title: 4.6  Evaluation on Complex Language\n",
            "\u001b[35mSection_title: 4.7  Ablation and Error Analysis\n",
            "\u001b[35mSection_title: 5  Discussion and Future Work\n",
            "\u001b[32mconclusion: we introduce prover, an interpretable joint model that answers binary questions over natural language rule-bases and generates corresponding proofs. the proofs are generated through the node and edge modules of the model in the presence of multiple global constraints during training and ilp inference. our model improves state-of-theart qa accuracy in the zero-shot scenario by 6% and generates proofs accurately. prover also generalizes much better to higher depth questions with up to 15% absolute improvement in qa performance over ruletakers. prover's modeling is relatively generic, and similar proof generation methods can be explored in traditional multi-hop qa tasks. prover can also be a helpful aid to formal reasoners in scenarios where rules are fuzzy and creating rule-bases in a formal language is tedious or infeasible. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 340  C A A T A A T T T C C C C A T A T G T G G A A T T T G A T A A G T T A G A G C A C C C A C T T C A A T T C A A G A A G A T T\n",
            "\u001b[35mSection_title: 340  C A A T A A T T T C C C C A T A T G T G G A A T T T G A T A A G T T A G A G C A C C C A C T T C A A T T C A A G A A G A T T\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Quantifying NLI Dataset Bias\n",
            "\u001b[35mSection_title: 2.1  Saturation and Bias Scoring\n",
            "\u001b[35mSection_title: 2.2  Relative Dataset Bias Scoring\n",
            "\u001b[35mSection_title: 2.3  Biased Model Results\n",
            "\u001b[35mSection_title: 2.4  Model Training\n",
            "\u001b[35mSection_title: 2.4.1  Replication Training Details\n",
            "\u001b[35mSection_title: 2.4.2  Single Sentence Condition Training\n",
            "\u001b[35mSection_title: 3  Analyzing NLI Dataset Bias\n",
            "\u001b[35mSection_title: 3.1  Sample-level Model Behavior\n",
            "\u001b[35mSection_title: 3.2  Cluster-based Bias Evaluation\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Result-Metric Correlations\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[35mSection_title: 6.1  PECO vs Dataset Cartography\n",
            "\u001b[32mconclusion: in the half decade since (poliak et al., 2018) single sentence relation leakage bias has proven to remain a difficult issue. efforts to debias nli have led to datasets that merely exhibit different kinds of bias than those shown before, or less saturated benchmarks that continue to exhibit cheating features. future work must prioritize reducing observable bias directly using a model-driven approach.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  HAC1 and Its Translation Control via Ire1p-Mediated Splicing\n",
            "\u001b[35mSection_title: 2.  HAC1 and Its Translation Control via Ire1p-Mediated Splicing\n",
            "\u001b[35mSection_title: 2.1.  Intron-Mediated Translation Control of HAC1 mRNA\n",
            "\u001b[35mSection_title: 2.1.  Intron-Mediated Translation Control of HAC1 mRNA\n",
            "\u001b[35mSection_title: 2.2.  Ire1p-Mediated Splicing of HAC1 mRNA\n",
            "\u001b[35mSection_title: 2.3.  Hac1p Triggers UPR and Is Autoregulated\n",
            "\u001b[35mSection_title: 2.3.  Hac1p Triggers UPR and Is Autoregulated\n",
            "\u001b[35mSection_title: 3.  Leaky Splicing of HAC1 mRNA in Non-UPR Yeast Cells\n",
            "\u001b[35mSection_title: 4.  Leaky Translation of HAC1 u\n",
            "\u001b[35mSection_title: 5.  Ire1p Domain Structure and Its Splicing Activity\n",
            "\u001b[35mSection_title: 7  of 17\n",
            "\u001b[35mSection_title: 6.  How Is Ire1p Activity Regulated in Response to Unfolded/Misfolded Proteins in Yeast?\n",
            "\u001b[35mSection_title: 7.  Conservation and Diversification of Ire1p-Mediated UPR Signaling\n",
            "\u001b[35mSection_title: 7.1.  Conservation and Diversification of the Ire1p-Hac1p Pathway\n",
            "\u001b[35mSection_title: 7.2.  Conservation and Diversity of Ire1p-Mediated Splicing\n",
            "\u001b[35mSection_title: 7.3.  Diversity in Translation Control\n",
            "\u001b[32mconclusion: (ire1p+hac1p)-mediated upr signaling represents a beautiful translation control mechanism created by nature. however, there are still many questions unanswered. what recognizes 3'be of hac1 pre -mrna and carries hac1 pre-mrna to ire1p foci to be spliced? how is translation regulated in hac1 homologues that do not have introns in fungal species? is ire1p oligomerization a consequence of ire1p phosphorylation or the opposite? how does ire1p dephosphorylation impact ire1p oligomerization and hac1 splicing activity? how do cells exit upr after its induction? how does this signaling system evolve and diverge in different lineages? i hope that this review will serve as an anchor for addressing all these questions in coming years. \n",
            "\u001b[35mSection_title: 8.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Generating Variables and Interactions with TreeNet and Association Rules Analysis for the Multinomial Logit Model\n",
            "\u001b[35mSection_title: 2.1.  TreeNet\n",
            "\u001b[35mSection_title: 2.2.  Association Rules Analysis (ASA)\n",
            "\u001b[35mSection_title: 3.  Proposed Method\n",
            "\u001b[35mSection_title: 4.  Illustrated Example: Thyroid Dataset\n",
            "\u001b[35mSection_title: 4.1.  Results from Model 1\n",
            "\u001b[35mSection_title: 4.3.  Performance Comparison Using the Training Set\n",
            "\u001b[35mSection_title: 4.4.  Performance Comparison Using the Test Set\n",
            "\u001b[35mSection_title: 4.5.  Performance Comparison with Other Methodologies\n",
            "\u001b[32mconclusion: our model-building framework advances the multinomial logit model by generating variables and interactions as candidate predictors. we demonstrate that the integration of three techniques-treenet, asa, and the multinomial logit model-constitutes a powerful and practical innovation for data analysis and also paves the way for additional innovative strategies. we have shown via our application example that these newly generated variables and interactions make a significant contribution to improving the multinomial logit model. our selected model from the thyroid dataset outperforms all of the 21 methods previously applied to the test data.\n",
            "\u001b[35mSection_title: 5.  Discussion and Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work in Linguistic Diversity\n",
            "\u001b[35mSection_title: 3  Data\n",
            "\u001b[35mSection_title: 4  Diversity Measures\n",
            "\u001b[35mSection_title: 4.1  Variety\n",
            "\u001b[35mSection_title: 4.2  Balance\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Diversity Measure Validation\n",
            "\u001b[35mSection_title: 5.2  PARSEME Shared Task Use Case\n",
            "\u001b[35mSection_title: 5.3  Limits of the Diversity Measures: Case Study of Turkish\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: in this paper, we borrowed the formalization of the notion of diversity from the literature in ecology. we focused on two out of the three main aspects of diversity, namely variety and balance. our contribution is to apply these measures to assess intralinguistic diversity, focusing on the particular phenomenon of multiword expressions. we not only formalize variety and balance measures in this context but we also put forward methods for selecting those variants of these measure which fit the nature of the mwe phenomenon. this validation methodology is based on corpus sampling with variable sample size. as a result, we retain richness and the e 2,1 evenness as the optimal variety and balance measures for mwes (among those studied by us). we apply these measures to the corpora and system results in the parseme shared task on automatic identification of mwes. the results show that richness of the correct annotations produced by the systems is roughly consistent with their f-measure performances. however, their balance is much less correlated with more traditional measures. we also display the limits of the richness and balance measures, when calculated on automatically annotated data, due to incorrect approximation of types under improper lemmatization in a morphologically rich language.\n",
            "\u001b[35mSection_title: 7  Conclusions and Future Works\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.3  Answer Generation\n",
            "\u001b[35mSection_title: 3.4  Human Annotation\n",
            "\u001b[35mSection_title: 3.5  Statistics\n",
            "\u001b[35mSection_title: 4  HAGRID Analysis\n",
            "\u001b[32mconclusion: generative search with the ability to cite supporting sources has gained a lot of traction lately. however, the absence of accessible high-quality data inhibits progress in building open-source information-seeking models. in this paper, we seek to bridge this gap in the community by introducing hagrid, a new dataset for building endto-end generative retrieval models. our dataset is collected via a human-machine collaboration that starts with generating explanatory answers to  information-seeking queries from gpt-3.5, followed by a human assessment of correctness and attributability of the generated answers. hagrid facilitates the development of open-source models for information-seeking scenarios. our human study has shed light on the room for improvement, i.e. around 40% of gpt-3.5 generated answers are not informative and over 20% fail to demonstrate attribution to the quotes. moving forward, future research endeavors may focus on building more accurate models, aimed at mitigating the errors commonly encountered in current llms.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.45  (\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  THE SaaC DATASET\n",
            "\u001b[35mSection_title: 2.1  Collecting Conversational Queries\n",
            "\u001b[35mSection_title: 2.2  Collecting Candidate Passages\n",
            "\u001b[35mSection_title: 2.3  Collecting Conversational Responses\n",
            "\u001b[35mSection_title: 2.4  Description of the SaaC dataset\n",
            "\u001b[35mSection_title: 3  A METHOD FOR CONVERSING WITH SEARCH ENGINES\n",
            "\u001b[35mSection_title: 3.1  Conversation and Passage Understanding\n",
            "\u001b[35mSection_title: 3.2  Relevant Passage Selection\n",
            "\u001b[35mSection_title: 3.3  Supporting Token Identification\n",
            "\u001b[35mSection_title: 3.4  Response Generation\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.1  Datasets and Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  Methods Used for Comparison\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 5.1  How Does CaSE Perform?\n",
            "\u001b[35mSection_title: 5.3  Is there Room for further Improvement?\n",
            "\u001b[35mSection_title: 5.4  Case study\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[35mSection_title: 6.1  Conversational Search\n",
            "\u001b[35mSection_title: 6.2  Conversational Agents\n",
            "\u001b[32mconclusion: in this paper, we propose conversations with search engines as task for the community to consider and we contribute two types of result: first, we release a new test set, saac, which is more suitable and challenging for this research than existing resources.second, we propose an end-to-end neural model, case, to advance the state-of-the-art.we implement state-of-the-art methods from related tasks and conduct extensive experiments to show that: (1) the proposeed case model can achieve state-of-the-art performance; (2) the proposed sti and ppg modules can bring large improvements; and (3) saac is a more challenging dataset than previously introduced ones, leaving significant room for further improvements.\n",
            "\u001b[35mSection_title: 7  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work and Background\n",
            "\u001b[35mSection_title: 2.1  Neural Retrieval\n",
            "\u001b[35mSection_title: 3  The BEIR Benchmark\n",
            "\u001b[35mSection_title: 3.1  Dataset and Diversity Analysis\n",
            "\u001b[35mSection_title: 3.2  BEIR Software and Framework\n",
            "\u001b[35mSection_title: 3.3  Evaluation Metric\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.  Strong training losses for dense retrieval leads to better out-of-distribution performances.\n",
            "\u001b[35mSection_title: 6  Impact of Annotation Selection Bias\n",
            "\u001b[32mconclusion: in this work, we presented beir: a heterogeneous benchmark for information retrieval. we provided a broader selection of target tasks ranging from narrow expert domains to open domain datasets. we included nine different retrieval tasks spanning 18 diverse datasets.\n",
            "\u001b[35mSection_title: 7  Conclusions and Future Work\n",
            "\u001b[35mSection_title: 1.  Multilingual Tasks:\n",
            "\u001b[35mSection_title: 2.  Long Document Retrieval:\n",
            "\u001b[35mSection_title: 4.  Multi-field Retrieval:\n",
            "\u001b[35mSection_title: 5.  Task-specific Models:\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Augmenting the Multi-Document\n",
            "\u001b[35mSection_title: 4  Experimental Setup and Results\n",
            "\u001b[35mSection_title: 4.1  Multi-Document Question Answering\n",
            "\u001b[35mSection_title: 4.2  Multi-Document Summarization (MDS)\n",
            "\u001b[35mSection_title: 4.3  Query-Focused Multi-Document Abstractive Summarization\n",
            "\u001b[35mSection_title: 4.4  Ablation Study\n",
            "\u001b[35mSection_title: 4.5  Comparison with Large Language Models\n",
            "\u001b[32mconclusion: in this work, we present a novel pre-training scheme for multi-document tasks. first, our approach suggests to augment the existing multidocument pre-training objectives into a crossdocument question answering task. second, we generate high-quality large-scale qa pre-training data using a controlled generation approach, in which each qa pair originates from a salient sentence in one of the documents in the set.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Overview\n",
            "\u001b[35mSection_title: 2.2  Hallucination Detection\n",
            "\u001b[35mSection_title: 2.2.1  Identify Key Concepts\n",
            "\u001b[35mSection_title: 2.2.2  Calculate Model's Uncertainty\n",
            "\u001b[35mSection_title: 2.2.3  Create Validation Question\n",
            "\u001b[35mSection_title: 2.2.4  Find Relevant Knowledge\n",
            "\u001b[35mSection_title: 2.2.5  Answer Validation Question\n",
            "\u001b[35mSection_title: 2.3  Hallucination Mitigation\n",
            "\u001b[35mSection_title: 2.4  Design Decisions\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 2  D > C\n",
            "\u001b[35mSection_title: 3  D > B\n",
            "\u001b[35mSection_title: 3.1  Motivating Findings\n",
            "\u001b[35mSection_title: 3.1.1  Hallucination Causes Further Hallucination\n",
            "\u001b[35mSection_title: 3.1.2  Logit Output Values Provide a Signal for Hallucination\n",
            "\u001b[35mSection_title: 3.2  Hallucination Detection Performance\n",
            "\u001b[35mSection_title: 3.3  Hallucination Mitigation Performance\n",
            "\u001b[35mSection_title: 3.4  Active Detection and Mitigation\n",
            "\u001b[35mSection_title: 4.2  Multi-hop Questions\n",
            "\u001b[35mSection_title: 4.3  False Premise Questions\n",
            "\u001b[35mSection_title: 4.4  Other Applications\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models. through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average. we also demonstrated the individual efficacy of our detection and mitigation techniques. specifically, our detection technique achieves a high recall and the mitigation technique successfully mitigates a large fraction of the correctly detected hallucinations. notably, the mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. we further demonstrated the effectiveness and wide applicability of our approach and presented several interesting studies including evaluation with another llm (vicuna) and answering multi-hop and false premise questions. overall, our work addresses the llms' hallucination problem and thus contributes to improving their reliability and trustworthiness, a crucial step en route to enabling their widespread adoption in real-world applications.  table 6 shows the instructional prompts used for different steps of our approach. we note that these techniques are the preferred techniques as they do not require calling an external task-specific tool to achieve the corresponding objectives.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A Motivating Example\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Implicit Knowledge of Taxonomic Relations\n",
            "\u001b[35mSection_title: 4.3  Counting over Implicit Facts\n",
            "\u001b[35mSection_title: 4.4  Generalizing to New Skill Combinations\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[35mSection_title: 7  Broader Impact\n",
            "\u001b[35mSection_title: 9.2  Implicit Knowledge of Taxonomic Relations\n",
            "\u001b[35mSection_title: 9.3  Analyzing systematicity\n",
            "\u001b[35mSection_title: 9.4  Generalizing to New Skill Combinations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  PROBLEM DEFINITION\n",
            "\u001b[35mSection_title: 3  MULTI-HOP MRC TECHNIQUES\n",
            "\u001b[35mSection_title: 3.1  Decomposition technique:\n",
            "\u001b[35mSection_title: 3.2  Recurrent reasoning-based technique\n",
            "\u001b[35mSection_title: 3.2.1  Path-based:\n",
            "\u001b[35mSection_title: 3.3  Graph-based technique\n",
            "\u001b[35mSection_title: 3.3.1  Entity-node graphs:\n",
            "\u001b[35mSection_title: 3.3.2  Multiple-node graphs:\n",
            "\u001b[35mSection_title: 3.4  Graph-free technique\n",
            "\u001b[35mSection_title: 4  COMPARISON\n",
            "\u001b[35mSection_title: 4.1  Techniques frequency\n",
            "\u001b[35mSection_title: 4.2  Models Performance:\n",
            "\u001b[35mSection_title: 4.2.1  HotpotQA\n",
            "\u001b[35mSection_title: 4.2.2  WikiHop\n",
            "\u001b[35mSection_title: 5  OPEN ISSUES\n",
            "\u001b[32mconclusion: in this study, we focused on the multi-hop mrc approaches. in this regard, after presenting the multi-hop mrc problem definition, the multi-hop mrc techniques had been explained based on 31 studies from 2018 to 2022. in addition to categorize the approaches based on the main technique, they also were reviewed in detail including the architecture, superiority, and motivations. in the following, a fine-grain comprehension of the approaches and techniques was prepared, and finally, some open issues in this field were discussed.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  UKP-SQUARE\n",
            "\u001b[35mSection_title: 2.1  Skills\n",
            "\u001b[35mSection_title: 2.2  Models\n",
            "\u001b[35mSection_title: 2.3  Datastores\n",
            "\u001b[35mSection_title: 2.4  Explainability\n",
            "\u001b[35mSection_title: 2.5  User Interface\n",
            "\u001b[35mSection_title: 3.1  Skill Publishing\n",
            "\u001b[35mSection_title: 3.2  Skill Querying\n",
            "\u001b[35mSection_title: 3.3  Behavioural Testing of Skills\n",
            "\u001b[35mSection_title: 4  User Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we introduce the ukp-square platform that enables researchers and developers to study and compare qa pipelines, i.e., skills, that comprises a selection of datastores, retrieval mechanisms and reader models. the platform enables querying ex-isting public skills, as well as implementing custom ones using ukp-square's microservices and utility functions that support a large collection of model types and datastores. furthermore, users can simultaneously query multiple skills, and analyze them through integrated behavioural tests. our architecture is scalable and flexible to incorporate most of the latest developments in the qa domain. future versions will include automated deployment of custom models and datastores, automated skill selection by incorporating previous works (puerto et al., 2021;geigle et al., 2021) and increasing the number of supported datastores (e.g., wikidata, vrandečić and krötzsch, 2014). we also plan to incorporate specialized models (e.g., using graph encoders, ribeiro et al., 2021), structured reasoning approaches (yasunaga et al., 2021) and interpretability techniques such as saliency maps .\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background & Related Work\n",
            "\u001b[35mSection_title: 3  Privacy-Aware ODQA Framework\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  PUBLIC-PRIVATE AUTOREGRESSIVE INFORMATION RETRIEVAL Framework\n",
            "\u001b[35mSection_title: 4  CONCURRENTQA for Multi-Domain Multi-Hop Reasoning\n",
            "\u001b[32mlimitation: we first adapt the widely used benchmark, hotpotqa [yang et al., 2018], to study our problem. hotpotqa contains multi-hop questions, which are each answerable by multiple wikipedia passages. we create hotpotqa-pair by splitting the wikipedia corpus into d g and d p by randomly assigning wikipedia articles to one or the other. this results in questions entirely reliant on p ∈ d p , entirely reliant on d ∈ d g , or reliant on a mix of one private and one public document, allowing us to evaluate performance under the pair constraints.ultimately however, d p and d g come from a single wikipedia distribution in hopotqa-pair. while it is possible that public and private data come from the same distribution (e.g., organizations routinely develop internal wikis in the style of public wikipedia), private and public data will intuitively often reflect different linguistic styles, structures, and topics, that further evolve over time [hawking, 2004]. we observe all existing textual multi-hop benchmarks focus on retrieving from a single distribution (table 1). additionally, we cannot combine existing benchmarks over two different corpora because this will not yield questions requiring one passage from each domain. methodologically, in the pair setting we likely will not have access to training data from all downstream (private) domains. to evaluate with a realistically private set of information and pair set up, we create a new benchmark concurrentqa. 6 following from the simple security property and *-property in the blp model. 7 single-hop can also avoid performance degradations arising from using two enclaves. recall that a non-private system retrieves the top k overall passages, so if for example kp = k 2 and kg = k 2 , such that kp + kg = k, the system may not retrieve the optimal k passages that the non-private system would have retrieved (e.g., consider when the overall top k passages for a question are in dg). however letting kp ∈ [0..k], kg ∈ [0..k] circumvents this challenge, at the cost of retrieving a few more passages per hop.\n",
            "\u001b[35mSection_title: 4.1  Adapting Existing Benchmarks to Privacy-Preserving QA and Limitations\n",
            "\u001b[35mSection_title: 4.2  CONCURRENTQA Overview\n",
            "\u001b[35mSection_title: 4.3  Benchmark Design\n",
            "\u001b[35mSection_title: 4.4  Benchmark Analysis\n",
            "\u001b[32mlimitation: concurrentqa, like hotpotqa, faces the limitation that crowdworkers see the gold supporting passages when creating questions, which can result in textual overlap between substrings in the questions and passages [trivedi et al., 2020]. we mitigate these effects through our validation task, and by limiting the allowable degree of overlap between passage pairs and questions through the frontend interface during the generation stage. further, our questions are not organic user searches as in kwiatkowski et al. [2019], however search logs do not contain questions over public and private data, and existing dialogue systems have not considered retrieval from a private corpus to our knowledge.additionally, enron was a major public corporation and many entities discussed in enron emails are public entities, so it is possible that public websites and news articles encountered during retriever and reader model pretraining, impact the distinction between public and private questions. we investigate the impact of dataset leakage further in section 6.\n",
            "\u001b[35mSection_title: 4.5  Benchmark Limitations\n",
            "\u001b[35mSection_title: 5  Evaluation in the PAIR Setting\n",
            "\u001b[35mSection_title: 3.  Document Privacy\n",
            "\u001b[35mSection_title: 4.  Query Privacy\n",
            "\u001b[35mSection_title: 6  Challenges in Enabling Public-Private Retrieval\n",
            "\u001b[35mSection_title: 2.  Research Question\n",
            "\u001b[35mSection_title: 6.1  Selective Prediction\n",
            "\u001b[35mSection_title: 6.2  Multi-Distribution Retrieval\n",
            "\u001b[35mSection_title: 6.3  Error Analysis of Retrieval Methods\n",
            "\u001b[35mSection_title: 7  Discussion and Future Work\n",
            "\u001b[32mconclusion: this work asks how to personalize retrieval-based systems in a privacy-preserving way and identifies that arbitrary autoregressive retrieval over public and private data poses a privacy concern. in summary, we define the pair privacy framework, present a new multi-domain multi-hop benchmark called concurrentqa for the novel retrieval setting, and demonstrate the privacy-performance tradeoffs faced by existing open-domain systems. we finally investigate two challenges towards realizing the potential of public-private retrieval systems: using selective prediction to manage the privacy-performance tradeoff and concurrently retrieving over multiple distributions. we hope this work inspires new privacy-preserving solutions for personalized retrieval-based systems.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Our Model\n",
            "\u001b[35mSection_title: 2.1  POS-Enhanced Encoder\n",
            "\u001b[35mSection_title: 2.2  Iterative Co-Attention Mechanism\n",
            "\u001b[35mSection_title: 2.2.1  Preliminary Interaction\n",
            "\u001b[35mSection_title: 2.2.2  t-th Turn Interaction\n",
            "\u001b[35mSection_title: 2.3  Attention Integration\n",
            "\u001b[35mSection_title: 2.4.2  Extractive MRC\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Setup & Dataset\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 4  Ablation Studies\n",
            "\u001b[35mSection_title: 4.1  Ablation\n",
            "\u001b[35mSection_title: 4.2  Role of POS\n",
            "\u001b[35mSection_title: 4.3  Research on the Robustness of POS Embedding\n",
            "\u001b[35mSection_title: 4.4  Role of Iterative Co-Attention Mechanism\n",
            "\u001b[35mSection_title: 4.5  Visualization\n",
            "\u001b[35mSection_title: 5  Related Studies\n",
            "\u001b[35mSection_title: 5.1  Semantic and Linguistic Embedding\n",
            "\u001b[35mSection_title: 5.2  Attention Mechanism\n",
            "\u001b[32mconclusion: in this work, we propose pos-enhanced iterative co-attention network (poi-net), as a lightweight unified modeling for multiple subcategories of discriminative mrc. poi-net utilizes pos embedding to encode pos attributes for the preciseness of answer boundary, and iterative co-attention mechanism with integration strategy is employed to highlight and integrate critical information at decoding aspect, with almost no additional parameter. as the first effective and unified modeling with pertinence for different types of discriminative mrc, evaluation results on four extractive and multi-choice mrc benchmarks consistently indicate the general effectiveness and applicability of our model.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Baseline\n",
            "\u001b[35mSection_title: 3.2  QAGAN\n",
            "\u001b[35mSection_title: 3.2.1  Heated Tanh Annealing\n",
            "\u001b[35mSection_title: 3.2.2  Conditional QA Prediction Head\n",
            "\u001b[35mSection_title: 3.2.3  Data Augmentation\n",
            "\u001b[35mSection_title: 3.3  Fine-tuning:\n",
            "\u001b[35mSection_title: 4  Experimental Details\n",
            "\u001b[35mSection_title: 4.1  Dataset Description\n",
            "\u001b[35mSection_title: 5  Qualitative Analysis\n",
            "\u001b[35mSection_title: 5.1  Quantitative Results\n",
            "\u001b[32mconclusion: we presented a method for training a question-answering language model in an adversarial fashion and showed through various experiments that it helps the model generalize well to out-of-domain dataset.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  QLORA Finetuning\n",
            "\u001b[35mSection_title: 4  QLoRA vs. Standard Finetuning\n",
            "\u001b[35mSection_title: 5  Pushing the Chatbot State-of-the-art with QLoRA\n",
            "\u001b[35mSection_title: 5.1  Experimental setup\n",
            "\u001b[35mSection_title: 5.2  Evaluation\n",
            "\u001b[35mSection_title: 5.3  Guanaco: QLORA trained on OASST1 is a State-of-the-art Chatbot\n",
            "\u001b[35mSection_title: 6  Qualitative Analysis\n",
            "\u001b[35mSection_title: 6.1  Qualitative Analysis of Example Generations\n",
            "\u001b[35mSection_title: 6.2  Considerations\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mlimitation: we have shown evidence that our method, qlora, can replicate 16-bit full finetuning performance with a 4-bit base model and low-rank adapters (lora). despite this evidence, we did not establish that qlora can match full 16-bit finetuning performance at 33b and 65b scales. due to the immense resource costs, we leave this study to future work.another limitation is the evaluation of instruction finetuning models. while we provide evaluations on mmlu, the vicuna benchmark, and the oa benchmark, we did not evaluate on other benchmarks such as bigbench, raft, and helm, and it is not ensured that our evaluations generalize to these benchmarks. on the other hand, we perform a very broad study on mmlu and develop new methods for evaluating chatbots.from the evidence presented, it appears that the performance of these benchmarks likely depends how similar the finetuning data is to the benchmark dataset. for example, flan v2 is similar to mmlu, but dissimilar to chatbot benchmarks and vice versa for the chip2 dataset and both models score accordingly on the mmlu and vicuna benchmarks. this highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place. do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? maybe something else? because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction. we should ensure as a community that the benchmarks measure what we care about.while we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible ai evaluation of guanaco. we evaluate the likelihood of guanaco-65b to generate a socially biased sequence of tokens compared to other models in table 8. we see that the average score in guanaco-65b is much lower than other raw pretrained models. as such, it seems that finetuning on the oasst1 dataset reduces the bias of the llama base model. while these results are encouraging, it is unclear if guanaco does also well when assessed on other types of biases. we leave further evaluation of analyzing biases in guanaco and similar chatbots to future work.an additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base models, or different adapter methods. besides lora, there is also a wide variety parameter efficient finetuning (peft) methods that have been shown to work well. however, it is unclear if these methods scale to large models. we used lora as many results established its robustness but other adapters might yield better performance. since finetuning after quantization seems to recover most of the information that is lost during quantization this might enable much more aggressive quantization. for example, 3-bit gptq quantization of the basemodel with lora might also yield 16-bit full finetuning performance after finetuning.\n",
            "\u001b[35mSection_title: 8  Limitations and Discussion\n",
            "\u001b[35mSection_title: 9  Broader Impacts\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  QLORA Finetuning\n",
            "\u001b[35mSection_title: 4  QLoRA vs. Standard Finetuning\n",
            "\u001b[35mSection_title: 5  Pushing the Chatbot State-of-the-art with QLoRA\n",
            "\u001b[35mSection_title: 5.1  Experimental setup\n",
            "\u001b[35mSection_title: 5.2  Evaluation\n",
            "\u001b[35mSection_title: 5.3  Guanaco: QLORA trained on OASST1 is a State-of-the-art Chatbot\n",
            "\u001b[35mSection_title: 6  Qualitative Analysis\n",
            "\u001b[35mSection_title: 6.1  Qualitative Analysis of Example Generations\n",
            "\u001b[35mSection_title: 6.2  Considerations\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mlimitation: we have shown evidence that our method, qlora, can replicate 16-bit full finetuning performance with a 4-bit base model and low-rank adapters (lora). despite this evidence, we did not establish that qlora can match full 16-bit finetuning performance at 33b and 65b scales. due to the immense resource costs, we leave this study to future work.another limitation is the evaluation of instruction finetuning models. while we provide evaluations on mmlu, the vicuna benchmark, and the oa benchmark, we did not evaluate on other benchmarks such as bigbench, raft, and helm, and it is not ensured that our evaluations generalize to these benchmarks. on the other hand, we perform a very broad study on mmlu and develop new methods for evaluating chatbots.from the evidence presented, it appears that the performance of these benchmarks likely depends how similar the finetuning data is to the benchmark dataset. for example, flan v2 is similar to mmlu, but dissimilar to chatbot benchmarks and vice versa for the chip2 dataset and both models score accordingly on the mmlu and vicuna benchmarks. this highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place. do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? maybe something else? because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction. we should ensure as a community that the benchmarks measure what we care about.while we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible ai evaluation of guanaco. we evaluate the likelihood of guanaco-65b to generate a socially biased sequence of tokens compared to other models in table 8. we see that the average score in guanaco-65b is much lower than other raw pretrained models. as such, it seems that finetuning on the oasst1 dataset reduces the bias of the llama base model. while these results are encouraging, it is unclear if guanaco does also well when assessed on other types of biases. we leave further evaluation of analyzing biases in guanaco and similar chatbots to future work.an additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base models, or different adapter methods. besides lora, there is also a wide variety parameter efficient finetuning (peft) methods that have been shown to work well. however, it is unclear if these methods scale to large models. we used lora as many results established its robustness but other adapters might yield better performance. since finetuning after quantization seems to recover most of the information that is lost during quantization this might enable much more aggressive quantization. for example, 3-bit gptq quantization of the basemodel with lora might also yield 16-bit full finetuning performance after finetuning.\n",
            "\u001b[35mSection_title: 8  Limitations and Discussion\n",
            "\u001b[35mSection_title: 9  Broader Impacts\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BEIR OVERVIEW\n",
            "\u001b[35mSection_title: 3  RETRIEVAL MODELS\n",
            "\u001b[35mSection_title: 3.1  Multi-Field BM25\n",
            "\u001b[35mSection_title: 3.2  Learned Dense Retrieval Models\n",
            "\u001b[35mSection_title: 3.3  Learned Sparse Retrieval Models\n",
            "\u001b[35mSection_title: 4  MAIN RESULTS\n",
            "\u001b[35mSection_title: 5  REPRODUCIBLE IMPLEMENTATIONS\n",
            "\u001b[35mSection_title: 6  LEADERBOARD\n",
            "\u001b[35mSection_title: 7  MODEL VARIANTS\n",
            "\u001b[35mSection_title: 7.1  Multi-Field Indexing\n",
            "\u001b[35mSection_title: 7.2  Searching Long Documents\n",
            "\u001b[35mSection_title: 7.3  Hybrid Fusion\n",
            "\u001b[32mconclusion: the beir benchmark provides an important instrument for evaluating the cross-domain robustness of retrieval models and has gained traction due to the growing recognition of retrieval as a form of representation learning. the efforts described in this paper address the two shortcomings that we have identified with beir: challenges in reproducibility and in the sharing of results. reproducible reference implementations in the pyserini ir toolkit tackle the first challenge. an official self-service leaderboard and best practices for sharing results target the second challenge.\n",
            "\u001b[35mSection_title: 8  CONCLUSIONS AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  K2R Model\n",
            "\u001b[35mSection_title: 4.1  Wizard of Wikipedia (WoW)\n",
            "\u001b[35mSection_title: 4.2  Natural Questions\n",
            "\u001b[35mSection_title: 4.3  LIGHT\n",
            "\u001b[35mSection_title: 4.3.1  LightQA\n",
            "\u001b[32mconclusion: in this work, we presented k2r: a modular approach for knowledge-based dialogue models. we showed that by decomposing the knowledge step and response generation into explicit sequence-tosequence subtasks, we could improve dialogue systems by incorporating knowledge or turning short qa model answers into an appropriate conversational form. in detailed experiments, we showed that this modular system helps with hallucination in knowledge-grounded dialogue, is rated by humans as more knowledgeable and engaging when answering questions, and improves generation metrics on open-domain dialogue. furthermore, it allows for more interpretable results and supports knowledge injection. future work should continue to investigate methods with modular reasoning steps to help in difficult language tasks. a genius is a person who displays exceptional intellectual ability, creative productivity, universality in genres or originality, typically to a degree that is associated with the achievement of new advances in a domain of knowledge. gold response universality in genres or originality typically to a degree that is associated with achievements bart a genius has a high iq.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Background\n",
            "\u001b[35mSection_title: 3.2  Overview\n",
            "\u001b[35mSection_title: 3.3  Constructing Pseudo-Labeled Data\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.2.1  No Labeled Data\n",
            "\u001b[35mSection_title: 4.2.2  Few Labeled Data\n",
            "\u001b[35mSection_title: 4.2.3  Full Labeled Data\n",
            "\u001b[35mSection_title: 4.2.4  Language Generation Tasks\n",
            "\u001b[35mSection_title: 4.3  Discussion\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Effect of Instruction Tuning\n",
            "\u001b[35mSection_title: 5.2  Effect of Domain Diversity\n",
            "\u001b[35mSection_title: 5.3  Effect of Data Amount\n",
            "\u001b[35mSection_title: 5.4  Effect of Individual Task Clusters\n",
            "\u001b[32mconclusion: in this work, we investigate performing it with unlabeled data for zero-shot cross-task generalization. we first empirically find that the it performance is largely restricted by the number of distinct tasks, instructions, and training samples in data-scarce tasks. then, we propose udit to take better advantage of the instructions by constructing pseudo-labeled data from the unlabeled plain texts. through udit, it is possible to perform it with unlabeled data when there are few or no humanannotated samples, which offers a better way to incorporate unlabeled data compared with other approaches. through comprehensive analysis, we find that the domain diversity and the matching between the pseudo-labeled data and corresponding instructions are essential for udit. in contrast, noises in individual task clusters and colossal data amount are less influential. there are three directions for future work: (1) designing automatic and generalizable methods to construct pseudo-labeled data for instruction tuning.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  IAPEK: Problem Formulation\n",
            "\u001b[35mSection_title: 2.2  Thrust: Measuring the Knowledgeability of PTLMs\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Performance of Using External Knowledge\n",
            "\u001b[35mSection_title: 3.3  Performance of Thrust\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[35mSection_title: 4.1  Primary Study: Distribution of Thrust scores\n",
            "\u001b[35mSection_title: 4.2  IAPEK-Thrust versus IAPEK-BM25\n",
            "\u001b[35mSection_title: 4.3  Layer ablation\n",
            "\u001b[35mSection_title: 4.4  Comparison with Full Knowledge Usage\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose instance-level adaptive propulsion of external knowledge (iapek) as a solution to propel model performance with external knowledge. accordingly, we propose a simple and effective instance-level metric, thrust, to perform the adaptive knowledge injection. extensive experiments show that thrust is a good indicator of models' knowledgeability and can improve the performance of utilizing external knowledge under various settings. understanding the delicate usage of potentially noisy knowledge for ptlms can further enable the models to conduct inference beyond the limitation of internal knowledge.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.  Introduction of a mutation into the SapI site in RNase A cDNA.\n",
            "\u001b[35mSection_title: 3.  Cloning of intein fusion RNase A (1-109) cDNA.\n",
            "\u001b[35mSection_title: 4.  Expression of intein-fused RNase\n",
            "\u001b[35mSection_title: 5.  Purification of intein-fused RNase\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Preliminaries\n",
            "\u001b[35mSection_title: 3.2  ControlRetriever\n",
            "\u001b[35mSection_title: 3.3  LLM-guided Instruction Data Synthesizing\n",
            "\u001b[35mSection_title: 3.4  Iterative Instruction-based Contrastive Learning\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Cooperating with Reranking Models\n",
            "\u001b[35mSection_title: 4.4  In-Depth Analysis\n",
            "\u001b[32mconclusion: in this paper, we present controlretriever, a generic and efficient approach capable of controlling retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions. with a parameter-isolated architecture, con-trolretriever effectively preserves the original capability of the retrieval model, meanwhile efficiently empowering it with a new facet of controllable retrieval conditioned on task-specific instructions. furthermore, we also propose a novel llm-guided instruction synthesizing and iterative training (list) strategy, which iteratively trains controlretriever based on extensive automatically-generated retrieval data with diverse instructions, capitalizing the advancement of large language models. extensive experiments demonstrate the superior zero-shot transferability of controlretriever on various retrieval tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  THE EXMIX TASK COLLECTION\n",
            "\u001b[35mSection_title: 2.1  TRANSFER RELATIONS BETWEEN EXMIX TASKS\n",
            "\u001b[35mSection_title: 2.2  CAN FINE-TUNING TASK RELATIONSHIPS HELP CURATE A PRE-TRAINING MIXTURE?\n",
            "\u001b[35mSection_title: 2.3  MULTI-TASK PRE-TRAINING VS PRE-FINETUNING\n",
            "\u001b[35mSection_title: 2.4  HOW MUCH LABELED DATA SHOULD BE MIXED?\n",
            "\u001b[35mSection_title: 2.5  DOES ADDING MORE TASKS HELP? TASK SCALING EXPERIMENTS\n",
            "\u001b[35mSection_title: 3  THE EXT5 MODEL\n",
            "\u001b[35mSection_title: 3.1  TRAINING EXT5\n",
            "\u001b[35mSection_title: 3.2  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.3  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[35mSection_title: 5  EPILOGUE\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Crowdsourcing for QA Dataset Creation\n",
            "\u001b[35mSection_title: 2.2  Crowdsourced Text Sequence Aggregation\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Text Sequence Aggregation for Answer Aggregation\n",
            "\u001b[35mSection_title: 3.3  Answer Quality Aware Answer Aggregation\n",
            "\u001b[35mSection_title: 3.3.1  Answer\n",
            "\u001b[35mSection_title: 3.3.2  The Joint Method (ACAF-SMS/SMV)\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Evaluation\n",
            "\u001b[35mSection_title: 5.1  Effectiveness of Answer Quality\n",
            "\u001b[35mSection_title: 5.2  Effectiveness of Answer Text Sequence Aggregation\n",
            "\u001b[35mSection_title: 5.3  Influence of Encoders\n",
            "\u001b[35mSection_title: 5.4  Case Study\n",
            "\u001b[32mconclusion: in this paper, we propose a novel answer annotation aggregation method for eqa crowdsourcing.we show that without any fine-tuning, our methods can achieve comparable performance with the trained qa and nli model using  4: an example from newsqa dataset.there are 7 different answer annotations for the question.some of the answers are overlapped.for each answer we report its ranking scores with ac af sms acaf-sms .\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: while many automatic answer aggregation methods take crowd worker's reliability into consideration (tian and zhu, 2015;li and fukumoto, 2019), to keep the proposed framework simple and concise, we focus on the influence of answer quality and ignore the worker reliability.moreover, we only use newsqa to evaluate the proposed method.although it is possible to consider more real or simulated datasets, as shown by the experiments on squad and natural questions in appendix a.3, newsqa is the only large extractive qa dataset that provides all actual annotations to the best of our knowledge.besides, this paper assumes there is only one correct answer for each question, while it is possible that there are multiple correct answers in some applications.we will explore crowd worker reliability aware answer aggregation methods and extend our work to multi-answer settings in future research.table 7: performance of answer agreement on primary-nc and test-nc using the bert-base-uncased model in terms of exact match (em) and f1.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Clue Extraction\n",
            "\u001b[35mSection_title: 3.2  Clue Reasoning\n",
            "\u001b[35mSection_title: 3.3  Multi-task prediction\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset and Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablation studies\n",
            "\u001b[35mSection_title: 4.4  Case Study\n",
            "\u001b[32mconclusion: in this paper, we apply explicit graph reasoning to extracted knowledge and contextual information for multi-hop reasoning. we extract clues at multiple levels of granularity relating entity nodes, and construct a semantic graph from these clues. we then combine a masked attention mechanism and two-stage graph reasoning to perform interpretable inference over the semantic graph. experimental results on hotpotqa dataset show the effectiveness of our model. in future work, we hope to extend the range and precision of the entity relations used, and we hope to extend our model to accommodate more complex multi-hop questions with unknown number of hops and non-linear reasoning.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Proposed model\n",
            "\u001b[35mSection_title: 3.1  Candidate answer extraction\n",
            "\u001b[35mSection_title: 3.2  Positive and Negative Sampling\n",
            "\u001b[35mSection_title: 3.3  ContrastiveQA using contrastive learning\n",
            "\u001b[35mSection_title: 4  Experiment Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Training details\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 5  Model Analysis\n",
            "\u001b[35mSection_title: 5.1  Extractive QA Results\n",
            "\u001b[35mSection_title: 5.2  Effect of Selecting Candidate sets\n",
            "\u001b[35mSection_title: 5.3  Effect of Contrastive Loss\n",
            "\u001b[35mSection_title: 5.4  Effect of Similarity Function\n",
            "\u001b[32mconclusion: in this work, we proposed a novel contrastiveqa to alleviate the problem of confusion in answer extraction using contrastive learning. the proposed model contrastiveqa consists of three tasks: 1) candidate answer extraction, 2) positive and negative sampling, 3) contrastiveqa using contrastive learning. we demonstrated the effectiveness of the proposed model by outperforming the performance of the baseline models on four benchmark datasets. in the future, we would like to adopt a method to specify the number of samples required in contrastive learning dynamically. this approach created an adaptive mechanism to use the number of samples for each input. this method could improve the overall performance of the model.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: we obtain notable qa performance through experiments. however, we conduct many experiments to find the optimal candidate for contrastiveqa. many of these experiments inevitably consume a lot of time and energy, and we have to heuristically determine the number of candidate sets through experiments in a limited environment. we intend to alleviate the current problems by adding a module that can solve these problems in our future research.  question what is the year of the event that occured first, making today a perfect day was produced, or frozen was produced? table a1: example of answer extraction of baseline and contrastiveqa.in table a.1, we classify into positive and negative samples using the information of the given correct answer and train them with contrastive loss to derive a result that is closer to the original correct answer when compared with the baseline.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Mintaka\n",
            "\u001b[35mSection_title: 4  Dataset Collection\n",
            "\u001b[35mSection_title: 4.1  Question Elicitation\n",
            "\u001b[35mSection_title: 4.2  Answer Entity Linking\n",
            "\u001b[35mSection_title: 4.3  Question Entity Linking\n",
            "\u001b[35mSection_title: 4.4  Translations\n",
            "\u001b[35mSection_title: 5  Dataset Analysis\n",
            "\u001b[35mSection_title: 5.1  Dataset Statistics\n",
            "\u001b[35mSection_title: 5.2  Naturalness Evaluation\n",
            "\u001b[35mSection_title: 5.3  Demographics of MTurk Workers\n",
            "\u001b[35mSection_title: 6.1  Models\n",
            "\u001b[35mSection_title: 6.2  Analysis\n",
            "\u001b[32mconclusion: in this paper, we introduce mintaka, an end-toend question answering dataset linked to wikidata. mintaka addresses an important gap in qa datasets by being large-scale, complex, naturally-elicited, and multilingual. our baselines show that there is room for improvement in existing methods to handle complex questions, especially in all languages. with the release of mintaka, we hope to encourage researchers to continue pushing the boundaries of question answering to handle more complex questions in more languages.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  DialDoc Shared Task Dataset\n",
            "\u001b[35mSection_title: 3  System Description\n",
            "\u001b[35mSection_title: 3.1  Question-Answering Model\n",
            "\u001b[35mSection_title: 3.2  Pretraining\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Pretraining Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Hyperparameters\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Pretraining on Different QA Datasets\n",
            "\u001b[35mSection_title: 5.2  Results on test set\n",
            "\u001b[35mSection_title: 5.3  Ensembling\n",
            "\u001b[35mSection_title: 5.4  Informed Data Selection\n",
            "\u001b[32mconclusion: our submission to the dialdoc subtask 1 performs continual pretraining of a transformer-based encoder on out-of-domain qa datasets. experiments with different qa datasets suggest that conversational qa datasets like coqa and quac are highly beneficial as their setup is substantially similar to doc2dial, the downstream dataset of interest. our final submission ensembles two albert-xl models independently pretrained on coqa and quac and achieves an f1-score of 70.9% and em-score of 53.5% on the competition test-set.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Module Diagrams\n",
            "\u001b[35mSection_title: 4  An important field of application.\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  Dataset Description\n",
            "\u001b[35mSection_title: 0  Irrelevant\n",
            "\u001b[35mSection_title: 5  Experiments: Explanation-as-Ranking\n",
            "\u001b[35mSection_title: 6  Experiments: Whole Explanations\n",
            "\u001b[35mSection_title: 6.1  Models\n",
            "\u001b[35mSection_title: 6.2  Automatic Evaluation Metrics\n",
            "\u001b[35mSection_title: 6.3  Results using Automated Metrics\n",
            "\u001b[35mSection_title: 6.4  Manual Evaluation of Completeness\n",
            "\u001b[32mconclusion: relevance performance is still undercounted: while the expert-generated relevance ratings produced in this work provide more accurate estimates of performance compared to single gold explanations when used in fully-automatic evaluations, these automatic estimates still undercount overall model performance. in our experiments we show the expert ratings primarily provide a vehicle for training better models, but that automatically evaluating relevance performance still remains a challenge, even with a large targeted increase in relevance annotation. further, annotators reported that determining relevance of single facts in isolation is challenging because it lacks the broader compositional context of the rest of the candidate explanation, suggesting ultimate limits to the utility of exhaustive annotation.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Parallel Context Windows\n",
            "\u001b[35mSection_title: 2.1  Positional Embeddings Modification\n",
            "\u001b[35mSection_title: 2.2  Attention Mask Modification\n",
            "\u001b[35mSection_title: 3  PCW for In-Context Learning\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Classification Tasks Results\n",
            "\u001b[35mSection_title: 3.3  Information Extraction Results\n",
            "\u001b[35mSection_title: 4  PCW for Question Answering\n",
            "\u001b[35mSection_title: 4.1  Retrieval Based Question Answering\n",
            "\u001b[35mSection_title: 4.2  Multi-hop Question Answering\n",
            "\u001b[35mSection_title: 5.2  Expanding the Context Window\n",
            "\u001b[32mconclusion: in recent years, a multitude of successful approaches have been proposed for allowing transformer-based language models to leverage large amounts of text during inference, leading to a variety of dedicated architectures. in parallel, however, the mainstream llm production line of new models with \"regular\"-up to several thousand tokens-context window sizes enjoys faster progress in the form of scaling, innovation, and data updating. this paper introduced parallel context windows (pcw): a simple approach for allowing any offthe-shelf llm to broaden the scope of text it can access during inference. we showed the effectiveness of pcw in the framework of in-context learning, where access to a context that is larger by a factor of b implies learning from b times more training examples. our results show that pcw is more effective than the vanilla single context window approach for in-context learning over a broad set of multi-class classification tasks, suggesting that pcw could improve in-context learning in tasks with diverse input or output spaces. we also showed promising signals for applying pcw for multiple retrieved document reading.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHODS\n",
            "\u001b[35mSection_title: 3.1  monoBERT\n",
            "\u001b[35mSection_title: 3.2  Late Interaction\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 5.1  Model Size\n",
            "\u001b[35mSection_title: 5.2  First-stage Retriever\n",
            "\u001b[35mSection_title: 5.3  Token Dimensions\n",
            "\u001b[35mSection_title: 6  ANALYSIS\n",
            "\u001b[35mSection_title: 6.1  Exact-matched Lexical Signal\n",
            "\u001b[35mSection_title: 6.2  Query Length\n",
            "\u001b[32mconclusion: in this work, we presented our finding that adding late interaction to existing rerankers brings visible improvement to out-of-distribution capacity without any degradation on in-domain effectiveness, even though the reranker already processes the token interaction via the attention mechanism at previous layers. extensive experiments on different model sizes and first-stage retrievers show that this improvement is consistent, and according to our analysis, the improvement is more prominent on longer queries. our findings suggest that boiling all information into the [cls] token may not be the optimal choice for neural rerankers, and more studies are required to better explore its capacity.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHODS\n",
            "\u001b[35mSection_title: 3.1  monoBERT\n",
            "\u001b[35mSection_title: 3.2  Late Interaction\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 5.1  Model Size\n",
            "\u001b[35mSection_title: 5.2  First-stage Retriever\n",
            "\u001b[35mSection_title: 5.3  Token Dimensions\n",
            "\u001b[35mSection_title: 6  ANALYSIS\n",
            "\u001b[35mSection_title: 6.1  Exact-matched Lexical Signal\n",
            "\u001b[35mSection_title: 6.2  Query Length\n",
            "\u001b[32mconclusion: in this work, we presented our finding that adding late interaction to existing rerankers brings visible improvement to out-of-distribution capacity without any degradation on in-domain effectiveness, even though the reranker already processes the token interaction via the attention mechanism at previous layers. extensive experiments on different model sizes and first-stage retrievers show that this improvement is consistent, and according to our analysis, the improvement is more prominent on longer queries. our findings suggest that boiling all information into the [cls] token may not be the optimal choice for neural rerankers, and more studies are required to better explore its capacity.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  PRE-TRAINING AND INSTRUCTION TUNING OF LEMUR\n",
            "\u001b[35mSection_title: 2.1  PRE-TRAINING\n",
            "\u001b[35mSection_title: 2.2  INSTRUCTION FINE-TUNING\n",
            "\u001b[35mSection_title: 3  FROM LANGUAGE MODEL TO LANGUAGE AGENT\n",
            "\u001b[35mSection_title: 3.1  FUNDAMENTAL LANGAUGE AND CODING CAPABILITIES\n",
            "\u001b[35mSection_title: 3.2  CONNECTING LLM AGENTS TO ENVIRONMENT\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 4.1  LANGUAGE AND CODE CAPABILITIES\n",
            "\u001b[35mSection_title: 4.2  AUGMENT WITH TOOLS\n",
            "\u001b[35mSection_title: 4.3  SELF-DEBUG WITH ENVIRONMENT FEEDBACK\n",
            "\u001b[35mSection_title: 4.4  FOLLOW NATURAL LANGUAGE FEEDBACK\n",
            "\u001b[35mSection_title: 4.5  EXPLORE IN PARTIALLY OBSERVABLE ENVIRONMENTS\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: in conclusion, this research underscores the pivotal role of harmonizing natural and programming language proficiencies in the evolution of language models to sophisticated language agents. through the development of lemur and lemur-chat, we demonstrated that the meticulous amalgamation of these competencies allows for elevated performance in diverse environments and applications, narrowing the existent capability divide between open-source and proprietary models. we open-sourced both models with the intention of fostering further research in the field of language models for agents. we performed pre-training on the top of llama-2. the detailed statistics of the pre-training data corpus are presented below in table 8. we train our model on a tpuv4-512 pod. our codebase is based on jax and easylm (geng, 2023). following the pretraining methodology of llama 2 (touvron et al., 2023), we used a batch size of 4m tokens. to improve training efficiency, we packed multiple shorter sequences into each batch entry when possible, an approach known as sequential packing (raffel et al., 2020).\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\u001b[35mSection_title: 7  ACKNOWLEDGMENTS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Problem Statement\n",
            "\u001b[35mSection_title: 1.2  Motivation of Approach\n",
            "\u001b[35mSection_title: 1.3  Human-human Spoken Conversations\n",
            "\u001b[35mSection_title: 1.4  Dialogue Comprehension Task\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Reading Comprehension\n",
            "\u001b[35mSection_title: 2.2  NLP for Healthcare\n",
            "\u001b[35mSection_title: 3.1  Data Preparation\n",
            "\u001b[35mSection_title: 3.2  Linguistic Characterization on Seed Data\n",
            "\u001b[35mSection_title: 4  Simulating Symptom Monitoring Dataset for Training\n",
            "\u001b[35mSection_title: 4.1  Template Construction\n",
            "\u001b[35mSection_title: 4.1.1  Linguistically-Inspired Templates\n",
            "\u001b[35mSection_title: 4.1.2  Topic Expansion & Symptom Customization\n",
            "\u001b[35mSection_title: 4.1.3  Expression Pool for Linguistic Diversity\n",
            "\u001b[35mSection_title: 4.2  Simulated Data Generation Framework\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Model Design\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 5.3  Evaluation Setup\n",
            "\u001b[35mSection_title: 5.4  Results\n",
            "\u001b[35mSection_title: 5.5  Ablation Analysis\n",
            "\u001b[32mconclusion: we formulated a dialogue comprehension task motivated by the need in telehealth settings to extract key clinical information from spoken conver-sations between nurses and patients. we analyzed linguistic characteristics of real-world humanhuman symptom checking dialogues, constructed a simulated dataset based on linguistically inspired and clinically validated templates, and prototyped a qa system. the model works effectively on a simulated test set using symptoms excluded during training and on real-world conversations between nurses and patients. we are currently improving the model's dialogue comprehension capability in complex reasoning and context understanding and also applying the qa model to summarization and virtual nurse applications.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Question Complexity Estimation\n",
            "\u001b[35mSection_title: 2.2  Question Generation\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Model Details\n",
            "\u001b[35mSection_title: 3.2  Training\n",
            "\u001b[35mSection_title: 4  Cross-Domain Complexity Estimator\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets and Complexity Annotation\n",
            "\u001b[35mSection_title: 5.2  Settings for CCQG\n",
            "\u001b[35mSection_title: 5.3  Results and Analysis for CCQG\n",
            "\u001b[35mSection_title: 5.4  Evaluation of Complexity Estimator\n",
            "\u001b[35mSection_title: 6  Discussion on MoE-based Architecture\n",
            "\u001b[32mconclusion: we propose a novel encoder-decoder model incorporating soft templates and moe to address the problem of complexity-controllable question generation. as most domains do not have training data for ccqg models, we propose a simple and effective cross-domain estimator to predict the missing complexity levels of questions. in the extensive experiments of both ccqg and complexity assessment tasks, our models achieve superior performance over the competitive baselines across all experimental settings. in the future, we will consider anaphora resolution and numerical reasoning in complexity estimator, and explore the performance of our model in different applications, such as examination and assisting qa systems.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Graph-Based Retrieval\n",
            "\u001b[35mSection_title: 3.2  Graph-Based Reading Comprehension\n",
            "\u001b[35mSection_title: 3.2.1  Initial Paragraph Representation\n",
            "\u001b[35mSection_title: 3.2.2  Fusing Paragraph Representations\n",
            "\u001b[35mSection_title: 3.2.3  Answering Questions\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Training Details\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 5  Analyses\n",
            "\u001b[35mSection_title: 5.1  Ablation Studies\n",
            "\u001b[35mSection_title: 5.2  Qualitative Results\n",
            "\u001b[32mconclusion: we proposed a general approach for open-domain question answering (qa) that models interactions between paragraphs using structural information from a knowledge base. unlike standard approaches where a model retrieves and reads a set of passages, we integrate graph structure at every stage to construct, retrieve and read a graph of passages. our approach consistently outperforms competitive baselines in three open-domain qa datasets, webquestions, natural ques-tions and triviaqa, and we also include a detailed qualitative analysis to illustrate where the cross paragraph reading contributes the most to the overall system performance.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 3.  Vacua and Dyons\n",
            "\u001b[32mconclusion: the results i have presented here are a rst step towards a microscopic understanding of the yang-mills vacuum. many of the intuitive ideas we have developed in the last years seem to prove to be correct nally. so, most likely, we are on the right track.\n",
            "\u001b[35mSection_title: 4.  Conclusions\n",
            "\u001b[35mSection_title: 5.  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Extension of the Multiple Filter Test to Weak Dependencies\n",
            "\u001b[35mSection_title: 2.1  The MFT for rate changes in renewal processes\n",
            "\u001b[35mSection_title: 2.2  The MFT for weak dependencies\n",
            "\u001b[35mSection_title: 2.3  Examples for practical application\n",
            "\u001b[35mSection_title: 2.3.1  Processes in P\n",
            "\u001b[35mSection_title: 2.3.2  Consistent estimators\n",
            "\u001b[35mSection_title: 3  Practical application of the MFT for weak dependencies\n",
            "\u001b[35mSection_title: 4  Application to spike train recordings\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 1.1  Oligonucleotide Primer\n",
            "\u001b[35mSection_title: 2.  METHODS\n",
            "\u001b[35mSection_title: 2.1  Materials\n",
            "\u001b[35mSection_title: 2.2  Quantification of DNA\n",
            "\u001b[35mSection_title: 2.3  Electropherogram of Genomic DNA (gDNA)\n",
            "\u001b[35mSection_title: 2.4  Standardization of the PCR\n",
            "\u001b[35mSection_title: 2.4.1  Cycling Conditions for\n",
            "\u001b[35mSection_title: 2.4.2  Electrophoresis Procedure\n",
            "\u001b[35mSection_title: 2.4.3  Sample Preparation\n",
            "\u001b[35mSection_title: 2.4.4  Running of Sample\n",
            "\u001b[35mSection_title: 2.4.5  PCR Product Purification\n",
            "\u001b[35mSection_title: 2.5  Sequencing of 16s-Sequencing Primer\n",
            "\u001b[35mSection_title: 2.6  Methods for Molecular Similarity and Links among the Isolated Strains\n",
            "\u001b[35mSection_title: 2.7  Molecular Identification of The Sequenced 16S rRNA of Selected Strains via Nucleotide BLASTn Protocol\n",
            "\u001b[35mSection_title: 2.8  Procedure for Molecular Identification\n",
            "\u001b[35mSection_title: 3.  RESULTS\n",
            "\u001b[35mSection_title: 3.1  Molecular typing using PCR\n",
            "\u001b[35mSection_title: 4.  DISCUSSION\n",
            "\u001b[32mconclusion: this study has indicated presence of sdia and fimh genes in salmonella typhimurium and e. coli o157:h7 respectively isolated from ready to eat chicken meats from public eateries in ibadan.\n",
            "\u001b[35mSection_title: 5.  CONCLUSION\n",
            "\u001b[35mSection_title: 6.  RECOMMENDATION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Datasets\n",
            "\u001b[35mSection_title: 3  QA performances with Gold Answer\n",
            "\u001b[35mSection_title: 3.1  Comparison Systems\n",
            "\u001b[35mSection_title: 3.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 4  Answerability Prediction\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  Annotating Unanswerability\n",
            "\u001b[35mSection_title: 5.1  Categories of Unanswerable Questions\n",
            "\u001b[35mSection_title: 5.2  Manual Study Setting\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 5.4  Per-category Performance\n",
            "\u001b[35mSection_title: 5.5  Discussion\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we provide the first in-depth analysis on information-seeking qa datasets to inspect where unanswerability arises and quantify the remaining modeling challenges. our controlled experiments identifies two remaining headrooms, answerability prediction and paragraph selection. observing a large percentage of questions are unanswerable, we provide manual analysis studying why questions are unanswerable and make suggestions to improve answer coverage: (1) going beyond wikipedia textual information as the only source of information, (2) addressing ambiguous queries instead of simply marking and leaving the questions as is, (3) enable accessing multiple documents and introducing abstractive answers for non-factoid questions. together, our work shed light on future work for information-seeking qa, both for modeling and dataset design.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Challenges in KG Querying\n",
            "\u001b[35mSection_title: 1.2  Related Work and Benefits of Our Survey\n",
            "\u001b[35mSection_title: 1.3  Roadmap\n",
            "\u001b[35mSection_title: 2  Taxonomy of KG Querying\n",
            "\u001b[35mSection_title: 2.1  KG Data Models\n",
            "\u001b[35mSection_title: 2.2  KG Query and Question Classification\n",
            "\u001b[35mSection_title: 2.3  KG Query Languages & Technologies\n",
            "\u001b[35mSection_title: 2.4  Benchmarks for KG Query & QA\n",
            "\u001b[35mSection_title: 3  KG Query Processing & QA: Recent Neural Methods\n",
            "\u001b[35mSection_title: 3.1  Embedding-based KG Query Processing\n",
            "\u001b[35mSection_title: 3.2  Multi-modal KG Embedding\n",
            "\u001b[35mSection_title: 3.3  Neural Methods for KG-QA\n",
            "\u001b[35mSection_title: 3.4  Conversational QA on KG\n",
            "\u001b[35mSection_title: 4  Graph Databases Support for KG Query\n",
            "\u001b[35mSection_title: 5  Future Directions\n",
            "\u001b[35mSection_title: 6  Acknowledgement\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Supervised Sentence Representations\n",
            "\u001b[35mSection_title: 2.2  Post-processing of BERT Representations\n",
            "\u001b[35mSection_title: 2.3  Self-supervised Sentence-level\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1.1  Phrase Extraction\n",
            "\u001b[35mSection_title: 3.1.2  Duplicate and Masking\n",
            "\u001b[35mSection_title: 3.1.3  Data Augmentation\n",
            "\u001b[35mSection_title: 3.2  Unsupervised PaSeR\n",
            "\u001b[35mSection_title: 3.2.1  Sentence Encoder\n",
            "\u001b[35mSection_title: 3.2.2  Decoding Signal\n",
            "\u001b[35mSection_title: 3.2.3  Generative Decoder\n",
            "\u001b[35mSection_title: 3.2.4  Combined with MLM\n",
            "\u001b[35mSection_title: 3.3  Supervised PaSeR\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Evaluation Datasets\n",
            "\u001b[35mSection_title: 4.2  Training Details\n",
            "\u001b[35mSection_title: 4.3  Results on Semantic Textual Similarity\n",
            "\u001b[35mSection_title: 4.4  Results on Semantic Retrieval/Reranking\n",
            "\u001b[35mSection_title: 4.5  Ablation Study\n",
            "\u001b[35mSection_title: 4.5.1  Complexity of Generative Decoder\n",
            "\u001b[35mSection_title: 4.5.2  Effectiveness of Data Augmentation\n",
            "\u001b[35mSection_title: 4.5.3  Choices of Phrases to Mask\n",
            "\u001b[35mSection_title: 5  Qualitative Analysis\n",
            "\u001b[35mSection_title: 5.1  Sentence Retrieval\n",
            "\u001b[35mSection_title: 5.2  Phrases Reconstructed by Decoder\n",
            "\u001b[32mconclusion: as most pre-trained language models fail to attach enough importance to sentence-level representation learning, it usually leads to unsatisfactory performance in downstream tasks when good sentence representation is right indispensable. based on investigating the intra-sentence relationship between components of sentences (important phrases) and the whole sentence representations, we propose a generative objective to align these phrases with their corresponding sentence representations. this idea leads to paser, a phrase-aware sentence representation model. as an effective alternative in sentence representation learning, our paser achieves comparable performance with strong contrastive learning baselines on sts tasks, and better performance on the downstream semantic retrieval and reranking tasks on datasets including qqp and askubuntu.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: we think our paser has the following limitations, and leave them for future work.• the combination of decoding signals is empirically designed. hyperparameters m and n are selected by grid search and lack technical analysis.  table 8: ablation for finding best m and n in the unsupervised setting.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Constructing QuAIL Corpus Collection\n",
            "\u001b[35mSection_title: 4  Human Evaluation\n",
            "\u001b[35mSection_title: 5  Model Evaluation\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: we presented quail 5 , the first multi-domain text comprehension challenge that is balanced and annotated for 9 types of verbal reasoning. quail aims to show the extent to which current models can generalize over different domains and reasoning strategies and handle questions that can be answered with the information in a given text, unanswerable questions and questions that require extra world knowledge. we hope that quail will stimulate efforts to develop generalist systems tackling different kinds of verbal reasoning, and that it will be useful in diagnostics and qualitative analysis for new qa systems.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The BoolQ Dataset\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Analysis\n",
            "\u001b[35mSection_title: 3.3  Annotation Quality\n",
            "\u001b[35mSection_title: 3.4  Question Types\n",
            "\u001b[35mSection_title: 3.5  Types of Inference\n",
            "\u001b[35mSection_title: 3.6  Discussion\n",
            "\u001b[35mSection_title: 4  Training Yes/No QA Models\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Shallow Models\n",
            "\u001b[35mSection_title: 5.2  Neural Models\n",
            "\u001b[35mSection_title: 5.3  Question/Passage Only Results\n",
            "\u001b[35mSection_title: 5.4  Transfer Learning Results\n",
            "\u001b[35mSection_title: 5.5  Multi-Step Transfer Results\n",
            "\u001b[35mSection_title: 5.6  Sample Efficiency\n",
            "\u001b[35mSection_title: 5.7  Discussion\n",
            "\u001b[32mconclusion: we have introduced boolq, a new reading comprehension dataset of naturally occurring yes/no questions. we have shown these questions are challenging and require a wide range of inference abilities to solve. we have also studied how transfer learning performs on this task, and found crowd-sourced entailment datasets can be leveraged to boost performance even on top of language model pre-training. future work could include building a document-level version of this task, which would increase its difficulty and its correspondence to an end-user application. embeddings h 1 , h 2 , ... . then pool these embeddings by computing attention scores a i = w · h i , p = sof tmax(a), and then the sum v * = i p i h i . likewise we compute p * from the premise.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Motivation\n",
            "\u001b[35mSection_title: 3  Background\n",
            "\u001b[35mSection_title: 3.1  Question answering software\n",
            "\u001b[35mSection_title: 3.1.1  QA Software based on information in knowledge base\n",
            "\u001b[35mSection_title: 3.1.2  QA Software based on information in textual materials\n",
            "\u001b[35mSection_title: 3.2  Metamorphic testing\n",
            "\u001b[35mSection_title: 4  Methodology\n",
            "\u001b[35mSection_title: 4.1  A recursive metamorphic testing method for QA software\n",
            "\u001b[35mSection_title: 4.2  Proposed metamorphic relations\n",
            "\u001b[35mSection_title: 4.3  Declarative sentence synthesis\n",
            "\u001b[35mSection_title: 4.3.1  Declarative sentence synthesis based on general question and its answer\n",
            "\u001b[35mSection_title: 4.3.2  Declarative sentence synthesis based on alternative question and its answer\n",
            "\u001b[35mSection_title: 4.3.3  Declarative sentence synthesis based on Wh-question and its answer\n",
            "\u001b[35mSection_title: 4.3.4  Declarative sentence synthesis based on Wh-question and extra information about its answer\n",
            "\u001b[35mSection_title: 4.4  Follow-up question sentence generation\n",
            "\u001b[35mSection_title: 4.4.1  General question sentence generation\n",
            "\u001b[35mSection_title: 4.4.2  Wh-question sentence generation\n",
            "\u001b[35mSection_title: 4.5  Violation measurement\n",
            "\u001b[35mSection_title: 4.5.1  Existence measurement\n",
            "\u001b[35mSection_title: 4.5.2  Affirmation measurement\n",
            "\u001b[35mSection_title: 5  Experimental setup\n",
            "\u001b[35mSection_title: 5.1  Research questions\n",
            "\u001b[35mSection_title: 5.2  Data preparation\n",
            "\u001b[35mSection_title: 5.3  Test objects\n",
            "\u001b[35mSection_title: 6  Results and analysis\n",
            "\u001b[35mSection_title: 6.1  RQ1: The overall effectiveness of QAAskeR +\n",
            "\u001b[35mSection_title: 6.2  RQ2: Effectiveness comparison with Non-recursive metamorphic relations\n",
            "\u001b[35mSection_title: 6.3  RQ3: Validity of the revealed violations\n",
            "\u001b[35mSection_title: 6.4  RQ4: Analysis on the revealed true violations\n",
            "\u001b[35mSection_title: 6.5  RQ5: Helpfulness to fix the answering issues revealed by MRs\n",
            "\u001b[35mSection_title: 7  Discussion on real-life usage\n",
            "\u001b[35mSection_title: 8  Threats to validity\n",
            "\u001b[35mSection_title: 9  Related works\n",
            "\u001b[35mSection_title: 9.1  Benchmark datasets for QA software\n",
            "\u001b[35mSection_title: 9.2  Metamorphic testing for deep learning software\n",
            "\u001b[32mconclusion: question answering (qa) software has been widely used in our daily life. in this paper, we propose a novel recursive metamorphic testing method qaasker + with five novel recursive metamorphic relations. qaasker + tests qa software by checking its behaviors on multiple recursively asked questions that are relevant to the same or some further enriched knowledge. it cuts off the reliance on the preannotated labels of test cases, thus enables both the flexible just-in-time test during usage and the extensible test with massive unlabeled data for qa software, which cannot be supported by the current reference-based test paradigm. we evaluate the effectiveness of qaasker + by using it to test four representative state-of-the-art qa software that covers two mainstream types of qa software, as well as a popular reallife qa application, the google search service. comprehensive results demonstrate that qaasker + can reveal quantities of valid violations that depict diverse answering issues for various kinds of mainstream qa software. besides, we also found that our recursive mrs have a better fault detection effectiveness than two representative non-recursive mrs and can even help to fix the revealed issues.\n",
            "\u001b[35mSection_title: 10  Conclusion and future work\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Motivation\n",
            "\u001b[35mSection_title: 3  Background\n",
            "\u001b[35mSection_title: 3.1  Question answering software\n",
            "\u001b[35mSection_title: 3.1.1  QA Software based on information in knowledge base\n",
            "\u001b[35mSection_title: 3.1.2  QA Software based on information in textual materials\n",
            "\u001b[35mSection_title: 3.2  Metamorphic testing\n",
            "\u001b[35mSection_title: 4  Methodology\n",
            "\u001b[35mSection_title: 4.1  A recursive metamorphic testing method for QA software\n",
            "\u001b[35mSection_title: 4.2  Proposed metamorphic relations\n",
            "\u001b[35mSection_title: 4.3  Declarative sentence synthesis\n",
            "\u001b[35mSection_title: 4.3.1  Declarative sentence synthesis based on general question and its answer\n",
            "\u001b[35mSection_title: 4.3.2  Declarative sentence synthesis based on alternative question and its answer\n",
            "\u001b[35mSection_title: 4.3.3  Declarative sentence synthesis based on Wh-question and its answer\n",
            "\u001b[35mSection_title: 4.3.4  Declarative sentence synthesis based on Wh-question and extra information about its answer\n",
            "\u001b[35mSection_title: 4.4  Follow-up question sentence generation\n",
            "\u001b[35mSection_title: 4.4.1  General question sentence generation\n",
            "\u001b[35mSection_title: 4.4.2  Wh-question sentence generation\n",
            "\u001b[35mSection_title: 4.5  Violation measurement\n",
            "\u001b[35mSection_title: 4.5.1  Existence measurement\n",
            "\u001b[35mSection_title: 4.5.2  Affirmation measurement\n",
            "\u001b[35mSection_title: 5  Experimental setup\n",
            "\u001b[35mSection_title: 5.1  Research questions\n",
            "\u001b[35mSection_title: 5.2  Data preparation\n",
            "\u001b[35mSection_title: 5.3  Test objects\n",
            "\u001b[35mSection_title: 6  Results and analysis\n",
            "\u001b[35mSection_title: 6.1  RQ1: The overall effectiveness of QAAskeR +\n",
            "\u001b[35mSection_title: 6.2  RQ2: Effectiveness comparison with Non-recursive metamorphic relations\n",
            "\u001b[35mSection_title: 6.3  RQ3: Validity of the revealed violations\n",
            "\u001b[35mSection_title: 6.4  RQ4: Analysis on the revealed true violations\n",
            "\u001b[35mSection_title: 6.5  RQ5: Helpfulness to fix the answering issues revealed by MRs\n",
            "\u001b[35mSection_title: 7  Discussion on real-life usage\n",
            "\u001b[35mSection_title: 8  Threats to validity\n",
            "\u001b[35mSection_title: 9  Related works\n",
            "\u001b[35mSection_title: 9.1  Benchmark datasets for QA software\n",
            "\u001b[35mSection_title: 9.2  Metamorphic testing for deep learning software\n",
            "\u001b[32mconclusion: question answering (qa) software has been widely used in our daily life. in this paper, we propose a novel recursive metamorphic testing method qaasker + with five novel recursive metamorphic relations. qaasker + tests qa software by checking its behaviors on multiple recursively asked questions that are relevant to the same or some further enriched knowledge. it cuts off the reliance on the preannotated labels of test cases, thus enables both the flexible just-in-time test during usage and the extensible test with massive unlabeled data for qa software, which cannot be supported by the current reference-based test paradigm. we evaluate the effectiveness of qaasker + by using it to test four representative state-of-the-art qa software that covers two mainstream types of qa software, as well as a popular reallife qa application, the google search service. comprehensive results demonstrate that qaasker + can reveal quantities of valid violations that depict diverse answering issues for various kinds of mainstream qa software. besides, we also found that our recursive mrs have a better fault detection effectiveness than two representative non-recursive mrs and can even help to fix the revealed issues.\n",
            "\u001b[35mSection_title: 10  Conclusion and future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Extrapolation to out-of-domain data\n",
            "\u001b[35mSection_title: 2.2  Selective prediction\n",
            "\u001b[35mSection_title: 2.3  Related goals and tasks\n",
            "\u001b[35mSection_title: 3  Problem Setup\n",
            "\u001b[35mSection_title: 3.1  Selective Prediction\n",
            "\u001b[35mSection_title: 3.2  Selective Prediction under Domain Shift\n",
            "\u001b[35mSection_title: 3.3  Selective Question Answering\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 4.1  MaxProb\n",
            "\u001b[35mSection_title: 4.2  Test-time Dropout\n",
            "\u001b[35mSection_title: 4.3  Training a calibrator\n",
            "\u001b[35mSection_title: 5  Experiments and Analysis\n",
            "\u001b[35mSection_title: 5.1  Experimental Details\n",
            "\u001b[35mSection_title: 5.2  Main results\n",
            "\u001b[35mSection_title: 5.3  Overconfidence of MaxProb\n",
            "\u001b[35mSection_title: 5.4  Extrapolation between datasets\n",
            "\u001b[35mSection_title: 5.5  Calibrator feature ablations\n",
            "\u001b[35mSection_title: 5.6  Error analysis\n",
            "\u001b[35mSection_title: 5.7  Relationship with Unanswerable Questions\n",
            "\u001b[35mSection_title: 5.8  Changing ratio of in-domain to OOD\n",
            "\u001b[35mSection_title: 5.9  Allowing access to q unk\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The ENTAILMENTBANK Dataset\n",
            "\u001b[35mSection_title: 3.1  Guidelines\n",
            "\u001b[35mSection_title: 3.2  Tool and Authoring Procedure\n",
            "\u001b[35mSection_title: 3.3  Overall Dataset\n",
            "\u001b[35mSection_title: 3.4  Dataset Analysis\n",
            "\u001b[35mSection_title: 4  Task Definitions\n",
            "\u001b[35mSection_title: 5  Model\n",
            "\u001b[35mSection_title: 5.1  Entailment Tree Encoding\n",
            "\u001b[35mSection_title: 5.2  Model Details\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 6.2  Results\n",
            "\u001b[35mSection_title: 6.3  Error Analysis and Future Work\n",
            "\u001b[35mSection_title: 6.3.1  Individual Entailment Steps\n",
            "\u001b[35mSection_title: 6.3.2  Errors in the Full Entailment Trees\n",
            "\u001b[35mSection_title: 7  Generality\n",
            "\u001b[32mconclusion: our goal is to enable machines to generate richer, more systematic explanations. to this end, we have developed a novel formulation of explanations as multistep entailment trees, and created entail-mentbank, the first large dataset of such trees.\n",
            "\u001b[35mSection_title: 8  Summary and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  BPB: Automatically Generating Semantic Question Perturbations\n",
            "\u001b[35mSection_title: 3.1  Question Decomposition\n",
            "\u001b[35mSection_title: 3.2  Decomposition Perturbation\n",
            "\u001b[35mSection_title: 3.3  Question Generation\n",
            "\u001b[35mSection_title: 3.4  Answer Generation\n",
            "\u001b[35mSection_title: 3.5  Answer Constraint Generation\n",
            "\u001b[35mSection_title: 4  Generated Evaluation Sets\n",
            "\u001b[35mSection_title: 5  Experimental Setting\n",
            "\u001b[35mSection_title: 5.1  Models\n",
            "\u001b[35mSection_title: 5.2  Evaluation\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 5.4  Data Augmentation\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Textual Question Answering\n",
            "\u001b[35mSection_title: 2.1  Reading Comprehension\n",
            "\u001b[35mSection_title: 2.2  Reasoning\n",
            "\u001b[35mSection_title: 2.3  Domain-Specific\n",
            "\u001b[35mSection_title: 2.4  Multi-hop\n",
            "\u001b[35mSection_title: 3  Image Question Answering\n",
            "\u001b[35mSection_title: 3.1  Recognition\n",
            "\u001b[35mSection_title: 3.2  Reasoning\n",
            "\u001b[35mSection_title: 3.3  Commonsense and Knowledge\n",
            "\u001b[35mSection_title: 3.4  Multi-hop\n",
            "\u001b[35mSection_title: 4  Video Question Answering\n",
            "\u001b[35mSection_title: 5  Challenges and Chances\n",
            "\u001b[35mSection_title: 5.1  Textual QA\n",
            "\u001b[35mSection_title: 5.2  Visual QA\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Setup\n",
            "\u001b[35mSection_title: 2.1  Task Formulation\n",
            "\u001b[35mSection_title: 2.2  Instructing LLMs with Natural Language Prompts\n",
            "\u001b[35mSection_title: 2.2.1  QA Prompting\n",
            "\u001b[35mSection_title: 2.2.2  Judgemental Prompting\n",
            "\u001b[35mSection_title: 2.3  Experimental Settings\n",
            "\u001b[35mSection_title: 2.3.1  Datasets\n",
            "\u001b[35mSection_title: 2.3.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 2.3.3  Retrieval Sources\n",
            "\u001b[35mSection_title: 2.3.4  Implementation Details\n",
            "\u001b[35mSection_title: 3  Experimental Analysis and Findings\n",
            "\u001b[35mSection_title: 3.1  To What Extent Can LLMs Perceive\n",
            "\u001b[35mSection_title: 3.2  What Impact Does Retrieval Augmentation Have on LLMs?\n",
            "\u001b[35mSection_title: 3.3  How do Different Supporting Document Characteristics Affect LLMs?\n",
            "\u001b[35mSection_title: 3.3.1  Sampling Strategies\n",
            "\u001b[35mSection_title: 3.3.2  Findings\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 1.1  Strategy for Literature Search\n",
            "\u001b[35mSection_title: 2  QUESTION ANSWERING APPROACHES\n",
            "\u001b[35mSection_title: 2.1  Interactive Query Answering as Exploration and Disambiguation\n",
            "\u001b[35mSection_title: 2.2  Conversational Query Answering\n",
            "\u001b[35mSection_title: 2.3  Task and Challenge based classification of Question Answering Systems\n",
            "\u001b[35mSection_title: 2.4  Interaction Modalities\n",
            "\u001b[35mSection_title: 3  INTERACTIVE QA SYSTEMS: ARCHITECTURE AND TECHNIQUES\n",
            "\u001b[35mSection_title: 3.1  Architecture\n",
            "\u001b[35mSection_title: 3.2  Interaction Engine\n",
            "\u001b[35mSection_title: 3.3  State Tracker\n",
            "\u001b[35mSection_title: 3.4  QA module\n",
            "\u001b[35mSection_title: 3.5  Knowledge source\n",
            "\u001b[35mSection_title: 4  EVALUATION AND DATASET\n",
            "\u001b[35mSection_title: 4.1  Evaluation protocols\n",
            "\u001b[35mSection_title: 4.2  Dataset\n",
            "\u001b[32mconclusion: in conclusion, we have reviewed a substantial collection of interactive question answering systems (iqass)-related literature published during the past decade. we discovered the literature to be diverse, beginning with adopted methodologies for addressing multiple qa tasks and concluding with a vast array of diverse resources (i.e. knowledge source, and datasets) that are typically utilized to create and evaluate question answering systems (qass). despite the fact that the state-of-the-art is defined by several types of qa solutions, we were able to determine the characteristics shared by the suggested systems that constitute a shared framework. to the best of our knowledge, we are the first to present a unified and comprehensive design that emphasizes the fundamental components and functions of iqass.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Training\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Expert Agents\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Evaluation\n",
            "\u001b[35mSection_title: 5  Results and Discussions\n",
            "\u001b[35mSection_title: 5.1  Comparison with the Baselines\n",
            "\u001b[35mSection_title: 5.1.1  TWEAC\n",
            "\u001b[35mSection_title: 5.1.2  UnifiedQA\n",
            "\u001b[35mSection_title: 5.1.3  MultiQA\n",
            "\u001b[35mSection_title: 5.1.4  Max-Voting\n",
            "\u001b[35mSection_title: 5.2  Leave-One-Out Ablation\n",
            "\u001b[35mSection_title: 5.3  Qualitative Analysis\n",
            "\u001b[35mSection_title: 5.4  Efficiency of MetaQA\n",
            "\u001b[35mSection_title: 5.5  Ablation Study\n",
            "\u001b[32mconclusion: in this work, we propose an alternative to multidataset models for multi-skill qa. we propose to combine expert agents to create a collaborative system for question answering (qa) called metaqa. it considers questions, answer predictions, and confidence scores from the agents to select the best answer to a question. through quantitative experiments, we show that our model avoids the limitations of multi-dataset models and outperforms the baselines thanks to the agent collaboration established. additionally, since metaqa learns to match questions with answers instead of end-to-end qa, it is highly data-efficient to train. we leave as future work: i) combining partially correct answer predictions to generate a better one, ii) adding new agents without retraining metaqa by fixing most of the weights and only training the weights of the new agent selection network, and iii) identifying a priori agents that are likely to give an incorrect answer to skip them at run-time.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Unsupervised MCQA\n",
            "\u001b[35mSection_title: 2.1  Candidates Choosing\n",
            "\u001b[35mSection_title: 2.2  Learning Methods\n",
            "\u001b[35mSection_title: 3  Experiments Setup\n",
            "\u001b[35mSection_title: 3.2  Model Description\n",
            "\u001b[35mSection_title: 4  Experiment Results\n",
            "\u001b[35mSection_title: 5  Analysis Candidate Set & Matching Methods\n",
            "\u001b[32mconclusion: in this paper, we proposed an unsupervised mcqa method, which exploits the pseudo labels generated by some basic rules or external non-mcqa datasets. the proposed method significantly outperforms the baseline approaches on race and is even comparable with the supervised learning performance on mc500. we hope this paper sheds light on unsupervised learning in nlp tasks.  \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset\n",
            "\u001b[35mSection_title: 2.1  Desiderata\n",
            "\u001b[35mSection_title: 2.2  Data Collection Method\n",
            "\u001b[35mSection_title: 2.2.1  Question Annotation\n",
            "\u001b[35mSection_title: 2.3  Dataset Split\n",
            "\u001b[35mSection_title: 2.4  Data Analysis and Statistics\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 3.1  Pipeline Model\n",
            "\u001b[35mSection_title: 3.2  End-to-End Model\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experiment Setup\n",
            "\u001b[35mSection_title: 4.2  Automatic Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Results and Discussions\n",
            "\u001b[35mSection_title: 4.4  Human Evaluation\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we introduced the task of generative table question answering with fetaqa, a table qa dataset consisting of complex questions that require free-form, elaborate answers. we also proposed two modeling approaches: (1) a pipeline model that incorporates a table semantic parser and a data-to-text generator, and (2) an end-to-end model that integrates query comprehension, reasoning and text generation. our experimental results indicate that the end-to-end model with a simple table encoding strategy achieves much higher scores than the pipeline model that requires table semantic parsing. furthermore, we show that fetaqa reveals the challenging nature of the table question answering task and calls for innovative model designs in the future.  \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Mention Reference Masking\n",
            "\u001b[35mSection_title: 3.2  Copy-based Training Objective\n",
            "\u001b[35mSection_title: 3.3  Training\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Training Details\n",
            "\u001b[35mSection_title: 4.2  Extractive Question Answering\n",
            "\u001b[35mSection_title: 4.3  Relation Extraction\n",
            "\u001b[35mSection_title: 4.4  Fact Extraction and Verification\n",
            "\u001b[35mSection_title: 4.5  Coreference Resolution\n",
            "\u001b[35mSection_title: 4.6  GLUE\n",
            "\u001b[35mSection_title: 4.7  Ablation Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present a language representation model named corefbert, which is trained on a novel task, mention reference prediction, for strengthening the coreferential reasoning ability of bert. experimental results on several downstream nlp tasks show that our corefbert significantly outperforms bert by considering the coreference information within the text. in the future, there are several prospective research directions: (1) we introduce a distant supervision (ds) assumption in our mention reference prediction training task. it is a feasible approach to introducing the coreferential signal to language representation models, but the automatic labeling mechanism inevitably accompanies with the wrong labeling problem. until now, mitigating noise in ds data is still an open question.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.1  Supplementary Command Lines\n",
            "\u001b[35mSection_title: 1.1.1  Rosetta FastRelax\n",
            "\u001b[35mSection_title: 1.1.2  Rosetta KIC\n",
            "\u001b[35mSection_title: 1.1.3  RosettaAntibody Homology Modeling\n",
            "\u001b[35mSection_title: 1.2  Sequences Used to Model Naïve-Reverted Antibodies\n",
            "\u001b[35mSection_title: 1.3  Comparison of Flexibility Calculations Across Ensemble Generation Methods\n",
            "\u001b[35mSection_title: 2.2  Supplementary Tables\n",
            "\u001b[35mSection_title: 2012  JBC\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  SELF-CONSISTENCY OVER DIVERSE REASONING PATHS\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 3.1  EXPERIMENT SETUP\n",
            "\u001b[35mSection_title: 3.2  MAIN RESULTS\n",
            "\u001b[35mSection_title: 3.3  SELF-CONSISTENCY HELPS WHEN CHAIN-OF-THOUGHT HURTS PERFORMANCE\n",
            "\u001b[35mSection_title: 3.4  COMPARE TO OTHER EXISTING APPROACHES\n",
            "\u001b[35mSection_title: 3.5  ADDITIONAL STUDIES\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[32mconclusion: we introduced a simple yet effective method called self-consistency, and observed that it significantly improves accuracy in a range of arithmetic and commonsense reasoning tasks, across four large language models with varying scales. beyond accuracy gains, self-consistency is also useful for collecting rationales when performing reasoning tasks with language models, and for providing uncertainty estimates and improved calibration of language model outputs.\n",
            "\u001b[35mSection_title: 5  CONCLUSION AND DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Explanatory Knowledge Acquisition\n",
            "\u001b[35mSection_title: 3.2  Cloze-style Question Answering\n",
            "\u001b[35mSection_title: 3.3  Zero-shot and Fine-tuning Settings\n",
            "\u001b[35mSection_title: 4.2  Overall Results on Zero-shot Science Question Answering\n",
            "\u001b[35mSection_title: 4.3  Ablation Analysis on Impact of Different Explanatory Knowledge Types\n",
            "\u001b[35mSection_title: 4.4  Evaluating Zero-shot N-XKT with\n",
            "\u001b[35mSection_title: 4.5  Improvement on Fine-tuning Convergence\n",
            "\u001b[32mconclusion: in this paper, we proposed a neural encoding mechanism for explanatory knowledge acquisition and transfer, n-xkt. we evaluated the impact of the encoding mechanism on downstream science qa. the proposed model delivers better generalisation and accuracy for qa tasks that require multi-hop and explanatory inference. the proposed encoding mechanism can be used to deliver zero-shot inference capabilities, providing comparable performance when compared to supervised models on qa. these results supports the hypothesis that pretraining tasks targeting abstract and explanatory knowledge acquisition can constitute and impor- \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Generation\n",
            "\u001b[35mSection_title: 3.1  Atomic Queries\n",
            "\u001b[35mSection_title: 3.2  Introducing set operations\n",
            "\u001b[35mSection_title: 3.3  Annotation Tasks\n",
            "\u001b[35mSection_title: 3.3.1  Paraphrasing\n",
            "\u001b[35mSection_title: 3.3.2  Validation\n",
            "\u001b[35mSection_title: 3.3.3  Relevance Labeling\n",
            "\u001b[35mSection_title: 3.4  Dataset Statistics\n",
            "\u001b[35mSection_title: 3.5  Additional Training Examples\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Task Definition\n",
            "\u001b[35mSection_title: 4.2  Evaluation\n",
            "\u001b[35mSection_title: 4.3  Baseline Systems\n",
            "\u001b[35mSection_title: 4.4  Manual Error Annotation\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[32mconclusion: we present quest, a new benchmark of queries which contain implicit set operations with corresponding sets of relevant entity documents. our experiments indicate that such queries present a challenge for modern retrieval systems. future work could consider approaches that have better inductive biases for handling set operations in natural language expressions (for example, vilnis et al. (2018)). the attributions in quest can be leveraged for building systems that can provide finegrained attributions at inference time. the potential of pretrained generative lms and multi-evidence aggregation methods to answer set-seeking selective queries, while providing attribution to sources, can also be investigated.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: naturalness. since our dataset relies on the wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations. further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios. however, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on quest should correlate with improvements on at least a fraction of natural search engine queries with set operations.recall. we also note that because wikipedia categories have imperfect recall of all relevant entities (that contain sufficient evidence in their documents), systems may be incorrectly penalised for predicted relevant entities assessed as false positive. we quantify this in section 5. we have also limited the trusted source for an entity to its wikipedia document but entities with insufficient textual evidence in their documents may still be relevant. ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions. romqa (zhong et al., 2022) takes a step in this latter direction although the evidence attribution is not manually verified.answer set sizes. to ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers. however, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible.on one hand, this is realistic for a search scenario because users may only be interested in a limited set of results. on the other hand, our dataset does not model a scenario where the answer set sizes are much larger.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methods\n",
            "\u001b[35mSection_title: 2.1  Data Collection\n",
            "\u001b[35mSection_title: 2.  Template expansion\n",
            "\u001b[35mSection_title: 1.  Retrieval\n",
            "\u001b[35mSection_title: 2.2  Model Training\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Baselines\n",
            "\u001b[35mSection_title: 3.2  Perturbation sets\n",
            "\u001b[35mSection_title: 3.3  QASC and EntailmentBank\n",
            "\u001b[35mSection_title: 3.4  Evaluation Criteria\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Results on Perturbation Sets\n",
            "\u001b[35mSection_title: 4.2  Results on QASC\n",
            "\u001b[35mSection_title: 4.3  Results on EntailmentBank\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: building systems that use natural language as a medium for reasoning will require operations to logically combine and transform natural language statements. in this work, we present parapattern, a method for creating such models with minimal manual effort by finetuning pretrained sequence-to-sequence language models on data generated through a three-step process of syntactic retrieval, template expansion, and automatic paraphrasing. our experimental results show that parapattern yields operation models capable of generating consistent logical transformations over a diverse range of natural language inputs, matching the performance of models trained with in-domain human supervision.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Partial Match in MRC\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  The Reader\n",
            "\u001b[35mSection_title: 3.2  The Corrector\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Comparison with Equal Parameters\n",
            "\u001b[35mSection_title: 5.2  Changes in Answers\n",
            "\u001b[32mconclusion: we describe a novel method for answer span correction in machine reading comprehension. the proposed method operates by marking an original, possibly incorrect, answer prediction in context and then making a new prediction using a corrector model. we show that this method corrects the predictions of a state-of-the-art english-language reader in different error categories. in our experiments, the approach also generalizes well to multilingual and cross-lingual mrc in seven languages. future work will explore joint answer span cor-\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Explainability in Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 3  Explanation-supporting Benchmarks\n",
            "\u001b[35mSection_title: 4  Explainable MRC Architectures\n",
            "\u001b[35mSection_title: 4.1  Modeling Explanatory Relevance for Knowledge-based Explanations\n",
            "\u001b[35mSection_title: 4.1.1  Explicit Models\n",
            "\u001b[35mSection_title: 4.1.2  Latent Models\n",
            "\u001b[35mSection_title: 4.1.3  Hybrid Models\n",
            "\u001b[35mSection_title: 4.2  Operational Explanation\n",
            "\u001b[35mSection_title: 5  Evaluation\n",
            "\u001b[35mSection_title: 5.1  Silver Explanations\n",
            "\u001b[32mconclusion: this survey has proposed a systematic categorisation of benchmarks and approaches for explainability in mrc. lastly, we outline a set of open research questions for future work:\n",
            "\u001b[35mSection_title: 6  Conclusion and Open Research Questions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  COMPOSITIONAL QA\n",
            "\u001b[35mSection_title: 3.1  Dataset Collection\n",
            "\u001b[35mSection_title: 3.1.1  Question-Document Collection.\n",
            "\u001b[35mSection_title: 3.1.2  Documents\n",
            "\u001b[35mSection_title: 3.1.3  Answer Annotation.\n",
            "\u001b[35mSection_title: 3.1.4  Quality Inspection.\n",
            "\u001b[35mSection_title: 3.1.5  Test Set Construction.\n",
            "\u001b[35mSection_title: 4  HIERARCHICAL GRAPH NEURAL NETWORKS FOR COMQA\n",
            "\u001b[35mSection_title: 4.1  Sequence Encoder\n",
            "\u001b[35mSection_title: 4.1.1  Embedding Layer.\n",
            "\u001b[35mSection_title: 4.2  Hierarchical Graph Neural Networks\n",
            "\u001b[35mSection_title: 4.2.1  Graph Construction.\n",
            "\u001b[35mSection_title: 4.2.2  Information Aggregation.\n",
            "\u001b[35mSection_title: 4.3  Prediction and Objective\n",
            "\u001b[35mSection_title: 4.4  Unsupervised Pre-training\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS 5.1 Common Setup\n",
            "\u001b[35mSection_title: 5.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.3  Baseline Methods\n",
            "\u001b[35mSection_title: 5.4  Main Results\n",
            "\u001b[35mSection_title: 5.5  The Effect of Pre-training\n",
            "\u001b[35mSection_title: 5.6  The Effect of Connection Type\n",
            "\u001b[35mSection_title: 5.7  Error Analysis\n",
            "\u001b[32mconclusion: in this paper, we study the compositional question answering where the answer is composed of discontiguous segments in the document. we present a large scale chinese comqa dataset containing more than 120k human-labeled questions. the data construction process has undergone rigid inspections to ensure high quality. to solve the comqa problem, we propose a hierarchical graph neural networks that incorporate document graph structure to the model. we also devise two novel tasks, i.e., question selection and node selection, to pre-train the model. the proposed methods achieve significant improvement over previous methods. we also conduct several ablation studies to demonstrate the superiority of the proposed pre-training tasks and the graph structure. however, there is still a large gap between our model with human performance, suggesting that there is still room for improvement in comqa. figure 8: a snapshot of the annotation interface. each row is a single node that could be served as the final answer component. the question is the page title in the top. note that the image or table could also be selected.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Multi-modal Learning\n",
            "\u001b[35mSection_title: 2.2  Multi-Hop Reasoning\n",
            "\u001b[35mSection_title: 2.3  Visual Question Answering\n",
            "\u001b[35mSection_title: 3  DVLQA Dataset\n",
            "\u001b[35mSection_title: 3.1  Task Overview\n",
            "\u001b[35mSection_title: 3.2  Constructing DVLQA Data collection and Annotation\n",
            "\u001b[35mSection_title: 4  Benchmarking\n",
            "\u001b[35mSection_title: 4.1  Best Existing Architectures\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Discussion\n",
            "\u001b[32mconclusion: in this work, we introduced the diverse visuo-linguistic question answering (dvlqa) challenge that we believe has the potential to open new research avenues in areas of joint vision & language. our experiments show that a system equipped with state-of-the-art vision-language models does not perform well on the task that requires joint visionlanguage inference. our future work would include extending this dataset to support more diverse visuo-linguistic tasks for future research on building generic ai models that can learn novel visual concepts through small set of examples. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Data Augmentation\n",
            "\u001b[35mSection_title: 3  RGF: Counterfactuals for Information-seeking Queries\n",
            "\u001b[35mSection_title: 3.1  Overview of RGF\n",
            "\u001b[35mSection_title: 3.2  Retrieval\n",
            "\u001b[35mSection_title: 3.3  Question Generation\n",
            "\u001b[35mSection_title: 3.4  Filtering for Data Augmentation\n",
            "\u001b[35mSection_title: 3.5  Semantic Filtering for Evaluation\n",
            "\u001b[35mSection_title: 4  Intrinsic Evaluation\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Reading Comprehension (RC)\n",
            "\u001b[35mSection_title: 5.3  Open-Domain Question Answering (OD)\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 6.1  Local Robustness\n",
            "\u001b[35mSection_title: 6.2  Effect of Perturbation Type\n",
            "\u001b[35mSection_title: 6.3  Effect of Training data size\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.2  Question Generation\n",
            "\u001b[35mSection_title: 3.3  QA Model\n",
            "\u001b[35mSection_title: 3.4  Contrastive Adaptation Loss\n",
            "\u001b[35mSection_title: 4  Experiment Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Training and Evaluation\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Performance on Target Questions\n",
            "\u001b[35mSection_title: 5.2  Sensitivity Analysis for Text Corpora Size\n",
            "\u001b[35mSection_title: 5.3  Comparison: Training Baselines with Contrastive Adaptation Loss\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: this work contributes a novel framework for domain adaptation of qa systems in settings with limited text corpora. we develop caqa in which we combine techniques from from question generation and domain-invariant learning to answer out-of-domain questions. different from existing works in question answering, we achieve this by proposing a contrastive adaptation loss. extensive experiments show that caqa is superior to other state-of-the-art approaches by achieving a substantially better performance on out-of-domain data. qagen-t5: we apply lm-filtering as in (shakeri et al., 2020) and select qa pairs with highest scores for each context paragraph. qagen-t5 models are trained similarly to aqgen and qagen, we separately keep the best qg and qa models according to validation performance on the squad dev set. hyperparameter search: in our experiments, we empirically search for hyperparameters β and σ in the contrastive adaptation loss through additional experiments. we experiment with different values of β in the range [10 −1 , 10 −2 , 10 −3 ] and gaussian noise n (0, σ) applied on all token embeddings with standard deviation σ ranging from 0 to 10 −2 . the best combination of β and σ as per the training set is then selected, these numbers can be found in table 4.  all parameters that have not been mentioned explicitly above were used as reported in their original paper b additional results\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Knowledge Acquisition\n",
            "\u001b[35mSection_title: 3.1.1  Retrieving Relevant Tweets\n",
            "\u001b[35mSection_title: 3.1.2  Gathering Relevant Concepts\n",
            "\u001b[35mSection_title: 3.1.3  Maintaining Hashtag Pool\n",
            "\u001b[35mSection_title: 3.1.4  Refining Topic Knowledge\n",
            "\u001b[35mSection_title: 3.2  Topic Knowledge Reader\n",
            "\u001b[35mSection_title: 3.2.1  Encoding Tweet and Question\n",
            "\u001b[35mSection_title: 3.2.2  Encoding Concepts\n",
            "\u001b[35mSection_title: 3.2.3  Topic Knowledge Fusion\n",
            "\u001b[35mSection_title: 3.2.4  Prediction\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  TweetQA Dataset\n",
            "\u001b[35mSection_title: 4.2  Implement Detail\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 4.5  Different Number of Concepts\n",
            "\u001b[35mSection_title: 4.6  Ablation Study\n",
            "\u001b[35mSection_title: 4.7  Extractive vs. Generative\n",
            "\u001b[35mSection_title: 4.8  Weakly Supervised Training\n",
            "\u001b[32mconclusion: in this paper, we focus on machine reading comprehension in social media domain. we propose a novel method to address the problem of lacking in background knowledge in this task. utilizing the nature of clustering of social media, we retrieve and refine topic knowledge from the relevant messages, and then integrate the knowledge into an mrc model, tkr. experimental results show that our proposed method outperforms the recently proposed models and the bert-based baselines, which proves the method effective overall. by introducing different amount of topic knowledge, we demonstrate the effectiveness of our refined knowledge. moreover, the ablation study further validates the contribution of the key modules of tkr for utilizing the knowledge.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction and Background\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[32mconclusion: in this work, we presented inpars-v2, an improved version of inpars [1] that uses a publicly available language model to generate queries and a better query-document pair selection process. our results show that we achieve effectiveness on par with the state of the art on beir. the synthetic data and finetuned models were publicly released.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Diverse Verifier on Reasoning\n",
            "\u001b[35mSection_title: 3.1  Diverse Prompts\n",
            "\u001b[35mSection_title: 3.2  Voting Verifier\n",
            "\u001b[35mSection_title: 3.3  Step-aware Voting Verifier\n",
            "\u001b[35mSection_title: 4.2  Details\n",
            "\u001b[35mSection_title: 5  Main Results\n",
            "\u001b[35mSection_title: 5.1  Effectiveness\n",
            "\u001b[35mSection_title: 5.2  Comparing to Previous SOTAs\n",
            "\u001b[35mSection_title: 5.3  Noises in Multiple Choice Tasks\n",
            "\u001b[35mSection_title: 6  Ablations\n",
            "\u001b[35mSection_title: 6.1  Diverse Prompts vs. Sampling Decoding\n",
            "\u001b[35mSection_title: 6.2  The Power of Voting Verifier\n",
            "\u001b[35mSection_title: 6.3  The Impact of\n",
            "\u001b[35mSection_title: 6.5  How Many Training Data Do We Need?\n",
            "\u001b[35mSection_title: 6.6  The Impact of the Number of Exemplars\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we introduce diverse, an effective and general method to make large language models better reasoners. as a continuation of the line of research that prompting language models using multi-step reasoning paths, the key insights of diverse are three-fold: diverse prompts, voting verifier, and step-level correctness. experimental results clearly show that diverse can bring significant and consistent improvements. for example, with codedavinci-002, diverse achieves new state-of-theart results in most of the reasoning tasks, outperforming the 540b palm model combined with previous prompting approaches.\n",
            "\u001b[35mSection_title: 8  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Title Generation\n",
            "\u001b[35mSection_title: 3.2  Passage Selection\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Metrics\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Boosting with Correction\n",
            "\u001b[32mconclusion: in this work, we propose an information retrieval (ir) pipeline built on the advances of query decomposition and event linking. specifically, we associate the input query with generated events relevant to the query through the decomposition step and then link these events to real-world facts in a knowledge base with an event-linking model. we show that with simple passage selection through bm-25 and cross-domain qa supervision, our pipeline outperforms existing state-of-the-art unsupervised or cross-domain ir models on five datasets by an average of 6%. compared with existing methods, our pipeline does not involve heavy pre-training, parameter tuning, or domain-specific supervision.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 8549790  FUSE 37° in vitro\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Problem Setup\n",
            "\u001b[35mSection_title: 2.2  Iterative Prompting Framework\n",
            "\u001b[35mSection_title: 2.3  Context-Aware Prompter\n",
            "\u001b[35mSection_title: 2.4  Learning and Inference\n",
            "\u001b[35mSection_title: 3.1  Datasets & Preprocessing\n",
            "\u001b[35mSection_title: 3.2  Evaluation Metric\n",
            "\u001b[35mSection_title: 3.3  Compared Methods\n",
            "\u001b[35mSection_title: 3.4  Implementation Details\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Effectiveness of iCAP\n",
            "\u001b[35mSection_title: 4.2  Faithfulness of Prompting\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we explore an iterative prompting framework towards driving a \"train of thought\" from plms for multi-step reasoning tasks. we show the superiority of this iterative scheme, and also effectiveness of our proposed context-aware prompter design, which addresses key limitations of previous prompting methods when applied in this new scheme. in addition, we conduct both quantitative & qualitative analysis on the faithfulness of the learned prompting behaviors. in the future, we aim to further extend and apply our ideas to language model pretraining, with the hope that plms can be inherently equipped with stronger multi-step reasoning capabilities. the iterative framework we explore here also opens the possibility of human intervention and interaction during inference; namely a human can track along the plm's train of thought and make edits and corrections at different steps, which improves the transparency and trustworthiness of inference and also helps reduce error propagation along the reasoning process. we leave these investigations as future work. \n",
            "\u001b[35mSection_title: 6  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Models and Datasets\n",
            "\u001b[35mSection_title: 3.2  HotpotQA and BoolQ\n",
            "\u001b[35mSection_title: 4  ChatGPT's Failures\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Abilities Behind Errors\n",
            "\u001b[35mSection_title: 7  Methods\n",
            "\u001b[35mSection_title: 7.1  Knowledge Memorization\n",
            "\u001b[35mSection_title: 7.2  Knowledge Recall\n",
            "\u001b[35mSection_title: 7.3  Knowledge Reasoning\n",
            "\u001b[35mSection_title: 8.2  Providing relevant background aids in recalling essential knowledge\n",
            "\u001b[35mSection_title: 9  Towards Truthfulness in Question Answering\n",
            "\u001b[32mconclusion: our paper investigates the common failures of chat-gpt in complex open-domain question answering. we identify four types of errors: comprehension, factualness, specificity, and inference. we also examine the key abilities knowledge memorization, knowledge recall, and knowledge reasoning, which are critical to these failures. additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on recall, and the effect of decomposition on reasoning. finally, we suggest several techniques to help users more effectively use chatgpt as a question-answering tool and enable system builders to develop better qa systems. our research contributes to the understanding of what influencing the truthfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more reliable llms.\n",
            "\u001b[35mSection_title: 10  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Models and Datasets\n",
            "\u001b[35mSection_title: 3.2  HotpotQA and BoolQ\n",
            "\u001b[35mSection_title: 4  ChatGPT's Failures\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Abilities Behind Errors\n",
            "\u001b[35mSection_title: 7  Methods\n",
            "\u001b[35mSection_title: 7.1  Knowledge Memorization\n",
            "\u001b[35mSection_title: 7.2  Knowledge Recall\n",
            "\u001b[35mSection_title: 7.3  Knowledge Reasoning\n",
            "\u001b[35mSection_title: 8.2  Providing relevant background aids in recalling essential knowledge\n",
            "\u001b[35mSection_title: 9  Towards Truthfulness in Question Answering\n",
            "\u001b[32mconclusion: our paper investigates the common failures of chat-gpt in complex open-domain question answering. we identify four types of errors: comprehension, factualness, specificity, and inference. we also examine the key abilities knowledge memorization, knowledge recall, and knowledge reasoning, which are critical to these failures. additionally, we investigate the impact of granularity on external knowledge provision, the influence of background knowledge on recall, and the effect of decomposition on reasoning. finally, we suggest several techniques to help users more effectively use chatgpt as a question-answering tool and enable system builders to develop better qa systems. our research contributes to the understanding of what influencing the truthfulness of question answering in chatgpt and provides practical insights for improving the performance of qa systems, ultimately paving the way for more reliable llms.\n",
            "\u001b[35mSection_title: 10  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Event Detection Definition\n",
            "\u001b[35mSection_title: 3  Data Collection of MAVEN\n",
            "\u001b[35mSection_title: 3.1  Event Schema Construction\n",
            "\u001b[35mSection_title: 3.2  Document Selection\n",
            "\u001b[35mSection_title: 3.3  Candidate Selection and Automatic Labeling\n",
            "\u001b[35mSection_title: 3.4  Human Annotation\n",
            "\u001b[35mSection_title: 4.2  Data Distribution\n",
            "\u001b[35mSection_title: 4.3  Multiple Events in One Sentence\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Benchmark Setting\n",
            "\u001b[35mSection_title: 5.2  Experimental Setting\n",
            "\u001b[35mSection_title: 5.3  Overall Experimental Results\n",
            "\u001b[35mSection_title: 5.4  Analyses on Data Size and #Event Types\n",
            "\u001b[35mSection_title: 5.5  Analyses on Transferability\n",
            "\u001b[35mSection_title: 5.6  Error Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present a massive general domain event detection dataset (maven), which significantly alleviates the data scarcity and low coverage problems of existing datasets. we conduct a thorough evaluation of the state-of-the-art ed models on maven. the results indicate that general domain ed is still challenging and maven may facilitate further research. we also explore some promising directions with analytic experiments, including modeling multiple event correlations (sec-tion 5.3), utilizing the hierarchical event schema to distinguish close types (section 5.6), and improving other ed tasks with transfer learning (section 5.5). in the future, we will extend maven to more event-related tasks like event argument extraction, event sequencing, etc. \n",
            "\u001b[35mSection_title: 7  Conclusion and Future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Training Dense Retrieval Models\n",
            "\u001b[35mSection_title: 2.2  A Unified Framework of Improved Dense\n",
            "\u001b[35mSection_title: 2.3  Settings for Empirical Studies\n",
            "\u001b[35mSection_title: 3  Pilot Studies on Data Augmentation\n",
            "\u001b[35mSection_title: 3.1  An Exploration of Data Augmentation\n",
            "\u001b[35mSection_title: 3.2  Training with Diverse Supervisions\n",
            "\u001b[35mSection_title: 3.3  Empirical Studies\n",
            "\u001b[35mSection_title: 3.4  Training our DRAGONs\n",
            "\u001b[35mSection_title: 4  Comparison with the State of The Art\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baseline Models\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 5  Discussions\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we present dragon, dense retriever trained with diverse augmentation. we propose a unified framework of data augmentation (da) to understand the recent progress of training dense retrievers. based on the framework, we extensively study how to improve dense retrieval training through query and relevance label augmentation. our experiments uncover some insights into training a dense retriever, which contradicts common wisdom that cross encoder is the most effective teacher and human-like queries are the most suitable training data for dense retrieval. instead, we propose a diverse data augmentation recipe, query augmentation with the mixture of sentence cropping and generative queries, and progressive relevance label augmentation with multiple teachers. with our recipe of da, we are the first to demonstrate that a single bert-base-sized dense retriever can achieve state-of-the-art effectiveness in both supervised and zero-shot retrieval tasks. we believe that dragon can serve as a strong foundation retrieval model for domain adaptation retrieval tasks (wang et al., 2022;dai et al., 2022) or the existing retrieval augmented language models (izacard et al., 2022;shi et al., 2023;mallen et al., 2022).\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 28  Oct 2022\n",
            "\u001b[35mSection_title: 2  Problem Description and Motivation\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Step Deduction Model\n",
            "\u001b[35mSection_title: 3.2  Search\n",
            "\u001b[35mSection_title: 3.3  Goal Entailment\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Evaluation: Goal Discrimination\n",
            "\u001b[35mSection_title: 4.2  End-to-end Baseline\n",
            "\u001b[35mSection_title: 4.3  Metrics\n",
            "\u001b[35mSection_title: 4.4  Implementation and Training\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Complete Deductions\n",
            "\u001b[35mSection_title: 5.2  Individual Step Validity\n",
            "\u001b[35mSection_title: 5.3  Goal Entailment\n",
            "\u001b[35mSection_title: 5.4  Error Analysis: Step Model\n",
            "\u001b[35mSection_title: 5.5  Error Analysis: Goal Entailment\n",
            "\u001b[35mSection_title: 6  Related work\n",
            "\u001b[32mconclusion: in this work, we propose a system that performs natural language reasoning through generative deduction and heuristic-guided search.\n",
            "\u001b[35mSection_title: 7  Discussion and Conclusion\n",
            "\u001b[32mlimitation: the baseline approach we consider in this work, end-to-end modeling of entailment tree generation, enjoys the convenience of simple inference and quadratic complexity. however, the computational overhead of sequence-to-sequence models places a hard limit on the tree size and premise count that can be handled in the end-to-end setting; moreover, recent results call into question how well end-to-end transformers can generalize this type of reasoning (zhang et al., 2022). our structured approach allows arbitrarily large premise sets and step counts. however, by discretizing the reasoning in the scsearch procedure, we do face a runtime theoretically exponential in proof size to do exhaustive search. in practice, we limit our search to a finite horizon and find that this suffices to provide a practical wall clock runtime, never exceeding 5 seconds for any single example. future work on higher tree depths may have to reckon with the theoretical limitations of this procedure, possibly through the use of better heuristics.our experiments are conducted exclusively on english datasets. while we hypothesize that our approach would work equally well for another language given a pretrained sequence-to-sequence model for that language with equivalent capacity, such models are not available universally across languages, representing an obstacle for transferring our results to languages beyond english.furthermore, the entailmentbank dataset on which we train and evaluate targets the elementary science domain, raising a question of domain specificity. in future work, we plan to evaluate deduction models on additional datasets with different style, conceptual content, and types of reasoning in order to verify that the factored approach is equally applicable across diverse settings.\n",
            "\u001b[35mSection_title: 8  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Architecture\n",
            "\u001b[35mSection_title: 2.1  Setup\n",
            "\u001b[35mSection_title: 2.2  Heuristics\n",
            "\u001b[35mSection_title: 2.3  Reflexion\n",
            "\u001b[35mSection_title: 2.4  Reward model\n",
            "\u001b[35mSection_title: 2.5  Action space\n",
            "\u001b[35mSection_title: 3  Evaluation\n",
            "\u001b[35mSection_title: 3.1  AlfWorld\n",
            "\u001b[35mSection_title: 3.2  HotPotQA\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[35mSection_title: 4.2  Hallucination vs. Inefficient Planning\n",
            "\u001b[35mSection_title: 4.3  Reflexion Enables More Intuitive Search Queries\n",
            "\u001b[32mlimitation: reflexion relies on the emergent property of self-reflection that is present in several large language models. in this study, we used gpt-3.0 and gpt 3.5 to power a react agent (yao et al., 2023) to learn from its past mistakes. while reflexion enabled the agent to discover new problem-solving techniques in alfworld decision-making tasks and hotpotqa knowledge-intensive tasks, we observed a shortcoming in its ability to improve on its baseline performance in a third benchmark, webshop (yao et al., rint).webshop is a text-based problem-solving benchmark that tests natural language agents to navigate an e-commerce website to find and purchase products given requests from a client. we tested the agent in 100 environments, giving the agent two few-shot examples of successful webshop trajectories using the react problem-solving technique. however, after only 4 trials, we terminated the baseline and reflexion runs as the agent did not show improvement in accuracy (fig. 6) and was not generating helpful, intuitive self-reflections. the agent achieved a 33% → 34% accuracy improvement in the baseline run and a mere 33% → 35% accuracy improvement in the reflexion run, which suggests that the agent only successfully completed 1 additional task relative to the baseline agent's performance.however, after analyzing the failed trajectories, we noted that the chance of a successful item purchase for an agent in a webshop environment was not necessarily dependent on the agent's ability to plan and execute a correct sequence of actions, but rather on the quality of the webshop search engine's results. this observation may not be a direct limitation of the reflexion approach, but it highlights the ability of a reflexion agent to optimize reasoning trace and action execution but not complete awareness of the quality of the tools that it may be using.\n",
            "\u001b[35mSection_title: 4.4  Limitations of Reflexion\n",
            "\u001b[32mconclusion: we proposed an approach that allows natural language agents to learn from past mistakes and redirect future decisions in planning sequences which removes the human trainer in a human-in-the-middle approach. we demonstrated learning curves on the alfworld and hotpotqa benchmarks that significantly outperform base react agents. in addition, we include an inconclusive attempt to improve performance on the webshop benchmark and provide a discussion that highlights a few limitations of this approach. reflexion is a highly applicable method to improve performance between trials on decision-making and knowledge-intensive tasks due to its sole dependence on a binary reward model. in the alfworld and hotpotqa experiments, we constrained the reward model to imitate environments in which informative reward models may be difficult to design or compute. we encourage others to apply reflexion to more complex tasks in which the agent must learn to develop new ideas, explore larger unseen state spaces, and form more accurate plans of action through its experiences in past environments. \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 6  Reproducibility\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS\n",
            "\u001b[35mSection_title: 3  PROBLEM FORMULATION\n",
            "\u001b[35mSection_title: 4  METHODS: SEARCH-ADAPTOR\n",
            "\u001b[35mSection_title: 4.1  RANKING OBJECTIVE\n",
            "\u001b[35mSection_title: 4.2  ADAPTING FIXED LLMS\n",
            "\u001b[35mSection_title: 4.3  REGULARIZATION\n",
            "\u001b[35mSection_title: 4.4  TRAINING\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  EXPERIMENTAL SETTINGS\n",
            "\u001b[35mSection_title: 5.2  ADAPTING WITH API-BASED LLMS\n",
            "\u001b[35mSection_title: 6.2  ABLATION STUDIES\n",
            "\u001b[32mconclusion: pre-trained llms have shown great potential in a variety of downstream tasks.in this paper, we focus on pushing the capabilities of llms for information retrieval and search.we propose a canonical efficient adaptation method, search-adaptor, that can also be applied to llms available only via apis.we demonstrate that search-adaptor significantly and consistently improves retrieval performance across diverse regimes of training data size, encoder type, and corpus set.important future directions include generalizing the adaptation method to include partial tuning of the embedding models, as well as extensions to multi-modal data.\n",
            "\u001b[35mSection_title: 7  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Case Retrieval\n",
            "\u001b[35mSection_title: 3.2  Case Reuse\n",
            "\u001b[35mSection_title: 3.2.1  Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Experiment Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.3.1  Supporting Evidence Identification\n",
            "\u001b[35mSection_title: 4.3.2  Robustness to Lexical Diversity\n",
            "\u001b[35mSection_title: 4.3.3  Few-shot Domain Adaptation\n",
            "\u001b[35mSection_title: 4.3.4  Effect of Retrieval Quantity\n",
            "\u001b[35mSection_title: 4.3.5  Effect of Modeling\n",
            "\u001b[32mconclusion: we present cbr-mrc, a semi-parametric model for machine reading comprehension that is simple, accurate, and interpretable.our model stores a collection of cases, retrieves the most relevant cases for a given test question, and then explicitly reuses the reasoning patterns encoded in the embeddings of these cases to predict an answer.we show that our model performs well for both extracting answers and identifying supporting evidence on several mrc tasks compared to fully-parametric baselines.we also demonstrate the ability of our model to transfer to new domains with limited labeled data.finally, we analyze our model under varying conditions of lexical diversity and find that it is robust to high lexical diversity, whereas fullyparametric models show a drop in performance.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: cbr-mrc, and the cbr framework more generally, relies on the existence of past cases to make predictions for a new problem.this can pose a challenge for composite questions, which require multi-hop reasoning, since the likelihood of enumerating each algebraic combination of reasoning patterns in the casebase is impractical.models, thus, may only be able to match a portion of the question, resulting in partially correct reasoning.to address these limitations, future work could explore methods to explicitly encourage such compositional generalization, such as question decomposition with a recursive application of cbr.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Model\n",
            "\u001b[35mSection_title: 2.1  Retrieval and Encoding\n",
            "\u001b[35mSection_title: 2.2  Document Explorer\n",
            "\u001b[35mSection_title: 2.3  Answer Proposer\n",
            "\u001b[35mSection_title: 2.4  Evidence Assembler\n",
            "\u001b[35mSection_title: 2.5  Joint Optimization\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 3.1  Datasets and Metrics\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 4  Analysis\n",
            "\u001b[35mSection_title: 4.1  Reasoning Chain Recovery Tests\n",
            "\u001b[35mSection_title: 4.2  Answer Proposer Comparisons\n",
            "\u001b[35mSection_title: 4.3  Assembler Ablations\n",
            "\u001b[35mSection_title: 4.4  Multi-hop Reasoning Example\n",
            "\u001b[35mSection_title: 5  Related Works\n",
            "\u001b[32mconclusion: we presented an interpretable 3-module, multihop, reading-comprehension system 'epar' which constructs a 'reasoning tree', proposes an answer candidate for every root-to-leaf chain, and merges key information from all reasoning chains to make the final prediction. on wikihop, our system outperforms all published models on the dev set, and achieves results competitive with the current stateof-the-art on the test set. on medhop, our system outperforms all previously published models on the leaderboard test set. we also presented multiple reasoning-chain recovery tests for the explainability of our system's reasoning capabilities.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Conventional Deep Learning\n",
            "\u001b[35mSection_title: 2.2  Pre-training in Natural\n",
            "\u001b[35mSection_title: 2.3  Pre-training in Computer Vision\n",
            "\u001b[35mSection_title: 2.4  Pre-training in Audio and Speech\n",
            "\u001b[35mSection_title: 3.1  Task Definition and Key Challenges\n",
            "\u001b[35mSection_title: 3.2  Advantages of MM-PTMs\n",
            "\u001b[35mSection_title: 3.3  Pre-training Data\n",
            "\u001b[35mSection_title: 3.4  Pre-training Objectives\n",
            "\u001b[35mSection_title: 3.5  Pre-training Network Architecture\n",
            "\u001b[35mSection_title: 3.5.1  Self-attention and Transformer\n",
            "\u001b[35mSection_title: 3.5.2  Single-and Multi-stream\n",
            "\u001b[35mSection_title: 3.5.3  Modality Interactive Learning\n",
            "\u001b[35mSection_title: 3.6  Pre-training using Knowledge\n",
            "\u001b[35mSection_title: 3.7  Characteristics of Different\n",
            "\u001b[35mSection_title: 4  Downstream Tasks\n",
            "\u001b[35mSection_title: 4.1  Generative Tasks\n",
            "\u001b[35mSection_title: 4.2  Classification Tasks\n",
            "\u001b[35mSection_title: 4.3  Regression Tasks\n",
            "\u001b[35mSection_title: 4.4  Prompt Learning\n",
            "\u001b[35mSection_title: 5  Experimental Analysis\n",
            "\u001b[35mSection_title: 5.1  Model Parameters and Training Information\n",
            "\u001b[35mSection_title: 5.2  Performance on Representative Downstream Tasks\n",
            "\u001b[35mSection_title: 6  Research Directions\n",
            "\u001b[32mconclusion: we give a comprehensive review of large-scale multi-modal pre-trained models (mm-ptms) in this paper.firstly, we introduce the background of mm-ptms, with a focus on conventional deep learning, and pre-training in nlp, cv, and speech.then, the task definition, key challenges, and benefits of mm-ptms are discussed.after that, we dive into the reviews of mm-ptms and discuss the pre-training data, objectives, networks, knowledge enhanced pre-training, etc.we review the downstream tasks including generative, classification, and regression tasks, and also give an overview of model parameters of mm-ptms and hardware for the pre-training.experimental results of several representative tasks are also discussed and visualized.finally, we point out some research directions that are worth to be focused on.we summarize this paper and hope our survey can provide some useful insights for the mm-ptms.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  BOSS Benchmark 2.1 Motivation\n",
            "\u001b[35mSection_title: 2.2  Protocol to Construct OOD benchmark.\n",
            "\u001b[35mSection_title: 2.3  Dataset Selection\n",
            "\u001b[35mSection_title: 3  Analysis of OOD Robustness\n",
            "\u001b[35mSection_title: 4  Evaluation of OOD Robustness 4.1 Robustness-enhanced Methods\n",
            "\u001b[35mSection_title: 4.2  Large Language Models\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we revisit ood robustness research in nlp, identifying deficiencies in benchmarks and evaluation.correspondingly, a benchmark construction protocol and an ood robustness evaluation suite are proposed to facilitate future research.the correlation between ood and id performance, the effectiveness of existing methods, and the challenges faced by llms are investigated.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  BACKGROUNDS\n",
            "\u001b[35mSection_title: 2.1  Bisulfite data benchmark and SMRT sequencing\n",
            "\u001b[35mSection_title: 2.2  Prediction of the methylation state from kinetic data\n",
            "\u001b[35mSection_title: 2.3  Handling intermediate or ambiguous methylation states\n",
            "\u001b[35mSection_title: 2.4  Genome-wide methylation pattern of repetitive elements in the human genome\n",
            "\u001b[35mSection_title: 2.5  Analysis of the Tol2 transposable element in medaka\n",
            "\u001b[35mSection_title: 3  DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.1  Scientific\n",
            "\u001b[35mSection_title: 2.3  Consumer Health\n",
            "\u001b[35mSection_title: 2.4  Examination\n",
            "\u001b[35mSection_title: 2.5  Related Surveys\n",
            "\u001b[35mSection_title: 3  BQA APPROACH OVERVIEW\n",
            "\u001b[35mSection_title: 4  CLASSIC BQA\n",
            "\u001b[35mSection_title: 5  INFORMATION RETRIEVAL BQA\n",
            "\u001b[35mSection_title: 5.2  IR BQA Datasets\n",
            "\u001b[35mSection_title: 5.3  IR BQA Methods\n",
            "\u001b[35mSection_title: 5.4  Comments\n",
            "\u001b[35mSection_title: 6  MACHINE READING COMPREHENSION BQA\n",
            "\u001b[35mSection_title: 6.1  MRC BQA Datasets\n",
            "\u001b[35mSection_title: 6.2  MRC BQA Methods\n",
            "\u001b[35mSection_title: 6.3  Comments\n",
            "\u001b[35mSection_title: 7  KNOWLEDGE BASE BQA\n",
            "\u001b[35mSection_title: 7.1  Existing Knowledge Bases\n",
            "\u001b[35mSection_title: 7.3  KB BQA Methods\n",
            "\u001b[35mSection_title: 7.4  Comments\n",
            "\u001b[35mSection_title: 8  QUESTION ENTAILMENT BQA\n",
            "\u001b[35mSection_title: 8.1  QE BQA Methods\n",
            "\u001b[35mSection_title: 8.2  Question-Question Pairs\n",
            "\u001b[35mSection_title: 8.3  Question-Answer Pairs\n",
            "\u001b[35mSection_title: 8.4  Comments\n",
            "\u001b[35mSection_title: 9  CHALLENGES AND FUTURE DIRECTIONS\n",
            "\u001b[35mSection_title: 9.1  Dataset Collection\n",
            "\u001b[35mSection_title: 9.2  Dataset Difficulty\n",
            "\u001b[35mSection_title: 9.3  Biomedical VQA\n",
            "\u001b[35mSection_title: 9.5  Answer Explainability\n",
            "\u001b[35mSection_title: 9.7  Fairness and Bias\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 15  nt of continuous base pairing interactions are sufficient for OrzO regulation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Material and Methods\n",
            "\u001b[35mSection_title: 2.1.  Trimming Modes\n",
            "\u001b[35mSection_title: 1.  ANY:\n",
            "\u001b[35mSection_title: 2.  LEFT:\n",
            "\u001b[35mSection_title: 3.  RIGHT:\n",
            "\u001b[35mSection_title: 2.2.  Quality Clipping and Read Filtering\n",
            "\u001b[35mSection_title: 2.3.  Program Usage\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\u001b[35mSection_title: 3.1.  Benchmark 1 Adapter Removal from miRNA Short Reads\n",
            "\u001b[35mSection_title: 3.2.  Benchmark 2 Trimming and Adapter Removal from Paired-End RNA-seq Data Set\n",
            "\u001b[35mSection_title: 3.3.  Benchmark 3 Barcode Recognition\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 2.1.  Pre-Training Language Processing and Background Knowledge for FV\n",
            "\u001b[35mSection_title: 2.2.  Graph Neural Network for FV\n",
            "\u001b[35mSection_title: 3.  Methodology\n",
            "\u001b[35mSection_title: 3.1.  External Knowledge for Retrieved Evidence\n",
            "\u001b[35mSection_title: 3.1.  External Knowledge for Retrieved Evidence\n",
            "\u001b[35mSection_title: 3.1.1.  Identifying the Knowledge Gaps\n",
            "\u001b[35mSection_title: 3.1.2.  Evidence Relevance\n",
            "\u001b[35mSection_title: 3.1.2.  Evidence Relevance\n",
            "\u001b[35mSection_title: 3.1.3.  Filling the Gap\n",
            "\u001b[35mSection_title: 3.1.3.  Filling the Gap\n",
            "\u001b[35mSection_title: 3.2.  Constructing the Entity Graph\n",
            "\u001b[35mSection_title: 3.3.  Claim Verification with the GNN Reasoning Method\n",
            "\u001b[35mSection_title: 4.  Experiments\n",
            "\u001b[35mSection_title: 4.1.  Experimental Setting\n",
            "\u001b[35mSection_title: 4.1.1.  Dataset\n",
            "\u001b[35mSection_title: 4.1.2.  Baselines\n",
            "\u001b[35mSection_title: 4.1.3.  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.1.4.  Data Processing\n",
            "\u001b[35mSection_title: 4.2.  Performance\n",
            "\u001b[35mSection_title: 4.3.  Further Expeeriments of FV System on Diverse Web Information\n",
            "\u001b[32mlimitation: limited by the size of knowledge bases, some knowledge gaps cannot be filled effectively in practice. we are studying a creative reasoning mechanism to solve this problem. in addition, fact verification is a very open and challenging task. it needs not only the support of linguistic features and background knowledge, but also the support of more complex multi-dimension information, such as social content and spatiotemporal information. for example, claims evolve over time, and what was fake yesterday is true today. \n",
            "\u001b[35mSection_title: 5.  Limitation of the Study\n",
            "\u001b[32mlimitation: limited by the size of knowledge bases, some knowledge gaps cannot be filled effectively in practice. we are studying a creative reasoning mechanism to solve this problem. in addition, fact verification is a very open and challenging task. it needs not only the support of linguistic features and background knowledge, but also the support of more complex multi-dimension information, such as social content and spatiotemporal information. for example, claims evolve over time, and what was fake yesterday is true today.\n",
            "\u001b[35mSection_title: 5.  Limitation of the Study\n",
            "\u001b[32mconclusion: in this study, a novel graph-based reasoning framework was proposed for complex fact verification (fv), which can dynamically supplement useful knowledge in the case of knowledge gaps. the framework retrieves and fills the knowledge gaps between the given claim and evidence to construct the collaborative graph before propagating and aggregating sequential information. experiments have shown that dkar can effectively solve the \"not enough information\" mislabeling problem in the fv task and outperform other baselines. in addition, our approach shows outstanding advantages in a small sample and heterogeneous web text sources. our research first illustrates that dynamic knowledge supplementation plays an important role in complex fv tasks, which contributes to the study of reasoning methods driven by data and knowledge for fact verification. it is expected that our first exploration encourages others to expand upon our work, and to further shed light on the broader and more challenging goal of complex and practical fv tasks with joint data and knowledge.\n",
            "\u001b[35mSection_title: 6.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  Adapting the EXP3 algorithm.\n",
            "\u001b[35mSection_title: 3.3  Adapting the UCB1 algorithm.\n",
            "\u001b[35mSection_title: 4  Experimental setup\n",
            "\u001b[35mSection_title: 5  Findings and analysis\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: recall the desiderata for our algorithm, expressed in the introduction: our algorithm should (1) make no assumptions on the available auxiliary data a-priori, (2) scale well with the number of auxiliary datasets, and (3) add minimal memory and computational overhead.(1) when designing our algorithm, we purposefully formulate the problem as a multi-armed bandit.mab algorithms, in general, make no assumptions on the quality of rewards and, in particular, exp3 even assumes that the auxiliary datasets will play an adversarial role when returning rewards.(2) as previously mentioned, our algorithms have a single-turn computational complexity that is independent of the number of auxiliary datasets.(3) finally, our method adds minimal computational overhead beyond usual training computations.every gradient that we utilize for our reward functions are also used to update the model, adding no additional computations.the only computational overhead is to compute gradient alignment (three vector dot products, two scalar square roots, and two scalar multiplications) or magnitude similarity (four vector dot products, two scalar square roots, three scalar multiplications, and one scalar addition).additionally, our method adds a small amount of memory overhead, used to store gradients between model updates.our rewards consider only the gradient w.r.t the language modelling head and, in practice, require 0.25gb per auxiliary gradient to store, slightly increasing the space complexity above standard fine-tuning.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Results and Discussion\n",
            "\u001b[35mSection_title: 2.1.  Galesong: The Patorani Community\n",
            "\u001b[35mSection_title: 2.2.  Wisdom and Religiosity: Micro and Macrocosm in Flying Fish Catch\n",
            "\u001b[35mSection_title: 2.2.1.  Erang Pa'boyaboyang\n",
            "\u001b[35mSection_title: 2.2.2.  Erang Passimombalang\n",
            "\u001b[35mSection_title: 2.3.  Pakkaja: Eco-Friendly Technology\n",
            "\u001b[35mSection_title: 2.4.  Pakkaja: Eco-Friendly Technology\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Description\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Metrics\n",
            "\u001b[35mSection_title: 3.3  Models\n",
            "\u001b[35mSection_title: 3.3.1  Single-hop models\n",
            "\u001b[35mSection_title: 3.3.2  Multi-hop models\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Overall performance\n",
            "\u001b[35mSection_title: 4.2  Performance for limited resources\n",
            "\u001b[35mSection_title: 4.3  Error Analysis\n",
            "\u001b[32mconclusion: in this work, we provided insights on the performance of state-of-the-art dense retrieval for multihop questions. we showed that rerank+dpr 2 (our hybrid model) outperforms mdr (the state-of-theart multi-hop dense retrieval model) in the low re-source setting, and it is competitive with mdr in the setting where mdr uses considerably more computational resources. finally, we highlighted that fully dense retrieval models get harmed when using limited computational resources. for future work, we plan to build on our insights to improve the performance of multi-hop models by combining the strengths of lexical and dense retrieval. also, we aim to develop less computationally expensive multi-hop retrieval models.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Explainable NLI\n",
            "\u001b[35mSection_title: 2.3  Evaluation and Human Ratings\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Base Model\n",
            "\u001b[35mSection_title: 3.2  Integration of Knowledge Sources\n",
            "\u001b[35mSection_title: 3.2.1  Background Knowledge from COMET\n",
            "\u001b[35mSection_title: 3.2.2  Modified Attention with ConceptNet\n",
            "\u001b[35mSection_title: 3.2.3  All-Text Prediction with GPT-2\n",
            "\u001b[35mSection_title: 3.3  Combined Models\n",
            "\u001b[35mSection_title: 4  Automatic Evaluation\n",
            "\u001b[35mSection_title: 4.1  Performance on e-SNLI\n",
            "\u001b[35mSection_title: 4.2  Stress Test Evaluation\n",
            "\u001b[35mSection_title: 5  Human Evaluation\n",
            "\u001b[35mSection_title: 5.1  Conditions\n",
            "\u001b[35mSection_title: 5.2  Dependent Variables\n",
            "\u001b[35mSection_title: 5.3  Study Design\n",
            "\u001b[35mSection_title: 5.4  Analysis\n",
            "\u001b[32mconclusion: in this paper, we addressed three research questions: whether integrating external knowledge can improve explainability for nli, how effective knowledge implicitly stored in language models is for reasoning, and how humans perceive explanation quality of state-of-the-art natural language inference models. to answer these questions, we proposed different methods of integrating various knowledge sources into deep learning models. we found that fine-tuned language models reach the highest performance on e-snli as well as the highest average accuracy within the nli stress test evaluation. however, their performance can break down on numerical reasoning and negations. in addition to automatic evaluation, we conducted a large-scale human crowdsourcing evaluation and found that high differences in accuracy, bleu or bleurt scores do not reflect in significant differences in human ratings of explanation correctness, commonsense inclusion, grammar or label correctness. this highlights an alarming disconnect between automatic evaluation scores and human ratings, that puts the real-world utility of recent model improvements into question and requires to re-think automatic evaluation across the field of explainable ai.  b study interface figure 4 shows an example of the study interface used to collect human ratings as discussed in section 5. table 4 lists the annotation guidelines used to decide on low/high levels of required external knowledge as discussed in section 5.3. table 5 shows example annotations.   the entailment can be decided by matching identical parts in the premise and the hypothesis. premise: a water scene with a sunset in the background.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Retriever-Reader Models\n",
            "\u001b[35mSection_title: 2.2  Question Generation\n",
            "\u001b[35mSection_title: 2.3  Memory-Augmented Language Models\n",
            "\u001b[35mSection_title: 3  Our Model: QAMAT\n",
            "\u001b[35mSection_title: 3.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Dense Retriever\n",
            "\u001b[35mSection_title: 3.3  Neural Memory Integration\n",
            "\u001b[35mSection_title: 3.4  Neural + Discrete Memory Integration\n",
            "\u001b[35mSection_title: 3.5  Multi-hop Extension\n",
            "\u001b[35mSection_title: 4  Training\n",
            "\u001b[35mSection_title: 4.1  Pre-training Corpus\n",
            "\u001b[35mSection_title: 4.2  End-to-End Training\n",
            "\u001b[35mSection_title: 4.3  Multihop Extension\n",
            "\u001b[35mSection_title: 5  QA Experiments\n",
            "\u001b[35mSection_title: 5.1  Implementation Details\n",
            "\u001b[35mSection_title: 5.2  Datasets\n",
            "\u001b[35mSection_title: 5.3  Baselines\n",
            "\u001b[35mSection_title: 5.4  Single-Hop Results\n",
            "\u001b[35mSection_title: 5.5  Multi-hop Results\n",
            "\u001b[35mSection_title: 5.6  Ablation Studies\n",
            "\u001b[32mconclusion: in this paper, we propose a more accurate and efficient architecture to utilize qa-pairs as representation units of knowledge. our proposed model qamat outperforms repaq significantly, while leveraging our less expensive training procedure. furthermore, we show how a qa-backed model can perform compositional reasoning and address more complex queries. in the future, we hope to further close the gap with state-of-the-art documentbased retrieve-and-read models and extend this approach to a broader set of tasks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Task Definition\n",
            "\u001b[35mSection_title: 4  The FINQA Dataset\n",
            "\u001b[35mSection_title: 4.1  Data Preparation\n",
            "\u001b[35mSection_title: 4.2  Annotation Procedure\n",
            "\u001b[35mSection_title: 4.3  Data Quality Assessment\n",
            "\u001b[35mSection_title: 5  Baseline Systems\n",
            "\u001b[35mSection_title: 5.1  The FinQANet Framework\n",
            "\u001b[35mSection_title: 5.2  Other Baselines\n",
            "\u001b[35mSection_title: 6  Experimental Results\n",
            "\u001b[35mSection_title: 6.1  QA Model Performance\n",
            "\u001b[35mSection_title: 6.2  Performance Breakdown\n",
            "\u001b[35mSection_title: 6.3  Error Analysis\n",
            "\u001b[32mconclusion: this paper introduces finqa, a new expertannotated qa dataset that aims to tackle numerical reasoning over real-world financial data. the questions in finqa pose great challenge for existing models to resolve domain-specific knowledge, as well as to acquire complex numerical reasoning abilities. we propose baseline frameworks and con-duct comprehensive experiments and analysis. the results show that current large pre-trained models still fall far behind the human expert performance. this encourages potential future work on developing pre-training tasks for such realistic, complex application domains. we believe finqa should serve as a valuable resource for the research community.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\u001b[35mSection_title: 8  Ethical Considerations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Methodology\n",
            "\u001b[35mSection_title: 3.  Data\n",
            "\u001b[35mSection_title: 4.  Empirical Analysis and discussions\n",
            "\u001b[32mconclusion: the random forest classification is a popular machine learning algorithm to predict bankruptcy (distress) of firms or business that has been considered in this study and evaluated against tree net algorithm that is also an extensively applied machine learning algorithm not only in bankruptcy prediction but also in information technology and other fields. random forest orders firms according to their propensity to default or to become distressed. the relative superiority of the different approaches has been verified in this study employing exhaustive information set from corporate india. the firms covering varied sectors like manufacturing, services etc. from 2006 to 2015 have been chosen for the purpose. on comparison amongst the two, it is observed that the tree net methodology has been producing better 'in-sample' classification accuracy in contrast to random forest methodology translating in estimation gain of around 3%. furthermore, tree net is showing superior predictive performance in contrast to the random forest consistently. the analysis provides useful insights for possible tools that may be used by management, regulators and researchers alike to forecast and ascertain the financial health of firms.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  QRelScore Metric\n",
            "\u001b[35mSection_title: 2.1  Local Relevance Matching\n",
            "\u001b[35mSection_title: 2.2  Global Relevance Generation\n",
            "\u001b[35mSection_title: 2.3  Reference-augmented QRelScore\n",
            "\u001b[35mSection_title: 3.1  Main Results\n",
            "\u001b[35mSection_title: 3.2  Ablation Analysis\n",
            "\u001b[35mSection_title: 3.3  Evaluating QRelScore Rewards for QG with Reinforcement Learning (RL)\n",
            "\u001b[35mSection_title: 3.4  Robustness Analysis\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: existing evaluation metrics for question generation are still reference-based and ignore the crucial input context of generation, lacking a deep understanding of the relevance between the generated questions and context. to address these issues, we propose qrelscore, which measures the word-and sentence-level relevance through the off-the-shelf language models. extensive experiments demonstrate that qrelscore achieves start-of-the-art correlation with human judgments and makes up for the shortcomings of existing reference-based metrics.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Document Ranking with PDRMM\n",
            "\u001b[35mSection_title: 2.2  PDRMM-based Pipelines for Document and Snippet Ranking\n",
            "\u001b[35mSection_title: 2.3  Joint PDRMM-based Models for Document and Snippet Ranking\n",
            "\u001b[35mSection_title: 2.4  Pipelines and Joint Models Based on\n",
            "\u001b[35mSection_title: 2.5  BM25+BM25 Baseline Pipeline\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Data and Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Experimental Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: our contributions can be summarized as follows:\n",
            "\u001b[35mSection_title: 5  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Preliminary\n",
            "\u001b[35mSection_title: 3.2  TransferNet\n",
            "\u001b[35mSection_title: 3.3  Training\n",
            "\u001b[35mSection_title: 3.4  Additional Modules\n",
            "\u001b[35mSection_title: 3.5  Relation Score Computation\n",
            "\u001b[35mSection_title: 3.5.1  Label Form\n",
            "\u001b[35mSection_title: 3.5.2  Text Form\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Implementations\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Results on Label-Formed Graph\n",
            "\u001b[35mSection_title: 5.2  Results on Text-Formed Graph\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.4  Interpretability\n",
            "\u001b[35mSection_title: 5.5  Model Efficiency\n",
            "\u001b[32mconclusion: we proposed transfernet, an effective and transparent framework for multi-hop qa over knowledge graph or text-formed relation graph.it achieved 100% accuracy on 2-hop and 3-hop questions of label-formed metaqa, nearly solving the dataset.on the more challenging webqsp, compwebq and text-formed metaqa, it also outperforms other state-of-the-art models significantly.qualitative analysis shows the good interpretability of trans-fernet.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Method\n",
            "\u001b[35mSection_title: 3.  Results and discussion\n",
            "\u001b[35mSection_title: 3.1.  The connection commonalities of the combined lessons\n",
            "\u001b[35mSection_title: 3.2.  The mathematical content connections for each four teachers\n",
            "\u001b[32mconclusion: the comparisons of the individual teachers' practices revealed how one particular teacher was noteworthy. according to the interview data collected from all four teachers, notably, only x-t2 mentioned the importance of making the relevant connections in his mathematics teaching. from the interview, x-t2 stressed that it was essential for all his students to see the connections of what reallife examples can bring to the topic of statistics. at the same time, he wanted his students to enjoy and learn mathematics from the enthusiasm he brought into his mathematics teaching. from the observations made, the same level of technique (the technique used to implement making connections problems) made by x-t2 was not detected in the lessons of the other three brunei teachers. then again, why is the least mathematically equipped teacher doing the most innovative teaching? a possible explanation that can be suggested is that, perhaps, since he did not have the relevant mathematical education background, he may have not seen the connection himself. we need to encourage teachers who are mathematical experts, not take for granted the connections that are obvious to them (and possibly, will be obvious to their students), and to include what x-t2 was doing, and to actually, very deliberately try to create the connections within and across their own lessons.\n",
            "\u001b[35mSection_title: 4.  Conclusion\n",
            "\u001b[35mSection_title: 5.  Acknowledgments\n",
            "\u001b[35mSection_title: 6.  References\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Design Desiderata\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Problem Statement\n",
            "\u001b[35mSection_title: 4  Solution Approach\n",
            "\u001b[35mSection_title: 4.1  Achieving Translucency via Loose Coupling\n",
            "\u001b[35mSection_title: 5  Architecture and Training of TAP\n",
            "\u001b[35mSection_title: 5.1  LoGIX\n",
            "\u001b[35mSection_title: 5.2  Answer Predictor\n",
            "\u001b[35mSection_title: 5.3  Pre-training\n",
            "\u001b[35mSection_title: 5.4  Joint Training\n",
            "\u001b[35mSection_title: 5.5  Ensemble\n",
            "\u001b[35mSection_title: 5.6  Evaluation\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[32mconclusion: tap is a novel architecture for the multi-hop reasoning based rcqa task. core to this system is logix, a new approach that effectively addresses the challenges of local context and global interactions present in multi-passage, multihop qa. we have shown that tap advances the state-of-theart on hotpotqa dataset, reaching rank-1 and rank-2 in its ensemble and single model variants at the time of submission. finally, by restricting the input of the ap to logix's selected supporting facts, tap admits interpretability that can be used to debug the model for performance enhancement purposes prior to its deployment.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Document Retrieval\n",
            "\u001b[35mSection_title: 3.1.1  Query Paraphrasing\n",
            "\u001b[35mSection_title: 3.1.2  Search Engine\n",
            "\u001b[35mSection_title: 3.2  Relevant Snippet Selector\n",
            "\u001b[35mSection_title: 3.2.1  QA as Evidence Selector\n",
            "\u001b[35mSection_title: 3.2.2  Answer Re-ranking and Highlight Generation\n",
            "\u001b[35mSection_title: 3.3  Query-focused Multi-document Summarization\n",
            "\u001b[35mSection_title: 3.3.1  Abstractive Summarization\n",
            "\u001b[35mSection_title: 3.3.2  Extractive Summarization\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Question Answering\n",
            "\u001b[35mSection_title: 4.1.1  Quantity Evaluation\n",
            "\u001b[35mSection_title: 4.1.2  Case Study\n",
            "\u001b[35mSection_title: 4.2  Query-focused Multi-document Summarization\n",
            "\u001b[35mSection_title: 4.2.1  Datasets\n",
            "\u001b[35mSection_title: 4.2.2  Model Setting\n",
            "\u001b[35mSection_title: 4.2.3  Results\n",
            "\u001b[32mconclusion: in this paper, we propose a general system, caire-covid, with open-domain qa and query focused multi-document summarization techniques for efficiently mining scientific literature given a query. the system has shown its efficiency on the kaggle cord-19 challenge, which was evaluated by medical researchers, and a series of experimental results also proved the effectiveness of our proposed methods and the competency of each module. the system is also easy to be generalized to general domain-agnostic literature information mining, especially for possible future pandemics. we have launched our website 2 for real-time interactions and released our code 3 for broader use.  \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.1  Background\n",
            "\u001b[35mSection_title: 1.2  Question-answer explanations as a KB\n",
            "\u001b[35mSection_title: 1.3  Discussion: QEDB's versus open IE\n",
            "\u001b[35mSection_title: 2  Methods\n",
            "\u001b[35mSection_title: 2.1  Question generation and filtering\n",
            "\u001b[35mSection_title: 2.2  Question\n",
            "\u001b[35mSection_title: 2.3  Question-answering using the QEDB\n",
            "\u001b[35mSection_title: 2.4  Question-answering using a KB\n",
            "\u001b[35mSection_title: 3  Experiments and Observations\n",
            "\u001b[35mSection_title: 3.1  What do the atomic elements of the QEDB look like?\n",
            "\u001b[35mSection_title: 3.2  Can elements of the QEDB be composed?\n",
            "\u001b[35mSection_title: 3.3  How useful are QA pairs compared to KB triples?\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 4.1  Extracting and querying structured KBs\n",
            "\u001b[35mSection_title: 4.2  Open IE\n",
            "\u001b[35mSection_title: 4.3  RePAQ, QAMAT, and other uses of QA data\n",
            "\u001b[35mSection_title: 4.4  Phrase-indexed QA and virtual KBs\n",
            "\u001b[35mSection_title: 4.5  Explainable QA\n",
            "\u001b[35mSection_title: 4.6  Automated question generation\n",
            "\u001b[35mSection_title: 4.7  Question-based document annotation\n",
            "\u001b[32mconclusion: symbolic kbs organize information into small modular components (e.g., entities, kg triples, wikidata statements) that can be combined compositionally to answer complex queries. while many recent papers have focused on tasks like open qa, where questions are answered from text without using a kb, broad-coverage symbolic kbs continue to be widely used in practice, and, despite recent progress in methods for \"multi-hop\" qa, are still the only computationally efficient way of answering questions that combine information for multiple documents. however, the broad-coverage kbs that are currently in wide use are largely collections of information that easily collected and integrated, and need not reflect the actual infor-mation needs of users. in this position paper, we advocate for a new approach to constructing kbs, and in particular, an approach to collecting modular, compositionally-combinable knowledge components from text, driven by a sample of user's questions and answers.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Graph Neural Networks\n",
            "\u001b[35mSection_title: 2.2  Interpretability of GNNs\n",
            "\u001b[35mSection_title: 3  GNNs as Embedded Graphical Models\n",
            "\u001b[35mSection_title: 4  A Unified Framework\n",
            "\u001b[35mSection_title: 4.1  Interpreting GCN and SGC\n",
            "\u001b[35mSection_title: 4.2  Interpreting GAT\n",
            "\u001b[35mSection_title: 4.3  Interpreting APPNP and GCNII\n",
            "\u001b[35mSection_title: 4.4  Interpreting JKNet and DGCN\n",
            "\u001b[35mSection_title: 4.5  Discussion on Non-linearity\n",
            "\u001b[35mSection_title: 5  On Designing Deep GNNs\n",
            "\u001b[35mSection_title: 5.1  Performance Degradation Problem\n",
            "\u001b[35mSection_title: 5.2  Coupling Graph Neural Network\n",
            "\u001b[35mSection_title: 6  Experiment\n",
            "\u001b[35mSection_title: 6.1  Citation Networks Dataset and Baselines\n",
            "\u001b[35mSection_title: 6.2  NLP Tasks\n",
            "\u001b[32mconclusion: in this paper, we developed a unified theoretical framework to understand the gnn baselines as well as various deep  gnn models using a graphical model representation. specifically, we obtained an iterative solution of variational inference on markov random fields, and the propagation operation of gnns can be represented as approximate forms of it. we also proposed a theoretically motivated and powerful gnn which performs well on both shallow and deep network layers. an interesting direction for future work is to establish the connection between the approximate sampling methods, and the graph neural network to pursue a faster and more powerful sample-based gnn hamilton et al., 2017;hasanzadeh et al., 2020]. to complete the picture, understanding and improving the general gnn with the help of other variational methods would also be interesting.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 2.1.  Healthcare Standards\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 2.1.  Healthcare Standards\n",
            "\u001b[35mSection_title: 2.2.  Emerging Technologies Not Only for Healthcare Applications\n",
            "\u001b[35mSection_title: 3.  Communication Protocols in Healthcare\n",
            "\u001b[35mSection_title: 3.1.  The DASTA Communication Protocol\n",
            "\u001b[35mSection_title: 3.2.  The HL7 Communication Protocol\n",
            "\u001b[35mSection_title: 3.2.1.  Overview of HL7\n",
            "\u001b[35mSection_title: 3.2.2.  The Usage of HL7 in the Case of Automated Drug Management System Swisslog (PillPick and BoxPicker)\n",
            "\u001b[35mSection_title: 3.2.2.  The Usage of HL7 in the Case of Automated Drug Management System Swisslog (PillPick and BoxPicker)\n",
            "\u001b[35mSection_title: 3.3.  Data Exchange in HL7\n",
            "\u001b[35mSection_title: 3.3.  Data Exchange in HL7\n",
            "\u001b[35mSection_title: 3.4.  Prescription Medication Requirement for the Patient (Standard)\n",
            "\u001b[35mSection_title: 3.4.  Prescription Medication Requirement for the Patient (Standard)\n",
            "\u001b[35mSection_title: 3.5.  Summary of Healthcare Communication Standards\n",
            "\u001b[35mSection_title: 4.  Transfer of Patient Medication Messages\n",
            "\u001b[35mSection_title: 4.1.  Medication Message, Its Contents, and Significance\n",
            "\u001b[35mSection_title: 4.1.1.  How a Message Is Generated in the DASTA Standard and What It Tells Us\n",
            "\u001b[35mSection_title: 4.1.2.  Why Convert Messages to the HL7 Standard?\n",
            "\u001b[35mSection_title: 4.1.3.  What Information Is Transferred between Reports and What Information is Drawn from the Internal Database?\n",
            "\u001b[35mSection_title: 5.  Implementation Options\n",
            "\u001b[35mSection_title: 5.1.  Option 1-HL7 Module for HIS/PIS\n",
            "\u001b[35mSection_title: 5.2.  Option 2-Communication through Integration Bridge\n",
            "\u001b[35mSection_title: 5.2.  Option 2-Communication through Integration Bridge\n",
            "\u001b[35mSection_title: 5.3.  Option 3-Communication via Data Bridge\n",
            "\u001b[35mSection_title: 5.3.  Option 3-Communication via Data Bridge\n",
            "\u001b[35mSection_title: 5.4.  Option 4-Integration Bus\n",
            "\u001b[35mSection_title: 5.4.  Option 4-Integration Bus\n",
            "\u001b[35mSection_title: 6.  The Interface between DASTA and HL7 (IDH)\n",
            "\u001b[35mSection_title: 6.1.  Basic Description of IDH\n",
            "\u001b[35mSection_title: 6.  The Interface between DASTA and HL7 (IDH)\n",
            "\u001b[35mSection_title: 6.1.  Basic Description of IDH\n",
            "\u001b[35mSection_title: 6.2.  Description of Interface Functionality\n",
            "\u001b[35mSection_title: 6.2.1.  Patient Information\n",
            "\u001b[35mSection_title: 6.2.2.  Form for Entering New Medication\n",
            "\u001b[35mSection_title: 6.2.3.  Preview of the Patient's Current Medications\n",
            "\u001b[35mSection_title: 6.2.4.  Selection of Day for the Generation of Medication Message\n",
            "\u001b[35mSection_title: 6.2.5.  Visualization of the Message Generated from the Input Data in the DASTA Standard\n",
            "\u001b[35mSection_title: 6.2.6.  An HL7 Message Generated from a DASTA Message\n",
            "\u001b[35mSection_title: 6.2.6.  An HL7 Message Generated from a DASTA Message\n",
            "\u001b[35mSection_title: 7.  Tests Performed and Results\n",
            "\u001b[35mSection_title: 7.1.  Patient 1\n",
            "\u001b[35mSection_title: 7.  Tests Performed and Results\n",
            "\u001b[35mSection_title: 7.1.  Patient 1\n",
            "\u001b[35mSection_title: 7.2.  Patient 2\n",
            "\u001b[35mSection_title: 7.2.  Patient 2\n",
            "\u001b[35mSection_title: 7.3.  Benefits of the Innovated Process\n",
            "\u001b[35mSection_title: 7.3.  Benefits of the Innovated Process\n",
            "\u001b[32mconclusion: the paper described the interface created between dasta and hl7 standards using a visual simulation presentable via a web interface that very simply simulates the process of a doctor's prescribing a drug in the ward and its automated preparation in a hospital pharmacy. the newly-created interface between dasta and hl7 (which has a working name abbreviated as idh) allows the selection of medication in a very simple simulation of the registration of a patient's medical record and treatment plan in his, a statement of the data sentence generated in the dasta standard, a statement of the data sentence in the hl7 standard from the dasta message, and showing the patient's prepared medication in a simple graphical output, simulating a robotically prepared \"circuit\" of the patient's personalized drug therapy with accompanying information about the patient and his or her medications for the next 24 h.\n",
            "\u001b[35mSection_title: 8.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Explainable Analogical Reasoning\n",
            "\u001b[35mSection_title: 3.1  Analogical Reasoning: A Psychological Perspective\n",
            "\u001b[35mSection_title: 3.2  Explanations for Analogical Reasoning\n",
            "\u001b[35mSection_title: 4  The E-KAR Benchmark\n",
            "\u001b[35mSection_title: 4.1  Dataset Collection\n",
            "\u001b[35mSection_title: 4.2  Manual Annotation of Explanations\n",
            "\u001b[35mSection_title: 4.3  Bilingual E-KAR: English and Chinese\n",
            "\u001b[35mSection_title: 4.4  Shared Tasks in E-KAR\n",
            "\u001b[35mSection_title: 5  Methods\n",
            "\u001b[35mSection_title: 5.1  Baselines for Analogical QA\n",
            "\u001b[35mSection_title: 5.2  Baselines for Explanation Generation\n",
            "\u001b[35mSection_title: 6  Results and Analysis\n",
            "\u001b[35mSection_title: 6.2  Can models rationalize analogical thinking?\n",
            "\u001b[32mconclusion: in this work, we propose a first-of-its-kind benchmark e-kar (in both chinese and english) for explainable analogical reasoning, which sets a concrete playground and evaluation benchmark to boost the development of human-like analogical reasoning algorithms. the e-kar benchmark is featured by its rich coverage in knowledge and welldesigned free-text explanations to rationalize the analogical reasoning process. preliminary experiments show that this benchmark provides a rather difficult challenge for prevailing language models.\n",
            "\u001b[35mSection_title: 7  Conclusion and Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Definition\n",
            "\u001b[35mSection_title: 3  The FAIRR Method\n",
            "\u001b[35mSection_title: 3.1  Approach Overview\n",
            "\u001b[35mSection_title: 3.2  FAIRR Modules\n",
            "\u001b[35mSection_title: 3.3  Training and Inference\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Experiment Results\n",
            "\u001b[35mSection_title: 5.1  Performance on Same Depth Reasoning\n",
            "\u001b[35mSection_title: 5.2  Robustness to Perturbed Theories\n",
            "\u001b[35mSection_title: 5.3  Study on Inference Efficiency\n",
            "\u001b[35mSection_title: 5.4  Error Analysis\n",
            "\u001b[35mSection_title: 5.5  ProofWriter Input Ablation\n",
            "\u001b[32mconclusion: in this paper, we proposed fairr, a faithful and robust deductive reasoning model based on three modular components: rule selection, fact selection, and knowledge composition. fairr ensures causality from proof generation to entailment prediction by design. we established the effectiveness of our approach through experiments on testing robustness to language variations and demonstrating the interpretability of the errors made by our model. we also show that fairr is faster and more precise at deductive reasoning than prior baselines.  \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 2.  The\n",
            "\u001b[35mSection_title: 3.1  Contextual Encoding\n",
            "\u001b[35mSection_title: 3.2  Reasoning\n",
            "\u001b[35mSection_title: 3.2.1  Prediction\n",
            "\u001b[35mSection_title: 3.2.2  Comparison\n",
            "\u001b[35mSection_title: 3.2.3  Deduction\n",
            "\u001b[35mSection_title: 3.2.4  Reasoning Type Classification\n",
            "\u001b[35mSection_title: 3.3  Answer Prediction\n",
            "\u001b[35mSection_title: 3.4  Model Training\n",
            "\u001b[35mSection_title: 4  Experiment Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 5  Experimental Results\n",
            "\u001b[35mSection_title: 5.1  Question Answering Performance\n",
            "\u001b[35mSection_title: 5.2  Reasoning Component Performance\n",
            "\u001b[35mSection_title: 5.3  Human Evaluation\n",
            "\u001b[35mSection_title: 5.4  Error Analysis\n",
            "\u001b[32mconclusion: in this paper, we aimed to solve the qualitative reasoning task in an interpretable manner. inspired by human cognition, we first summarized the questions into two categories, prediction and comparison. then an end-to-end trained reasoning component that contains two reasoning chains was designed. both reasoning chains contained multiple neural modules that provide transparent intermediate predictions for the understanding and reasoning process. the experimental results showed the effectiveness of our approach, and the analysis of each module and case study demonstrated the superior interpretability compared with the \"blackbox\" model. moreover, we found that some questions could be solved by both reasoning chains, thus increasing the default tolerance and generalization capability. furthermore, a human evaluation was conducted to validate the function of the synthetic text and provide an additional explanation for the superior performance achieved by our method. however, the error analysis showed the inadequacy under complicated scenarios. therefore, our future work will focus on applying interpretable reasoning on complex reasoning tasks. the annotated data and models are shared publicly 3 . \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Knowledge-guided Pre-training\n",
            "\u001b[35mSection_title: 3.2  Generative Pre-training\n",
            "\u001b[35mSection_title: 3.3  Discriminative Pre-training\n",
            "\u001b[35mSection_title: 3.4  Learning Schemes\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Pre-training Setup\n",
            "\u001b[35mSection_title: 4.2  Cloze-style Question Answering\n",
            "\u001b[35mSection_title: 4.3  Machine Reading for Question Answering\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Pre-training Schemes\n",
            "\u001b[35mSection_title: 5.2  Continue Pre-training\n",
            "\u001b[35mSection_title: 5.3  Effects of Generator and Discriminator\n",
            "\u001b[32mconclusion: we have proposed a pre-training method by cooperatively modeling the generative and discriminative knowledge injecting approaches. our model can be easily extended to larger pre-training corpus and does not introduce any modifications for downstream tasks during finetuning. experiments show our model consistently outperforms all base models on a variety of question answering datasets, demonstrating that our kgplm is a preferred choice for the knowledge intensive nlp tasks.  \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Standard Adversarial Training\n",
            "\u001b[35mSection_title: 3  Adversarial Training for MRC\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Overall Results\n",
            "\u001b[35mSection_title: 5.2  Comparison\n",
            "\u001b[35mSection_title: 5.3  Robustness on Adversarial Datasets\n",
            "\u001b[32mconclusion: we have applied adversarial training on a wide range of mrc tasks, including span-based extractive rc and multiple-choice rc. especially, we have proposed a novel adversarial training method pqat, which uses virtual p/q-embedding matrices to generate global and role-aware perturbations that consider the characteristics of mrc tasks. our experiments demonstrate that adversarial training improves the mrc model performance universally and consistently, even over the strong pre-trained model baseline. the pqat method further improves the model performance over the standard at on both normal datasets and adversarial datasets.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Context Filter\n",
            "\u001b[35mSection_title: 2.2  Intermediate Hop Reasoning\n",
            "\u001b[35mSection_title: 2.3  Final Hop Inference\n",
            "\u001b[35mSection_title: 2.4  Optimization & Inference\n",
            "\u001b[35mSection_title: 2.5  Exposure Bias Mitigation\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experimental Dataset\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Overall Performance\n",
            "\u001b[35mSection_title: 3.4  Further Analysis\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we study the task of multi-hop question answering and propose to stepwise locate the single-hop supporting sentences and generate more fact-grounded single-hop questions for better interpretable multi-hop reasoning. we present a stepwise reasoning framework to incorporate both single-hop supporting sentence identification and the corresponding single-hop question generation for each intermediate step until inferring a final result. it employs a pre-trained simple question generator and takes the identified single-hop supporting sentences as base to generate the single-hop question, which obviates the necessity of constructed supervision and helps generate more fact-based single-hop questions. it utilizes a unified reader to jointly learn both intermediate hop reasoning and final hop inference for better fault tolerance. experimental results validate the general effectiveness and interpretability of our stepreasoner.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Learning and Interaction Scenario\n",
            "\u001b[35mSection_title: 3  Simulation Setup\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Online Learning\n",
            "\u001b[35mSection_title: 6  Offline Learning\n",
            "\u001b[35mSection_title: 7  Domain Adaptation\n",
            "\u001b[35mSection_title: 8  Related Work\n",
            "\u001b[32mconclusion: we present a simulation study of learning from user feedback for extractive qa. we formulate the problem as contextual bandit learning. we conduct experiments to show the effectiveness of such feedback, the robustness to feedback noise, the impact of initial model performance, the trade-offs between online and offline learning, and the potential for domain adaptation. our study design emphasizes the potential for reducing annotation costs by annotating few examples or by utilizing existing datasets for new domains.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Formulation\n",
            "\u001b[35mSection_title: 2.1  The CLIF Problem\n",
            "\u001b[35mSection_title: 2.2  Tasks and Data Streams\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Base NLP Models\n",
            "\u001b[35mSection_title: 3.2  Baseline Learning Algorithms\n",
            "\u001b[35mSection_title: 3.3  Our Extension: Bi-level Hypernetworks for Adapters with Regularization\n",
            "\u001b[35mSection_title: 4  Results and Analysis\n",
            "\u001b[35mSection_title: 4.1  Examining Knowledge Accumulation\n",
            "\u001b[35mSection_title: 4.2  Effect of Continual Learning Algorithms\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we present the continual learning of few-shot learners (clif) challenge to simulate the scenario where a learner continually accumulate (generalizable) knowledge over a sequence of nlp tasks, while retaining its performance on the seen tasks. we propose evaluation protocols to study the performance of existing continual learning algorithm, and present our method bihnet-reg. we demonstrate the potentials of building a nlp system that, through continual training, can perform more tasks and also become more efficient in mastering new tasks. future works include extending our work to task agnostic scenarios where the distribution of data may shift continuously and studying algorithms for continual refinement of large-scale pre-trained models with emerging unlabeled data.   \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Materials\n",
            "\u001b[35mSection_title: 2.2  Potentiometric measurements\n",
            "\u001b[35mSection_title: 2.3  Spectral measurements 2.3.1 NMR spectroscopy\n",
            "\u001b[35mSection_title: 2.3.2  IR spectroscopy\n",
            "\u001b[35mSection_title: 2.3.3  Vis spectroscopy\n",
            "\u001b[35mSection_title: 2.3.4  EPR spectroscopy\n",
            "\u001b[35mSection_title: 3  Results and discussion\n",
            "\u001b[35mSection_title: 3.1  Non-covalent interactions in the Glu/3,3tri and Glu/Spd systems\n",
            "\u001b[35mSection_title: 3.1.1  Glu/3,3-tri system\n",
            "\u001b[35mSection_title: 3.2  Ternary Cu(II)/Glu/triamine systems\n",
            "\u001b[35mSection_title: 3.2.1  Cu(II)/Glu/3,3-tri system\n",
            "\u001b[35mSection_title: 3.2.2.  Cu(II)/Glu/Spd system\n",
            "\u001b[32mconclusion: the main centres of interactions in the adducts forming in the systems of glu-triamine are the oxygen atoms from carboxyl groups and the nitrogen atom from the amine group of glu and the nitrogen atoms from the amine groups of triamine. moreover, these centres are also potential sites of metal ions coordination. at lower ph in the metal-free systems, the terminal amine groups of the two triamines studied, the oxygen atoms from -c (5) ooand the amine group from glu are not engaged in the weak noncovalent interactions between ligands, in contrast to the situation in the system asp-triamine [43]). thus, not only the length of the polyamine carbon chain [42][43][44] but also the length of the amino acid carbon chain influences the interactions between the bioligands. in the adducts forming above ph 7, all available active centres of the ligands are involved in the interaction and the inversion effect is observed, similarly as in the system of asp-triamine [43]). the amine groups of the polyamine could act either as positive or negative centres of interaction, depending on ph (the character of interaction depends on the degree of protonation). in the system cu(ii)-glu-triamine the following species are formed: mlhl', mll' and mll'oh (where l = glu, l' = triamine). in contrast to the situation in the systems with a shorter chain asp [42,43], no formation of ml … l' type molecular complexes was observed, with polyamine in the outer coordination sphere engaged in noncovalent interactions with the anchoring binary complex ml. in the monoprotonated species, the coordination of 3,3-tri and spd to copper(ii) ions is of the same character, but in disparity to the 3,3-tri species, in the complex cu(glu)h(spd) the oxygen atoms from -c (5) ooof the amino acid are not engaged in the metallation.\n",
            "\u001b[35mSection_title: 4  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background: causal inference\n",
            "\u001b[35mSection_title: 3  Theory and methods\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Calibrated estimation\n",
            "\u001b[35mSection_title: 3.3  Regularized calibrated estimation\n",
            "\u001b[35mSection_title: 3.3.1  Computation\n",
            "\u001b[35mSection_title: 3.3.2  High-dimensional analysis\n",
            "\u001b[35mSection_title: 4  Simulation study\n",
            "\u001b[35mSection_title: 5  Application to a medical study\n",
            "\u001b[35mSection_title: 6  Additional discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Plug-and-Play Modules for PTMs\n",
            "\u001b[35mSection_title: 2.2  Language Representation Learning\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Plug-and-Play Document Modules\n",
            "\u001b[35mSection_title: 3.2  Overall Framework\n",
            "\u001b[35mSection_title: 3.3  Plugin Learning\n",
            "\u001b[35mSection_title: 3.4  Plugging Strategies\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Evaluation Settings\n",
            "\u001b[35mSection_title: 4.2  Training Details\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Plugging during Tuning\n",
            "\u001b[35mSection_title: 4.5  Plugging after Tuning\n",
            "\u001b[35mSection_title: 4.6  Ablation Study\n",
            "\u001b[35mSection_title: 4.7  Transferability Analysis\n",
            "\u001b[32mconclusion: in this paper, we explore a new paradigm, which aims to represent documents as pluggable modules for ptms. in this setting, we can get rid of encoding the same document multiple times for different tasks. the extensive experiments prove that our proposed plugd can significantly reduce the computational cost and effectively inject document knowledge into ptms to improve performance. in the future, we will explore more effective plugin learning tasks and further attempt to represent knowledge graphs, and figures as plugins to provide knowledge for ptms.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Multihop Reasoning Desiderata\n",
            "\u001b[35mSection_title: 4  Connected Reasoning via Composition\n",
            "\u001b[35mSection_title: 4.1  Multihop via Single-Hop Composition\n",
            "\u001b[35mSection_title: 4.2  Ensuring Connected Reasoning\n",
            "\u001b[35mSection_title: 4.3  Reading Comprehension Setting\n",
            "\u001b[35mSection_title: 5  Dataset Construction Pipeline\n",
            "\u001b[35mSection_title: 6  Dataset Quality Assessment\n",
            "\u001b[35mSection_title: 7.1  Datasets\n",
            "\u001b[35mSection_title: 7.2  Models\n",
            "\u001b[35mSection_title: 7.2.1  Multihop Models\n",
            "\u001b[35mSection_title: 7.2.2  Artifact-based Models\n",
            "\u001b[35mSection_title: 7.2.3  Cheatability Score\n",
            "\u001b[35mSection_title: 7.3  Human Performance\n",
            "\u001b[35mSection_title: 8  Empirical Findings\n",
            "\u001b[35mSection_title: 8.1  MuSiQue is a Challenging Dataset\n",
            "\u001b[35mSection_title: 8.2  Dataset Construction Steps are Valuable\n",
            "\u001b[35mSection_title: 8.3  Potential Avenues for Improvement\n",
            "\u001b[32mconclusion: constructing multihop datasets is a tricky process. it can introduce shortcuts and artifacts that models can exploit to circumvent the need for multihop reasoning. a bottom-up process of constructing multihop from single-hop questions allows systematic exploration of a large space of multihop candidates and greater control over which questions we compose. we showed how to use such a carefully controlled process to create a challenging dataset that, by design, requires connected reasoning by reducing potential reasoning shortcuts, minimizing train-test leakage, and including harder distractor contexts. empirical results show that -ans has a substantially higher human-model gap and is significantly less cheatable via disconnected reasoning than previous datasets. the dataset also comes with unanswerable questions, and question decompositions which we hope spurs further work in developing models that get right answers for the right reasons.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Dataset Collection\n",
            "\u001b[35mSection_title: 2.2  Human Annotation\n",
            "\u001b[35mSection_title: 2.3  Dataset Statistics and Analysis\n",
            "\u001b[35mSection_title: 3  DETLLM: Diverse Retrieval Tool\n",
            "\u001b[35mSection_title: 3.1  Question Decomposition and Planning\n",
            "\u001b[35mSection_title: 3.2  Multi-Source Knowledge Retrieval\n",
            "\u001b[35mSection_title: 3.3  Multi-Source Knowledge Ranking\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Discussion\n",
            "\u001b[35mSection_title: 5.1  Assessing the Reasoning Ability of LLMs\n",
            "\u001b[35mSection_title: 5.2  Retrieval-Augmented LLMs\n",
            "\u001b[32mconclusion: we introduce the divknowqa, designed to evaluate the proficiency of question-answering systems, especially those enhanced by retrieval tools, in addressing knowledge-intensive questions with a strong emphasis on multi-hop multi-source retrieval.this dataset is constructed through automated data generation and subsequent human verification, minimizing manual effort.our evaluation encompasses both standard llms and llms augmented with retrieval tools.notably, we identify that this task presents a new challenge for state-of-the-art models due to the demand for structured knowledge retrieval and the inherent lack of prior knowledge in this context.to tackle this challenge, we propose the detllm, which incorporates diverse retrieval tools including innovative symbolic query generation for retrieving information from the structured knowledge source.in the future, we are keen on enhancing llms' capabilities in understanding and generating symbolic language, as well as exploring methods to improve performance on knowledge-intensive and complex question-answering tasks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Retrieval Augmentation\n",
            "\u001b[35mSection_title: 2.2  Zero-shot Dense Retrieval\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Mixture-of-Memory Augmentation\n",
            "\u001b[35mSection_title: 3.2  Joint Learning in MoMA and Inference with Plug In Memory\n",
            "\u001b[35mSection_title: 4  Experimental Methodologies\n",
            "\u001b[35mSection_title: 5  Evaluation Results\n",
            "\u001b[35mSection_title: 5.1  Zero-Shot Retrieval Accuracy and Efficiency\n",
            "\u001b[35mSection_title: 5.2  Performance with Different Memories\n",
            "\u001b[35mSection_title: 5.3  Effect of Memory Mixture Learning\n",
            "\u001b[35mSection_title: 5.4  Generalization of Plug-In Memory\n",
            "\u001b[35mSection_title: 5.5  Case Studies\n",
            "\u001b[32mconclusion: in this paper we propose a new plug-in mixture-ofmemory mechanism for the retrieval augmented language models to improve their zero-shot ability on the dense retrieval task. to learn the memory mixture we develop a new joint learning approach that trains the augmentation component using the positive signals from the end task, the language model's attention scores, and  wood jr. (october 10, december 10, 1978) was an american filmmaker, actor, writer, producer, and director. hard negatives retrieved from the mixture of augmentation corpora. this leads to our final model moma (t5-ance) and moma (coco) that achieve strong zero-shot accuracy on 18 retrieval tasks included in beir. our analysis shows the importance of augmenting with diverse memory sources and in-domain information for robust generalization. we also share our observations and insights on how the model learns to leverage the augmentation information from multiple corpora during training and testing. we hope our findings and illustrations can inspire more future research in better augmenting language models, to provide other alternatives to achieve generalization ability beyond solely relying on model scale.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  DEVELOPMENT OF OPENQA\n",
            "\u001b[35mSection_title: 2.1  Origin of OpenQA\n",
            "\u001b[35mSection_title: 2.2  Traditional Architecture of OpenQA\n",
            "\u001b[35mSection_title: 2.2.1  Question Analysis\n",
            "\u001b[35mSection_title: 2.2.2  Document Retrieval\n",
            "\u001b[35mSection_title: 2.2.3  Answer Extraction\n",
            "\u001b[35mSection_title: 2.3  Application of Deep Neural Networks in OpenQA\n",
            "\u001b[35mSection_title: 3  MODERN OPENQA: RETRIEVING AND READING\n",
            "\u001b[35mSection_title: 3.1  Retriever\n",
            "\u001b[35mSection_title: 3.1.1  Sparse Retriever\n",
            "\u001b[35mSection_title: 3.1.2  Dense Retriever\n",
            "\u001b[35mSection_title: 3.1.3  Iterative Retriever\n",
            "\u001b[35mSection_title: 3.2  Document Post-processing\n",
            "\u001b[35mSection_title: 3.3  Reader\n",
            "\u001b[35mSection_title: 3.3.1  Extractive Reader\n",
            "\u001b[35mSection_title: 3.3.2  Generative Reader\n",
            "\u001b[35mSection_title: 3.4  Answer Post-processing\n",
            "\u001b[35mSection_title: 3.5  End-to-end Methods\n",
            "\u001b[35mSection_title: 3.5.1  Retriever-Reader\n",
            "\u001b[35mSection_title: 3.5.2  Retriever-only\n",
            "\u001b[35mSection_title: 3.5.3  Retriever-free\n",
            "\u001b[35mSection_title: 4  CHALLENGES AND BENCHMARKS\n",
            "\u001b[35mSection_title: 4.1  Challenges to OpenQA\n",
            "\u001b[35mSection_title: 4.1.1  Distant Supervision\n",
            "\u001b[35mSection_title: 4.1.2  Retrieval Effectiveness and Efficiency\n",
            "\u001b[35mSection_title: 4.1.3  Knowledge Incorporation\n",
            "\u001b[35mSection_title: 4.1.4  Conversational OpenQA\n",
            "\u001b[35mSection_title: 4.2  Benchmarks\n",
            "\u001b[32mconclusion: in this work we presented a comprehensive survey on the latest progress of open-domain qa (openqa) systems. in particular, we first reviewed the development of openqa and illustrated a \"retriever-reader\" architecture. moreover, we reviewed a variety of existing openqa systems as well as their different approaches. finally, we discussed some salient challenges towards openqa followed by a summary of various qa benchmarks, hoping to reveal the research gaps so as to push further progress in this field. based on  our review of prior research, we claim that openqa would continue to be a research hot-spot. in particular, single-step and multi-step neural retrievers will attract increasing attention due to the demand for more accurate retrieval of related documents. also, more end-to-end openqa systems will be developed with the advancement of deep learning techniques. knowledge enhanced openqa is very promising not only because it is helpful to generating the answer but also because it serves as the source for interpreting the obtained answer. however, how to represent and make full use of the knowledge for openqa still needs more research efforts. furthermore, to equip openqa with a dialogue-like interface that enables interaction between human users and the system for information exchange is expected to attract increasing attention, which well aligns with real world application scenarios.  \n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Background\n",
            "\u001b[35mSection_title: 3.1  The FM-Index\n",
            "\u001b[35mSection_title: 5  Experimental Setting\n",
            "\u001b[35mSection_title: 5.1  Data\n",
            "\u001b[35mSection_title: 5.2  SEAL configuration\n",
            "\u001b[35mSection_title: 5.3  Retriever Baselines\n",
            "\u001b[35mSection_title: 5.4  Reader\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\u001b[32mconclusion: in this paper we present seal, a novel retrieval system that combines an autoregressive language model with a compressed full-text substring index. such combination allows to constraint the generation of existing ngrams in a corpus and to jointly retrieve all the documents containing them. empirically, we show an improvement of more than 10 points in average passage-level r-precision on kilt, and establish new state-of-the-art downstream performance on 4 out 7 datasets when paired with a reader model. while our results show that seal could already compete with more established retrieval systems, we believe there is potential in exploring the use of existing (or yet to come) larger autoregressive models.   \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Multi-dataset Fine-tuning\n",
            "\u001b[35mSection_title: 3.3  MADE\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  In-domain Performance\n",
            "\u001b[35mSection_title: 4.3  Zero-shot Generalization\n",
            "\u001b[35mSection_title: 4.4  Transfer Learning\n",
            "\u001b[32mconclusion: made combines the benefits of single-and multidataset training, resulting in better in-domain accuracy and transfer performance than either multidataset models or ensembles of single-dataset models, especially in low resource settings. for future work we plan to explore ensembling methods for better zero-shot prediction and interpolating made weights for better transfer learning. \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  LONGBENCH: TASKS AND CONSTRUCTION\n",
            "\u001b[35mSection_title: 3.1  PROBLEM DEFINITION\n",
            "\u001b[35mSection_title: 3.2  DATASET CONSTRUCTION\n",
            "\u001b[35mSection_title: 3.2.1  DATA COLLECTION AND ANNOTATION\n",
            "\u001b[35mSection_title: 3.2.2  DATA EXTRACTION\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.2  IMPACT OF CONTEXT COMPRESSION TECHNIQUES\n",
            "\u001b[32mconclusion: in this paper, we introduce longbench, a multi-task bilingual benchmark tailored for gauging long context understanding abilities of llms. longbench covers six key categories and a total of 21 tasks, with data lengths extending from thousands of tokens up to tens of thousands of tokens. we also develop longbench-e which features a more evenly data length distribution. we conduct extensive experiments on longbench and longbench-e, yielding insightful conclusions about the capabilities of current llms on long context understanding. moreover, our analysis suggests that longbench and longbench-e serve as ideal testbeds for future research in long context modeling. table 6 lists the instantiation of (i, c, a) for each dataset in longbench. table 7 reports the number of data on each task that falls in the length range of 0-4k, 4k-8k, and 8k+ in longbench-e.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  RALLE Usage\n",
            "\u001b[35mSection_title: 2.1  Document Embedding and Indexing\n",
            "\u001b[35mSection_title: 2.2  Chain Construction\n",
            "\u001b[35mSection_title: 2.3  Prompt Engineering\n",
            "\u001b[35mSection_title: 2.4  Experiment Tracking\n",
            "\u001b[35mSection_title: 2.5  Chat AI\n",
            "\u001b[35mSection_title: 3  Experimental Settings\n",
            "\u001b[35mSection_title: 3.1  Tasks and Datasets\n",
            "\u001b[35mSection_title: 3.2  Models\n",
            "\u001b[35mSection_title: 3.2.1  LLMs\n",
            "\u001b[35mSection_title: 3.2.2  Retrievers\n",
            "\u001b[35mSection_title: 3.3  Prompts\n",
            "\u001b[35mSection_title: 4  KILT Benchmark Results\n",
            "\u001b[35mSection_title: 4.1  Baseline\n",
            "\u001b[35mSection_title: 4.2  Downstream Performance\n",
            "\u001b[35mSection_title: 4.3  Retrieval Performance\n",
            "\u001b[35mSection_title: 4.4  Speed Analysis\n",
            "\u001b[32mconclusion: this paper introduces ralle, an accessible framework for developing and evaluating r-llms.we also report evaluation results of several r-llms built using open-source retrievers and llms on knowledge-intensive tasks.overall, ralle offers a significant advancement in retrieval-augmented generation research, enabling efficient development, evaluation, and improvement of r-llms.we hope that ralle will contribute to the development of best practices for r-llms.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Models and Methods\n",
            "\u001b[35mSection_title: 3.1  BIDAF\n",
            "\u001b[35mSection_title: 3.2  DocumentQA\n",
            "\u001b[35mSection_title: 3.3  ALBERT and the Retro-Reader\n",
            "\u001b[35mSection_title: 3.3.1  ALBERT\n",
            "\u001b[35mSection_title: 3.3.2  Retro Reader\n",
            "\u001b[35mSection_title: 3.3.3  Novel fine-tuning of a shared-parameter model\n",
            "\u001b[35mSection_title: 4.2  DocQA\n",
            "\u001b[35mSection_title: 4.2.1  Novel idea:\n",
            "\u001b[35mSection_title: 4.3  Retro-Reader with ALBERT\n",
            "\u001b[35mSection_title: 4.3.1  Novel fine-tuning method: Specific re-initializing of a parameter-shared network\n",
            "\u001b[32mconclusion: our research covered the development of nlp in the past 3 years, when the attention mechanism largely displaced recurrent architectures, using the squad2.0 dataset [1] as the centerpiece of our discussion. through our experiments with bidaf [2], docqa [3] and albert retro-reader [4], we examined the architectural differences and the advantages of the using state-of-the-art pretrained models and then fine-tuning for the question-answering task. our major contribution is our specific method of re-initializing weights in a parameter-shared network. future work can include applying our re-initialization method to electra [10]. as we mentioned before, we divide the bidaf model [2] into 3 part. in the first \"embedding\" part, there are character embedding, word embeddin, and contextual embedding three layers. the words in both context and query are embedded in character-level and word-level respectively.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 6.1.2  Hyper-parameters tuning\n",
            "\u001b[35mSection_title: 6.2.2  The 4 output vectors from the Attention layer\n",
            "\u001b[35mSection_title: 6.2.3  Pros and cons for each confidence method\n",
            "\u001b[35mSection_title: 6.2.4  In Table 3(hyper-parameters in Appendix)\n",
            "\u001b[35mSection_title: 6.2.5  Novel idea\n",
            "\u001b[35mSection_title: 6.2.6  Code reference\n",
            "\u001b[35mSection_title: 6.3.1  ALBERT\n",
            "\u001b[35mSection_title: 6.3.2  Retro Reader\n",
            "\u001b[35mSection_title: 6.3.3  Hyperparameter Tuning of ALBERT Retro-Reader\n",
            "\u001b[35mSection_title: 6.3.4  Our implementation of the ALBERT Retro-Reader\n",
            "\u001b[35mSection_title: 7  Attributions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Analysis\n",
            "\u001b[35mSection_title: 2.1  Unanswerable Questions\n",
            "\u001b[35mSection_title: 2.  False Premise\n",
            "\u001b[35mSection_title: 3.  Topic Error\n",
            "\u001b[35mSection_title: 2.2  Dialog Features\n",
            "\u001b[35mSection_title: 4.  Clarification\n",
            "\u001b[35mSection_title: 5.  Definition\n",
            "\u001b[35mSection_title: 2.3  Abstractive Answers\n",
            "\u001b[35mSection_title: 2.  Coref\n",
            "\u001b[35mSection_title: 3.  Count\n",
            "\u001b[35mSection_title: 3  New Extractive Baseline for CoQA\n",
            "\u001b[35mSection_title: 4  Cross-Dataset Experiments\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Teaching Broad-Coverage Reasoning Skills in a Robust Fashion\n",
            "\u001b[35mSection_title: 4  TEABREAC Dataset Construction\n",
            "\u001b[35mSection_title: 4.1  Instance Generator\n",
            "\u001b[35mSection_title: 4.1.1  QDMR to Typed Program:\n",
            "\u001b[35mSection_title: 4.2  Dataset Generator\n",
            "\u001b[35mSection_title: 4.3  Additional QA Instances for Primitives\n",
            "\u001b[35mSection_title: 4.4  Final Dataset\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Results\n",
            "\u001b[32mconclusion: despite large lms' impressive reading abilities and the availability of large scale multi-step qa datasets requiring a rich set of reasoning skills, lmbased qa models do not reliably learn to use such skills for answering complex questions. in this work, we show that the greater control that synthetic contexts offer can be leveraged to create a teaching dataset where models can learn a broad range of reasoning skills in a reliable manner, especially for more complex questions.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Formal Definitions\n",
            "\u001b[35mSection_title: 3.  Estimating the Rule Probabilities\n",
            "\u001b[35mSection_title: 4.  Determining the Most Probable Derivation\n",
            "\u001b[35mSection_title: 5.  Application\n",
            "\u001b[35mSection_title: 5.1.  Data\n",
            "\u001b[35mSection_title: 5.2.  Grammars\n",
            "\u001b[35mSection_title: 5.3.  Observations and Dicussion\n",
            "\u001b[35mSection_title: 5.4.  Runtime\n",
            "\u001b[32mconclusion: we introduced an extension to the concept of stochastic context-free grammars that allows the probabilities of the productions to depend on the length of the generated subword.\n",
            "\u001b[35mSection_title: 6.  Conclusions\n",
            "\u001b[35mSection_title: 6.1.  Possible Other Applications\n",
            "\u001b[35mSection_title: 6.2.  Language Theoretical Considerations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Single-document MRC\n",
            "\u001b[35mSection_title: 2.2  Multi-document MRC\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Accurate Word Semantic Meaning Understanding\n",
            "\u001b[35mSection_title: 3.2  Interaction Understanding\n",
            "\u001b[35mSection_title: 3.3  Answer Supporting Cue Understanding\n",
            "\u001b[35mSection_title: 3.4  Answer Prediction\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  Datasets and Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Main Experimental Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Experiments\n",
            "\u001b[35mSection_title: 4.4  Parameter Efficiency\n",
            "\u001b[35mSection_title: 4.5  Error Analyses\n",
            "\u001b[35mSection_title: 4.6  Case Study\n",
            "\u001b[35mSection_title: 4.7  Discussions\n",
            "\u001b[32mconclusion: in this paper, we propose a simple but effective deep understanding based multi-document mrc model. it uses neither any sophisticated technologies nor any pretrained language models. we evaluate our model on dureader and triviaqa web, two widely used benchmark multi-document mrc datasets. experiments show that our model achieves very competitive results on both datasets.\n",
            "\u001b[35mSection_title: 5  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Related work\n",
            "\u001b[35mSection_title: 2  Methods\n",
            "\u001b[35mSection_title: 3  Experimental setting\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Multiple choice setting\n",
            "\u001b[35mSection_title: 4.2  Extractive QA setting\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[32mlimitation: our work comes with some limitations. it is uncertain whether our results in two specific settings, multiple choice and extractive qa, would extend to more general settings for nli, although the use of contradictions for factual consistency by laban et al. (2022) suggests that they could. additionally, 3-class nli is not sufficient to capture all the natural language relations that might be needed to verify an answer. as such more challenging datasets in other settings and more granular nli settings should be attempted.another limitation involves answer ranking and the associated computational cost. the main reason we did not test answer ranking in extractive qa is that we did not generate diverse outputs, but another reason is that such a procedure grows prohibitively expensive as the domain becomes more open. in a fully open domain, ranking would require a quadratic evaluation for each context passage against each reformulated answer candidate (schuster et al., 2022). future work should look at comparison approaches that amortize this cost, such as nli-based dense passage retrieval (reimers and gurevych, 2019).  table 1: multiple choice setting. accuracy scores (best per column in bold, second best underlined, statistical significance (pairwise students t-test) is indicated by asterix) after answer ranking with the mnli-large nli model. the top three rows show the accuracy of using only the qa models' confidence score; \"qa\" refers to the scores of the roberta-race model, which was used for calibration. the bottom rows add the entailment and/or contradiction scores to the roberta-race score. for other nli models, and for just e, c, and e+c without calibration with roberta-race, see table 8 in the appendix.   \n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Transformation Systems for Software Development\n",
            "\u001b[35mSection_title: 2.  Functions\n",
            "\u001b[35mSection_title: 3.  Working Mode\n",
            "\u001b[35mSection_title: 4.  Type of transformation\n",
            "\u001b[35mSection_title: 3  Program Transformation for Software Maintenance\n",
            "\u001b[35mSection_title: 5.  A Concept Recognition-Based Program\n",
            "\u001b[35mSection_title: 4  Reverse Engineering and Reusing COBOL Programs\n",
            "\u001b[35mSection_title: 4.1  Extension of WSL\n",
            "\u001b[35mSection_title: 4.2  A Method for COBOL Program Reverse Engineering and Reuse\n",
            "\u001b[35mSection_title: 2.  Transformations for Manipulating Data Items\n",
            "\u001b[35mSection_title: 3.  Files\n",
            "\u001b[35mSection_title: 4.  Aliases\n",
            "\u001b[35mSection_title: 5.  Foreign Keys\n",
            "\u001b[35mSection_title: 6.  Abstract Data Types\n",
            "\u001b[35mSection_title: 7.  Functional Relationships\n",
            "\u001b[35mSection_title: 4.4  Reuse of COBOL Code and Design\n",
            "\u001b[35mSection_title: 4.5  An Illustration\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Construction\n",
            "\u001b[35mSection_title: 2.1  Dataset Analysis\n",
            "\u001b[35mSection_title: 3  Evaluation Method\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Model\n",
            "\u001b[35mSection_title: 4.2  Setting\n",
            "\u001b[35mSection_title: 4.3  Human Self-Knowledge\n",
            "\u001b[35mSection_title: 4.4  Analysis\n",
            "\u001b[32mconclusion: this study investigates the self-knowledge of llms by evaluating their ability to identify unanswerable questions. through the introduction of a novel dataset and an automated method for detecting uncertainty in the models' responses, we are able to accurately measure the self-knowledge of llms such as gpt-3, instructgpt and llama. our results reveal that while these models possess a certain degree of self-knowledge, there is still an apparent disparity in comparison to human selfknowledge. this highlights the need for further research in this area to enhance the ability of llms to understand their own limitations on the unknows. such efforts will lead to more accurate and reliable responses from llms, which will have a positive impact on their applications in diverse fields.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Implementation\n",
            "\u001b[35mSection_title: 3.1  System 1\n",
            "\u001b[35mSection_title: 3.2  System 2\n",
            "\u001b[35mSection_title: 3.3  Predictor\n",
            "\u001b[35mSection_title: 3.4  Training\n",
            "\u001b[35mSection_title: 3.4.1  Task #1: Span Extraction\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Experimental Details\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 5  Related work\n",
            "\u001b[32mconclusion: we present a new framework cogqa to tackle multi-hop machine reading problem at scale.the reasoning process is organized as cognitive graph, reaching unprecedented entity-level explainability.our implementation based on bert and gnn obtains state-of-art results on hotpotqa dataset, which shows the efficacy of our framework.\n",
            "\u001b[35mSection_title: 6  Discussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Fact Verification\n",
            "\u001b[35mSection_title: 2.2  Model Explainability\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 3.1  Claim Annotation\n",
            "\u001b[35mSection_title: 3.2  Explanation Annotation\n",
            "\u001b[35mSection_title: 3.3  Overall Annotating Process\n",
            "\u001b[35mSection_title: 3.4  Data characteristic\n",
            "\u001b[35mSection_title: 4.1  Experimental results\n",
            "\u001b[35mSection_title: 5  Prompt based approach\n",
            "\u001b[35mSection_title: 5.1  Experimental results\n",
            "\u001b[32mconclusion: we recognize the challenge of generating highquality explanations in a realistic setting, especially in terms of no post-hoc methods and multi-hop reasoning.however, there is still significant potential for further research in leveraging the capabilities of large language models (llms) for fact-checking systems.overcoming these challenges is essential for advancing fact-checking as a whole, improving reliability, trustworthiness, and facilitating better decision-making across different domains.\n",
            "\u001b[35mSection_title: 6  Dicussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Construction\n",
            "\u001b[35mSection_title: 2.1  Simple Questions\n",
            "\u001b[35mSection_title: 2.2  Complex Questions\n",
            "\u001b[35mSection_title: 2.3  Questions from Wikipedia Tables\n",
            "\u001b[35mSection_title: 2.4  Data Split\n",
            "\u001b[35mSection_title: 2.5  Crowdsourcing\n",
            "\u001b[35mSection_title: 4  Experimental Evaluation\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Related work\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Construction\n",
            "\u001b[35mSection_title: 2.1  Simple Questions\n",
            "\u001b[35mSection_title: 2.2  Complex Questions\n",
            "\u001b[35mSection_title: 2.3  Questions from Wikipedia Tables\n",
            "\u001b[35mSection_title: 2.4  Data Split\n",
            "\u001b[35mSection_title: 2.5  Crowdsourcing\n",
            "\u001b[35mSection_title: 4  Experimental Evaluation\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Experimental Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Related work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Setup\n",
            "\u001b[35mSection_title: 3.1  Question-Evidence Similarity\n",
            "\u001b[35mSection_title: 3.2  Incorporating Question-Type Projections\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results and Analysis\n",
            "\u001b[32mconclusion: in this work, we proposed an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts. we demonstrate consistent improvement when using our approach on three different models over two different benchmarks. for future work, we propose exploring variations of our proposed supervised loss on other long-context tasks, such as longdocument and multi-document summarization, and integrating our method into information retrieval re-ranker models. depends on the size of the model). in order to determine the temperature hyperparameter τ , we searched over {0.2, 0.4, 0.6, 0.8, 1.0} per question type (if applicable). we also applied dropout with a rate of p = 0.1 over the linear projections, which consistently improved the results over all the benchmarks. finally, we searched for the best performing λ hyperparameter over the values of {0.2, 0.4, 0.6, 0.8, 1.0}.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND\n",
            "\u001b[35mSection_title: 3  TASK AND DATASET\n",
            "\u001b[35mSection_title: 3.1  QUESTION AND ANSWER ANNOTATIONS\n",
            "\u001b[35mSection_title: 3.2  DATASET STATISTICS\n",
            "\u001b[35mSection_title: 4  MODEL\n",
            "\u001b[35mSection_title: 4.1  FUSION RETRIEVER\n",
            "\u001b[35mSection_title: 4.2  CROSS-BLOCK READER\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[32mconclusion: we focus on the problem of performing open question answering over tables and text in this paper. one interesting question we would like to ask in the future is: can we extend open question answering system to more modalities? some questions can be better answered by images and other resources, but the task can be drastically more challenging by including more modalities, as we have learned from this paper. finally, we believe the techniques we proposed might be useful for other open-qa setting, especially the comparisons between iterative retriever and fusion retriever. \n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Hop Encoding\n",
            "\u001b[35mSection_title: 3.3  Iterative Retrieval of Hops\n",
            "\u001b[35mSection_title: 3.4  Fine-Grained Sentence-Level Retrieval\n",
            "\u001b[35mSection_title: 3.5  Objective Functions of HopRetriever\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[32mconclusion: in this paper, we propose the hopretriever to collect reasoning evidence over wikipedia for multi-hop question answering. both the structured knowledge indicated by hyperlinks and the unstructured knowledge presented as introduc-tory documents in wikipedia, are involved and leveraged together in hopretriever to help the evidence collection. the experiment on the hotpotqa dataset shows that the performance of hopretriever improved observably as a result of combining the structured knowledge with unstructured knowledge, and outperforms all the published models on the leaderboard. moreover, by inspecting the proportion of the two kinds of knowledge in hops, which kind of knowledge leads the retrieving of each evidence piece can be observed directly, which also provides extra intuitive interpretations for the selection of each evidence.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Reasoning Skill Modules\n",
            "\u001b[35mSection_title: 3.2  Modular Skill Specialization\n",
            "\u001b[35mSection_title: 3.3  Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Settings\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Task Interference\n",
            "\u001b[35mSection_title: 5.2  Benefit of Chain-of-Skills Inference\n",
            "\u001b[35mSection_title: 5.3  Effect of Pretraining\n",
            "\u001b[35mSection_title: 5.4  Swapping Experts\n",
            "\u001b[35mSection_title: 6  Question Answering Experiments\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose a modular model chain-of-skills (cos) that learns five reusable skills for odqa via multi-task learning. to reduce task interference, we design a new parameterization for skill modules. we also show that skills learned by cos can be flexibly chained together to better fit the target task. cos can directly perform superior zero-shot retrieval using multitask self-supervision on wikipedia. when finetuned on multiple datasets, cos achieves sota results across the board. for future work, we are interested in exploring scaling up our method and other scenarios, e.g., commonsense reasoning (talmor et al., 2022) and biomedical retrieval (nentidis et al., 2020;zhang et al., 2022b). group at microsoft research for their helpful discussions and anonymous reviewers for their valuable suggestions on this paper.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Background\n",
            "\u001b[35mSection_title: 2.1.  General Concepts\n",
            "\u001b[35mSection_title: 2.1.1.  Language Models\n",
            "\u001b[35mSection_title: 2.1.2.  Transfer Learning\n",
            "\u001b[35mSection_title: 2.1.3.  Multilingual and Cross-Lingual Analysis\n",
            "\u001b[35mSection_title: 2.1.4.  Zero-shot Learning\n",
            "\u001b[35mSection_title: 2.2.  Neural language models\n",
            "\u001b[35mSection_title: 2.3.  Pre-trained Language Models\n",
            "\u001b[35mSection_title: 2.3.1.  Base Models\n",
            "\u001b[35mSection_title: 2.3.2.  Multilingual Models\n",
            "\u001b[35mSection_title: 2.3.3.  Language-Specific Models\n",
            "\u001b[35mSection_title: 3.  Multilingual\n",
            "\u001b[35mSection_title: 3.1.  Importance of the multilingual tasks\n",
            "\u001b[35mSection_title: 3.2.  Multilingual Models Performance\n",
            "\u001b[35mSection_title: 3.3.  Technology Evolution\n",
            "\u001b[35mSection_title: 3.3.1.  Historical Review\n",
            "\u001b[35mSection_title: 3.3.2.  Model Perspective\n",
            "\u001b[35mSection_title: 3.4.  Architecture and Hardware Requirements\n",
            "\u001b[35mSection_title: 3.5.  Multilingual Datasets\n",
            "\u001b[35mSection_title: 3.6.  Categorization of Multilingual Tasks\n",
            "\u001b[35mSection_title: 4.  Multilingual Applications and Tasks\n",
            "\u001b[35mSection_title: 4.1.  Translation\n",
            "\u001b[35mSection_title: 4.2.  Speech Recognition\n",
            "\u001b[35mSection_title: 4.3.  Sentiment Analysis\n",
            "\u001b[35mSection_title: 4.4.  Intent detection and Slot filling\n",
            "\u001b[35mSection_title: 4.5.  Dependency Parsing\n",
            "\u001b[35mSection_title: 4.6.  NER\n",
            "\u001b[35mSection_title: 4.7.  Question Answering\n",
            "\u001b[35mSection_title: 5.  Challenges and Outlook\n",
            "\u001b[35mSection_title: 5.1.  Existing Challenges\n",
            "\u001b[35mSection_title: 5.1.1.  On Modeling aspect\n",
            "\u001b[35mSection_title: 5.1.2.  On Practical aspects\n",
            "\u001b[35mSection_title: 5.1.3.  On Application\n",
            "\u001b[35mSection_title: 5.2.  Future Directions\n",
            "\u001b[32mconclusion: this survey provides an comprehensive overview of the existing studies on leveraging transfer learning models to tackle the multi-lingual and cross-lingual tasks. in addition to the models, we also reviewed the main available datasets in the community and investigated different approaches in term of the architectures and applications to identify the existing research challenges in the domain and later we provide few potential future directions.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\u001b[35mSection_title: 7.  Acknowledgments\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Proposed Model\n",
            "\u001b[35mSection_title: 3.1  Generative Modeling of Multi-Hop QG\n",
            "\u001b[35mSection_title: 3.2  Generative Modeling of Single-Hop QG\n",
            "\u001b[35mSection_title: 3.3  Planning Guided Question Generation\n",
            "\u001b[35mSection_title: 3.4  Learning and Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Settings\n",
            "\u001b[35mSection_title: 4.3  Discussion\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we proposed a jointly optimized two-phase model named plar for low-resource question generation. plar effectively utilizes non-parallel singlehop and multi-hop question answering data to perform optimization. we further designed a planning mechanism to guide the generation process of subquestions so that the generation results are valid to compose a multi-hop question. experimental results confirm that plar achieves better performance compared with the state-of-the-art under various metrics, especially in a question answering based evaluation. for future work, we will explore the heterogeneous multi-hop qg task that requires reasoning beyond plain texts, e.g., tables.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Knowledge Abilities Syllabus\n",
            "\u001b[35mSection_title: 4  Dataset Construction\n",
            "\u001b[35mSection_title: 4.1  Knowledge Collection Schema Design\n",
            "\u001b[35mSection_title: 4.2  Knowledge Collection\n",
            "\u001b[35mSection_title: 4.3  Question Generation\n",
            "\u001b[35mSection_title: 4.4  Quality Control\n",
            "\u001b[35mSection_title: 4.5  Evaluation Metric\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[32mconclusion: this paper introduces kmir benchmark for evaluating knowledge memorization, identification, and reasoning abilities. kmir includes a systematic evaluation syllabus to summarize the knowledgerelated abilities of plms, and has 184,348 questions involving 4 types of questions covering 3 types of knowledge. we also find many interesting phenomena through extensive experiments: 1) the memorization ability of plms depends more on the number of parameters than training schemes. 2) current plms are struggling to robustly remember the facts. 3) model compression technology retains the amount of knowledge well, but hurts the identification and reasoning ability, etc.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Logical Probe\n",
            "\u001b[35mSection_title: 3.2  Qualitative Diagnosis\n",
            "\u001b[35mSection_title: 3.3  Quantitative Diagnosis\n",
            "\u001b[35mSection_title: 4  Diagnosis\n",
            "\u001b[35mSection_title: 4.1  NLU Datasets to Be Diagnosed\n",
            "\u001b[35mSection_title: 4.2  Probe Dataset\n",
            "\u001b[35mSection_title: 4.3  Probe Model\n",
            "\u001b[35mSection_title: 4.4  Preliminaries\n",
            "\u001b[35mSection_title: 5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.1  Overall Results of Diagnosis\n",
            "\u001b[35mSection_title: 5.2  Exception Analysis\n",
            "\u001b[35mSection_title: 5.3  Paired Analysis\n",
            "\u001b[35mSection_title: 5.4  Case Study\n",
            "\u001b[32mconclusion: in this work, we present a novel framework, which can diagnose the correlation between the nlu dataset and a specific skill and we probe a fundamental reasoning skill, logical reasoning, on 11 nlu datasets. our framework involves a logical probe to conduct diagnosis and defines a qualitative process and a quantitative process to calculate two indicators. from the results, we observe that 1) most nli datasets have a relatively strong correlation with logical reasoning. 2) the correlations between type 1 mrc datasets and logical reasoning are moderate because logical reasoning is not the only dominant skill in these datasets. 3) the dependences of type 2 mrc datasets are not always exactly consistent with their intended purpose. based on the analysis, although there are several limitations in our proposed method, this work is still a reasonable attempt to deeply understand the relationship between the dataset and a specific nlu skill. in future works, we will focus on: 1) exploring the solution to the limitations of the proposed method; 2) build associations for different datasets that require the same nlu capabilities.\n",
            "\u001b[35mSection_title: 6  Conclusions and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Eχαµs Dataset\n",
            "\u001b[35mSection_title: 3.1  Dataset Statistics\n",
            "\u001b[35mSection_title: 3.2  Subjects and Categories\n",
            "\u001b[35mSection_title: 3.3  Collection and Preparation\n",
            "\u001b[35mSection_title: 3.4  Data Splits\n",
            "\u001b[35mSection_title: 3.5  Reasoning and Knowledge Types\n",
            "\u001b[35mSection_title: 4  Baseline Models\n",
            "\u001b[35mSection_title: 4.1  No Additional Training\n",
            "\u001b[35mSection_title: 4.2  Fine-Tuned Models\n",
            "\u001b[35mSection_title: 5  Experiments and Results\n",
            "\u001b[35mSection_title: 5.1  Multilingual Evaluation\n",
            "\u001b[35mSection_title: 5.2  Knowledge Evaluation\n",
            "\u001b[35mSection_title: 5.3  Cross-lingual Evaluation\n",
            "\u001b[35mSection_title: 5.4  Per-subject Fine-grained Evaluation\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: we presented eχαµs, a new challenging crosslingual and multilingual benchmark for science qa in 16 languages and 24 subjects from high school examinations.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Motivation for this survey\n",
            "\u001b[35mSection_title: 2  Why use Text Worlds?\n",
            "\u001b[35mSection_title: 3  Text World Simulators\n",
            "\u001b[35mSection_title: 3.1  Text World Modeling Paradigms\n",
            "\u001b[35mSection_title: 3.1.1  Environment Modelling\n",
            "\u001b[35mSection_title: 3.1.2  Agent Modelling\n",
            "\u001b[35mSection_title: 4  Text World Environments\n",
            "\u001b[35mSection_title: 4.1  Single Environment Benchmarks\n",
            "\u001b[35mSection_title: 4.2  Domain-specific Environments\n",
            "\u001b[35mSection_title: 4.3  Environment Collections as Benchmarks\n",
            "\u001b[35mSection_title: 4.4  Generative Environments\n",
            "\u001b[35mSection_title: 5  Text World Agents\n",
            "\u001b[35mSection_title: 6  Contemporary Focus Areas\n",
            "\u001b[32mlimitation: environment complexity is limited, and it's currently difficult to author complex worlds. two competing needs are currently at odds: the desire for complex environments to learn complex skills, and the desire for environment variation to encourage robustness in models. current tooling emphasizes creating varied procedural environments, but those environments have limited complexity, and require agents to complete straightforward tasks. economically creating complex, interactive environments that simulate a significant fraction of real world interactions is still well beyond current simulators or libraries -but required for higher-fidelity interactive worlds that have multiple meaningful paths toward achieving task goals. generating these environments semi-automatically (e.g. ammanabrolu et al., 2020a) may offer a partial solution. independent of tooling, libraries and other middleware offer near-term solutions to more complex environment modeling, much in the same way 3d game engines are regularly coupled with physics engine middleware to dramatically reduce the time required to implement forces, collisions, lighting, and other physics-based modeling. currently, few analogs exist for text worlds. the addition of a chemistry engine that knows ice warmed above the freezing point will change to liquid water, or a generator engine that knows the sun is a source of sunlight during sunny days, or an observation engine that knows tools (like microscopes or thermometers) can change the observation model of a pomdp -may offer tractability in the form of modularization. efforts using large-scale crowdsourcing to construct knowledge bases of commonsense knowledge (e.g., atomic, sap et al., 2019) may be required to support these efforts.current planning languages offer a partial solution for environment modelling. while simulators partially implement facilities for world modeling, some (e.g. côté et al., 2018;shridhar et al., 2020b) suggest using mature planning languages like strips (fikes and nilsson, 1971) or pddl (mcdermott et al., 1998) for more full-featured modeling. this would not be without significant development effort -existing implementations of planning languages typically assume full-world observability (in conflict with pomdp modelling), and primarily agent-directed state-space changes, making complex world modeling with partial ob-servability, and complex environment processes (such as plants that require water and light to survive, or a sun that rises and sets causing different items to be observable in day versus night) outside the space of being easily implemented with off-the-shelf solutions. in the near-term, it is likely that a domain-specific language specific to complex text world modeling would be required to address these needs while simultaneously reducing the time investment and barrier-to-entry for end users.analyses of environment complexity can inform agent design and evaluation. text world articles frequently emphasize agent modeling contributions over environment, methodological, or analysis contributions -but these contributions are critical, especially in the early stages of this subfield. agent performance in easy environments has increased incrementally, while medium-to-hard environments have seen comparatively modest improvements. agent performance is typically reported as a distribution over a large number of environments, and the methodological groundwork required to understand when different models exceed others in time or performance over these environment distributions is critical to making forward progress. transfer learning in the form of training on one set of environments and testing on others has become a standard feature of benchmarks (e.g. hausknecht et al., 2020), but focused contributions that work to precisely characterize the limits of what can be learned from (for example) omniquest and transferred to zork, and what capacities must be learned elsewhere, will help inform research programs in agent modeling and environment design.transfer learning between text world and 3d environments. tasks learned at a high-level in text worlds help speed learning when those same models are transferred to more complex 3d environments (shridhar et al., 2020b). this framing of transfer learning may resemble how humans can converse about plans for future actions in locations remote from those eventual actions (as when we apply knowledge learned in classrooms to the real world). as such, text-plus-3d environment rendering shows promise as a manner of controlling for different sources of complexity in multi-modal task learning (from high-level task-specific knowledge to low-level perceptual knowledge), and appears a promising research methodology for imparting complex task knowledge on agents that are able to navigate high-fidelity virtual environments. \n",
            "\u001b[35mSection_title: 7  Contemporary Limitations and Challenges\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Material and Methods\n",
            "\u001b[35mSection_title: 2.1  Sample Collection and Isolation of Staphylococcus Aureus\n",
            "\u001b[35mSection_title: 2.2  Antibiotic Suspetibility Testing\n",
            "\u001b[35mSection_title: 2.3  DNA Extraction and Identifying gyrA Gene:\n",
            "\u001b[35mSection_title: 2.4  DNA Sequencing\n",
            "\u001b[35mSection_title: 2.5  Nucleotide Analysing\n",
            "\u001b[35mSection_title: 2.6  Translate Nucleotide Sequences to Amino Acid Sequences\n",
            "\u001b[35mSection_title: 2.7  Molecular Docking\n",
            "\u001b[35mSection_title: 8  Ligand Preparation\n",
            "\u001b[35mSection_title: 2.9  Protein Preparation\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Approach\n",
            "\u001b[35mSection_title: 3.1.  Noun Phrase Extraction\n",
            "\u001b[35mSection_title: 3.2.  Noun-phrase Graph Construction\n",
            "\u001b[35mSection_title: 3.3.  Steiner Tree Computation\n",
            "\u001b[35mSection_title: 3.4.  Query Expansion and Retrieval\n",
            "\u001b[35mSection_title: 3.5.  Answer Prediction\n",
            "\u001b[35mSection_title: 4.  Experiments and Results\n",
            "\u001b[35mSection_title: 4.1.  Experiment 1: Evidence Retrieval\n",
            "\u001b[35mSection_title: 4.2.  Experiment 2: Question Answering\n",
            "\u001b[35mSection_title: 4.3.  Experiment 3: Bridge Phrases\n",
            "\u001b[35mSection_title: 4.4.  Experiment 4: Post-hoc Explanations\n",
            "\u001b[32mconclusion: we proposed an unsupervised approach for the identification of bridge phrases in multi-hop question answering. our method constructs a graph of noun phrases from the question and the available context, and applies the steiner tree algorithm to identify the minimal subgraph that connects all question phrases. we extract as bridge phrases nodes in this graph that are not any of the question phrases. our method can be coupled with any downstream qa component, i.e., it can be used as query expansion for evidence retrieval; it can be used to generate enhanced context for answer prediction; and it can be used to generate post-hoc explanations for given answers. using the hotpotqa dataset, we demonstrate that our method yields improved results in all these scenarios, for multiple types of downstream components. 5. ravi sethi: he is best known as one of three authors of the classic computer science textbook \"\", also known as the \"dragon book\". \n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\u001b[35mSection_title: 6.  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Introducing Abstraction Inductive Biases\n",
            "\u001b[35mSection_title: 3.1  Abstraction as an additional embedding\n",
            "\u001b[35mSection_title: 3.2  Abstraction as an additional sequence to encode\n",
            "\u001b[35mSection_title: 3.3  Abstraction as an auxiliary task\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Compositional generalization with CLUTRR\n",
            "\u001b[35mSection_title: 4.1.1  Testing Compositional generalization\n",
            "\u001b[35mSection_title: 4.1.2  Results\n",
            "\u001b[35mSection_title: 4.2  Abductive Reasoning with ProofWriter\n",
            "\u001b[35mSection_title: 4.3  Multi-hop Question Answering with HotpotQA\n",
            "\u001b[35mSection_title: 4.4  Conversational Question Answering with CoQA\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[32mconclusion: conclusion. we presented various ways to incorporate abstract knowledge into transformer language models. focusing on entity types, this work evaluated model performance on reasoning tasks requiring compositional generalization and multi-hop reasoning. overall our results demonstrate three things: (i) incorporating abstract knowledge significantly improves reasoning and compositional generalization in both interpolation and extrapolation when the environment is formally defined in a logical reasoning setting; (ii) different ways to incorporate abstraction yields different performance boosts: enc-sum and dec-loss are generally performing better than others; (iii) abstraction is not beneficial when the task at hand is more natural, less procedural, and not requiring long reasoning chains. this last result is due to the noisy entity tagging from \"off-the-shelf\" taggers, and due to the nature of the task at hand.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  READONCE Transformers\n",
            "\u001b[35mSection_title: 3.1  Document Encoder\n",
            "\u001b[35mSection_title: 3.2  Representation+Text Model\n",
            "\u001b[35mSection_title: 3.3  Training READONCE via QA\n",
            "\u001b[35mSection_title: 3.4  Downstream Usage of READONCE\n",
            "\u001b[35mSection_title: 4  Representation Learning Experiments\n",
            "\u001b[35mSection_title: 4.1  Training Setup\n",
            "\u001b[35mSection_title: 4.2  Architecture Evaluation\n",
            "\u001b[35mSection_title: 4.2.1  Representation+Text Model\n",
            "\u001b[35mSection_title: 4.2.2  Document Encoder\n",
            "\u001b[35mSection_title: 4.3  Final READONCE Architecture\n",
            "\u001b[35mSection_title: 5  Downstream Task Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Representation Quality\n",
            "\u001b[35mSection_title: 5.3  Model Efficiency\n",
            "\u001b[35mSection_title: 5.4  Efficiency-Accuracy Tradeoff\n",
            "\u001b[35mSection_title: 5.5  Handling Long Documents\n",
            "\u001b[32mconclusion: this work introduced readonce transformers, a novel approach for using large scale transformerbased language models to both build and consume reusable document representations. akin to humans' ability to read a document and extract useful information without knowing the enduse, readonce representations are compact, information-capturing document representations that can be pre-computed once, in a task-and example-independent fashion.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  ELUE: A Standard Benchmark for Efficient NLP Models\n",
            "\u001b[35mSection_title: 3.1  Design Considerations\n",
            "\u001b[35mSection_title: 3.2  Task and Dataset Selection\n",
            "\u001b[35mSection_title: 3.3  Leaderboard\n",
            "\u001b[35mSection_title: 4  ElasticBERT: A Strong Baseline for Efficient Inference\n",
            "\u001b[35mSection_title: 5.2  Evaluating ElasticBERT on ELUE\n",
            "\u001b[32mconclusion: in this work, we present elue, which is a public benchmark and platform for efficient models, and elasticbert, which is a strong baseline (backbone) for efficient static (dynamic) models. both of the two main contributions are aimed to build the pareto frontier for nlu tasks, such that the position of existing work can be clearly recognized, and future work can be easily and fairly measured. our future work is mainly in four aspects: (1) including more baselines in elue, (2) supporting the evaluation for more frameworks such as tensor-flow (abadi et al., 2016) \n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Experimental Setup\n",
            "\u001b[35mSection_title: 2.1  Models\n",
            "\u001b[35mSection_title: 2.2  Adaptation strategies\n",
            "\u001b[35mSection_title: 2.2.1  Fine-tuning (baseline)\n",
            "\u001b[35mSection_title: 2.2.2  Prompt fine-tuning\n",
            "\u001b[35mSection_title: 2.2.3  Parameter-efficient fine-tuning\n",
            "\u001b[35mSection_title: 2.2.4  Robustness enhancing fine-tuning\n",
            "\u001b[35mSection_title: 2.2.5  In-context learning\n",
            "\u001b[35mSection_title: 2.2.6  Zero-shot inference\n",
            "\u001b[35mSection_title: 2.3  Distribution shifts\n",
            "\u001b[35mSection_title: 2.4  Measuring robustness\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 3.1  Performance drops under distribution shift\n",
            "\u001b[35mSection_title: 3.2  Role of model\n",
            "\u001b[35mSection_title: 3.3  Role of the adaptation method\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[35mSection_title: 4.1  How do the findings compare to robustness evaluations in vision?\n",
            "\u001b[35mSection_title: 4.2  How do individual distribution shifts differ from aggregate trends?\n",
            "\u001b[35mSection_title: 5  Related work\n",
            "\u001b[32mconclusion: we conduct an extensive evaluation of the robustness of different model and adaptation methods on 15 distribution shifts in question answering. our in-depth analysis suggests several concrete directions for future work: improving the in-distribution performance of icl methods and understanding why different few-shot fine-tuning methods yield the squadshifts wiki dataset is derived from the same data source (wikipedia) as squad. as a result, models lie closer to the y = x diagonal than on other distribution shifts. (middle) progress on squad is a weaker indicator for progress on searchqa for fully fine-tuned models and few-shot fine-tuned models. we find that zero-shot and icl models are less robust than fine-tuned and few-shot models with the exception of larger language models. (right) on the squad→drop distribution shift, we observe that progress beyond 70 f1 on squad yields quick progress on drop for fine-tuned models.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: experimenting with different in-distribution datasets. we choose squad as a representative in-distribution dataset since it is one of the largest and most popular qa datasets. one limitation of squad is that the training set is mainly collected from wikipedia articles which may not be optimal for building a qa model that generalizes to many domains. future work could explore the robustness of models trained on datasets from other domains for increased coverage.specialized modeling methods. our work does not evaluate models with task or data specific components. as an example andor et al. (2019) improved performance on drop (dua et al., 2019) by using arithmetic programs to improve a model's mathematical reasoning. evaluating the robustness of methods like these are an exciting area for future investigations.few-shot gpt evaluations. our results indicate that large gpt models fine-tuned on a smaller number of samples are more robust to distribution shifts compared to other few-shot fine-tuned models that use a prompt or span prediction. however, gpt-2 xl and gpt-neo, which both have more than one billion parameters, are larger than all few-shot models we evaluate. future work could examine the impact of architecture on this trend by evaluating other models with more than a billion parameters like t5.multiple fine-tuning runs. for fine-tuned models we include a single data-point for each model. however, previous work (phang et al., 2018;dodge et al., 2020) has shown that different data ordering and weight initialization can lead to large variance in model performance. in figure 11 we evaluate the robustness of roberta large models fine-tuned with different data ordering and initialization for the span prediction head (devlin et al., 2019). we find that on average the robustness of these models does not differ substantially. further investigation into the effect of random seeds on robustness would improve our understanding of the robustness of individual data points. as at most other universities, notre dame's students run a number of news media outlets. the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. question: how many student news papers are found at notre dame? answer: three figure 9: a sample from squad with the input formatting used for fine-tuning decoder-only models, incontext learning, and zero-shot inference. 14 figure 10: a sample from squad with the input formatting used for fine-tuning decoder-only models, incontext learning, and zero-shot inference.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.  Train a model\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 4.1  Uncertainty Methods\n",
            "\u001b[35mSection_title: 4.2  H-Divergence Methods\n",
            "\u001b[35mSection_title: 4.3  Reverse Classification Accuracy\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Comparing Acquisition Methods\n",
            "\u001b[35mSection_title: 6.2  Properties of Optimal Example Selection\n",
            "\u001b[32mconclusion: we examine a challenging variant of active learning where target data is scarce, and multiple shifted domains operate as the source set of unlabeled data. for practitioners facing multi-domain active learning, we benchmark 18 acquisition functions, demonstrating the h-divergence family of methods and our proposed variant dal-e achieve the best results. our analysis shows the importance of example selection in existing methods, and also the surprising potential of domain budget allocation strategies. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Problem Definition\n",
            "\u001b[35mSection_title: 2.2  SPANDROP\n",
            "\u001b[35mSection_title: 2.3  Beta-SPANDROP\n",
            "\u001b[35mSection_title: 3  FINDANIMALS: Distilling Supervision from Long-Sequences\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Results and Analysis\n",
            "\u001b[35mSection_title: 4  Experiments on Natural Language Data\n",
            "\u001b[35mSection_title: 4.1  Setup and Main Results\n",
            "\u001b[35mSection_title: 4.2  Analysis\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we presented spandrop, a simple and effective method for learning from long sequences, which ablates parts of the sequence at random to generate counterfactual data to distill the sparse supervision signal that is predictive of the desired output. we show via theoretical analysis and carefully designed synthetic datasets that spandrop and its variant based on the beta-bernoulli distribution, beta-spandrop, help models achieve competitive performance with a fraction of the data by introducing diverse augmented training examples, and generalize better to previously unseen data. our experiments on four real-world nlp datasets confirm these theoretical findings, and demonstrate span-drop's efficacy on strong neural models even when data is abundant.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Retrieve-and-Rerank\n",
            "\u001b[35mSection_title: 2.2  Re-ranker Relevance Feedback\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Setup\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 3.2.1  English Retrieval in Multiple Domains\n",
            "\u001b[35mSection_title: 3.2.2  Retrieval in More Languages\n",
            "\u001b[35mSection_title: 3.2.3  Multi-modal Retrieval\n",
            "\u001b[35mSection_title: 3.3  Discussion and Analysis\n",
            "\u001b[35mSection_title: 3.3.1  Query vectors: the original and the new\n",
            "\u001b[35mSection_title: 3.3.2  How do smaller K values affect results?\n",
            "\u001b[35mSection_title: 3.3.3  How much additional latency does our approach introduce?\n",
            "\u001b[35mSection_title: 3.3.4  Can multiple rounds of relevance feedback further improve results?\n",
            "\u001b[35mSection_title: 3.3.5  Further Discussion\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we demonstrate that query representations can be improved using feedback from a cross-encoder reranker at inference time for better performance of dual-encoder retrieval. this work proposes for distillation using relevance feedback from the re-ranker as a better and faster alternative to the traditional strategy of re-ranking a larger pool of candidates for improving recall. our proposed distillation process is lightweight and im-proves retrieval accuracy across different domains, languages and modalities over a state-of-the-art retrieve-and-rerank pipeline with comparable latency. future work will explore relevance feedback for token-level query representations as well as disentangling term importance scores from query representations for better interpretability.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.  METODE PENELITIAN\n",
            "\u001b[35mSection_title: 2.1.  APRS\n",
            "\u001b[35mSection_title: 2.2  Paket Radio AFSK 1.200\n",
            "\u001b[35mSection_title: 2.3  Perangkat Lunak Tracking Satelit\n",
            "\u001b[35mSection_title: 2.4  Perangkat Lunak APRSDroid\n",
            "\u001b[35mSection_title: 2.5  Perangkat Keras Rotator Antena\n",
            "\u001b[35mSection_title: 2.6  Rancangan Perangkat Keras\n",
            "\u001b[35mSection_title: 2.7.2  Program Demodulator FM\n",
            "\u001b[35mSection_title: 3.2.2  Program rtl_fm\n",
            "\u001b[35mSection_title: 3.3.3  Pengujian pada satelit LAPAN-A2\n",
            "\u001b[35mSection_title: 3.3.4  Pengujian pada stasiun cuaca\n",
            "\u001b[35mSection_title: 5.  SARAN\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 1.1  Definitions of Interpretability\n",
            "\u001b[35mSection_title: 1.1.2  Local and Global\n",
            "\u001b[35mSection_title: 1.1.3  Post-hoc vs In-built Interpretations. Another important distinction is whether an interpretability method is applied\n",
            "\u001b[35mSection_title: 1.2  Paper layout\n",
            "\u001b[35mSection_title: 2  ASPECTS OF INTERPRETABILITY\n",
            "\u001b[35mSection_title: 2.1  Interpretability requirements\n",
            "\u001b[35mSection_title: 2.2  Dimensions of Interpretability\n",
            "\u001b[35mSection_title: 2.2.2  Stability.\n",
            "\u001b[35mSection_title: 3  INTERPRETABLE METHODS\n",
            "\u001b[35mSection_title: 3.1  Feature Importance\n",
            "\u001b[35mSection_title: 3.1.2  Input Perturbation. Another method for identifying important features of textual inputs is input perturbation.\n",
            "\u001b[35mSection_title: 3.1.3  Attention weights. Extracting attention weights for inputs to provide local interpretations for predictions is\n",
            "\u001b[35mSection_title: 3.1.4  Attribution Methods.\n",
            "\u001b[35mSection_title: 3.1.5  Datasets.\n",
            "\u001b[35mSection_title: 3.2  Natural Language Explanation\n",
            "\u001b[35mSection_title: 3.2.2  Text-only NLE.\n",
            "\u001b[35mSection_title: 3.2.3  NLE in Dialog.\n",
            "\u001b[35mSection_title: 3.3  Probing\n",
            "\u001b[35mSection_title: 3.3.1  Embedding Probes.\n",
            "\u001b[35mSection_title: 3.3.2  Model\n",
            "\u001b[32mlimitation: the continued growth of probing-based papers has also led to recent work examining best practices for probes, and how to interpret their results. hewitt and liang [66] considered how to ensure that a probe is truly reflective of the underlying information present in a model, and proposed the use of a control task, a randomised version of a probe task in which high performance is only possible by memorisation of inputs. hence, a faithful probe should perform well on a probe task and poorly on a corresponding control task if the underlying model does indeed contain the information being probed for. the authors found that most probes (including linear classifiers)are over-parameterised, and discuss methods for constraining complex probes (e.g. multilayer perceptrons) to improve faithfulness while still allowing them to achieve similar results.while most papers we have discussed above follow the intuition that probes should avoid complex probes to prevent memorisation, pimentel et al. [127] suggest that instead the probe with the best score on a given task should be chosen as the tightest estimate, since simpler models may simply be unable to extract the linguistic information present in a model, and such linguistic information cannot be 'added' by more complex probes (since their only input are hidden representations). in addition, the authors argue that memorisation is an important part of linguistic competence, and as such probes should not be artificially punished (via control tasks) for doing this. recent work has also presented methods that avoid making assumptions about probe complexity, such as mdl probing [101,170], which directly measures 'amount of effort' needed to achieve some extraction task, or directprobe [198], which directly examines intermediate representations of models to avoid having to deal with additional classifiers.finally, hall maudslay et al. [59] compared the structural probe [67] with a lightweight dependency parser (both given the same inputs), and demonstrate that the parser is generally able to extract more syntactic information from bert embedding. in contrast, the probe performs better with a different metric, showing that the choice of metric is important for probes: when testing for evidence of linguistic information, one should not only consider the nature of the probe, but also the metric used to evaluate it. furthermore, the significance of well-performing probes is not clear: models may encode linguistic information not actually used by the end-task [138], showing that the presence of linguistic information does not imply it is being used for prediction. more causal approaches such as amnesiac probing [50], which directly intervene in the underlying model's representations, may better distinguish between these cases.\n",
            "\u001b[35mSection_title: 3.3.3  Probe Considerations and Limitations.\n",
            "\u001b[35mSection_title: 3.3.4  Interpretability of Probes and Future Work.\n",
            "\u001b[35mSection_title: 4.1.2  Human Evaluation.\n",
            "\u001b[35mSection_title: 4.2  Evaluation of NLE\n",
            "\u001b[35mSection_title: 4.2.2  Human Evaluation.\n",
            "\u001b[35mSection_title: 4.3  Evaluation of Probing\n",
            "\u001b[32mconclusion: this paper focused on the local interpretable methods commonly used for natural language processing models. in this survey, we have divided these methods into three different categories based on their underlying characteristics: 1) explaining the model's outputs from the input features, where these features could be identified through rationale extraction, perturbing inputs, traditional attribution methods, and attention weight extraction; 2) generating the natural language explanations corresponding to each inputs; 3) using diagnostic classifiers to analyse the hidden information stored within a model. for each method type, we have also outlined the common datasets used for different nlp tasks and different evaluation methods for examining the validity and efficacy of the explanations provided.\n",
            "\u001b[35mSection_title: 5  DISCUSSION AND CONCLUSION\n",
            "\u001b[35mSection_title: 6  FUTURE TREND OF INTERPRETABILITY\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Dataset\n",
            "\u001b[35mSection_title: 3.1  Conversational QA Data\n",
            "\u001b[35mSection_title: 3.2  Standard QA Data\n",
            "\u001b[35mSection_title: 4  Metrics\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Reinforcement\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\u001b[32mconclusion: asking inquisitive questions allows humans to learn from each other and increase their knowledge. we thus proposed a new task: curiosity-driven question generation, which attempts to address such a key component for several human-machine interaction scenarios. in absence of data directly usable for this task, we proposed an automatic method to derive it from conversational qa datasets. further, recognizing that the great majority of qa datasets are not conversational, we also extended the method to standard qa data. our experiments, which include learning strategies such as pretraining and reinforcement, show promising results under both automatic and human evaluation. in future works, we plan to extend the approach to conditional generation of curiosity-driven questions.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Artifacts in NLP datasets\n",
            "\u001b[35mSection_title: 2.2  Joint QA and Coreference Reasoning\n",
            "\u001b[35mSection_title: 3  How Well Quoref Presents Coreference\n",
            "\u001b[35mSection_title: 5  Improving Coreference Reasoning\n",
            "\u001b[35mSection_title: 5.1  Coreference-to-QA Conversion\n",
            "\u001b[35mSection_title: 5.2  Experimental Setups\n",
            "\u001b[35mSection_title: 5.3  Data\n",
            "\u001b[35mSection_title: 5.4  Results\n",
            "\u001b[35mSection_title: 5.5  Analysis\n",
            "\u001b[32mconclusion: we show that the high performance of recent models on the quoref dataset does not necessarily indicate that they are adept at performing coreference reasoning, and that qa based on coreference reasoning is a greater challenge than current scores suggest. we then propose a methodology for creating a dataset that better presents the coreference reasoning challenge for mrc. we provide our methodology to an annotator and create a sample dataset. our analysis shows that our dataset contains fewer biases compared to quoref, and the performance of state-of-the-art quoref models drops considerably on this evaluation set.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related work\n",
            "\u001b[35mSection_title: 2.1.  Machine reading comprehension\n",
            "\u001b[35mSection_title: 2.2.  Ensemble learning for improving generalization\n",
            "\u001b[35mSection_title: 3.  Proposed approach\n",
            "\u001b[35mSection_title: 3.1.  Weighting\n",
            "\u001b[35mSection_title: 3.2.  Stacking\n",
            "\u001b[35mSection_title: 4.  Experiments\n",
            "\u001b[35mSection_title: 4.1.  Evaluation setting\n",
            "\u001b[35mSection_title: 4.2.  Preprocessing\n",
            "\u001b[35mSection_title: 4.3.  Results and discussion\n",
            "\u001b[35mSection_title: 4.3.2.  Different ensemble approaches\n",
            "\u001b[35mSection_title: 4.3.3.  Out-of-distribution generalization robustness\n",
            "\u001b[32mconclusion: the common paradigm in the natural language processing community to develop models for a new benchmark is to either train a new model or fine-tune a pre-trained one. in addition to their high computational costs and environmental effects [8], their accuracies drop significantly for a new data distribution [5]. in this paper, we investigated    the effect of light-weight ensemble-based approach on the generalization of machine reading comprehension models to out-of-distribution data. the experiments were conducted using eight different datasets, six mrc models, and three settings including heterogeneous (different base models with the same training dataset), homogeneous (the same base model trained on different datasets), and hybrid.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Multiple choice reading comprehension\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Results\n",
            "\u001b[32mlimitation: we propose an approach that can automatically flag questions that can be answered without contextual information. however, the remaining questions are not necessarily high-quality questions, since many other aspects make up question quality. second, the experiments are conducted using only the electra model, though it is expected similar trends will be picked up by alternative transformer-based language models. further, exams might be aimed at a level where a lack of specific knowledge may be assumed. our work does not consider variable candidate knowledge levels, and our evaluation was only done by highly educated (we'd like to think) graduate students. finally, we acknowledge that our human evaluation was limited in size and questions, however it is clearly demonstrated that for low 'shortcut entropy' questions, comprehension is not necessarily required.\n",
            "\u001b[35mSection_title: 5  Limitations\n",
            "\u001b[35mSection_title: 6  Acknowledgements\n",
            "\u001b[35mSection_title: 7  Ethics Statement\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  PRIOR LITERATURE\n",
            "\u001b[35mSection_title: 3.  DATA\n",
            "\u001b[35mSection_title: 4.  MODEL\n",
            "\u001b[35mSection_title: 5.  METHODS\n",
            "\u001b[35mSection_title: 5.1.  Evaluation Metrics\n",
            "\u001b[35mSection_title: 6.  RESULTS\n",
            "\u001b[35mSection_title: 7.  ANALYSIS\n",
            "\u001b[32mconclusion: financial analysis is an important method for assessing the performance of firms. advanced quantitative analyses are used by practitioners to answer financial queries on reports and make lucrative investment decisions. as a result, financial question answering (qa) is an important question-answering task requiring in-depth numerical reasoning. a retriever must extract critical details about the financial issue from the text, and a generator must construct a legitimate computational tree and a final response. large language models like gpt-3 have recently achieved the state-of-the-art performance on similar tasks. however, due to the nature of financial inquiries and the extensive information held in financial documents, we come up with a financial qa system that requires a retriever and program generator. our work produces highly promising results in terms of answers to detailed numerical financial questions that may contribute as a supporting tool to the finance industry.\n",
            "\u001b[35mSection_title: 8.  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Data Pre-processing\n",
            "\u001b[35mSection_title: 3.2  Task Definition\n",
            "\u001b[35mSection_title: 3.3  Model\n",
            "\u001b[35mSection_title: 3.3.1  Query Representation\n",
            "\u001b[35mSection_title: 3.3.2  Surrounding Sentence Prediction\n",
            "\u001b[35mSection_title: 3.3.3  Retrieval based MLM\n",
            "\u001b[35mSection_title: 3.4  Optimization\n",
            "\u001b[35mSection_title: 3.5  Fine-tuning\n",
            "\u001b[35mSection_title: 3.5.1  Multiple Choice QA\n",
            "\u001b[35mSection_title: 3.5.2  Span Extraction\n",
            "\u001b[35mSection_title: 4.1.2  Span Extraction\n",
            "\u001b[35mSection_title: 4.2  Implementation Detail\n",
            "\u001b[35mSection_title: 4.3  Baseline\n",
            "\u001b[35mSection_title: 4.3.1  Multiple Choice QA\n",
            "\u001b[35mSection_title: 4.3.2  Hotpot QA\n",
            "\u001b[35mSection_title: 5  Results and Analyses\n",
            "\u001b[35mSection_title: 5.1  Results for Multiple Choice QA\n",
            "\u001b[35mSection_title: 5.2  Performance on Span Extraction QA\n",
            "\u001b[35mSection_title: 5.3  Evaluation of Evidence Extraction\n",
            "\u001b[35mSection_title: 5.4  Effect of Different Masking Ratio During\n",
            "\u001b[35mSection_title: 5.5  Performance in Low Resource Scenario\n",
            "\u001b[32mconclusion: in this paper, we present a novel pre-training approach, rept, to bridge the gap between pretrained language models and machine reading comprehension through retrieval-based pre-training. specifically, we design two retrieval-based pretraining tasks equipped with self-supervised learning, namely surrounding sentences prediction (ssp) and retreval based masked language modeling (rmlm), to enhance plms with the capability of evidence extraction for mrc. the experiments over five different datasets validate the effectiveness of our proposed method. in the future, we plan to extend the proposed pre-training approach to the more challenging open-domain settings.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  MULTIHIERTT Dataset\n",
            "\u001b[35mSection_title: 3.1  Data Collection and Preprocessing\n",
            "\u001b[35mSection_title: 3.2  Question-Answer Pair Annotation\n",
            "\u001b[35mSection_title: 3.3  Quality Control\n",
            "\u001b[35mSection_title: 3.4  Dataset Analysis\n",
            "\u001b[35mSection_title: 4  MT2Net Model\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Baseline Systems\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 5.3  Human Performance\n",
            "\u001b[35mSection_title: 5.4  Model Performance\n",
            "\u001b[35mSection_title: 5.5  Further Analysis\n",
            "\u001b[32mlimitation: although the proposed mt2net model outperforms other baseline models, it still performs significantly worse than human experts, which reflects the challenge of multihiertt. primarily, we find that models do not perform well on certain types of questions: 1) questions requiring reasoning across multiple tables; 2) questions requiring multi-step reasoning; 3) questions requiring reasoning over tables with complex hierarchical structures; and 4) questions requiring external financial knowledge.to deal with these challenges, we believe that four main directions of work may be workable: 1) designing a specialized module to handle multitable reasoning; 2) decomposing a complex question requiring multi-step reasoning into several simpler sub-questions that qa models can handle (perez et al., 2020;chen et al., 2020); 3) applying a more advanced table-encoding method. for example, a pre-trained model with specialized table structure-aware mechanisms cheng et al., 2021a;yang et al., 2022) can be utilized in the facts retrieving module to better understand hierarchical tables; and 4) leveraging structured knowledge (xie et al., 2022) to inject external financial knowledge to models.\n",
            "\u001b[35mSection_title: 5.6  Limitations and Future Work\n",
            "\u001b[32mconclusion: we have proposed multihiertt, a new largescale qa dataset that aims to solve complicated qa tasks that require numerical reasoning over documents containing multiple hierarchical tables and paragraphs. to address the challenge of multi-hiertt, we introduce a baseline framework named mt2net. the framework first retrieves supporting facts from financial reports and then generates executable reasoning programs to answer the question. the results of comprehensive experiments showed that current qa models (best f 1 : 38.43%) still lag far behind the human expert performance (f 1 : 87.03%). this motivates further research on developing qa models for such complex hybrid data with multiple hierarchical tables.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 7  Ethics Considerations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Operators\n",
            "\u001b[35mSection_title: 3.2  Reasoning Graphs\n",
            "\u001b[35mSection_title: 3.3  Question Filtration\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Unsupervised QA Results\n",
            "\u001b[35mSection_title: 4.2  Ablation Study\n",
            "\u001b[35mSection_title: 4.3  Few-shot Multi-hop QA\n",
            "\u001b[35mSection_title: 4.4  Analysis of Generated Questions\n",
            "\u001b[35mSection_title: 4.5  Effects of Question Paraphrasing\n",
            "\u001b[32mconclusion: in this work, we study unsupervised multi-hop qa and propose a novel framework mqa-qg to generate multi-hop questions via composing reasoning graphs built upon basic operators. the experiments show that our model can generate human-like questions that help to train a well-performing multi-hop qa model in both the unsupervised and the fewshot learning setting. further work is required to include more flexible paraphrasing at the fusion stage. we can also design more reasoning graphs and operators to generate more complex questions and support more input modalities.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Works\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  AUGMENTED LANGUAGE AGENT ARCHITECTURE\n",
            "\u001b[35mSection_title: 2.2  WEB AGENT\n",
            "\u001b[35mSection_title: 2.3  TOOL AGENT\n",
            "\u001b[35mSection_title: 3  AGENT ARCHITECTURES\n",
            "\u001b[35mSection_title: 3.1  SOLO AGENTS\n",
            "\u001b[35mSection_title: 4  EXPERIMENT\n",
            "\u001b[35mSection_title: 4.1  ENVIRONMENT BENCHMARK\n",
            "\u001b[35mSection_title: 4.2  EVALUATION METRICS\n",
            "\u001b[35mSection_title: 4.3  LLM UTILIZATION\n",
            "\u001b[35mSection_title: 4.4  DECISION-MAKING SIMULATION\n",
            "\u001b[35mSection_title: 4.5  KNOWLEDGE REASONING SIMULATION\n",
            "\u001b[32mconclusion: in this paper, we systematically investigate the performances of various laa architecture paired with different llm backbones. we also provide one novel orchestrating method for multiple agents, i.e. bolaa. the benchmarking results provide experimental justification for the laa investigation and verify the potential benefits of bolaa architecture. during the investigation, we also identify the challenge of designing bolaa architecture for environments with compounding actions. in the future, we will explore whether we can harness llms in the controller such that selection and communication with labor agents is also fully autonomous. we will continue developing more laa architectures and include more llms and environments for evaluations.\n",
            "\u001b[35mSection_title: 5  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Overview of the emrQA dataset\n",
            "\u001b[35mSection_title: 3  In-depth Qualitative Analysis\n",
            "\u001b[35mSection_title: 3.1  How clean are the emrQA answers?\n",
            "\u001b[35mSection_title: 3.2  How challenging are the emrQA pairs?\n",
            "\u001b[35mSection_title: 4  Comprehensive Quantitative Analysis\n",
            "\u001b[35mSection_title: 4.1  How redundant are the emrQA pairs?\n",
            "\u001b[35mSection_title: 4.2  Little room for improvement\n",
            "\u001b[35mSection_title: 4.3  BERT does not always win\n",
            "\u001b[35mSection_title: 5  Desiderata in Real-World CliniRC\n",
            "\u001b[35mSection_title: 5.1  External domain knowledge is needed\n",
            "\u001b[35mSection_title: 5.2  Generalizing to unseen questions and documents\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we study the clinical reading comprehension (clinirc) task with the recently created emrqa dataset. our qualitative and quantitative analysis as well as exploration of the two desired aspects of clinirc systems show that future clinical qa datasets should not only be large-scale but also less noisy and more diverse. moreover, questions that involve complex relations and are across different domains should be included, and then more advanced external knowledge incorporation methods as well as domain adaptation methods can be carefully designed and systematically evaluated.  \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Conditional Question Answering\n",
            "\u001b[35mSection_title: 2.2  Self-Supervised Learning\n",
            "\u001b[35mSection_title: 3  Preliminaries: Structured Documents\n",
            "\u001b[35mSection_title: 4  Our Approach\n",
            "\u001b[35mSection_title: 4.1  Decomposed Conditional Question Generation with Document Extraction\n",
            "\u001b[35mSection_title: 4.1.1  Selective Extraction\n",
            "\u001b[35mSection_title: 4.1.2  Question Generation\n",
            "\u001b[35mSection_title: 4.2  Purturbation-based Document Contrastive Learning\n",
            "\u001b[35mSection_title: 4.3  Document Perturbation\n",
            "\u001b[35mSection_title: 4.3.1  Positive Pair Generation\n",
            "\u001b[35mSection_title: 4.3.2  Contrastive Loss Computation\n",
            "\u001b[35mSection_title: 4.4  Pipeline for Answering Conditional Questions\n",
            "\u001b[35mSection_title: 4.4.1  Document Encoding\n",
            "\u001b[35mSection_title: 4.4.2  Multiple Answer Extraction\n",
            "\u001b[35mSection_title: 4.4.3  Condition Determination\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Datasets and Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.2.1  Main Result\n",
            "\u001b[35mSection_title: 5.2.2  Conditional Accuracy\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 6.1  Ablation Study\n",
            "\u001b[35mSection_title: 6.2  State Generator's Output Quality\n",
            "\u001b[35mSection_title: 6.3  Label Generator's Output Accuracy\n",
            "\u001b[32mconclusion: in this paper, we present learning on structured documents (lsd), a self-supervised learning method for conditional question answering. lsd uses a conditional question generation method to leverage massive structured documents while improving conciseness, and applies contrastive learning to learn effective semantic representations from complex documents. we further propose a pipeline that could generate multiple answers and conditions to better handle the cqa task. we verify the effectiveness of the proposed method on the conditionalqa dataset. for future work, we plan to investigate how to better generate conditional questions and improve our model's performance in providing correct answers. despite the effectiveness of lsd in utilizing the structure of massive unsupervised data, there are still some potential points for improvement. one issue is that the state generator is only trained on answerable questions, leading to a distribution bias that there might be unanswerable questions. in addition, our pipeline can still not handle the position where a sentence has more than one answer, which limits our model's performance for broader scenarios. we will resolve these issues in future work. table 7: statistics of our scraped dataset. we present document count, average document length measured by word (avg. w) and sentences (avg. s), average sentence length (avg w/s) and tag distribution (h:p:li/tr).\n",
            "\u001b[35mSection_title: 7  Conclusion and Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  VIP-Token Centric Compression (VCC)\n",
            "\u001b[35mSection_title: 3.1  Elevating the Importance of a Few Tokens: VIP-Tokens\n",
            "\u001b[35mSection_title: 3.2  VIP-Token Centric Compression (VCC): An Initial Proposal\n",
            "\u001b[35mSection_title: 3.3  A Specific Instantiation via Multi-Resolution Compression\n",
            "\u001b[35mSection_title: 3.4  Efficient Data Structure for Compression/Decompression\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[32mconclusion: we propose a vip-token centric sequence compression method to compress/decompress the input/output sequences of transformer layers thereby reducing the complexity dependency on the sequence length n without sacrificing the model accuracy. specifically, we design the compression our empirical evaluation shows that our method can be directly incorporated into existing pretrained models with some additional training. also, it often has much higher efficiency compared to baselines with the same sequence length while offering better or competitive model accuracy. for future work, we believe that extending our method to the decoder of the encoder-decoder models will further boost the efficiency of transformers while maintaining similar model performance.    we provide a table 4 of notations that are used for more than once so that the readers can refer to their definition easily.\n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\u001b[35mSection_title: 6.2  Details of Multi-Resolution Compression\n",
            "\u001b[35mSection_title: 6.2.1  Basic Problem Setup\n",
            "\u001b[35mSection_title: 6.2.2  Plugging Our Problem into the Setup\n",
            "\u001b[35mSection_title: 6.2.3  Efficient Approximation\n",
            "\u001b[35mSection_title: 6.2.4  How to Construct J for S c and S c C?\n",
            "\u001b[35mSection_title: 6.2.5  How Good is This Approximation?\n",
            "\u001b[35mSection_title: 6.2.6  Relation to [36] that Inspires Multi-Resolution Compression\n",
            "\u001b[35mSection_title: 6.3  Details of Proposed Data Structure\n",
            "\u001b[35mSection_title: 6.3.1  Why\n",
            "\u001b[35mSection_title: 6.3.3  Algorithm for Making T (C) into T (C new )\n",
            "\u001b[35mSection_title: 6.4  Complexity Analysis\n",
            "\u001b[35mSection_title: 6.4.1  Preparing Input Sequence to T (C): O(nd)\n",
            "\u001b[35mSection_title: 6.4.4  Updating T (C) into T (C new ): O(lrd)\n",
            "\u001b[35mSection_title: 6.6.2  Downstream Finetuning\n",
            "\u001b[35mSection_title: 6.7  Practical Questions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Overview of the CORE Framework\n",
            "\u001b[35mSection_title: 3  Intermediary Modules\n",
            "\u001b[35mSection_title: 3.1  Linker\n",
            "\u001b[35mSection_title: 3.2  Chainer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Experimental Settings\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Ablation Study\n",
            "\u001b[35mSection_title: 5.2  Impact of Linker & Chainer\n",
            "\u001b[35mSection_title: 5.3  Alternative Linking Strategy\n",
            "\u001b[35mSection_title: 5.4  Linker Performance\n",
            "\u001b[35mSection_title: 5.5  Case Study\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present a new framework, core, for odqa over heterogeneous knowledge sources.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Setup\n",
            "\u001b[35mSection_title: 3  Sequence-level contrastive loss\n",
            "\u001b[35mSection_title: 4  Experimental Setup and Results\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[32mconclusion: in this work, we propose an effective sequencelevel contrastive loss for improving the performance of long-range transformers in solving qa tasks that require reasoning over long contexts. we demonstrate consistent improvement when using our approach on three different models over two different benchmarks. in future work, we would like to explore variations of our proposed supervised loss on other long-context tasks, such as long document summarization.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Task and Dataset\n",
            "\u001b[35mSection_title: 4.  Methodology\n",
            "\u001b[35mSection_title: 4.1.  Baseline approaches\n",
            "\u001b[35mSection_title: 4.1.2.  CompAggr-kMax.\n",
            "\u001b[35mSection_title: 4.1.3.  CompClip-LM-LC.\n",
            "\u001b[35mSection_title: 4.1.4.  IWAN.\n",
            "\u001b[35mSection_title: 4.1.5.  sCARNN.\n",
            "\u001b[35mSection_title: 4.2.  Propagate-Selector\n",
            "\u001b[35mSection_title: 4.3.  Optimization\n",
            "\u001b[35mSection_title: 5.  Experiments\n",
            "\u001b[35mSection_title: 5.1.  Implementation Details\n",
            "\u001b[35mSection_title: 5.2.  Comparisons with Other Methods\n",
            "\u001b[35mSection_title: 5.3.  Hop Analysis\n",
            "\u001b[35mSection_title: 5.4.  Impact of Various Graph Topologies\n",
            "\u001b[35mSection_title: 5.5.  Impact of Node Representation\n",
            "\u001b[35mSection_title: 5.5.1.  Word Representation\n",
            "\u001b[35mSection_title: 5.5.2.  Node Representation\n",
            "\u001b[35mSection_title: 6.  Discussion\n",
            "\u001b[32mconclusion: in this paper, we propose a graph neural network that finds the sentences crucial for answering a question. the experiments demonstrate that the model correctly classifies supporting sentences by iteratively propagating the necessary information through its novel architecture. we believe that our approach will play an important role in building a qa pipeline in combination with other mrqa models trained in an end-to-end manner.\n",
            "\u001b[35mSection_title: 7.  Conclusion\n",
            "\u001b[35mSection_title: 8.  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 500  bp 640 bp NEE\n",
            "\u001b[35mSection_title: 956  bp CDS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 5.  Complete the attached Word document (see additional information below)\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Formulation\n",
            "\u001b[35mSection_title: 3  Our LEAP Framework\n",
            "\u001b[35mSection_title: 3.1  Base System\n",
            "\u001b[35mSection_title: 3.2  Improvement-A: Inference with Planning\n",
            "\u001b[35mSection_title: 3.3  Improvement-B: Refined Verification Model\n",
            "\u001b[35mSection_title: 4  Small and Large Model Versions\n",
            "\u001b[35mSection_title: 4.1  SLM Version\n",
            "\u001b[35mSection_title: 4.2  LLM Version\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  SLM Experiments on Entailment Bank\n",
            "\u001b[35mSection_title: 6.2  SLM Experiments on QASC\n",
            "\u001b[35mSection_title: 6.3  LLM Experiments on PrOntoQA\n",
            "\u001b[32mconclusion: in this paper, we presented leap, an lm-based logical reasoning system that integrates explicit planning into the inference method.we also proposed a method that learns to prevent the explicit planning from being misguided.our proposed methods exhibit intriguing technical connections to other reasoning systems and can be likened to the deliberative system 2 in \"dual process\" theories of reasoning.in our experiments, our planning-based system outperforms strong baseline methods including the selection-inference method and chain-of-thought prompting.we will discuss several exciting avenues for further improvements in appendix a.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  The Potential of LLMs as Black-Box Models\n",
            "\u001b[35mSection_title: 2.2  Retrieval-Augmented Framework\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Two-Stage Training for PRCA\n",
            "\u001b[35mSection_title: 3.2  Contextual Extraction Stage\n",
            "\u001b[35mSection_title: 3.3  Reward-Driven Stage\n",
            "\u001b[35mSection_title: 4.2  Baseline Retrievers and Generators\n",
            "\u001b[35mSection_title: 4.3  GPT-4 Assessment\n",
            "\u001b[35mSection_title: 4.4  Hyperparameter Configurations\n",
            "\u001b[35mSection_title: 5.1  Overall Performance\n",
            "\u001b[35mSection_title: 5.2  Efficiency of PRCA\n",
            "\u001b[35mSection_title: 5.3  Impact of Top-K Selection\n",
            "\u001b[35mSection_title: 5.4  Case Study\n",
            "\u001b[35mSection_title: 5.5  Ablation Study of PRCA\n",
            "\u001b[32mconclusion: in conclusion, this research successfully introduces a prca-based paradigm for reqa tasks, tackling the inherent challenges of fine-tuning llms in the retrieval-enhancement framework, especially given their vast parameter size and closed-source natures.prca innovatively distills retrieved documents via generator rewards, leading to a marked improvement in the reqa task's performance.experimental outcomes consistently demonstrate the robustness and effectiveness of prca when paired with various retrievers and generators, indicating its potential to be widely deployed as an adapter on the reqa task.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Framework\n",
            "\u001b[35mSection_title: 3.1  Retrieval Module\n",
            "\u001b[35mSection_title: 3.2  Refinement Module\n",
            "\u001b[35mSection_title: 3.3  Reading Comprehension Module\n",
            "\u001b[35mSection_title: 4  Label Smoothing\n",
            "\u001b[35mSection_title: 4.1  Linear Decay Label Smoothing\n",
            "\u001b[35mSection_title: 4.2  F1 Smoothing\n",
            "\u001b[35mSection_title: 5  Experiment\n",
            "\u001b[35mSection_title: 5.1  Dataset\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 5.3  Experimental Results\n",
            "\u001b[35mSection_title: 5.4  Label Smoothing Analysis\n",
            "\u001b[35mSection_title: 5.5  Error Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Setup\n",
            "\u001b[35mSection_title: 3.1  Natural Instructions V2 Dataset\n",
            "\u001b[35mSection_title: 3.2  Baseline Model with Standard Training\n",
            "\u001b[35mSection_title: 3.3  Evaluation Settings\n",
            "\u001b[35mSection_title: 4  Meta-Learning with Instructions\n",
            "\u001b[35mSection_title: 4.1  Standard Training + MAML\n",
            "\u001b[35mSection_title: 4.2  Standard Training + HNET\n",
            "\u001b[35mSection_title: 4.2.1  The HNet Language Model (HNet-LM)\n",
            "\u001b[35mSection_title: 4.2.2  HNet-FF Projections\n",
            "\u001b[35mSection_title: 4.2.3  Alternating Training Schedule\n",
            "\u001b[35mSection_title: 4.3  Standard Training + HNet + MAML\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Summary of Results\n",
            "\u001b[35mSection_title: 5.3  Performance of Different Models\n",
            "\u001b[35mSection_title: 5.4  Effect of Instructions on Tasks with Different Difficulty Levels\n",
            "\u001b[35mSection_title: 5.5  Detailed Task Level Analysis of Models\n",
            "\u001b[32mconclusion: in this paper we investigate whether meta-learning applied to multi-task instructional learning (mtil) can boost the generalizability of lms to unseen tasks in a zero-shot setting. specifically, we eval-  figure 6: % differences from standard training with other models with bart-large. hnet has the best performance for the strong generalization set.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\u001b[32mlimitation: there are several limitations of the proposed metalearning based approaches in its present form.• computation and memory overhead: metalearning approaches have higher resource requirements which can limit the usage specially for larger models. for example with bartlarge, the hnet-maml model on a single gpu is inefficient to train since we have to use small batch sizes which leads to lower performance.• regressions with easy tasks: we see some regression in metrics for the easy tasks. further analysis and research is needed to understand the factors and improve the models such that model enhancements are uniform across tasks.• hyper-parameter tuning: meta learning models have more hyper-parameters and thus might be more difficult to tune than the standard training approach.• overall zero-shot performance: the zero-shot performance even with the best meta-learning approaches is quite far from state-of-the-art results. it will be interesting to see at what point (e.g. with k-shot learning) the performance can match a fully supervised model.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 5  SINGLE-TASK, SINGLE-METHOD FINE-TUNING\n",
            "\u001b[35mSection_title: 5.1  PERFORMANCE AND EFFICIENCY\n",
            "\u001b[35mSection_title: 5.2  ROBUSTNESS AND GENERALIZATION\n",
            "\u001b[35mSection_title: 5.3  ANALYSIS OF VARIOUS FINE-TUNING FACTORS\n",
            "\u001b[35mSection_title: 6  MULTI-METHOD FINE-TUNING\n",
            "\u001b[35mSection_title: 7  MULTI-TASK FINE-TUNING\n",
            "\u001b[35mSection_title: 8  DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  EXPLICIT, IMPLICIT, AND NO CHAIN-OF-THOUGHT REASONING\n",
            "\u001b[35mSection_title: 2.1  NO CHAIN-OF-THOUGHT REASONING\n",
            "\u001b[35mSection_title: 2.2  EXPLICIT CHAIN-OF-THOUGHT REASONING\n",
            "\u001b[35mSection_title: 3  APPROACH TO IMPLICIT CHAIN OF THOUGHT REASONING\n",
            "\u001b[35mSection_title: 3.1  MIND-READING THE TEACHER\n",
            "\u001b[35mSection_title: 3.2  THOUGHT EMULATION\n",
            "\u001b[35mSection_title: 3.3  COUPLE AND OPTIMIZE\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.1  DATA\n",
            "\u001b[35mSection_title: 4.2  BASELINES\n",
            "\u001b[35mSection_title: 4.3  MODELS\n",
            "\u001b[35mSection_title: 5  RESULTS\n",
            "\u001b[35mSection_title: 6  ANALYSIS\n",
            "\u001b[35mSection_title: 7  RELATED WORK\n",
            "\u001b[32mlimitation: lack of transparency and interpretability one of the main advantages of explicit cot is its inherent transparency: the intermediate steps allow for easy interpretation of the model's reasoning process.in contrast, implicit cot, by virtue of its internal processing within hidden states, lacks this transparency.while it achieves compactness and efficiency in generation, it sacrifices human interpretability, making it challenging to understand how the model arrives at its conclusions.reliance on the teacher's thought process our current three-step strategy is, at a high level, trying to distill the teacher model's horizontal reasoning process into the vertical reasoning process of the student and the emulator.while the ultimate goal of implicit reasoning is to allow models to develop their own unique trajectories of reasoning, our initial approach still relies heavily on the teacher's thought processes for a starting point.performance discrepancies our current results of implicit cot still lag behind the performance of explicit cot.however, this work is just a first step towards building implicit cot, and there exists ample room for further optimization.\n",
            "\u001b[35mSection_title: 8  LIMITATIONS\n",
            "\u001b[32mconclusion: in this work, we proposed the concept of implicit chain of thought reasoning for transformer-based language models, where reasoning is performed \"vertically\" among the transformer hidden states, instead of being performed \"horizontally\" in the form of generating intermediate tokens.this concept potentially enables the model to break away from the human-like reasoning process and develop its own internal reasoning process.\n",
            "\u001b[35mSection_title: 9  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  The Task\n",
            "\u001b[35mSection_title: 3.1  Corpus\n",
            "\u001b[35mSection_title: 3.2  Input\n",
            "\u001b[35mSection_title: 3.3  Output\n",
            "\u001b[35mSection_title: 3.4  Evaluation\n",
            "\u001b[35mSection_title: 4  Data Collection\n",
            "\u001b[35mSection_title: 4.1  Documents\n",
            "\u001b[35mSection_title: 4.2  Questions\n",
            "\u001b[35mSection_title: 4.3  Find Answers\n",
            "\u001b[35mSection_title: 4.4  Move Conditions to Scenario\n",
            "\u001b[35mSection_title: 4.5  Train / Dev / Test Splits\n",
            "\u001b[35mSection_title: 5  Evaluation\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[32mconclusion: we propose a challenging dataset conditionalqa that contains questions with conditional answers.  table 4: em/f1 w/ conditions on the subset of questions with conditional answers. \"best overall\" uses the best checkpoints/hyper-parameters on the full dataset, while \"best conditional\" uses the best ones on the subset of questions.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 7  Ethics Statements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Consistent Question Answering\n",
            "\u001b[35mSection_title: 3.2  Logic-guided Data Augmentation\n",
            "\u001b[35mSection_title: 3.3  Logic-guided Consistency Regularization\n",
            "\u001b[32mconclusion: we introduce a logic guided data augmentation and consistency-based regularization framework for accurate and globally consistent qa, especially under limited training data setting. our approach significantly improves the state-of-the-art models across three substantially different qa datasets. notably, our approach advances the state-of-the-art on quarel and wiqa, two standard benchmarks requiring rich logical and language understanding. we further show that our approach can effectively learn from extremely limited training data.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RESEARCH QUESTIONS\n",
            "\u001b[35mSection_title: 2.1  Designing a Wizard-of-Oz System by Using a Multi-Hop Dataset\n",
            "\u001b[35mSection_title: 2.2  Study design\n",
            "\u001b[35mSection_title: 2.3  Measures\n",
            "\u001b[35mSection_title: 3  RESULTS\n",
            "\u001b[32mconclusion: large general-purpose language models, such as gpt family of models [brown et al. 2020;openai 2023] , lamda [thoppilan et al. 2022], palm [anil et al. 2023;chowdhery et al. 2022], and others, have propagated into informationseeking workflows of a general audience. a vast host of existing and ongoing work in nlp examine the deficiencies of these language models, ranging from hallucinated generations  with background with background and highlights fig. 4. users' subjective rating of the system for the usefulness of highlights, background, their confidence in the ai system, selfconfidence, and satisfaction with ai. users rate self-confidence marginally higher in the condition with background than the condition without (left). however, users rate their satisfaction with ai in the condition with background slightly lower than without. users' satisfaction rating is slightly lower even after introducing highlights with the background (right), with a slightly higher rating of background utility in the condition with highlights than without. however, there are no other discernible differences in ratings in the background condition with or without highlight.\n",
            "\u001b[35mSection_title: 4  DISCUSSION AND CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Moving Average Equipped\n",
            "\u001b[35mSection_title: 3.1  Problem Definition\n",
            "\u001b[35mSection_title: 3.2  Independent Encoding of Homogeneous Data: the Basic Encoder for FiD\n",
            "\u001b[35mSection_title: 3.2.1  Single-row Heterogeneous Reasoning\n",
            "\u001b[35mSection_title: 3.3  Multi-row Heterogeneous Reasoning by the Low-dimensional EMA\n",
            "\u001b[35mSection_title: 3.4  Gated Fusion-in-Decoder\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Main Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[35mSection_title: 4.5  Error Analysis\n",
            "\u001b[32mconclusion: in this paper, we address long range-reasoning for the multi-hop table-and-text qa and propose mafid, which extends fid by equipping ema and the gated cross-attention layer for the encoder and decoder parts, respectively, to design an effective way of combining various types of encoded representations. the experimental results on hy-bridqa showed that the proposed mafid achieved state-of-the-art performances in both the development and blind test sets. in future work, we will extend mafid to open-domain table-and-text qa and explore a unified approach that integrates single-row and multi-row reasoning.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task and Hypothesis\n",
            "\u001b[35mSection_title: 3  Dataset\n",
            "\u001b[35mSection_title: 4  Event Likelihood Prediction\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Representing Procedures as Python Code\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[35mSection_title: 5  Causal Reasoning with Entities\n",
            "\u001b[35mSection_title: 5.1  Predicted Entity States as CoT\n",
            "\u001b[35mSection_title: 5.2  Annotated Entity States as CoT\n",
            "\u001b[35mSection_title: 5.3  Externally Predicted Entity States\n",
            "\u001b[35mSection_title: 6  Performance Analysis\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we present crepe, a benchmark for causal reasoning about events and entities in procedural texts. we show that mainstream llms such as gpt-3 perform close to chance on crepe, while using code-like event representation as a prompt to code language model codex greatly improves the performance. further, we experiment with various ways to encode entity information into this representation and find that eliciting chain-of-thought reasoning from codex further improves performance while existing cot approaches with gpt-3 are ineffective. we clearly show that llms benefit from lower-level entity information when making predictions about higher-level events. future work should explore related tasks such as next-event prediction, event temporal ordering, etc., by injecting relevant information about entities into our representation. our code-representation of events allows more powerful expressions than simply entailment and negation considered in this work. future work may explore other forms of code chain-of-thought such as first-order logic. these expressions generated by llms can be computed objectively, thus ameliorating llms' hallucinations and improving the interpretability and faithfulness of predictions.\n",
            "\u001b[35mSection_title: 8  Conclusion and Future Work\n",
            "\u001b[32mlimitation: despite our best efforts, our crepe dataset has inherent limitations. first, the choice of studying procedure texts, despite many discussed advantages, limits the domain, writing style, and other semantic features of the texts. as a result, porting our methods and findings to other text styles such as stories or news might require domain adaptation. second, we prioritize quality over quantity when creating this benchmark, which suffers from small size and contains biases from the annotators, even though we address the latter by having different annotators label a test set.when annotating the hypothetical events, our intention is that they represent a wild variety that doers of the procedures, humans or machines, would care about. however, we also have to ensure these events are unambiguously bound to some entities in order to challenge models for their causal reasoning ability. while we do our utmost to balance these two conflicting objectives, the issue might still persist.in crepe, each event likelihood change is caused by exactly one entity state change. this is an over-simplification made to facilitate evaluation. in real life, many complex events require many entity states to be reasoned about, which in turn may have complex logical relations among them. we leave this for future work.while we intend our representation of events and entities to be a general and effective one, we have only shown that it works well empirically using codex, which is one of the only code language models at present. whether the idea of our structured representation applies to other models remains to be explored. in section 4 and 5, we have discussed our bestperforming prompts for gpt-3 and codex. here, we elaborate on inferior codex prompts and shed light on why they do not work well empirically.best prompt as discussed, our best-performing prompt represents procedures as classes and steps as functions.class wash_sneakers: # init # remove shoelaces # rinse def __init__(self, event0): self.event0 = event0 # my feet get wet by wearing the sneakers. def remove_shoelaces(self): self.event0.change = \"equally likely\" # my feet get wet by wearing the sneakers. def rinse(self):self.event0.change = \"more likely\" # my feet get wet by wearing the sneakers.nested functions instead of representing procedures as classes as in our best-performing prompt, we can also represent them as nested functions.def wash_sneakers(event0): # init # remove shoelaces # rinse event0 = event0 # my feet get wet by wearing the sneakers. def remove_shoelaces(self): event0.change = \"equally likely\" # my feet get wet by wearing the sneakers. def rinse(self): event0.change = \"more likely\" # my feet get wet by wearing the sneakers.no step comments the comments displaying the steps immediately after the class declaration are removed.class wash_sneakers: def __init__(self, event0): self.event0 = event0 # my feet get wet by wearing the sneakers. def remove_shoelaces(self): self.event0.change = \"equally likely\" # my feet get wet by wearing the sneakers. def rinse(self):self.event0.change = \"more likely\" # my feet get wet by wearing the sneakers.\n",
            "\u001b[35mSection_title: 9  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Setup\n",
            "\u001b[35mSection_title: 4  The QADA Framework\n",
            "\u001b[35mSection_title: 4.1  Overview\n",
            "\u001b[35mSection_title: 2.  Hidden space augmentation:\n",
            "\u001b[35mSection_title: 3.  Contrastive adaptation:\n",
            "\u001b[35mSection_title: 4.2  Pseudo Labeling\n",
            "\u001b[35mSection_title: 4.3  Hidden Space Augmentation\n",
            "\u001b[35mSection_title: 4.4  Contrastive Adaptation\n",
            "\u001b[35mSection_title: 4.5  Learning Algorithm\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6.1  Adaptation Performance\n",
            "\u001b[35mSection_title: 6.2  Ablation Study for QADA\n",
            "\u001b[35mSection_title: 6.3  Sensitivity Analysis for Hidden Space Augmentation\n",
            "\u001b[32mconclusion: in this paper, we propose a novel self-supervised framework called qada for qa domain adaptation. qada introduces: (1) hidden space augmentation tailored for qa data to enrich target training corpora; and (2) an attention-based contrastive adaptation to learn domain-invariant features that generalize across source and target domain. our experiments demonstrate the effectiveness of qada: it achieves a superior performance over state-ofthe-art baselines in qa domain adaptation.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[32mlimitation: despite having introduced hidden space augmentation in qada, we have not discussed different choices of α values for multi-hop synonyms to exploit the potential benefits of the dirichlet distribution. for context cutoff, dropping multiple context spans in each qa example may bring additional benefits to improve context understanding and the answer extraction process of the qa model. combined with additional question value estimation in pseudo labeling, we plan to explore such directions in adaptive qa systems as our future work. a combination of question augmentation ratio ζ and context cutoff ratio φ. specifically, we empirically search for the best combination in the range of [0.1, 0.2, 0.3, 0.4] for both ζ and φ. eventually, the best hyperparameter combination is selected.\n",
            "\u001b[35mSection_title: 8  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data Collection and Question Type Annotation\n",
            "\u001b[35mSection_title: 3.1  Open-ended Question Datasets\n",
            "\u001b[35mSection_title: 3.2  Question Type Ontology and Annotation\n",
            "\u001b[35mSection_title: 4  Type-aware Open-ended Question Generation\n",
            "\u001b[35mSection_title: 4.1  Joint Focus Prediction and Question Generation (JOINTGEN)\n",
            "\u001b[35mSection_title: 4.2  Diversifying Questions with Templates (EXPLGEN & TPLGEN)\n",
            "\u001b[35mSection_title: 5  Experiment Results\n",
            "\u001b[35mSection_title: 5.1  Automatic Evaluation\n",
            "\u001b[35mSection_title: 5.2  Human Evaluation\n",
            "\u001b[35mSection_title: 5.3  Further Analyses\n",
            "\u001b[32mconclusion: we present a new question type ontology which better captures the nuances of questions to support the study of open-ended question generation. we further annotate a new dataset with 4,959 questions based on the proposed ontology. we describe a joint question focus detection and question generation framework with a novel semantic graphaugmented representation, which is directly built on large pre-trained models. based on this framework, we also enhance the controllability and diversity of generated questions by employing template exemplars or automatically generated templates. experiments on two large datasets show that questions generated by our models have better quality and higher diversity than non-trivial comparisons, with similar results rated by human judges.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3.1  Named Entity Recognition & Entity Typing\n",
            "\u001b[35mSection_title: 3.2  Sentence Graph Construction\n",
            "\u001b[35mSection_title: 3.3  Minimal Dominating Set Approximation\n",
            "\u001b[35mSection_title: 3.4  Question Generation\n",
            "\u001b[35mSection_title: 3.5  Prompt-style Data Augmentation\n",
            "\u001b[35mSection_title: 3.6  Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Performance Comparison\n",
            "\u001b[35mSection_title: 4.3  Effect of Deriving the Dominating Set\n",
            "\u001b[35mSection_title: 4.4  Case Study\n",
            "\u001b[32mconclusion: in this paper, we present minprompt, a robust data augmentation framework that leverages a graph-based algorithm and unsupervised question generation to extract minimally meaningful qa training samples from raw text.our contributions reside in the application of minimal data augmentation, enhancing computational efficiency and model performance while mitigating overfitting.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  METHODOLOGY\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[32mconclusion: this paper introduces dc-bert to decouple question and document for efficient contextual encoding. dc-bert has been successfully applied to document retrieval, a key component in opendomain qa, achieving 10x speedup while retaining most of the qa performance. with the capability of processing high-throughput of questions each with a large collection of retrieved documents, dc-bert brings open-domain qa one step closer to serving real-world applications.\n",
            "\u001b[35mSection_title: 4  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and methods\n",
            "\u001b[35mSection_title: 2.1.  Ethics statement\n",
            "\u001b[35mSection_title: 2.2.  Drug preparation\n",
            "\u001b[35mSection_title: 2.3.  Human blood samples\n",
            "\u001b[35mSection_title: 2.4.  Microarray analysis\n",
            "\u001b[35mSection_title: 2.5.  Bioinformatic studies\n",
            "\u001b[35mSection_title: 2.6.  qRT-PCR validation of miRNA expression in serum samples\n",
            "\u001b[35mSection_title: 2.7.  Cell culture\n",
            "\u001b[35mSection_title: 2.8.  Construction of the miR-1281 lentiviral expression vector and KXS intervention\n",
            "\u001b[35mSection_title: 2.9.  Detection of survival rate by CCK8 and apoptosis by flow cytometry (FCM)\n",
            "\u001b[35mSection_title: 2.10.  Luciferase reporter assay\n",
            "\u001b[35mSection_title: 2.11.  qRT-PCR assay for miRNA and mRNAs expression in SY5Y cells\n",
            "\u001b[35mSection_title: 2.12.  Western blotting analysis\n",
            "\u001b[35mSection_title: 2.13.  Statistical analysis\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Clinical characteristics of subjects\n",
            "\u001b[35mSection_title: 3.2.  Differential expression of miRNAs between groups\n",
            "\u001b[35mSection_title: 3.3.  PCR and gene regulatory network\n",
            "\u001b[35mSection_title: 3.4.  Prediction and validation of the target genes of miR-1281\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[32mconclusion: collectively, based on the discovery of clinical-level mirna expression profiles, the biological function of mir-1281 was elucidated in vitro from the perspective of posttranscriptional regulation. we found that the target genes of mir-1281 (adcy1 and dvl1) were closely related to the occurrence and development of depression. it was demonstrated in vitro that kxs may activate the camp/pka/ erk/creb and wnt/β-catenin signal transduction pathways by downregulating mir-1281, which targets adcy1 and dvl1, to achieve its role in neuronal cell protection. these results provide a novel way of perceiving depression disorder, shedding light on the development of new therapeutic approaches.\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Structure and functionality of the code\n",
            "\u001b[35mSection_title: 2.1  The distribution directory\n",
            "\u001b[35mSection_title: 2.1.2  Two templates of the user projects\n",
            "\u001b[35mSection_title: 2.2  Persistency with the help of ROOT\n",
            "\u001b[35mSection_title: 2.3  The workflow\n",
            "\u001b[35mSection_title: 2.4  Structure of the source code\n",
            "\u001b[35mSection_title: 2.4.1  The generator class TMCgen\n",
            "\u001b[35mSection_title: 2.4.2  The MC run and analysis module class TRobol\n",
            "\u001b[35mSection_title: 3  Running stochastic simulation within the MCdevelop framework\n",
            "\u001b[35mSection_title: 3.1  Example 1 -for an impatient user (5 min. time)\n",
            "\u001b[35mSection_title: 3.2  Example 2 -running a graphical analysis program\n",
            "\u001b[35mSection_title: 3.3  Example 3 -running more advanced programs\n",
            "\u001b[35mSection_title: 3.3.1  Running jobs on a computer farm\n",
            "\u001b[35mSection_title: 3.3.2  Configuring farming scripts for other applications\n",
            "\u001b[35mSection_title: 3.4  Developing a new project in MCdevelop framework\n",
            "\u001b[35mSection_title: 4.2  Configuring for the use of ROOT package\n",
            "\u001b[35mSection_title: 5  Using KDevelop IDE\n",
            "\u001b[35mSection_title: 5.1  Importing the Project into KDevelop\n",
            "\u001b[35mSection_title: 5.2  Using KDevelop for compiling and running the Project\n",
            "\u001b[35mSection_title: 6  Future developments\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Tasks\n",
            "\u001b[35mSection_title: 2.1  Definition of Typical MRC Tasks\n",
            "\u001b[35mSection_title: 2.2  Discussion on MRC Tasks\n",
            "\u001b[35mSection_title: 2.2.1  Multi-modal MRC vs. Textual MRC\n",
            "\u001b[35mSection_title: 2.2.2  Machine Reading Comprehension vs. Question Answering\n",
            "\u001b[35mSection_title: 2.3  Classification of MRC Tasks\n",
            "\u001b[35mSection_title: 2.3.1  Existing Classification Methods of MRC tasks\n",
            "\u001b[35mSection_title: 2.3.2  A More Precise Classification Method\n",
            "\u001b[35mSection_title: 2.3.3  Type of Corpus\n",
            "\u001b[35mSection_title: 2.3.4  Type of Questions\n",
            "\u001b[35mSection_title: 2.3.5  Type of Answers\n",
            "\u001b[35mSection_title: 2.3.6  Source of Answers\n",
            "\u001b[35mSection_title: 2.3.7  Statistics of MRC Tasks\n",
            "\u001b[35mSection_title: 2.3.8  Mixed Tasks\n",
            "\u001b[35mSection_title: 2.3.9  Form of Task vs. Content of Task\n",
            "\u001b[35mSection_title: 3.1  Overview of Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.2  Accuracy\n",
            "\u001b[35mSection_title: 3.3  Exact Match\n",
            "\u001b[35mSection_title: 3.4  Precision\n",
            "\u001b[35mSection_title: 3.4.1  Token-level Precision\n",
            "\u001b[35mSection_title: 3.4.2  Question-level Precision\n",
            "\u001b[35mSection_title: 3.5  Recall\n",
            "\u001b[35mSection_title: 3.5.1  Token-level Recall\n",
            "\u001b[35mSection_title: 3.5.2  Question-level Recall\n",
            "\u001b[35mSection_title: 3.6  F1\n",
            "\u001b[35mSection_title: 3.6.2  Question-level F1\n",
            "\u001b[35mSection_title: 3.7  ROUGE\n",
            "\u001b[35mSection_title: 3.8  BLEU\n",
            "\u001b[35mSection_title: 3.9  Meteor\n",
            "\u001b[35mSection_title: 3.10  HEQ\n",
            "\u001b[35mSection_title: 3.11  Statistics of Evaluation Metrics\n",
            "\u001b[35mSection_title: 4  Benchmark Dataset\n",
            "\u001b[35mSection_title: 4.1  The Size of Datasets\n",
            "\u001b[35mSection_title: 4.2  The Generation Method of Datasets\n",
            "\u001b[35mSection_title: 4.3  The Source of Corpus\n",
            "\u001b[35mSection_title: 4.4  The Type of Context\n",
            "\u001b[35mSection_title: 4.5  The Availability of Datasets, Leaderboards and Baselines\n",
            "\u001b[35mSection_title: 4.6  Statistical Analysis\n",
            "\u001b[35mSection_title: 4.7  Prerequisite Skills\n",
            "\u001b[35mSection_title: 4.8  Citation Analysis\n",
            "\u001b[35mSection_title: 4.9  Characteristics of Datasets\n",
            "\u001b[35mSection_title: 4.9.1  Overview\n",
            "\u001b[35mSection_title: 4.9.2  MRC with Unanswerable Questions\n",
            "\u001b[35mSection_title: 4.9.3  Multi-hop Reading Comprehension\n",
            "\u001b[35mSection_title: 4.9.4  Multi-modal Reading Comprehension\n",
            "\u001b[35mSection_title: 4.9.5  Reading Comprehension Require Commonsense or World knowledge\n",
            "\u001b[35mSection_title: 4.9.6  Complex Reasoning MRC\n",
            "\u001b[35mSection_title: 4.9.7  Conversational Reading Comprehension\n",
            "\u001b[35mSection_title: 4.9.8  Domain-specific Datasets\n",
            "\u001b[35mSection_title: 4.9.9  MRC with Paraphrased Paragraph\n",
            "\u001b[35mSection_title: 4.9.10  Large-scale MRC Dataset\n",
            "\u001b[35mSection_title: 4.9.11  MRC dataset for Open-Domain QA\n",
            "\u001b[35mSection_title: 4.10  Descriptions of each MRC dataset\n",
            "\u001b[35mSection_title: 4.10.1  WikiQA\n",
            "\u001b[35mSection_title: 4.10.2  SQuAD 2.0\n",
            "\u001b[35mSection_title: 4.10.3  Natural Questions\n",
            "\u001b[35mSection_title: 4.10.5  DuoRC\n",
            "\u001b[35mSection_title: 4.10.6  Who-did-What\n",
            "\u001b[35mSection_title: 4.10.7  ARC\n",
            "\u001b[35mSection_title: 4.10.8  MCScript\n",
            "\u001b[35mSection_title: 4.10.9  OpenBookQA\n",
            "\u001b[35mSection_title: 4.10.10  ReCoRD\n",
            "\u001b[35mSection_title: 4.10.11  CommonSenseQA\n",
            "\u001b[35mSection_title: 4.10.12  WikiReading\n",
            "\u001b[35mSection_title: 4.10.13  WikiMovies\n",
            "\u001b[35mSection_title: 4.10.14  MovieQA\n",
            "\u001b[35mSection_title: 4.10.15  COMICS\n",
            "\u001b[35mSection_title: 4.10.16  TQA\n",
            "\u001b[35mSection_title: 4.10.17  RecipeQA\n",
            "\u001b[35mSection_title: 4.10.18  HotpotQA\n",
            "\u001b[35mSection_title: 4.10.19  NarrativeQA\n",
            "\u001b[35mSection_title: 4.10.20  Qangaroo\n",
            "\u001b[35mSection_title: 4.10.21  MultiRC\n",
            "\u001b[35mSection_title: 4.10.22  CNN/Daily Mail\n",
            "\u001b[35mSection_title: 4.10.24  MCTest\n",
            "\u001b[35mSection_title: 4.10.25  CuratedTREC\n",
            "\u001b[35mSection_title: 4.10.27  SearchQA\n",
            "\u001b[35mSection_title: 4.10.28  SciQ\n",
            "\u001b[35mSection_title: 4.10.29  CliCR\n",
            "\u001b[35mSection_title: 4.10.30  PaperQA (Hong et al.,2018)\n",
            "\u001b[35mSection_title: 4.10.32  ReviewQA\n",
            "\u001b[35mSection_title: 4.10.33  SciTail\n",
            "\u001b[35mSection_title: 4.10.34  DROP\n",
            "\u001b[35mSection_title: 4.10.35  Facebook CBT\n",
            "\u001b[35mSection_title: 4.10.36  Google MC-AFP\n",
            "\u001b[35mSection_title: 4.10.40  RACE\n",
            "\u001b[35mSection_title: 4.10.41  TriviaQA\n",
            "\u001b[35mSection_title: 4.10.45  CoQA\n",
            "\u001b[35mSection_title: 4.10.46  QuAC\n",
            "\u001b[35mSection_title: 4.10.47  ShARC\n",
            "\u001b[35mSection_title: 5  Open Issues\n",
            "\u001b[35mSection_title: 5.1  What needs to be improved?\n",
            "\u001b[35mSection_title: 5.1.1  Multi-modal MRC\n",
            "\u001b[35mSection_title: 5.1.2  Commonsense and World Knowledge\n",
            "\u001b[35mSection_title: 5.1.3  Complex Reasoning\n",
            "\u001b[35mSection_title: 5.1.4  Robustness\n",
            "\u001b[35mSection_title: 5.1.5  Interpretability\n",
            "\u001b[35mSection_title: 5.1.6  Evaluation of the quality of MRC datasets?\n",
            "\u001b[35mSection_title: 5.2.1  What is understanding?\n",
            "\u001b[35mSection_title: 5.2.2  Understanding from the perspective of cognitive neuroscience\n",
            "\u001b[32mconclusion: we conducted a comprehensive survey of recent efforts on the tasks, evaluation metrics and benchmark datasets of machine reading comprehension (mrc). we discussed the definition and taxonomy of mrc tasks, and proposed a new classification method for mrc tasks. the computing methods of different mrc evaluation metrics have been introduced with their usage in each type of mrc tasks also analyzed. we also introduced attributes and characteristics of mrc datasets, with 47 mrc datasets described in detail. finally, we discussed the open issues for future research of mrc and we argued that high-quality multi-modal mrc datasets and the research findings of cognitive neuroscience may help us find better ways to construct more challenging datasets and develop related mrc algorithms to achieve the ultimate goal of human-level machine reading comprehension.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Reasoning Performance of Neural Language Models\n",
            "\u001b[35mSection_title: 2.2  Dynamic Templating and Context Retrieval\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Synthesizing the ChainRuler Data\n",
            "\u001b[35mSection_title: 3.2  Generating Problem Elaborations\n",
            "\u001b[35mSection_title: 3.3  Predicting Answers\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 5  Analysis and Discussion\n",
            "\u001b[35mSection_title: 5.1  Do generated elaborations facilitate the application of the simple heuristic?\n",
            "\u001b[35mSection_title: 5.2  Do generated elaborations cohere with the problem to be solved?\n",
            "\u001b[35mSection_title: 5.3  Do generated elaborations resemble ideal elaborations?\n",
            "\u001b[35mSection_title: 5.4  Are piecemeal elaborations syntactically redundant and internally coherent?\n",
            "\u001b[32mconclusion: in this paper, we introduce chainruler, a dataset for multi-hop deductive argumentation, and assess gpt-2's zero-shot ability both to solve the inference tasks and to generate effective problem elaborations, i.e., texts which -once added to the context -improve performance. our main findings are:\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Energy Refactorings\n",
            "\u001b[35mSection_title: 2.1  ViewHolder: View Holder Candidates\n",
            "\u001b[35mSection_title: 2.2  DrawAllocation: Allocations within drawing code\n",
            "\u001b[35mSection_title: 2.3  WakeLock: Incorrect wake lock usage\n",
            "\u001b[35mSection_title: 3  Automatic Refactoring Tool\n",
            "\u001b[35mSection_title: 4  Empirical evaluation\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: our work shows the potential of using automatic refactoring tools to improve energy efficiency of mobile applications. we have analyzed 140 foss android apps and as an outcome we have fixed 222 energy related anti-patterns. in total, we improved the energy footprint of 45 apps.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  A\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  DEFINITIONS\n",
            "\u001b[35mSection_title: 2.1  Categorization\n",
            "\u001b[35mSection_title: 2.2  Task Comparison\n",
            "\u001b[35mSection_title: 2.3  Terminology Clarification\n",
            "\u001b[35mSection_title: 3  CONTRIBUTORS TO HALLUCINATION IN NLG\n",
            "\u001b[35mSection_title: 3.1  Hallucination from Data\n",
            "\u001b[35mSection_title: 3.2  Hallucination from Training and Inference\n",
            "\u001b[35mSection_title: 4  METRICS MEASURING HALLUCINATION\n",
            "\u001b[35mSection_title: 4.1  Statistical Metric\n",
            "\u001b[35mSection_title: 4.2  Model-based Metric\n",
            "\u001b[35mSection_title: 4.2.1  Information Extraction (IE)-based.\n",
            "\u001b[35mSection_title: 4.2.2  QA-based.\n",
            "\u001b[35mSection_title: 4.2.3  Natural Language Inference (NLI) Metrics.\n",
            "\u001b[35mSection_title: 4.2.4  Faithfulness Classification Metrics.\n",
            "\u001b[35mSection_title: 4.2.5  LM-based Metrics.\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[35mSection_title: 5  HALLUCINATION MITIGATION METHODS\n",
            "\u001b[35mSection_title: 5.1  Data-Related Methods\n",
            "\u001b[35mSection_title: 5.1.1  Building a Faithful Dataset.\n",
            "\u001b[35mSection_title: 5.1.2  Cleaning Data\n",
            "\u001b[35mSection_title: 5.1.3  Information Augmentation.\n",
            "\u001b[35mSection_title: 5.2  Modeling and Inference Methods\n",
            "\u001b[35mSection_title: 5.2.1  Architecture.\n",
            "\u001b[35mSection_title: 5.2.2  Training.\n",
            "\u001b[35mSection_title: 5.2.3  Post-Processing.\n",
            "\u001b[35mSection_title: 6  FUTURE DIRECTIONS\n",
            "\u001b[35mSection_title: 6.1  Future Directions in Metrics Design\n",
            "\u001b[35mSection_title: 6.2  Future Directions in Mitigation Methods\n",
            "\u001b[35mSection_title: 7  HALLUCINATION IN ABSTRACTIVE SUMMARIZATION\n",
            "\u001b[35mSection_title: 7.1  Hallucination Definition in Abstractive Summarization\n",
            "\u001b[35mSection_title: 7.2  Hallucination Metrics in Abstractive Summarization\n",
            "\u001b[35mSection_title: 7.2.1  Unsupervised Metrics.\n",
            "\u001b[35mSection_title: 7.2.2  Semi-Supervised\n",
            "\u001b[35mSection_title: 7.3  Hallucination Mitigation in Abstractive Summarization\n",
            "\u001b[35mSection_title: 7.3.2  Training Method.\n",
            "\u001b[35mSection_title: 7.3.3  Post-Processing\n",
            "\u001b[35mSection_title: 7.4  Future Directions in Abstractive Summarization\n",
            "\u001b[35mSection_title: 8  HALLUCINATION IN DIALOGUE GENERATION\n",
            "\u001b[35mSection_title: 8.1  Hallucination Definition in Dialogue Generation\n",
            "\u001b[35mSection_title: 8.2  Open-domain Dialogue Generation\n",
            "\u001b[35mSection_title: 8.2.1  Self-Consistency.\n",
            "\u001b[35mSection_title: 8.2.2  External\n",
            "\u001b[35mSection_title: 8.2.3  Hallucination Metrics. For generation-based dialogue systems, especially open-domain chatbots, the hallucination evaluation method remains an open problem\n",
            "\u001b[35mSection_title: 8.2.4  Mitigation Methods.\n",
            "\u001b[35mSection_title: 8.3  Task-oriented Dialogue Generation\n",
            "\u001b[35mSection_title: 8.3.1  Hallucination Metrics.\n",
            "\u001b[35mSection_title: 8.3.2  Mitigation\n",
            "\u001b[35mSection_title: 8.4  Future Directions in Dialogue Generation\n",
            "\u001b[35mSection_title: 9  HALLUCINATION IN GENERATIVE QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 9.1  Hallucination Definition in GQA\n",
            "\u001b[35mSection_title: 9.2  Hallucination-related Metrics in GQA\n",
            "\u001b[35mSection_title: 9.3  Hallucination Mitigation in GQA\n",
            "\u001b[35mSection_title: 9.4  Future Directions in GQA\n",
            "\u001b[35mSection_title: 10  HALLUCINATION IN DATA-TO-TEXT GENERATION\n",
            "\u001b[35mSection_title: 10.1  Hallucination Definition in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 10.2  Hallucination Metrics in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 10.3  Hallucination Mitigation in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 10.4  Future Directions in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 11  HALLUCINATIONS IN NEURAL MACHINE TRANSLATION\n",
            "\u001b[35mSection_title: 11.1  Hallucinations Definition and Categories in NMT\n",
            "\u001b[35mSection_title: 11.2  Hallucination Metrics in NMT\n",
            "\u001b[35mSection_title: 11.2.1  Statistical Metrics.\n",
            "\u001b[35mSection_title: 11.2.2  Model-Based Metrics.\n",
            "\u001b[35mSection_title: 11.3  Hallucination Mitigation Methods in NMT\n",
            "\u001b[35mSection_title: 11.3.1  Data-Related\n",
            "\u001b[35mSection_title: 11.3.2  Modeling and Inference.\n",
            "\u001b[35mSection_title: 11.4  Future Directions in NMT\n",
            "\u001b[35mSection_title: 12  HALLUCINATION IN VISION-LANGUAGE GENERATION\n",
            "\u001b[35mSection_title: 12.1  Object Hallucination in Image Captioning\n",
            "\u001b[35mSection_title: 12.2  Hallucination in Other VL Tasks\n",
            "\u001b[35mSection_title: 12.3  Future Directions in VL\n",
            "\u001b[32mconclusion: in this survey, we provide the first comprehensive overview of the hallucination problem in nlg, summarizing existing evaluation metrics, mitigation methods, and the remaining challenges for future research. hallucination is an artifact of neural-based nlg and is of concern because they appear fluent and can therefore be misleading to users. in some scenarios and tasks, hallucination can cause harm. we survey various contributors to hallucination, ranging from noisy data, erroneous parametric knowledge, incorrect attention mechanism, inappropriate training strategy, to inference exposure bias, etc. we show that there are two categories of hallucinations, namely intrinsic hallucination and extrinsic hallucination, and they need to be treated differently with diverse mitigation strategies. hallucination is relatively easy to detect in abstractive summarization and in nmt against the evidence in the source. for dialogue systems, it is important to balance diversity vs consistency in dialogue responses. hallucination in gqa and vl tasks is detrimental to the performance, but research on mitigation methods is still very preliminary in these areas. for datato-text generation, hallucination arises from the discrepancy between the input and output format. most methods to mitigate hallucinations in nmt either aim to reduce dataset noise or alleviate exposure bias. in the vl domain, models also generate unfaithful output given the visual scene, and recent works have mainly focused on the object hallucination problem. there remain many challenges ahead in identifying and mitigating hallucinations in nlg, and we hope research in this area can benefit from this survey.\n",
            "\u001b[35mSection_title: 13  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  DEFINITIONS\n",
            "\u001b[35mSection_title: 2.1  Categorization\n",
            "\u001b[35mSection_title: 2.2  Task Comparison\n",
            "\u001b[35mSection_title: 2.3  Terminology Clarification\n",
            "\u001b[35mSection_title: 3  CONTRIBUTORS TO HALLUCINATION IN NLG\n",
            "\u001b[35mSection_title: 3.1  Hallucination from Data\n",
            "\u001b[35mSection_title: 3.2  Hallucination from Training and Inference\n",
            "\u001b[35mSection_title: 4  METRICS MEASURING HALLUCINATION\n",
            "\u001b[35mSection_title: 4.1  Statistical Metric\n",
            "\u001b[35mSection_title: 4.2  Model-based Metric\n",
            "\u001b[35mSection_title: 4.2.1  Information Extraction (IE)-based.\n",
            "\u001b[35mSection_title: 4.2.2  QA-based.\n",
            "\u001b[35mSection_title: 4.2.3  Natural Language Inference (NLI) Metrics.\n",
            "\u001b[35mSection_title: 4.2.4  Faithfulness Classification Metrics.\n",
            "\u001b[35mSection_title: 4.2.5  LM-based Metrics.\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[35mSection_title: 5  HALLUCINATION MITIGATION METHODS\n",
            "\u001b[35mSection_title: 5.1  Data-Related Methods\n",
            "\u001b[35mSection_title: 5.1.1  Building a Faithful Dataset.\n",
            "\u001b[35mSection_title: 5.1.2  Cleaning Data\n",
            "\u001b[35mSection_title: 5.1.3  Information Augmentation.\n",
            "\u001b[35mSection_title: 5.2  Modeling and Inference Methods\n",
            "\u001b[35mSection_title: 5.2.1  Architecture.\n",
            "\u001b[35mSection_title: 5.2.2  Training.\n",
            "\u001b[35mSection_title: 5.2.3  Post-Processing.\n",
            "\u001b[35mSection_title: 6  FUTURE DIRECTIONS\n",
            "\u001b[35mSection_title: 6.1  Future Directions in Metrics Design\n",
            "\u001b[35mSection_title: 6.2  Future Directions in Mitigation Methods\n",
            "\u001b[35mSection_title: 7  HALLUCINATION IN ABSTRACTIVE SUMMARIZATION\n",
            "\u001b[35mSection_title: 7.1  Hallucination Definition in Abstractive Summarization\n",
            "\u001b[35mSection_title: 7.2  Hallucination Metrics in Abstractive Summarization\n",
            "\u001b[35mSection_title: 7.2.1  Unsupervised Metrics.\n",
            "\u001b[35mSection_title: 7.2.2  Semi-Supervised\n",
            "\u001b[35mSection_title: 7.3  Hallucination Mitigation in Abstractive Summarization\n",
            "\u001b[35mSection_title: 7.3.2  Training Method.\n",
            "\u001b[35mSection_title: 7.3.3  Post-Processing\n",
            "\u001b[35mSection_title: 7.4  Future Directions in Abstractive Summarization\n",
            "\u001b[35mSection_title: 8  HALLUCINATION IN DIALOGUE GENERATION\n",
            "\u001b[35mSection_title: 8.1  Hallucination Definition in Dialogue Generation\n",
            "\u001b[35mSection_title: 8.2  Open-domain Dialogue Generation\n",
            "\u001b[35mSection_title: 8.2.1  Self-Consistency.\n",
            "\u001b[35mSection_title: 8.2.2  External\n",
            "\u001b[35mSection_title: 8.2.3  Hallucination Metrics. For generation-based dialogue systems, especially open-domain chatbots, the hallucination evaluation method remains an open problem\n",
            "\u001b[35mSection_title: 8.2.4  Mitigation Methods.\n",
            "\u001b[35mSection_title: 8.3  Task-oriented Dialogue Generation\n",
            "\u001b[35mSection_title: 8.3.1  Hallucination Metrics.\n",
            "\u001b[35mSection_title: 8.3.2  Mitigation\n",
            "\u001b[35mSection_title: 8.4  Future Directions in Dialogue Generation\n",
            "\u001b[35mSection_title: 9  HALLUCINATION IN GENERATIVE QUESTION ANSWERING\n",
            "\u001b[35mSection_title: 9.1  Hallucination Definition in GQA\n",
            "\u001b[35mSection_title: 9.2  Hallucination-related Metrics in GQA\n",
            "\u001b[35mSection_title: 9.3  Hallucination Mitigation in GQA\n",
            "\u001b[35mSection_title: 9.4  Future Directions in GQA\n",
            "\u001b[35mSection_title: 10  HALLUCINATION IN DATA-TO-TEXT GENERATION\n",
            "\u001b[35mSection_title: 10.1  Hallucination Definition in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 10.2  Hallucination Metrics in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 10.3  Hallucination Mitigation in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 10.4  Future Directions in Data-to-Text Generation\n",
            "\u001b[35mSection_title: 11  HALLUCINATIONS IN NEURAL MACHINE TRANSLATION\n",
            "\u001b[35mSection_title: 11.1  Hallucinations Definition and Categories in NMT\n",
            "\u001b[35mSection_title: 11.2  Hallucination Metrics in NMT\n",
            "\u001b[35mSection_title: 11.2.1  Statistical Metrics.\n",
            "\u001b[35mSection_title: 11.2.2  Model-Based Metrics.\n",
            "\u001b[35mSection_title: 11.3  Hallucination Mitigation Methods in NMT\n",
            "\u001b[35mSection_title: 11.3.1  Data-Related\n",
            "\u001b[35mSection_title: 11.3.2  Modeling and Inference.\n",
            "\u001b[35mSection_title: 11.4  Future Directions in NMT\n",
            "\u001b[35mSection_title: 12  HALLUCINATION IN VISION-LANGUAGE GENERATION\n",
            "\u001b[35mSection_title: 12.1  Object Hallucination in Image Captioning\n",
            "\u001b[35mSection_title: 12.2  Hallucination in Other VL Tasks\n",
            "\u001b[35mSection_title: 12.3  Future Directions in VL\n",
            "\u001b[32mconclusion: in this survey, we provide the first comprehensive overview of the hallucination problem in nlg, summarizing existing evaluation metrics, mitigation methods, and the remaining challenges for future research. hallucination is an artifact of neural-based nlg and is of concern because they appear fluent and can therefore be misleading to users. in some scenarios and tasks, hallucination can cause harm. we survey various contributors to hallucination, ranging from noisy data, erroneous parametric knowledge, incorrect attention mechanism, inappropriate training strategy, to inference exposure bias, etc. we show that there are two categories of hallucinations, namely intrinsic hallucination and extrinsic hallucination, and they need to be treated differently with diverse mitigation strategies. hallucination is relatively easy to detect in abstractive summarization and in nmt against the evidence in the source. for dialogue systems, it is important to balance diversity vs consistency in dialogue responses. hallucination in gqa and vl tasks is detrimental to the performance, but research on mitigation methods is still very preliminary in these areas. for datato-text generation, hallucination arises from the discrepancy between the input and output format. most methods to mitigate hallucinations in nmt either aim to reduce dataset noise or alleviate exposure bias. in the vl domain, models also generate unfaithful output given the visual scene, and recent works have mainly focused on the object hallucination problem. there remain many challenges ahead in identifying and mitigating hallucinations in nlg, and we hope research in this area can benefit from this survey.\n",
            "\u001b[35mSection_title: 13  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  SciMRC\n",
            "\u001b[35mSection_title: 2.1  Data Preparation\n",
            "\u001b[35mSection_title: 2.2  Multi-perspective Collection\n",
            "\u001b[35mSection_title: 3  Analysis About SciMRC\n",
            "\u001b[35mSection_title: 3.1  Question types\n",
            "\u001b[35mSection_title: 3.2  Evidence Selection\n",
            "\u001b[35mSection_title: 3.3  Answer types\n",
            "\u001b[35mSection_title: 4  Modeling\n",
            "\u001b[35mSection_title: 4.1  Training objective\n",
            "\u001b[35mSection_title: 4.2  Metrics\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Results\n",
            "\u001b[35mSection_title: 5.2  Analysis\n",
            "\u001b[35mSection_title: 5.3  Case Study\n",
            "\u001b[35mSection_title: 6  Related work\n",
            "\u001b[32mconclusion: in this paper, we proposed a novel multiperspective scientific machine (smrc) reading comprehension dataset, called scimrc, with different perspectives of readers, including begin-ners, students and experts. extensive experimental results suggest the inner relations and differences among different perspectives, suggesting the importance of analyzing perspectives. the extensive results suggest that scimrc could serve as a testbed for evaluating smrc research.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Characteristics of Explanation Quality\n",
            "\u001b[35mSection_title: 2.1  Explanation Quality is User-dependent\n",
            "\u001b[35mSection_title: 2.2  Explanation Quality has (Orthogonal) Dimensions\n",
            "\u001b[35mSection_title: 3  Case Study on the HotpotQA Leaderboard\n",
            "\u001b[35mSection_title: 3.1  Task, Models and Automatic Evaluation\n",
            "\u001b[35mSection_title: 3.2  Human Evaluation\n",
            "\u001b[35mSection_title: 4  Shortcomings of Current Evaluations\n",
            "\u001b[35mSection_title: 4.1  Unvalidated Proxy Scores\n",
            "\u001b[35mSection_title: 4.1.1  Low Correlations Between Proxy Scores and Human Ratings\n",
            "\u001b[35mSection_title: 4.1.2  Proxy Scores Conflate Different Dimensions\n",
            "\u001b[35mSection_title: 4.1.3  Goodhart's Law: Validity Can Change Over Time\n",
            "\u001b[35mSection_title: 4.2  Neglecting Users\n",
            "\u001b[35mSection_title: 4.3  Single-score Leaderboards\n",
            "\u001b[35mSection_title: 5  Guidelines\n",
            "\u001b[35mSection_title: 5.1  Validate Proxy Scores Against Humans\n",
            "\u001b[35mSection_title: 5.2  Do Human Evaluation\n",
            "\u001b[35mSection_title: 5.3  Report Various Scores Without Averaging\n",
            "\u001b[32mconclusion: this paper aims at increasing the awareness of the shortcomings and open challenges that today's explanation quality evaluation practices face. we discuss general characteristics of explanation quality, describe current practices and point out to which extent they violate the discussed characteristics. we support our arguments with empirical evidence of a crowdsourced case study that we conducted for the example of explainable question answering systems from the hotpotqa leaderboard.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  UKP-SQuARE\n",
            "\u001b[35mSection_title: 3.1  Target Users and Scenarios\n",
            "\u001b[35mSection_title: 4  Multi-Agent Systems\n",
            "\u001b[35mSection_title: 4.1  Skill Selection\n",
            "\u001b[35mSection_title: 4.2  Early-Fusion of Skills\n",
            "\u001b[35mSection_title: 4.3  Late-Fusion of Skills\n",
            "\u001b[35mSection_title: 4.4  Comparison of Multi-Skill Models\n",
            "\u001b[32mconclusion: in this work, we have extended ukp-square to support multi-agent models. in particular, we deployed a routing system, tweac (geigle et al., 2021), a method to combine adapter weights, made (friedman et al., 2021), and a model that combines the prediction of multiple skills, metaqa (puerto et al., 2023). we have conducted experiments on these three models and unifiedqa (khashabi et al., 2020), a multi-dataset system, to analyze the trade-off between the performance, efficiency, and flexibility of these systems. we showed that in scenarios where new domains or expertise are often needed, metaqa provides the best tradeoff since its performance is close to the best model, it is compatible with any qa format, cheap to train, and its inference runtime is close to tweac and made using the parallel engine provided by ukp-square. however, when simple deployment is needed or the model is not expected to be updated, made and unifiedqa might be more appropriate.\n",
            "\u001b[35mSection_title: 5  Conclusions and Discussions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Does Prompting with Explanations Improve In-Context Learning?\n",
            "\u001b[35mSection_title: 2.1  Datasets\n",
            "\u001b[35mSection_title: 2.2  Baselines\n",
            "\u001b[35mSection_title: 2.3  Setup\n",
            "\u001b[35mSection_title: 2.4  Results\n",
            "\u001b[35mSection_title: 3  Can LLMs Generate Factual and Consistent Explanations?\n",
            "\u001b[35mSection_title: 3.1  Reliability of Explanations and Prediction Accuracy\n",
            "\u001b[35mSection_title: 4.1  Motivating Example: Improving SYNTH Dataset\n",
            "\u001b[35mSection_title: 4.2  Learning-based Calibration Framework\n",
            "\u001b[35mSection_title: 4.3  Calibrating E-SNLI\n",
            "\u001b[35mSection_title: 4.4  Calibrating ADVHOTPOT\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: caveats and risks of explanations from large language models our analysis suggests that llms' internal \"reasoning\" does not always align with explanations that it generates, as shown by our consistency results. more concerning, the explanations might not be factually grounded in the provided prompt. this shortcoming should caution against any deployment of this technology in practice: because the explanations are grammatical english and look very convincing, they may deceive users into believing the system's responses even when those responses are incorrect. section 6 of bender et al. (2021) discusses these risks in additional detail. the fact that language models can hallucinate explanations is also found in other work (zhou and tan, 2021). this result is unsurprising in some sense: without sufficient supervision or grounding, language models do not learn meaning as distinct from form (bender and koller, 2020), so we should not expect their explanations to be strongly grounded.\n",
            "\u001b[35mSection_title: 6  Discussion & Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Cognitive Origins of Tool Use\n",
            "\u001b[35mSection_title: 2.2  Tool Categorization: A User-Interface Perspective\n",
            "\u001b[35mSection_title: 2.2  Tool Categorization: A User-Interface Perspective\n",
            "\u001b[35mSection_title: 2.3  Paradigm Shift of Foundation Models\n",
            "\u001b[35mSection_title: 2.4  Complementary Roles of Tools and Foundation Models\n",
            "\u001b[35mSection_title: 3  Tool Learning\n",
            "\u001b[35mSection_title: 3.1  Components of Tool Learning\n",
            "\u001b[35mSection_title: 3.1.1  Understanding the Components\n",
            "\u001b[35mSection_title: 3.1.2  Connecting the Components\n",
            "\u001b[35mSection_title: 3.2  The General Procedure: From Intent to Plan\n",
            "\u001b[35mSection_title: 3.2.1  Understanding Intent and Tools\n",
            "\u001b[35mSection_title: 3.2.2  Planning with Reasoning\n",
            "\u001b[35mSection_title: 3.2.2  Planning with Reasoning\n",
            "\u001b[35mSection_title: 3.3  Training Models for Improved Tool Learning\n",
            "\u001b[35mSection_title: 3.3  Training Models for Improved Tool Learning\n",
            "\u001b[35mSection_title: 3.3.1  Learning from Demonstrations\n",
            "\u001b[35mSection_title: 3.3.2  Learning from Feedback\n",
            "\u001b[35mSection_title: 3.3.3  Generalizable Tool Learning\n",
            "\u001b[35mSection_title: 4  Application and Experiment\n",
            "\u001b[35mSection_title: 4.1  Evaluated Tools\n",
            "\u001b[35mSection_title: 4.2  Experiments\n",
            "\u001b[35mSection_title: 4.2  Experiments\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Safe and Trustworthy Tool Learning\n",
            "\u001b[35mSection_title: 5.2  Tool Learning for Large Complex Systems\n",
            "\u001b[35mSection_title: 5.2  Tool Learning for Large Complex Systems\n",
            "\u001b[35mSection_title: 5.3  From Tool User to Tool Maker: AI's Evolutionary Role\n",
            "\u001b[35mSection_title: 5.4  From General Intelligence to Personalized Intelligence\n",
            "\u001b[35mSection_title: 5.5  Tool Learning and Embodied Learning\n",
            "\u001b[35mSection_title: 5.6  Knowledge Conflicts in Tool Augmentation\n",
            "\u001b[35mSection_title: 5.6  Open Problems\n",
            "\u001b[35mSection_title: 5.7  Open Problems\n",
            "\u001b[35mSection_title: 5.6  Open Problems\n",
            "\u001b[35mSection_title: 5.7  Open Problems\n",
            "\u001b[32mconclusion: this paper studies the paradigm of tool learning with foundation models. we first recapitulate the cognitive origins of tool use in human history and categorize tools from the perspective of the user interface. then we review the ai paradigm shift brought about by foundation models and discuss the complementary roles of tools and foundation models. we perform a comprehensive literature review for existing exploration in tool learning and start with formulating a general tool learning framework. then we highlight core research problems such as bridging user intents with appropriate tools, better planning by leveraging the reasoning abilities of foundation models, training strategies for tool learning, and how to facilitate generalization for tool learning. finally, we discuss important research topics, including safe and trustworthy tool learning, tool learning for large complex systems, ai tool creation, personalized tool learning, embodied tool learning, knowledge conflict issue in tool augmentation, etc. in general, this paper serves as a systematic investigation of tool learning. we hope this paper could facilitate research in integrating tools with foundation models in the future.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  NLU Benchmark\n",
            "\u001b[35mSection_title: 2.2  FOL Reasoning Benchmark\n",
            "\u001b[35mSection_title: 3  Task Definition\n",
            "\u001b[35mSection_title: 4  LogicNLI\n",
            "\u001b[35mSection_title: 4.1  Overview\n",
            "\u001b[35mSection_title: 4.2  Dataset Gerneration and Statistics\n",
            "\u001b[35mSection_title: 4.3  Diagnosis\n",
            "\u001b[35mSection_title: 4.4  Degraded LogicNLI\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Settings\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Overall Diagnosis\n",
            "\u001b[35mSection_title: 5.4  Analysis of Each FOLs\n",
            "\u001b[35mSection_title: 5.5  Analysis of \"Paradox\"\n",
            "\u001b[35mSection_title: 5.6  Discussion\n",
            "\u001b[32mconclusion: in this paper, we propose a diagnostic method to diagnose lms' fol reasoning ability. this method introduces a novel proposed benchmark, logicnli, that disentangles the fol reasoning from commonsense inference. specifically, it includes four evaluations to measure the fol reasoning ability from different perspectives. results on three lms show that although some lms (roberta) own a certain interpretable fol reasoning ability, they still cannot make sensible fol reasoning like humans. detailed analysis motivates us to enhance specific reasoning abilities or explore new methods to make neural models understand more refined logic.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  PROBLEM DEFINITION\n",
            "\u001b[35mSection_title: 3  SELF-SUPERVISED ENTITY ALIGNMENT\n",
            "\u001b[35mSection_title: 3.1  The SelfKG Framework\n",
            "\u001b[35mSection_title: 3.2  Relative Similarity Metric\n",
            "\u001b[35mSection_title: 3.3  Self-Negative Sampling\n",
            "\u001b[35mSection_title: 3.4  Multiple Negative Queues\n",
            "\u001b[35mSection_title: 4.2  Ablation Study\n",
            "\u001b[35mSection_title: 4.3  SelfKG v.s. Supervised SelfKG\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[32mconclusion: in this work, we re-examine the use and effect of supervision in the entity alignment problem, which targets aligning entities with identical meanings across different knowledge graphs.based on the three insights we derive-uni-space learning, relative similarity metric, and self-negative sampling, we develop a self-supervised entity alignment algorithm-selfkg-to automatically align entities without training labels.the experiments on two widely-used benchmarks dwy100k and dbp15k show that selfkg is able to beat or match most of the supervised alignment methods which leverage the 100% of the training datasets.our discovery indicates a huge potential to get rid of supervision in the entity alignment problem, and more studies are expected for a deeper understanding of self-supervised learning.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  Construction of ReasonGraphQA\n",
            "\u001b[35mSection_title: 4.1  Question-related Triples Finding\n",
            "\u001b[35mSection_title: 4.2  Textual Facts Generation\n",
            "\u001b[35mSection_title: 4.3  Textual Database Construction\n",
            "\u001b[35mSection_title: 4.4  Evidence Graph Generation\n",
            "\u001b[35mSection_title: 4.6  Quality Evaluation\n",
            "\u001b[35mSection_title: 5  Methods\n",
            "\u001b[35mSection_title: 5.1  Bidirectional Retrieval\n",
            "\u001b[35mSection_title: 5.2  Subgraph Reconstruction\n",
            "\u001b[35mSection_title: 5.3  Answer Generation\n",
            "\u001b[35mSection_title: 6  Experimental\n",
            "\u001b[35mSection_title: 6.1  Compared Baselines\n",
            "\u001b[35mSection_title: 6.2  Implementation\n",
            "\u001b[35mSection_title: 6.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 6.4  Results and Analysis\n",
            "\u001b[35mSection_title: 6.5  Ablation Study\n",
            "\u001b[32mconclusion: our study introduces the reasongraphqa dataset, the first textual database qa dataset with an explanation graph, which provides complex structured retrieval assistance for graph retrieval systems.we have tested various traditional evidence retrieval methods on the reasongraphqa dataset and evaluated them manually.additionally, we propose the graph-hop retrieval paradigm and develop a bidirectional graph retrieval model, which significantly improves the evidence retrieval and graph construction capabilities of complex question answering by reconstructing reasoning paths in different directions.future research utilizing the rea-songraphqa dataset can enable fine-grained analysis of the explanation graph output from models, leading to further advancements in real and complex qa environments.while the current methods have several limitations, this presents opportunities for future research to improve upon them.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.3  Complex QA in other domains\n",
            "\u001b[35mSection_title: 3  Data Collection\n",
            "\u001b[35mSection_title: 3.1  Knowledge Base Selection\n",
            "\u001b[35mSection_title: 3.2  Question Generation\n",
            "\u001b[35mSection_title: 3.2.1  Locating Stage\n",
            "\u001b[35mSection_title: 3.2.2  Asking Stage\n",
            "\u001b[35mSection_title: 3.2.3  SPARQLs\n",
            "\u001b[35mSection_title: 3.2.4  Functions and Programs\n",
            "\u001b[35mSection_title: 3.3  Question Paraphrasing\n",
            "\u001b[35mSection_title: 4  Dataset Analysis\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[32mconclusion: we introduce kqa pro, a new benchmark of complex kbqa with following features: 1) with explicit reasoning process, including sparqls and programs; 2) large-scale, with about 120k natural questions; 3) with rich kinds of knowledge, including relational, literal, and high-level. we create a unified codebase to implement the baselines and state-of-the-arts of complex kbqa. extensive experiments reveal a huge gap between machines and humans, demonstrating that kqa pro is very challenging. kqa pro is the first kbqa benchmark that provides the explicit reasoning process for complex questions. we hope that these additional information can help machines develop the compositional reasoning ability and learn to tackle complex questions like a human.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Bi-ZsRE\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Data Statistics\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results & Analyses\n",
            "\u001b[35mSection_title: 4.3  The Influence of Language Mismatch\n",
            "\u001b[32mconclusion: in this paper, we first explore the cross-lingual effect of knowledge editing. to achieve that, we automatically construct bi-zsre dataset by translating the previous zsre dataset from english to chinese. based on bi-zsre, we conduct experiments on various knowledge editing methods and multilingual llms, and study the cross-lingual effect from english to chinese and vice versa. our results indicate that (1) the language modeling gaps of different languages might influence the efficiency of knowledge editing in different languages; (2) it is still hard for existing knowledge editing methods to transfer the edited knowledge from one language to another in a multi-lingual llm; (3) when editing llms in a language, the locality in the other languages could also be influenced. we also analyze the inconsistent behaviors of the edited models and discuss their specific challenges to provide a deeper understanding of the cross-lingual effect in knowledge editing.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  CS1QA Dataset\n",
            "\u001b[35mSection_title: 3.1  Data Source\n",
            "\u001b[35mSection_title: 3.2  Question Type Categorization\n",
            "\u001b[35mSection_title: 3.3  Collecting Question-Answer Pairs with Question Types\n",
            "\u001b[35mSection_title: 3.4  Selecting Code Lines\n",
            "\u001b[35mSection_title: 3.5  Quality Control and Validation\n",
            "\u001b[35mSection_title: 4  Task Definition\n",
            "\u001b[35mSection_title: 5.1  Text Lengths\n",
            "\u001b[35mSection_title: 5.2  Question Type Distribution\n",
            "\u001b[35mSection_title: 5.3  Code Line Distribution\n",
            "\u001b[35mSection_title: 5.4  Machine Translation Quality\n",
            "\u001b[35mSection_title: 6  Experimental Setup\n",
            "\u001b[35mSection_title: 7  Results\n",
            "\u001b[35mSection_title: 7.1  Type Classification\n",
            "\u001b[35mSection_title: 7.2  Code Line Selection\n",
            "\u001b[35mSection_title: 7.3  Answer Retrieval\n",
            "\u001b[35mSection_title: 7.4  Qualitative Analysis\n",
            "\u001b[32mconclusion: in this paper, we present cs1qa, a dataset for codebased question answering in introductory programming course. cs1qa's crowdsourced data from a programming course provide rich information that code understanding models need to consider to correctly answer the given questions. we introduce three tasks for cs1qa, whose output can help students debug and reduce workloads for the teaching staff. results from the baseline models indicate that tasks for cs1qa are challenging for current language understanding models. cs1qa promotes further research to better represent and understand source code for code-based question answering.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[35mSection_title: 9  Ethical Consideration\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.2  Grounding via according-to Prompting\n",
            "\u001b[35mSection_title: 3.3  Datasets\n",
            "\u001b[35mSection_title: 3.4  Models and Prompting\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Results from Other Models\n",
            "\u001b[35mSection_title: 4.2  Impact of Model Size\n",
            "\u001b[35mSection_title: 4.3  Impact of Instruction Tuning\n",
            "\u001b[35mSection_title: 1.  Question (ELI5)\n",
            "\u001b[35mSection_title: 2.  Question (NQ)\n",
            "\u001b[35mSection_title: 3.  Question (ELI5)\n",
            "\u001b[35mSection_title: 4.  Question (TriviaQA) According to Smokey The Bear, Remember... only you can what?\n",
            "\u001b[35mSection_title: 5.  Question (HotpotQA)\n",
            "\u001b[35mSection_title: 4.4  Impact of Entity Popularity\n",
            "\u001b[35mSection_title: 4.5  Qualitative Examples\n",
            "\u001b[35mSection_title: 5  Discussion and Future Implications\n",
            "\u001b[32mconclusion: large language models struggle with hallucination, or generating incorrect information, despite the large amount of factual pre-training data they were trained on. to help alleviate this problem, we proposed according-to prompts, asking language models to ground their output to their pretraining corpus. to quantify the extent to which models achieve this goal, we introduced a new metric, quip-score, that efficiently and quickly measures the percent of the model's generation that exists as exact quotes in the pre-training corpus. we showed that prompting models with grounding prompts greatly improves the quip-score while anti-grounding prompts reduces the quip-score. our analysis also shows that quip-score increases with instruction-tuning, popularity of the entity in the question, and model size. we hope that this work brings more attention to the positive aspects of llm memorization and encourages more work into understanding how and when language model output is grounded to its pre-training data.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[32mlimitation: our proposed metric only accounts for exact lexical match and will miss other types of grounded statements -thus we view quip-score as a lower bound on grounding where grounding is defined only by quoting from source material.we also recognize the possibility of a discrepancy between the pre-training data of private models like chatgpt and the wikipedia version we use for analysis, due to limited information on their pretraining. however, this might not be a significant concern, as although wikipedia is not completely static, a substantial part of the information in this knowledge source remains consistent over a short span of years. furthermore, our results with chat-gpt are similar compared with models for which we do have the exact pre-training data (like gpt-j).\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Formulation\n",
            "\u001b[35mSection_title: 3.1  Relevance Estimator\n",
            "\u001b[35mSection_title: 3.2  Evidence Estimator\n",
            "\u001b[35mSection_title: 3.3  Centrality Estimator\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Automatic Evaluation\n",
            "\u001b[35mSection_title: 6  Human Evaluation\n",
            "\u001b[32mconclusion: in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization. we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments. experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more relevant and less redundant. we have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query. in the future, we would like to generate abstractive summaries following an unsupervised approach (baziotis et al., 2019;chu and liu, 2019) and investigate how recent advances in open domain qa qi et al., 2019) can be adapted for query focused summarization.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Comparison With Existing Datasets\n",
            "\u001b[35mSection_title: 2.1  Implicit Relation Decomposition\n",
            "\u001b[35mSection_title: 3  Multihop Question Collection\n",
            "\u001b[35mSection_title: 3.1  Input Facts\n",
            "\u001b[35mSection_title: 3.2  Baseline QA Systems\n",
            "\u001b[35mSection_title: 3.3  Question Validation\n",
            "\u001b[35mSection_title: 4  Challenges\n",
            "\u001b[35mSection_title: 4.1  Retrieval Challenges\n",
            "\u001b[35mSection_title: 4.2  Reasoning Challenges\n",
            "\u001b[35mSection_title: 5  Question Answering Model\n",
            "\u001b[35mSection_title: 5.1  Retrieval: Two-step IR\n",
            "\u001b[35mSection_title: 5.2  Reasoning: BERT Models\n",
            "\u001b[35mSection_title: 5.3  Results on Crowd-Sourced Questions\n",
            "\u001b[35mSection_title: 6  Adversarial Choice Generation\n",
            "\u001b[35mSection_title: 6.1  Distractor Options\n",
            "\u001b[35mSection_title: 6.2  Multi-Adversary Choice Selection\n",
            "\u001b[35mSection_title: 6.3  Evaluating Dataset Difficulty\n",
            "\u001b[35mSection_title: 6.4  QASC Dataset\n",
            "\u001b[35mSection_title: 7  Experiments\n",
            "\u001b[32mconclusion: we present qasc, the first qa dataset for multi-hop reasoning beyond a single paragraph where two facts needed to answer a question are annotated for training, but questions cannot be easily syntactically decomposed into these facts. instead, models must learn to retrieve and compose candidate pieces of knowledge. qasc is generated via a crowdsourcing process, and further enhanced via multi-adversary distractor choice selection. state-of-the-art bert models, even with massive fine-tuning on over 100k questions from previous relevant datasets and using our proposed two-step retrieval, leave a large margin to human performance levels, thus making qasc a new challenge for the community.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approaches\n",
            "\u001b[35mSection_title: 3.1  Preprocessing\n",
            "\u001b[35mSection_title: 3.2  Supergraph Construction\n",
            "\u001b[35mSection_title: 3.3  Encoder\n",
            "\u001b[35mSection_title: 3.3.1  Context Encoder\n",
            "\u001b[35mSection_title: 3.3.2  Super Graph Encoder\n",
            "\u001b[35mSection_title: 3.3.3  Question-Option-aware Interaction\n",
            "\u001b[35mSection_title: 3.4  Hierarchical Decoder\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Ablation Study\n",
            "\u001b[35mSection_title: 5.2  Effects of Fact Units Numbers\n",
            "\u001b[35mSection_title: 5.3  Interpretability: a Case Study\n",
            "\u001b[32mconclusion: in this work, we propose a novel method named focal reasoner for logical reasoning in the machine reading comprehension task. our method not only better uncovers the logical structures within the context, which can be a general method for other sophisticated reasoning tasks but also better captures the logical interactions between context and options. the experimental results verify the effectiveness of our method. in the future, we intend to design more elaborate mechanisms to cope with different question types and logical types as well as combine the symbolic and neural approaches.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Sequence Analysis and Model Development\n",
            "\u001b[35mSection_title: 2.2.  Transcription Factor Binding Factor Sites Prediction\n",
            "\u001b[35mSection_title: 2.3.  Structural Modelling\n",
            "\u001b[35mSection_title: 2.4.  Mammalian Cell Transfection and Expression of tbx22 Proteins\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Sequence Identification\n",
            "\u001b[35mSection_title: 3.2.  Transcription Factor Binding Site Prediction\n",
            "\u001b[35mSection_title: 3.3.  T-Box Genes and Brachury Gene Matrix Family\n",
            "\u001b[35mSection_title: 3.4.  Protein Homology Modeling\n",
            "\u001b[35mSection_title: 3.5.  Tbx22 Protein Expression in Mammalian Cell System\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[32mconclusion: protein prediction methods have allowed biologists to identify protein structure, functionality, orientation and other important general information.available knowledge of existing proteins can guide us with a template to approach a solution to unknown structures and function.uncharacterized proteins can now be defined with the advantage of multiple public databases, with increased set of established data sources combined with bioinformatics tools.as more tools are accessed computational biologists can unravel mysteries of unidentified/unpredicted biological molecules to solve known problems leading to advancements and new ideas.the goal of this paper was to show the sequence similarity between t-box22 of 2 different species and show how they are sequentially and structurally similar to each other.supporting documentation available from databases and the usage of the right tools has shown enough evidence to prove that the zf tbx22 tf shares functional homology to the human tbx22 tf.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Answer uncertainty\n",
            "\u001b[35mSection_title: 2.2  Unanswerability\n",
            "\u001b[35mSection_title: 3  Uncertainty\n",
            "\u001b[35mSection_title: 4  Data and Experimental Set-Up\n",
            "\u001b[35mSection_title: 4.1  Training and evaluation data\n",
            "\u001b[35mSection_title: 4.2  Data construction\n",
            "\u001b[35mSection_title: 4.3  Performance criteria\n",
            "\u001b[35mSection_title: 5  Results and Discussion\n",
            "\u001b[35mSection_title: 5.1  Baseline results\n",
            "\u001b[35mSection_title: 5.2  Answer uncertainty\n",
            "\u001b[35mSection_title: 5.3  Unanswerability\n",
            "\u001b[32mconclusion: this paper addresses answer uncertainty and unanswerability in multiple-choice mrc. measures of answer uncertainty are required to identify examples that the system may struggle to get correct and hence should abstain from answering such questions. unanswerability detection is required for when the answer cannot be deduced using the information provided. an electra prlm achieve competitive results on the default reclor dataset, achieving up to 67.1% accuracy on the evaluation split. ensemble-based predictive uncertainty measures are explored for both modes of operation: answer uncertainty for negative marking schemes and the presence of unanswerability. it is shown that uncertainty in the prediction such as expected entropy is correlated with the error rate of the mrc system allowing better than vanilla performance with an aggressive negative marking scheme for reclor and race. interestingly, it is found that expected entropy from the predictions of an implicitly trained system is competitive at unanswerability detection and is able to out-compete map decoding from an explicitly trained system that has been trained with unanswerable examples for reclor.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 7  Acknowledgements\n",
            "\u001b[35mSection_title: 8  Ethics\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Explanation Reconstruction as a Ranking Problem\n",
            "\u001b[35mSection_title: 3.  Selecting the top k elements belonging to\n",
            "\u001b[35mSection_title: 3.1  Modelling Explanatory Relevance\n",
            "\u001b[35mSection_title: 4  Empirical Evaluation\n",
            "\u001b[35mSection_title: 4.1  Explanation Reconstruction\n",
            "\u001b[35mSection_title: 4.2  Explanation Analysis\n",
            "\u001b[35mSection_title: 4.3  Qualitative analysis.\n",
            "\u001b[35mSection_title: 4.4  Question Answering\n",
            "\u001b[32mconclusion: this paper proposed a novel framework for multihop explanation reconstruction based on explanatory unification. an extensive evaluation on the worldtree corpus led to the following conclusions:\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 2.  k = 100\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Contemporary Challenges in\n",
            "\u001b[35mSection_title: 3  Task Description\n",
            "\u001b[35mSection_title: 4  Training and Evaluation Dataset\n",
            "\u001b[35mSection_title: 5  Shared Task Online Competition Setup\n",
            "\u001b[35mSection_title: 7  Extended Evaluation and Analysis\n",
            "\u001b[35mSection_title: 7.1  Performance by Lexical Overlap / Multiple Hops\n",
            "\u001b[35mSection_title: 7.2  Manual Evaluation of Explanation Quality\n",
            "\u001b[35mSection_title: 7.3  Additional Performance Evaluation\n",
            "\u001b[35mSection_title: 7.3.1  Performance by Explanatory Roles\n",
            "\u001b[35mSection_title: 7.3.2  Performance by Table Knowledge Types\n",
            "\u001b[32mconclusion: the textgraphs 2019 shared task on multi-hop inference for explanation regeneration received four team submissions that exceeded the performance of the baseline system. the systems used a variety of methods from additional knowledge resources (such as conceptnet or framenet) to directly training language models to perform multihop inference by predicting chains of facts. the top-performing system increased baseline performance by nearly a factor of two on this task, achieving a new state-of-the-art.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[35mSection_title: 9  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary\n",
            "\u001b[35mSection_title: 2.1  Test-time Adaptation for Question Answering\n",
            "\u001b[35mSection_title: 2.2  The Risk of TTA Leading to Model Collapse\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Entropy Minimization with Source Constraints\n",
            "\u001b[35mSection_title: 3.2  Efficient Side Block\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets and Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 5.1  Effects of the Learning Rate\n",
            "\u001b[35mSection_title: 5.2  Effects of α\n",
            "\u001b[35mSection_title: 5.3  Effects of Warmup Data\n",
            "\u001b[35mSection_title: 5.5  Applications in Generative QA\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we attempt to improve the robustness of qa models by testing time adaptation (tta) but find that tta causes the models collapse.we thoroughly investigate why previous tta methods cause the model collapse and find that the imbalanced label distribution is the main reason.we address this problem by adding constraints between the source and adapted model during the tta process.we also design an efficient side block to speed up the inference time.sufficient experimental results show that our proposed method is effective and efficient, making tta a big step closer to being applied in real-world scenarios.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The HaluEval Benchmark\n",
            "\u001b[35mSection_title: 2.1  Automatic Generation\n",
            "\u001b[35mSection_title: 2.2  Human Annotation\n",
            "\u001b[35mSection_title: 2.3  Benchmark Analysis and Usage\n",
            "\u001b[35mSection_title: 3.2  Results and Analysis\n",
            "\u001b[35mSection_title: 3.2.1  Hallucination Recognition\n",
            "\u001b[35mSection_title: 3.2.2  Improvement Strategies\n",
            "\u001b[35mSection_title: 3.3  Case Study\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we introduce halueval, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing hallucinations.to automatically generate large-scale samples, we propose a two-step approach, i.e., sampling-then-filtering.we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one.besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.we find that, existing llms mostly fail to recognize the hallucinations in text and tend to generate hallucinated content.finally, we suggest several strategies to help llms recognize hallucinations.our benchmark can facilitate research in understanding what types of content and to which extent llms tend to hallucinate, ultimately paving the way for building more effective and reliable llms in the future.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: in our approach, we leverage a llm, i.e., chatgpt, to automatically generate the hallucinated samples.therefore, the quality of our hallucinated samples is limited by the capacity of chatgpt in following the complex instruction of hallucination sampling.although we design the high-quality hallucination filtering process, it is still necessary to apply quality control to the generation of hallucinated samples.besides, our benchmark focuses on evaluating the ability of llms in recognizing the hallucinations in text but does not investigate the underlying reasons behind the appearance of hallucinations like prior work (zheng et al., 2023;das et al., 2023).as for the potential issue, since the hallucinated samples in our benchmark looks highly similar to the ground-truth samples, which might be misused for an unexpected purpose than we planned.to alleviate this issue, we should monitor and regulate the spread and usage of our benchmark.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background and Related Work 2.1 Large Pretrained Models\n",
            "\u001b[35mSection_title: 2.2  ChatGPT\n",
            "\u001b[35mSection_title: 2.3  LLM benchmark and evaluation\n",
            "\u001b[35mSection_title: 3.1  Evaluating the Multitask Ability of ChatGPT\n",
            "\u001b[35mSection_title: 3.1.2  ChatGPT on Dialogue Tasks\n",
            "\u001b[35mSection_title: 3.2  Evaluating Multilinguality of ChatGPT\n",
            "\u001b[35mSection_title: 3.2.1  Language Understanding\n",
            "\u001b[35mSection_title: 3.2.2  Language Generation\n",
            "\u001b[35mSection_title: 3.3  Evaluating Multimodality of ChatGPT\n",
            "\u001b[35mSection_title: 3.3.1  Flag Drawing Task\n",
            "\u001b[35mSection_title: 3.3.2  Findings\n",
            "\u001b[35mSection_title: 4  Reasoning Evaluations of ChatGPT\n",
            "\u001b[35mSection_title: 4.1  Logical Reasoning\n",
            "\u001b[35mSection_title: 4.1.1  Deductive vs. Inductive Reasoning\n",
            "\u001b[35mSection_title: 4.1.2  Abductive Reasoning\n",
            "\u001b[35mSection_title: 4.2  Non-textual semantic reasoning\n",
            "\u001b[35mSection_title: 4.3  Commonsense Reasoning\n",
            "\u001b[35mSection_title: 4.4  Causal, Multi-Hop, and Analogical Reasoning\n",
            "\u001b[35mSection_title: 5.1  Factuality in ChatGPT\n",
            "\u001b[35mSection_title: 5.2  Hallucination in ChatGPT\n",
            "\u001b[35mSection_title: 6  Evaluating Interactivity in ChatGPT\n",
            "\u001b[35mSection_title: 6.1  Interactivity on Summarization\n",
            "\u001b[35mSection_title: 6.2  Interactivity on Machine Translation\n",
            "\u001b[35mSection_title: 6.3  Interactivity on Multimodal Generation\n",
            "\u001b[32mconclusion: multitask, multilingual, multimodalchatgpt outperforms multiple state-of-the-art zero-shot llms on various tasks and even surpasses fine-tuned models on some tasks. although chatgpt performs well in most of the tasks, there are still some failure cases on each task ( §3.1). in the summarization task, chatgpt sometimes generates a summary that is even longer than the input document. in the machine translation task, chatgpt sometimes produces an incorrect translation for some words, making the meaning slightly shifted. therefore, dealing with these special cases is a complex but important task.\n",
            "\u001b[35mSection_title: 7  Conclusion and Discussion\n",
            "\u001b[35mSection_title: 7.1  Multitask, Multilingual, Multimodal\n",
            "\u001b[35mSection_title: 7.2  Reasoning\n",
            "\u001b[35mSection_title: 7.3  Factuality and Hallucinations\n",
            "\u001b[35mSection_title: 7.4  Interactivity\n",
            "\u001b[35mSection_title: 7.5  Responsible Generative AI\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Generating Natural Language Proofs\n",
            "\u001b[35mSection_title: 4  Our Method: NLProofS\n",
            "\u001b[35mSection_title: 4.1  Stepwise Prover\n",
            "\u001b[35mSection_title: 4.2  Verifier\n",
            "\u001b[35mSection_title: 4.3  Proof Search\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Proof Generation on EntailmentBank\n",
            "\u001b[35mSection_title: 5.3  Generating Answers and Proofs on RuleTaker\n",
            "\u001b[35mSection_title: 6  Analyses\n",
            "\u001b[35mSection_title: 6.1  Ablation Studies\n",
            "\u001b[35mSection_title: 6.2  NLProofS with Oracles\n",
            "\u001b[35mSection_title: 6.3  Impact of Proof Length\n",
            "\u001b[35mSection_title: 6.4  Reduced Hallucination\n",
            "\u001b[32mconclusion: we have introduced nlproofs for stepwise proof generation in natural language. it learns to generate relevant proof steps conditioning on the hypothesis. to prevent hallucination, nlproofs searches for proofs that maximize a validity score judged by a verifier. our method has achieved state-of-the-art performance on en-tailmentbank and ruletaker, demonstrating the promise of stepwise proof generation for human-authored proofs. in the future, we hope to see increasing applications of verifiers and proof search in various reasoning tasks.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Existing data sets\n",
            "\u001b[35mSection_title: 2.1  Short-Context Question Answering\n",
            "\u001b[35mSection_title: 2.2  Long-Context Question Answering\n",
            "\u001b[35mSection_title: 3  Data Set Design\n",
            "\u001b[35mSection_title: 3.1  Data Curation\n",
            "\u001b[35mSection_title: 3.2  Data Set Statistics\n",
            "\u001b[35mSection_title: 4  Baseline Models\n",
            "\u001b[35mSection_title: 4.1  BERT and RoBERTa\n",
            "\u001b[35mSection_title: 4.2  Longformer\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 5.2  Results and Discussion\n",
            "\u001b[35mSection_title: 5.3  Human Evaluation\n",
            "\u001b[35mSection_title: 5.4  Error Analysis\n",
            "\u001b[32mconclusion: we introduce nlquad, a non-factoid long question answering data set from bbc news articles. nlquad's question types and the long lengths of its context documents as well as answers, make it a challenging real-world task. we propose to use intersection over union (iou) as an evaluation metric for long question answering. to establish a baseline performance, we experimented with the bert, roberta, and longformer question answering models. longformer outperforms the other methods with an iou of 73.57%, but the results show that the performance of state-of-the-art question answering systems is far from perfect. we hope nlquad will inspire more research in the area of document-level language understanding and question answering.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Example\n",
            "\u001b[35mSection_title: 3  Related Work\n",
            "\u001b[35mSection_title: 4  The Meaning-Language Interface\n",
            "\u001b[35mSection_title: 4.1  Meaning-to-Language mapping\n",
            "\u001b[35mSection_title: 4.2  Generative Model of Linguistic Graphs\n",
            "\u001b[35mSection_title: 4.3  Noisy Similarity Metric\n",
            "\u001b[35mSection_title: 5  Hypothesis Testing Setup: Reasoning About Meaning via Words\n",
            "\u001b[35mSection_title: 6  Main Results (Informal)\n",
            "\u001b[35mSection_title: 7  Empirical Validation\n",
            "\u001b[35mSection_title: 8  Implications and Practical Lessons\n",
            "\u001b[32mconclusion: this work is the first attempt to develop a formal framework for understanding the behavior of complex natural language reasoning in the presence of linguistic noise. the importance of this work is two-fold. first, it proposes a novel graph-theoretic paradigm for studying reasoning, inspired by the symbol-meaning problem in the presence of redundancy, ambiguity, incompleteness, and inaccuracy of language. second, it shows how to use this framework to analyze a class of reasoning algorithms. we expect our findings, as well as those from future extensions to other classes of reasoning algorithms, to have important implications on how we study problems in language comprehension.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  BEAMDR: Beam Dense Retriever\n",
            "\u001b[35mSection_title: 2.1  Preliminaries\n",
            "\u001b[35mSection_title: 2.2  Finding Evidence Chains with BEAMDR\n",
            "\u001b[35mSection_title: 3  Experiments: Retrieval and Answering\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Passage Chain Retrieval Evaluation\n",
            "\u001b[35mSection_title: 3.3  Answer Extraction Evaluation\n",
            "\u001b[35mSection_title: 4  Exploring How we Hop\n",
            "\u001b[35mSection_title: 4.1  Qualitative Analysis\n",
            "\u001b[35mSection_title: 4.2  Hop Analysis\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation on Model Errors and Case Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: we introduce a simple yet effective multi-step dense retrieval method, beamdr. by conducting beam search and globally refreshing negative chains during training, beamdr finds reasoning chains in dense space. beamdr is competitive to more complex sota systems albeit not using semi-structured information.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  BACKGROUND AND RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Dense Lexical Representations\n",
            "\u001b[35mSection_title: 3.2  Independent Model Fusion\n",
            "\u001b[35mSection_title: 3.3  Single Model Fusion\n",
            "\u001b[35mSection_title: 3.4  End-to-End Retrieval with DLRs\n",
            "\u001b[35mSection_title: 3.5  Choice of Lexical Representation Models\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP 4.1 Dataset Descriptions\n",
            "\u001b[35mSection_title: 4.2  Models\n",
            "\u001b[35mSection_title: 4.3  Retrieval Implementation and Settings\n",
            "\u001b[35mSection_title: 5  RESULTS\n",
            "\u001b[35mSection_title: 5.1  Quality of DLR Approximations\n",
            "\u001b[35mSection_title: 5.2  Evaluation of Independent Model Fusion\n",
            "\u001b[35mSection_title: 5.3  Evaluation of Single Model Fusion\n",
            "\u001b[35mSection_title: 5.4  Performance of Two-Stage Retrieval\n",
            "\u001b[32mconclusion: we present a simple yet effective approach to densifying lexical representations for passage retrieval. this work introduces a dense representation framework and proposes a new scoring function to compute relevance scores between dense lexical representations (dlrs) derived from queries and passages. using our framework, we can combine lexical and semantic representations into dense hybrid representations (dhrs) for hybrid retrieval. our experiments show that dlrs can accurately approximate any \"off-the-shelf\" lexical model. furthermore, when combined with other semantic representations (as dhrs), the resulting models can achieve comparable effectiveness to existing state-of-the-art hybrid retrieval methods.\n",
            "\u001b[35mSection_title: 6  CONCLUSIONS AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  METHOD\n",
            "\u001b[35mSection_title: 2.1  ARCHITECTURE\n",
            "\u001b[35mSection_title: 2.2  FINE-TUNING DATASETS\n",
            "\u001b[35mSection_title: 2.3  RETRIEVAL AUGMENTED LANGUAGE MODEL FINE-TUNING\n",
            "\u001b[35mSection_title: 2.4  RETRIEVER FINE-TUNING\n",
            "\u001b[35mSection_title: 3  EXPERIMENT SETUP\n",
            "\u001b[35mSection_title: 3.1  RETRIEVER\n",
            "\u001b[35mSection_title: 3.2  BASELINES\n",
            "\u001b[35mSection_title: 3.3  EVALUATION\n",
            "\u001b[35mSection_title: 4  MAIN RESULTS\n",
            "\u001b[35mSection_title: 5  ANALYSIS\n",
            "\u001b[35mSection_title: 5.1  FINE-TUNING STRATEGIES\n",
            "\u001b[35mSection_title: 5.2  DUAL INSTRUCTION TUNING ABLATION\n",
            "\u001b[35mSection_title: 5.3  RETRIEVER SETTINGS\n",
            "\u001b[35mSection_title: 6  RELATED WORK\n",
            "\u001b[32mconclusion: in this paper, we propose ra-dit, a lightweight retrieval-augmented dual instruction tuning framework that can effectively retrofit any pre-trained llm with retrieval capabilities.ra-dit updates the llm with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information.it also fine-tunes the retriever with supervision from the llm to retrieve texts that can better help the llm generate correct outputs.ra-dit achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context ralm approaches such as replug and compete effectively against methods that require extensive pre-training such as atlas.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Modeling and Training\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Tasks and Datasets\n",
            "\u001b[35mSection_title: 4.2  Metrics\n",
            "\u001b[35mSection_title: 5  Results on Benchmarks\n",
            "\u001b[35mSection_title: 6  Analysis and Ablations\n",
            "\u001b[35mSection_title: 6.1  Ablation Studies\n",
            "\u001b[35mSection_title: 6.2  Sub-Module Change Analysis\n",
            "\u001b[35mSection_title: 6.2.1  Effects of Paragraph-level Retrieval\n",
            "\u001b[35mSection_title: 6.2.2  Effects of Sentence-level Retrieval\n",
            "\u001b[35mSection_title: 6.3  Answer Breakdown\n",
            "\u001b[35mSection_title: 6.4  Examples\n",
            "\u001b[32mconclusion: we proposed a simple yet effective hierarchical pipeline system that achieves state-of-the-art results on two mrs tasks. ablation studies demonstrate the importance of semantic retrieval at both paragraph and sentence levels in the mrs system. the work can give general guidelines on mrs modeling and inspire future research on the relationship between semantic retrieval and downstream comprehension in a joint setting. table 6: hyper-parameter selection for the full pipeline system. h and k are the retrieval filtering hyperparameters mentioned in the main paper. p-level and s-level indicate paragraph-level and sentence-level respectively. \"{}\" means values enumerated from a set. \"[]\" means values enumerated from a range with inter-val=0.1 \"bs.\"=batch size \"# e.\"=number of epochs\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  ITERATIVE RETRIEVAL-GENERATION SYNERGY\n",
            "\u001b[35mSection_title: 2.1.  Overview\n",
            "\u001b[35mSection_title: 2.2.  Generation Augmented Retrieval\n",
            "\u001b[35mSection_title: 2.3.  Retrieval Augmented Generation\n",
            "\u001b[35mSection_title: 2.3.1.  Refine\n",
            "\u001b[35mSection_title: 2.3.2.  Refresh\n",
            "\u001b[35mSection_title: 3.  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.1.  Datasets\n",
            "\u001b[35mSection_title: 3.2.  Baselines\n",
            "\u001b[35mSection_title: 3.3.  Details\n",
            "\u001b[35mSection_title: 4.  RESULTS\n",
            "\u001b[35mSection_title: 4.1.  Main Results\n",
            "\u001b[35mSection_title: 4.2.  Performance at Different Iterations\n",
            "\u001b[32mconclusion: in this paper, we present itrg, which is an iterative retrievalgeneration synergy framework, containing two important steps: generation-augmented retrieval and retrieval-augmented generation.they form a closed loop, and can improve each other via multiple iterations.we propose a simple and effective generation-augmented retrieval strategy and two retrieval-augmented generation strategies.empirical results show our approach significantly exceeds several strong baselines, including gpt 3.5, on four open domain question answering datasets, which indicates that our method can significantly improve the reasoning ability of large language models.\n",
            "\u001b[35mSection_title: 5.  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Simultaneously transforming and rounding\n",
            "\u001b[35mSection_title: 2.1  The transformation g\n",
            "\u001b[35mSection_title: 2.2  Model properties\n",
            "\u001b[35mSection_title: 2.3  Posterior inference\n",
            "\u001b[35mSection_title: 3  Regression modeling with STAR\n",
            "\u001b[35mSection_title: 3.1  Additive models\n",
            "\u001b[35mSection_title: 3.2  Bayesian Additive Regression Trees\n",
            "\u001b[35mSection_title: 4  Simulation studies\n",
            "\u001b[35mSection_title: 4.1  Linear mean functions\n",
            "\u001b[35mSection_title: 4.2  Nonlinear mean functions\n",
            "\u001b[35mSection_title: 5  Predicting the demand for healthcare utilization\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Existing MRC dataset designs\n",
            "\u001b[35mSection_title: 2.1  Manually written questions\n",
            "\u001b[35mSection_title: 2.2  Naturally occurring questions\n",
            "\u001b[35mSection_title: 2.3  Questions from tests designed for humans\n",
            "\u001b[35mSection_title: 2.4  Automatically generated questions\n",
            "\u001b[35mSection_title: 2.5  Summary: What is missing\n",
            "\u001b[35mSection_title: 3  Defining deep story understanding\n",
            "\u001b[35mSection_title: 3.1  The case for stories\n",
            "\u001b[35mSection_title: 3.2  A ToU for stories\n",
            "\u001b[35mSection_title: 4  Towards a story understanding task\n",
            "\u001b[35mSection_title: 4.1  Approach 1: Annotating ToU answers\n",
            "\u001b[35mSection_title: 4.1.1  Preliminary notes on RoU agreement\n",
            "\u001b[35mSection_title: 4.1.2  Free-text evaluation\n",
            "\u001b[35mSection_title: 4.1.3  Thorough multiple-choice evaluation\n",
            "\u001b[35mSection_title: 4.2  Approach 2: Competing to satisfy judges\n",
            "\u001b[35mSection_title: 5  Current MRC systems do not comprehend stories\n",
            "\u001b[35mSection_title: 5.1  Data and experimental setup\n",
            "\u001b[35mSection_title: 5.2  Results and Discussion\n",
            "\u001b[35mSection_title: 6  Taking the ToU idea forward\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Bucketing-based Calibration and ECE\n",
            "\u001b[35mSection_title: 2.2  Temperature Scaling\n",
            "\u001b[35mSection_title: 3  Calibration in Open-Domain Question Answering\n",
            "\u001b[35mSection_title: 3.1  The ODQA Model\n",
            "\u001b[35mSection_title: 3.2  Temperature Scaling For ODQA\n",
            "\u001b[35mSection_title: 3.3  Temperature Scaling Results\n",
            "\u001b[35mSection_title: 4  Flaws in ECE and Better Alternatives\n",
            "\u001b[35mSection_title: 4.1  What's Wrong With ECE?\n",
            "\u001b[35mSection_title: 4.2  New Metric: MACROCE\n",
            "\u001b[35mSection_title: 4.3  Re-Evaluating Calibration with New Metrics\n",
            "\u001b[35mSection_title: 5  Toward Better Calibration Methods\n",
            "\u001b[35mSection_title: 5.1  Existing Calibration Baselines\n",
            "\u001b[35mSection_title: 5.2  CONSCAL: Calibration Through Consistency\n",
            "\u001b[35mSection_title: 5.3  Experimental Results\n",
            "\u001b[35mSection_title: 5.4  Human Study\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: this paper investigates calibration in the realistic application of odqa where users need to decide whether to trust the model prediction based on the confidence scores. although confidence scores produced by existing calibration methods improves the popular ece metric, these confidence scores do not help distinguish correct and wrong predictions. we propose to use the macroce metric to remedy the flaws, and existing calibration methods fail on our macroce metric. we further propose a simple and effective calibration method conscal that leverages training consistency. our human study confirms both the effectiveness of conscal as well as the alignment between macroce and human preference. our work advocates and paves the path for user-centric calibration, and our con-scal method is a promising direction for better calibration. future work can adapt our calibration metric and method to more diverse tasks (such as generative tasks) and explore other ways to further improve user-centric calibration.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  RELATED WORK\n",
            "\u001b[35mSection_title: 3.  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1.  BART\n",
            "\u001b[35mSection_title: 3.2.  Answer Relevance Attention\n",
            "\u001b[35mSection_title: 3.2.1.  Soft attention\n",
            "\u001b[35mSection_title: 3.2.2.  Hard attention\n",
            "\u001b[35mSection_title: 3.3.  Enhanced Cross-Attention\n",
            "\u001b[35mSection_title: 4.  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 4.1.  Training Settings\n",
            "\u001b[35mSection_title: 4.2.  Baselines\n",
            "\u001b[35mSection_title: 5.  RESULTS AND ANALYSIS\n",
            "\u001b[35mSection_title: 5.1.  Ablation Study\n",
            "\u001b[32mconclusion: in this paper, we propose a novel framework, qa4qg, a qaaugmented bart-based framework for mqg. it is the first work to explore large pre-trained language models for mqg and takes advantage of an additional multi-hop qa module to further constrain the question generation. our results on the hotpotqa dataset show that qa4qg outperforms all state-of-the-art models, with an increase of 8 bleu-4 and 8 rouge points compared to the best results previously reported. our work suggests the advantage of introducing pre-trained language models and qa modules for the mqg task.\n",
            "\u001b[35mSection_title: 6.  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Evaluation\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 6.1  Influence of Training Steps\n",
            "\u001b[35mSection_title: 6.2  Influence of Corpus Size\n",
            "\u001b[35mSection_title: 6.3  Robustness against Query Generation\n",
            "\u001b[35mSection_title: 6.4  Sensitivity to Starting Checkpoints\n",
            "\u001b[35mSection_title: 7  Case Study: Fine-Grained Labels\n",
            "\u001b[35mSection_title: 8.2  0\n",
            "\u001b[32mconclusion: in this work we propose gpl, a novel unsupervised domain adaptation method for dense retrieval models. it generates queries for a target corpus and pseudo labels these with a cross-encoders. pseudolabeling overcomes two important short-comings of previous methods: not all generated queries are of high quality and pseudo-labels efficiently detects those. further, training with mined hard negatives is possible as the pseudo labels performs efficient denoising.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  MATERIALS AND METHODS\n",
            "\u001b[35mSection_title: 2.1.  Identification of GIs\n",
            "\u001b[35mSection_title: 2.2.  tRNA and lncRNA Content of GIs\n",
            "\u001b[35mSection_title: 2.3.  Human GIs Displaying Regions of Pairwise Non-alignment to 38 Other Vertebrates and Their lncRNAs Content\n",
            "\u001b[35mSection_title: 2.4.  Non-aligning Regions in GIs Gene Content and Enrichment Analysis\n",
            "\u001b[35mSection_title: 2.5.  Identification of Possible Donors of lncRNA in GIs and Sequence Similarity Analysis\n",
            "\u001b[35mSection_title: 2.6.  Fitting a Logistic Function\n",
            "\u001b[35mSection_title: 3.  RESULTS\n",
            "\u001b[35mSection_title: 3.1.  Location and Distribution of GIs\n",
            "\u001b[35mSection_title: 3.2.  tRNAs in GIs\n",
            "\u001b[35mSection_title: 3.3.  Genome Specific Insertions in Predicted GIs\n",
            "\u001b[35mSection_title: 3.4.  Analysis of Gene Content of GIs Identified in the Human Genome\n",
            "\u001b[35mSection_title: 3.5.  Analysis of lncRNA Content of GIs\n",
            "\u001b[35mSection_title: 4.  DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Data Collection\n",
            "\u001b[35mSection_title: 2.1  Human-Annotated Data Collection\n",
            "\u001b[35mSection_title: 2.2  Distantly Supervised Data Collection\n",
            "\u001b[35mSection_title: 3  Data Analysis\n",
            "\u001b[35mSection_title: 4  Benchmark Settings\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: to promote re systems from sentence level to document level, we present docred, a large-scale document-level re dataset that features the data size, the requirement for reading and reasoning over multiple sentences, and the distantly supervised data offered for facilitating the development of weakly supervised document-level re. experiments show that human performance is significantly higher than re baseline models, which suggests ample opportunity for future improvement.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Methods\n",
            "\u001b[35mSection_title: 2.1.  Data Source and Study Population.\n",
            "\u001b[35mSection_title: 2.2.  Data Element.\n",
            "\u001b[35mSection_title: 2.3.  Statistical Analyses.\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Patient Characteristics.\n",
            "\u001b[35mSection_title: 3.2.  Independent Prognostic Factors for OS.\n",
            "\u001b[35mSection_title: 3.3.  Nomogram Construction and Validation.\n",
            "\u001b[35mSection_title: 3.4.  Risk Classifcation Systems for CSS.\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[32mconclusion: to summarize, we established and validated a novel nomogram for os patients of cya, which could serve as concise and practical tools for clinicians to anticipate the 1-, 3-, and 5-years css. lss for patients with os exhibited signifcant beneft on css compared with amputation. while new chemotherapy regimens will be required to increase survivorship in the setting of os, patients with tumor features suitable to lss had a much higher survival rate than those suitable for amputation. [28][29][30].\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Contrastive Estimation for Reading Comprehension\n",
            "\u001b[35mSection_title: 2.1  Choosing a neighborhood\n",
            "\u001b[35mSection_title: 2.1.1  Single Neighborhood Models\n",
            "\u001b[35mSection_title: 2.1.2  Multiple Neighbourhood Models\n",
            "\u001b[35mSection_title: 2.2  Alternative Ways to Use Bundles\n",
            "\u001b[35mSection_title: 3  Bundling Heuristics\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Main results\n",
            "\u001b[35mSection_title: 4.2  Joint Inference\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Relation between MLE, UL and CE\n",
            "\u001b[35mSection_title: 5.2  Importance of close instance bundles\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we have presented a way to use contrastive estimation in a supervised manner to learn from distinguishing cues between multiple related qa pairs, or instance bundles. our experiments with multiple ce-based loss functions, defined over a joint neighborhood of questions and answers, have shown that these models outperform existing methods on two datasets: ropes and hotpotqa. apart from presenting several ways to create instance bundles, we also explore theoretical connections between unlikelihood training and contrastive estimation, and initial exploration into when instance bundles are likely to be effective with these methods. we believe our results give strong motivation for further work in techniques to both create and use instance bundles in nlp datasets. the code is available at https://github.com/ddua/ contrastive-estimation.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset and Data Extraction\n",
            "\u001b[35mSection_title: 3  Methods to generate NQ-like Questions\n",
            "\u001b[35mSection_title: 3.1  Quality Control\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Generated Data\n",
            "\u001b[35mSection_title: 4.2  Question Answering Systems\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[32mconclusion: we clearly observe from the results that adding filtered nq-like questions from the qb data has given a boost over using only nq questions. in finer detail, we observe that questions from the last sentence of the qb are of higher quality than from intermediate sentences and therefore provide a higher boost to performance even with less sam-ples. even by simply adding questions generated from last sentence, we increase the exact match accuracy by nearly 2 points. we also observe that the bleu score of answers generated from quality controlled nq like system is 16 points more than the bleu score of the baseline qb system for the rag system and by 13 points for the drqa system. this shows that our algorithm to generate nq-like questions has been effective in improving the quality of the training dataset.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  ENTITY RETRIEVAL\n",
            "\u001b[35mSection_title: 3  METHOD\n",
            "\u001b[35mSection_title: 3.1  INFERENCE WITH CONSTRAINED BEAM SEARCH\n",
            "\u001b[35mSection_title: 3.2  AUTOREGRESSIVE END-TO-END ENTITY LINKING\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  SETTINGS\n",
            "\u001b[35mSection_title: 4.2  RESULTS\n",
            "\u001b[35mSection_title: 5  RELATED WORKS\n",
            "\u001b[32mconclusion: in this work, we propose genre, a novel paradigm to addresses entity retrieval: generate entity names autoregressively. entity names have several properties that might help (even humans) retrieving them, including a compositional structure and a predictable interaction with the context. the autoregressive formulation allows us to directly capture some of these properties, leading to several advantages with respect to current solutions, including an efficient way to cross encode men-tion context and entity candidates, a much smaller memory footprint, and the ability to compute an exact softmax without the need to subsample negative data. we empirically show that these characteristics, combined with constrained decoding strategies, led to state-of-the-art performance on a plethora of entity retrieval datasets, spanning entity disambiguation, end-to-end entity linking, and page-level document retrieval, while resulting in systems with a remarkably contained memory footprint, a space reduction by a factor of twenty on average. we additionally demonstrate that new entities can be effectively considered in our system by simply appending their unambiguous name to the candidate set.\n",
            "\u001b[35mSection_title: 6  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.3.  Preprocessing & Quality Checks\n",
            "\u001b[35mSection_title: 2.4.  Split Criteria\n",
            "\u001b[35mSection_title: 3.  Data statistics\n",
            "\u001b[35mSection_title: 4.  Data Analysis\n",
            "\u001b[35mSection_title: 4.1.  Difficulty and Diversity of Questions\n",
            "\u001b[35mSection_title: 4.2.  Answer types\n",
            "\u001b[35mSection_title: 4.3.  Subject & Topic Analysis\n",
            "\u001b[35mSection_title: 4.4.  Reasoning Types\n",
            "\u001b[35mSection_title: 5.  Baseline Models\n",
            "\u001b[35mSection_title: 5.1.  SciBERT\n",
            "\u001b[35mSection_title: 5.2.  BioBERT\n",
            "\u001b[35mSection_title: 5.3.  PubMedBERT\n",
            "\u001b[35mSection_title: 5.4.  Retriever models\n",
            "\u001b[35mSection_title: 6.  Experiments\n",
            "\u001b[35mSection_title: 6.1.  Pubmed Data Preprocessing\n",
            "\u001b[35mSection_title: 6.2.  Retriever\n",
            "\u001b[35mSection_title: 6.3.  Reader finetuning\n",
            "\u001b[35mSection_title: 7.  Error Analysis\n",
            "\u001b[35mSection_title: 8.  Result & Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Relevance-Based Confidence Sampling\n",
            "\u001b[35mSection_title: 3.  Experiment Design\n",
            "\u001b[35mSection_title: 4.  Results\n",
            "\u001b[35mSection_title: 5.  Related Work\n",
            "\u001b[32mconclusion: we proposed a simple yet effective approach for multi-task training of the fid retrieval-augmented generation model on the kilt benchmark. we cleaned (and downsampled were necessary) the training set by removing query-answer pairs with low relevance confidence. we demonstrated that this approach substantially improves two imbalanced tasks, and has a smaller benefit on two of the remaining five tasks. by scaling the model capacity we achieve state-of-the-art results on five kilt tasks evaluated by the leaderboard.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  ProQA\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Structural Prompt\n",
            "\u001b[35mSection_title: 3.3  Structural Prompt-based Pre-training\n",
            "\u001b[35mSection_title: 4.1  Datasets and Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  Approaches\n",
            "\u001b[35mSection_title: 5  Results and Analyses\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Continual Learning via Soft Prompt\n",
            "\u001b[35mSection_title: 5.3  Convergence Analysis\n",
            "\u001b[35mSection_title: 5.4  Ablation Study\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: we introduce proqa, a unified qa paradigm that adopts a single model for solving various qa tasks with the bridge of a structural prompt. structural prompt simultaneously models the common ability required for various tasks and keeps the speciality of each task, through a structurally designed learnable input schema. we further conduct structural prompt-based pre-training, seeking to empower the model with general qa-centric ability and injects the semantic knowledge of the structural prompt into the pre-training model. experimental results on 11 qa benchmarks demonstrate that proqa can significantly boost performance on all settings. further analyses show that our method can better mitigate the catastrophic forgetting issue during continual learning, and our method can be adapted to a newly involved task more quickly, by taking the advantages of the structural prompt. in the future, we hope our analysis could inspire more explorations on the unified qa methods, or the unification of distinct tasks with complex inputs modeling by the structural prompt. we also hope structural prompt can be further utilized into the unification of more tasks with complex inputs.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 8  Acknowledgments\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Attention Pattern\n",
            "\u001b[35mSection_title: 3.2  CUDA Kernels for our Attention Pattern\n",
            "\u001b[35mSection_title: 4  Autoregressive Language Modeling\n",
            "\u001b[35mSection_title: 4.1  Attention Pattern\n",
            "\u001b[35mSection_title: 4.2  Experiment Setup\n",
            "\u001b[35mSection_title: 4.2.1  Results\n",
            "\u001b[35mSection_title: 4.2.2  Ablation Study\n",
            "\u001b[35mSection_title: 5  Pretraining and Finetuning\n",
            "\u001b[35mSection_title: 5.1  Tasks\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Ablations on WikiHop\n",
            "\u001b[32mconclusion: we present longformer, a transformer-based model that is scalable for processing long documents and that makes it easy to perform a wide range of document-level nlp tasks without chunking/shortening the long input and without complex architecture to combine information across these chunks. longformer employs an attention pattern that combines local and global information while also scaling linearly with the sequence length. longformer achieves state-of-the-art results on the character-level language modeling tasks of text8 and enwik8. when pretrained, longformer consistently outperforms roberta on long document tasks and sets new state-of-the-art results on wiki-hop and triviaqa. for future work, we would like to explore other attention patterns that are more efficient by dynamically adapting to the input. we also would like to apply our model to other relevant long document tasks such as summarization.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.1  Reading Comprehension\n",
            "\u001b[35mSection_title: 1.1.3  History\n",
            "\u001b[35mSection_title: 1.2  Entity and Relation\n",
            "\u001b[35mSection_title: 1.2.1  Knowledge base population\n",
            "\u001b[35mSection_title: 2.1  Related work\n",
            "\u001b[35mSection_title: 2.2  Dataset construction\n",
            "\u001b[35mSection_title: 2.3  Performance Benchmarks\n",
            "\u001b[32mconclusion: we presented a large-scale person-centered cloze dataset. the dataset is not anonymized, and each passage is a raw text which is not only natural but also easier to be pre-processed by syntactic and semantic parsers. in the dataset construction, we used baseline suppression,\n",
            "\u001b[35mSection_title: 2.4  Conclusion\n",
            "\u001b[35mSection_title: 3.1  Related work\n",
            "\u001b[35mSection_title: 3.2  Emergent Predication Structure\n",
            "\u001b[35mSection_title: 3.3  Pointer Annotation Readers\n",
            "\u001b[35mSection_title: 3.4  Discussion\n",
            "\u001b[32mconclusion: in this work, we claimed and empirically showed that the success of aggregation readers and explicit readers could be explained by equation finally, we proposed one-hop pointer annotation to helps aggregation readers whose performance indicates that these neural networks are benefited from externally provided linguistic features, including externally annotated reference information.\n",
            "\u001b[35mSection_title: 3.5  Conclusion\n",
            "\u001b[35mSection_title: 4.1  Wikihop dataset\n",
            "\u001b[35mSection_title: 4.2  Related work\n",
            "\u001b[35mSection_title: 4.3  Explicit reference transformer\n",
            "\u001b[35mSection_title: 4.4  Experiments\n",
            "\u001b[35mSection_title: 4.4.1  Main result\n",
            "\u001b[35mSection_title: 4.4.2  Ablation studies\n",
            "\u001b[32mconclusion: we proposed the explicit reference transformer that has a simple sum layer on the top of a pre-trained transformer encoder. the sum layer, called explicit reference structure, performs over contextual token embeddings referring to each candidate answer. our model is simple and efficiently fine-tuned over wikihop, and its performance is significantly better than that of models with the similar parameter size.\n",
            "\u001b[35mSection_title: 4.5  Conclusion\n",
            "\u001b[35mSection_title: 5.1  Related work\n",
            "\u001b[35mSection_title: 5.2  Preliminary\n",
            "\u001b[35mSection_title: 5.3  System description\n",
            "\u001b[35mSection_title: 5.4  Experiment for relation identification\n",
            "\u001b[35mSection_title: 5.6  End-to-end system\n",
            "\u001b[32mconclusion: in this study, we developed and tested our knowledge extraction and representation system intended to support material design, by representing knowledge as relationships. knowledge was represented as relationships in pspp design charts. we leveraged weakly supervised learning for relation extraction. the end-to-end system proved our concept, and its relation extraction performance was superior to that of other baseline models.\n",
            "\u001b[35mSection_title: 5.7  Conclusions and contribution\n",
            "\u001b[35mSection_title: 5.8  Follow-up work\n",
            "\u001b[35mSection_title: 6.1  Future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Problem and methods\n",
            "\u001b[35mSection_title: 2.1.  Dataset and evaluation metrics\n",
            "\u001b[35mSection_title: 2.2.  Training: Overview\n",
            "\u001b[35mSection_title: 2.3.  Supervised finetuning\n",
            "\u001b[35mSection_title: 2.4.  Reward models\n",
            "\u001b[35mSection_title: 2.5.  Decoding\n",
            "\u001b[35mSection_title: 2.6.  RL via Expert Iteration\n",
            "\u001b[35mSection_title: 2.7.  Data annotation\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  No reward model\n",
            "\u001b[35mSection_title: 3.2.  Reward model for reranking only\n",
            "\u001b[35mSection_title: 3.3.  Reinforcement learning with a reward model\n",
            "\u001b[35mSection_title: 3.4.  Selective prediction\n",
            "\u001b[35mSection_title: 3.5.  OOD generalization\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[35mSection_title: 4.1.  When to use process-vs. outcome-based feedback?\n",
            "\u001b[35mSection_title: 4.1.1.  Final-answer vs. trace error\n",
            "\u001b[35mSection_title: 4.1.2.  Process-based approaches both require and facilitate human understanding\n",
            "\u001b[35mSection_title: 4.1.3.  Process-based approaches avoid tampering incentives\n",
            "\u001b[32mlimitation: we generally expect process-based and outcome-based feedback to align more closely for math compared to other domains. for math problems, incorrect traces are typically harmful for reaching correct final answers. this matches our earlier finding that outcome-supervised rms approximate process-based feedback. in contrast, in other domains, undesirable behaviors may be helpful for highly-rated outcomes, e.g., manipulation may increase reported user satisfaction. as a result, we believe optimizing for outcomes (final-answer correctness) for math problems has a stronger effect on inducing a correct process than it would in other domains.\n",
            "\u001b[35mSection_title: 4.2.  Limitations to generalizability of our results\n",
            "\u001b[35mSection_title: 4.3.  Concepts related to process-and outcome-based feedback\n",
            "\u001b[35mSection_title: 5.  Related work\n",
            "\u001b[32mconclusion: in this work, we run the first comprehensive comparison between process-and outcome-based supervision on a natural language task. we find that both types of supervision lead to similar finalanswer error rates, with our best models improving the state-of-the-art final-answer error on gsm8k from 16.8% to 13.8% when using outcome-based supervision and to 12.9% when using processbased supervision. in contrast, we find that obtaining low trace error requires either process-based supervision, or a reward model that emulates it. a purely process-based approach of sft with prm reranking reduces the state-of-the-art trace error rate from 14.0% to 3.4%, while its outcome-based analogue achieves 12.7% trace error. however, somewhat surprisingly, we find that reward models trained with outcome-based labels result in predictions that agree more closely with the process-based labels than they do with the outcome-based labels themselves. by using this reward model during rl training, we close most of this gap, reducing trace error from 12.7% to 5.5%. while some of these conclusions may be specific to our setting of math word problems, we hope that future work explores the extent to which they generalize to other domains. \n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Continual Model Refinement\n",
            "\u001b[35mSection_title: 2.1  Problem Formulation\n",
            "\u001b[35mSection_title: 2.2  Basic Evaluation Metrics\n",
            "\u001b[35mSection_title: 2.3  Unique Challenges of CMR\n",
            "\u001b[35mSection_title: 3  A Comprehensive Evaluation Protocol\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Creating Dynamic OOD Data Streams\n",
            "\u001b[35mSection_title: 3.3  Benchmarking CMR Methods\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 4.1  Base Model & Continual Fine-Tuning\n",
            "\u001b[35mSection_title: 4.2  Regularization-based methods\n",
            "\u001b[35mSection_title: 4.3  Replay Methods\n",
            "\u001b[35mSection_title: 5  Evaluation & Analysis\n",
            "\u001b[35mSection_title: 5.1  Setup\n",
            "\u001b[35mSection_title: 5.2  Main Results and Findings\n",
            "\u001b[35mSection_title: 5.3  Analysis on Memory Replaying\n",
            "\u001b[35mSection_title: 5.4  Orthogonal Improvement for CMR\n",
            "\u001b[35mSection_title: 5.5  Additional Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we propose a novel continual learning formulation named continual model refinement (cmr). the cmr problem aims to efficiently fix prediction errors when learning in outof-distribution data streams without catastrophically forgetting the acquired knowledge. for studying such a realistic and complex problem, we presented a dedicated evaluation protocol with a general method to create non-stationary, diverse ood data streams for analysis. also, we design multiple evaluation metrics to deliver a comprehensive yet concise measurement of cmr methods. the proposed cmr problem with our comprehensive analysis opens up a range of new opportunities for studying continual learning problems that are closer to real-world applications for the nlp community and beyond. for example, based on our results and analysis about (q3) and (q6), we find that it is promising to study how we can integrate both regularization methods and replay methods for mitigating the forgetting issue while improving the generalization ability. the analysis about (q5) suggests that developing more stable ranking criteria is also important to conditional replay methods (e.g., our simple extension maxloss can outperform mir under specific settings). developing cmr methods of which the configurations can generalize to diverse types of streams is also an important challenge. we release our codebase and processed datasets for supporting the reproducibility of our experiments and future research.\n",
            "\u001b[35mSection_title: 7  Conclusion & Future Directions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Adversarial Attack\n",
            "\u001b[35mSection_title: 2.2  Unanswerable Questions in MRC\n",
            "\u001b[35mSection_title: 3  Tasks and Models\n",
            "\u001b[35mSection_title: 3.1  Extractive Question Answering\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.3  Models\n",
            "\u001b[35mSection_title: 4  Adversarial Attacks\n",
            "\u001b[35mSection_title: 4.1  Robustness Evaluation\n",
            "\u001b[35mSection_title: 4.2  Attack Construction\n",
            "\u001b[35mSection_title: 5.2  Categories of Responses\n",
            "\u001b[35mSection_title: 5.3  Force To Answer\n",
            "\u001b[35mSection_title: 6  Attacks in Out-Of-Domain Settings: Results\n",
            "\u001b[35mSection_title: 7  New Attack\n",
            "\u001b[32mconclusion: in this work, we investigate the effects of training mrc models with unanswerable questions on their robustness against adversarial attacks. we construct adversarial samples from answerable and unanswerable questions in squad 2.0 and evaluate three mrc models fine-tuned on either squad 1.1 (v1 models) or squad 2.0 (v2 models) independently. adversarial attacks on answerable questions reveal that v2 models initially show little improved robustness over v1 models yet possess a latent ability to deal with these attacks that v1 models do not; the correct responses are often hidden as second-best answers, an indicator of the \"hidden robustness\" of v2 models resulting from additional training on unanswerable questions. by eliminating the \"unanswerable\" option and forcing v2 models to output an answer to any answer-  able questions, we leverage this hidden robustness to improve the performance of mrc models to attacks on answerable questions. furthermore, we also show that this robustness translates well to out-of-domain test sets.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[35mSection_title: 9  Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  OVERVIEW: LANGUAGE MODEL PROGRAMMING\n",
            "\u001b[35mSection_title: 2.1  Background: (Large) Language Models\n",
            "\u001b[35mSection_title: 2.2  Key Challenges\n",
            "\u001b[35mSection_title: 2.3  Language Model Programming in LMQL\n",
            "\u001b[35mSection_title: 3  THE LMQL LANGUAGE\n",
            "\u001b[35mSection_title: 3.1  Built-in Functions\n",
            "\u001b[35mSection_title: 4  THE LMQL RUNTIME: QUERY EXECUTION & DECODING\n",
            "\u001b[35mSection_title: 5  VALIDATION AND CONSTRAINT DECODING\n",
            "\u001b[35mSection_title: 5.1  Partial Evaluation\n",
            "\u001b[35mSection_title: 5.2  Generating Token Masks using FollowMaps\n",
            "\u001b[35mSection_title: 6  EVALUATION\n",
            "\u001b[35mSection_title: 6.1  Case Study 1: Chain-of-Thought Prompting\n",
            "\u001b[35mSection_title: 6.2  Case Study 2: Interactive Prompting\n",
            "\u001b[35mSection_title: 6.3  Case Study 3: Arithmetic Reasoning\n",
            "\u001b[35mSection_title: 6.4  Discussion\n",
            "\u001b[35mSection_title: 7  RELATED WORK\n",
            "\u001b[32mconclusion: in this work, we introduce the concept of language model programming, a novel way to interact with (large) language models. we presented lmql, a high-level query language, offering a concise and intuitive syntax. lmql implements purpose-designed evaluation semantics, which enable efficient query execution. we have substantiated this claim in a series of case studies, where we demonstrate that complex, state-of-the-art prompting techniques can be implemented as intuitive, concise and efficient lmql programs that reduce (compute) costs by up to 80%.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  A Framework for Semi-Parametric Learning Based on Neural Inducing Points\n",
            "\u001b[35mSection_title: 3.2  Semi-Parametric Inducing Point Networks\n",
            "\u001b[35mSection_title: 3.2.1  Encoder Architecture\n",
            "\u001b[35mSection_title: 3.2.2  Predictor Architecture\n",
            "\u001b[35mSection_title: 3.3  Objective Function\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Genotype Imputation Experiments\n",
            "\u001b[35mSection_title: 4.2  UCI Experiments\n",
            "\u001b[35mSection_title: 4.3  Ablation Analysis\n",
            "\u001b[35mSection_title: 4.4  Effect of Batching\n",
            "\u001b[35mSection_title: 4.5  Compute Resources\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mlimitation: the spin model achieves linear complexity via advanced self-attention mechanisms; this also introduces additional hyper-parameters into the model, potentially increasing tuning time. the resulting architecture remains overparametrized even after with small numbers of inducing points h, f and may overfit.the primary source of expressivity is attention between datapoints, which has a query dimension he.highly expressive models may learn to ignore the training set and operate in a fully-parametric mode; this failure mode is best avoided via regularization and large datasets. interestingly, our approach benefits from big data, while classical non-parametric models work best on small datasets due to their computational complexity. regularization via small h, f , dropout, and feature masking control overfitting; we will explore more compact architectures in future work.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\u001b[32mconclusion: in this paper, we introduce a domain-agnostic general-purpose architecture, the semi-parametric inducing point network (spin). unlike previous semi-parametric approaches whose computational cost grows quadratically with the size of the dataset, our approach scales linearly in the size and dimensionality of the data by leveraging a cross attention mechanism between datapoints and induced latents, allowing it to scale to large datasets.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Multi-Head Transformer Models\n",
            "\u001b[35mSection_title: 3  Overview: Experiments & Findings\n",
            "\u001b[35mSection_title: 4  Setting 1: Emerging Computation Arguments in Span Extraction\n",
            "\u001b[35mSection_title: 4.1  Experimental Setting\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.3  Influence of Extracted Spans on Generation\n",
            "\u001b[35mSection_title: 5  Setting 2: Emerging Supporting Facts in Span Extraction\n",
            "\u001b[35mSection_title: 5.1  Experimental Setting\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 6  Setting 3: Emerging Query-based Summaries\n",
            "\u001b[35mSection_title: 6.1  Experimental Setting\n",
            "\u001b[35mSection_title: 6.2  Results\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we show that training multiple heads on top of a pre-trained language model creates a steering effect, where the target head influences the behaviour of another head, steering it towards capabilities beyond its training objective. in three multi-task settings, we find that without any dedicated training, the steered head often outputs explanations for the model predictions. moreover, modifying the input representation based on the outputs of the steered head can lead to predictable changes in the target head predictions.\n",
            "\u001b[35mSection_title: 8  Conclusions and Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Overview\n",
            "\u001b[35mSection_title: 2.2  QA models\n",
            "\u001b[35mSection_title: 2.3  QA2D\n",
            "\u001b[35mSection_title: 2.4  NLI\n",
            "\u001b[35mSection_title: 2.5  Calibration models\n",
            "\u001b[35mSection_title: 2.6  Answer ranking\n",
            "\u001b[35mSection_title: 2.7  Datasets\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 3.1  Ranking multiple choice\n",
            "\u001b[35mSection_title: 3.2  Selective QA\n",
            "\u001b[35mSection_title: 3.2.1  Selective QA for multiple choice QA\n",
            "\u001b[35mSection_title: 3.3  Answer Rejection on SQuAD 2.0\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[32mlimitation: despite the results above, multiple choice qa and extractive qa with a provided context is a limited setting that doesn't indicate the results would extend to other popular settings where nli. given that laban et al. (2022) shows similar results that contradiction is an important signal in factual consistency we are hopeful that it would.\n",
            "\u001b[35mSection_title: 5  Limitations\n",
            "\u001b[35mSection_title: 5.1  Context Length and NLI datasets\n",
            "\u001b[35mSection_title: 5.2  Ranking requires alternatives and time\n",
            "\u001b[35mSection_title: 6  Summary\n",
            "\u001b[35mSection_title: 7  Ethics Statement\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  LONG-RANGE ARENA (LRA)\n",
            "\u001b[35mSection_title: 2.1  DESIDERATA\n",
            "\u001b[35mSection_title: 3.  Challenging:\n",
            "\u001b[35mSection_title: 4.  Long inputs:\n",
            "\u001b[35mSection_title: 2.2  TASKS\n",
            "\u001b[35mSection_title: 2.2.1  LONG LISTOPS\n",
            "\u001b[35mSection_title: 2.2.2  BYTE-LEVEL TEXT CLASSIFICATION\n",
            "\u001b[35mSection_title: 2.2.3  BYTE-LEVEL DOCUMENT RETRIEVAL\n",
            "\u001b[35mSection_title: 2.2.4  IMAGE CLASSIFICATION ON SEQUENCES OF PIXELS\n",
            "\u001b[35mSection_title: 2.2.5  PATHFINDER (LONG-RANGE SPATIAL DEPENDENCY)\n",
            "\u001b[35mSection_title: 2.2.6  PATHFINDER-X (LONG-RANGE SPATIAL DEPENDENCIES WITH EXTREME LENGTHS)\n",
            "\u001b[35mSection_title: 2.3  REQUIRED ATTENTION SPAN OF LRA TASKS\n",
            "\u001b[35mSection_title: 3.1  MODELS\n",
            "\u001b[35mSection_title: 3.2  PHILOSOPHY BEHIND THE BENCHMARK\n",
            "\u001b[35mSection_title: 3.3  QUANTITATIVE RESULTS\n",
            "\u001b[35mSection_title: 3.4  EFFICIENCY BENCHMARKS\n",
            "\u001b[35mSection_title: 3.5  OVERALL RESULTS: NO ONE-SIZE-FITS-ALL\n",
            "\u001b[35mSection_title: 4  RELATED WORK\n",
            "\u001b[35mSection_title: 4.1  EFFICIENT TRANSFORMERS\n",
            "\u001b[35mSection_title: 4.2  EXISTING BENCHMARKS\n",
            "\u001b[32mconclusion: we proposed long range arena (lra), a new benchmark for evaluating progress on efficient transformer research. our new benchmark is challenging and probes at model capabilities in dealing with diverse data types and structures such as text, mathematics, and visual data. our benchmark comprises of tasks ranging from 1k to 16k tokens. for the first time, we conduct an extensive side-by-side comparison of ten recently proposed efficient transformer models. the experimental results show that these tasks are very challenging even for long-range transformer models. the overall results show that there is no one-size-fits-all solution and trade-offs have to be made in terms of model quality and speed/memory. we plan to open source our code and benchmarks to facilitate future benchmarking, research and model development.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Overview\n",
            "\u001b[35mSection_title: 2.2  Hallucination Detection\n",
            "\u001b[35mSection_title: 2.2.1  Identify Key Concepts\n",
            "\u001b[35mSection_title: 2.2.2  Calculate Model's Uncertainty\n",
            "\u001b[35mSection_title: 2.2.3  Create Validation Question\n",
            "\u001b[35mSection_title: 2.2.4  Find Relevant Knowledge\n",
            "\u001b[35mSection_title: 2.2.5  Answer Validation Question\n",
            "\u001b[35mSection_title: 2.3  Hallucination Mitigation\n",
            "\u001b[35mSection_title: 2.4  Design Decisions\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 2  D > C\n",
            "\u001b[35mSection_title: 3  D > B\n",
            "\u001b[35mSection_title: 3.1  Motivating Findings\n",
            "\u001b[35mSection_title: 3.1.1  Hallucination Causes Further Hallucination\n",
            "\u001b[35mSection_title: 3.1.2  Logit Output Values Provide a Signal for Hallucination\n",
            "\u001b[35mSection_title: 3.2  Hallucination Detection Performance\n",
            "\u001b[35mSection_title: 3.3  Hallucination Mitigation Performance\n",
            "\u001b[35mSection_title: 3.4  Active Detection and Mitigation\n",
            "\u001b[35mSection_title: 4.2  Multi-hop Questions\n",
            "\u001b[35mSection_title: 4.3  False Premise Questions\n",
            "\u001b[35mSection_title: 4.4  Other Applications\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models. through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average. we also demonstrated the individual efficacy of our detection and mitigation techniques. specifically, our detection technique achieves a high recall and the mitigation technique successfully mitigates a large fraction of the correctly detected hallucinations. notably, the mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. we further demonstrated the effectiveness and wide applicability of our approach and presented several interesting studies including evaluation with another llm (vicuna) and answering multi-hop and false premise questions. overall, our work addresses the llms' hallucination problem and thus contributes to improving their reliability and trustworthiness, a crucial step en route to enabling their widespread adoption in real-world applications.  table 6 shows the instructional prompts used for different steps of our approach. we note that these techniques are the preferred techniques as they do not require calling an external task-specific tool to achieve the corresponding objectives.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Overview\n",
            "\u001b[35mSection_title: 2.2  Hallucination Detection\n",
            "\u001b[35mSection_title: 2.2.1  Identify Key Concepts\n",
            "\u001b[35mSection_title: 2.2.2  Calculate Model's Uncertainty\n",
            "\u001b[35mSection_title: 2.2.3  Create Validation Question\n",
            "\u001b[35mSection_title: 2.2.4  Find Relevant Knowledge\n",
            "\u001b[35mSection_title: 2.2.5  Answer Validation Question\n",
            "\u001b[35mSection_title: 2.3  Hallucination Mitigation\n",
            "\u001b[35mSection_title: 2.4  Design Decisions\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 2  D > C\n",
            "\u001b[35mSection_title: 3  D > B\n",
            "\u001b[35mSection_title: 3.1  Motivating Findings\n",
            "\u001b[35mSection_title: 3.1.1  Hallucination Causes Further Hallucination\n",
            "\u001b[35mSection_title: 3.1.2  Logit Output Values Provide a Signal for Hallucination\n",
            "\u001b[35mSection_title: 3.2  Hallucination Detection Performance\n",
            "\u001b[35mSection_title: 3.3  Hallucination Mitigation Performance\n",
            "\u001b[35mSection_title: 3.4  Active Detection and Mitigation\n",
            "\u001b[35mSection_title: 4.2  Multi-hop Questions\n",
            "\u001b[35mSection_title: 4.3  False Premise Questions\n",
            "\u001b[35mSection_title: 4.4  Other Applications\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models. through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average. we also demonstrated the individual efficacy of our detection and mitigation techniques. specifically, our detection technique achieves a high recall and the mitigation technique successfully mitigates a large fraction of the correctly detected hallucinations. notably, the mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. we further demonstrated the effectiveness and wide applicability of our approach and presented several interesting studies including evaluation with another llm (vicuna) and answering multi-hop and false premise questions. overall, our work addresses the llms' hallucination problem and thus contributes to improving their reliability and trustworthiness, a crucial step en route to enabling their widespread adoption in real-world applications.  table 6 shows the instructional prompts used for different steps of our approach. we note that these techniques are the preferred techniques as they do not require calling an external task-specific tool to achieve the corresponding objectives.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Task Description\n",
            "\u001b[35mSection_title: 4  Training and Evaluation Dataset\n",
            "\u001b[35mSection_title: 5  System Descriptions and Performance\n",
            "\u001b[35mSection_title: 6  Detailed Analysis\n",
            "\u001b[35mSection_title: 6.1  Performance by Table Knowledge Types\n",
            "\u001b[35mSection_title: 6.2  Performance by Relevance Ratings\n",
            "\u001b[35mSection_title: 6.3  Precision@k\n",
            "\u001b[35mSection_title: 6.4  Performance by Lexical Overlap\n",
            "\u001b[32mconclusion: the 2021 edition of the shared task on multi-hop inference for explanation regeneration was a success, with 4 participating teams each substantially improving performance over the baseline model. the best performing team, deepblueai, produced a system that improves absolute performance by 32%, up to 0.820 ndcg, bringing overall state-ofthe-art performance at this relevancy ranking aspect of multi-hop inference to a moderate level. we hope that future systems for many-hop multi-hop inference that aim to build large detailed explanations for question answering will be able to leverage these results to build strong relevancy retrieval subcomponents to augment their compositional inference algorithms. award #1815948, \"explainable natural language inference\"). this edition of the shared task would not have been possible without the hard work of a number of relevance annotators, and their generous offer to anonymously use their data while their work is under review. a special thanks to andré freitas for the helpful discussions. additionally, we would like to thank the computational shared facility of the university of manchester for providing the infrastructure to run our experiments.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Generative Multi-Hop QA\n",
            "\u001b[35mSection_title: 3.1  Model\n",
            "\u001b[35mSection_title: 3.2  Training and Inference\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[32mconclusion: we present hug, a probabilistic, principled approach for explainable multi-hop reasoning without rationale supervision. hug explicitly models multi-hop reasoning by considering the dependency between documents and between sentences within a document. experimental results demonstrate that hug outperforms other state-of-the-art methods that do not rely on rationale labels.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  BERT UNDER THE MICROSCOPE\n",
            "\u001b[35mSection_title: 3.1  Analysis of Transformed Tokens\n",
            "\u001b[35mSection_title: 3.2  Probing BERT's Layers\n",
            "\u001b[35mSection_title: 4  DATASETS AND MODELS 4.1 Datasets\n",
            "\u001b[35mSection_title: 4.2  BERT and GPT-2\n",
            "\u001b[35mSection_title: 4.3  Applying BERT to Question Answering\n",
            "\u001b[35mSection_title: 5  RESULTS AND DISCUSSION\n",
            "\u001b[35mSection_title: 5.1  Phases of BERT's Transformations\n",
            "\u001b[35mSection_title: 5.2  Comparison to GPT-2\n",
            "\u001b[35mSection_title: 5.3  Additional Findings\n",
            "\u001b[32mconclusion: our work reveals important findings about the inner functioning of transformer networks. the impact of these findings and how future work can build upon them is described in the following:\n",
            "\u001b[35mSection_title: 6  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  MultiFactor Question Generation\n",
            "\u001b[35mSection_title: 3.2  Phrase Enhanced Transformer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  The Roles of Q-model and FA-model\n",
            "\u001b[35mSection_title: 4.5  Human Evaluation\n",
            "\u001b[35mSection_title: 4.6  Comparison with LLM-based QG\n",
            "\u001b[32mconclusion: this paper presents multifactor, a novel qg method with multi-level content planning.specifically, multifactor consists of a fa-model, which simultaneously select important phrases and generate an answer-aware summary (a full answer), and q-model, which takes the generated full answer into account for question generation.both fa-model and q-model are formalized as our simple yet effective pet.experiments on hotpotqa and squad 1.1 demonstrate the effectiveness of our method.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Works\n",
            "\u001b[32mlimitation: our work may have some limitations.first, the experiments are only on english corpus.the effectiveness of multifactor is not verified on the datasets of other languages.second, the context length in sentence-level qg task is not very long as shown in table 8.for particularly long contexts (> 500 or 1000), it needs more explorations.multifactor aims to improve the performance of the answer-aware qg task, especially the complex qg.during our research, we did not collect any other datasets, instead conduct our experiments and construct the corresponding full answer on these previously works.our generation is completely within the scope of the datasets.even the result is incorrect, it is still controllable and harmless, no potential risk.the model is currently english language only, whose practical applications is limited in the real world.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2  BioMed Research International\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.2.  Animals and Phenotypes.\n",
            "\u001b[35mSection_title: 2.3.  Small RNA Library Construction and Sequencing.\n",
            "\u001b[35mSection_title: 2.4.  Expression\n",
            "\u001b[35mSection_title: 2.5.  Quantitative RT-PCR.\n",
            "\u001b[35mSection_title: 2.7.  Integration Analysis of miRNA and mRNA Sequencing Data.\n",
            "\u001b[35mSection_title: 2.8.  Functional Enrichment of Target Genes.\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  Small RNA Diversity in Sheep Adipose\n",
            "\u001b[35mSection_title: 3.2.  Differential miRNA Expression in Sheep Adipose Tissues.\n",
            "\u001b[35mSection_title: 3.3.  Validation of miRNA by qPCR.\n",
            "\u001b[35mSection_title: 3.4.  Differential Expression of mRNAs in TS and KS\n",
            "\u001b[35mSection_title: 3.5.  GO and Pathway Enrichment Analyses of Differentially Expressed Genes.\n",
            "\u001b[35mSection_title: 3.6.  Identification of Potential mRNA Targets of miRNAs.\n",
            "\u001b[35mSection_title: 3.7.  Validation of the Target Gene of miR-125a-5p.\n",
            "\u001b[35mSection_title: 3.8.  Functional Enrichment Based on miRNA-Associated Target Genes.\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\u001b[32mconclusion: in conclusion, we identified a number of mirnas that are differentially expressed between the fat-tailed and short-tailed sheep breeds. we further highlighted gene targets of related mirnas that may be involved in regulating fat deposition and adiposeness, in sheep and other livestock. this occurs via the key signaling pathways including focal adhesion, pyruvate metabolism, and the mapk, foxo, and tnf signaling pathway. further studies are needed to verify the correlation between key mirnas and their target genes by in vitro approach and elucidate the functional impacts that mirnas serve during adiposeness. our results also provide evidence for the interaction of mirnas and genes in the regulation of obesity and metabolic syndromes, which suggests that this may serve as an animal model for human' obesity and metabolic syndromes researches.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Pre-Training Tasks\n",
            "\u001b[35mSection_title: 2.2  Variants of the MCROSS Method\n",
            "\u001b[35mSection_title: 3.1  Baselines\n",
            "\u001b[35mSection_title: 3.2  Experiment Results\n",
            "\u001b[32mconclusion: this paper presents a novel pre-training method mcross for extractive qa which contains two tasks: 1) contrastive learning and 2) answer term prediction. specifically, mcross adapts moco frameworks to maintain consistency in answering cloze-like and natural questions, enabling pretrained models to have a more comprehensive understanding of supporting passages. the empirical experiments on three public datasets demonstrate that our approach can obtain noticeable improvements in extractive qa tasks in supervised and zero-shot scenarios.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\u001b[32mlimitation: although mcross can already obtain satisfactory qa performance, due to limited time and computational resources, we only use 5 million cloze-like samples for pre-training, which is one-twentieth of the scale of original sspt experiments.\n",
            "\u001b[35mSection_title: 5  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[32mconclusion: in this work we investigate whether or not there exists a systematic performance difference between prompting plms with code or with text. we confirm that there are indeed tasks for which code prompting is significantly more effective than text prompting and that this finding holds across different types of models. however, for most tasks, we find that text prompting is still the best method for eliciting few-shot generalization from plms. given this result it seems reasonable to attempt to predict which tasks will benefit from code prompts and which tasks will not. however, we show that making such predictions based on simple heuristics such as domain and task category is difficult and that the larger trends remain unclear. future work should seek to investigate the core mechanism behind what makes code prompting effective for certain tasks.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Gotta: The Proposed Framework\n",
            "\u001b[35mSection_title: 3.1  Entity-aware Text Masking\n",
            "\u001b[35mSection_title: 3.2  Prompt-style Data Augmentation\n",
            "\u001b[35mSection_title: 3.3  Generative Prompt-Tuning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup Datasets. Following\n",
            "\u001b[35mSection_title: 4.2  Performance Comparison\n",
            "\u001b[35mSection_title: 4.3  Analysis and Discussions\n",
            "\u001b[35mSection_title: 4.3.1  Entity Masking\n",
            "\u001b[35mSection_title: 4.3.2  Prompt-tuning vs. Multi-task Learning\n",
            "\u001b[35mSection_title: 4.3.3  Question Templates\n",
            "\u001b[35mSection_title: 4.3.4  Case Study\n",
            "\u001b[35mSection_title: 4.3.5  Effect of Augmented Data As shown in\n",
            "\u001b[32mconclusion: in this work, we propose to incorporate the cloze task to improve neural machine question answering with a few training examples. the key idea is to identify and mask the informative entities in the passage and make the model predict them correctly. through empirical experimental studies on various qa benchmarks and different few-shot settings, we show that the cloze task indeed benefits the qa task due to its commonalities. we find different ways of incorporating the cloze task improve the qa task while prompt-tuning brings the most. looking forward, it is of interest to explore qa-dedicated pre-training and ways of pipelining pretraining and prompt-tuning for downstream few-shot qa needs.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  From Question Answering to\n",
            "\u001b[35mSection_title: 3  Humans as meta-answerers\n",
            "\u001b[35mSection_title: 3.1  Human Answering Framework\n",
            "\u001b[35mSection_title: 3.2  Comparing Human meta-answerers to NQ Annotators and the QA system\n",
            "\u001b[35mSection_title: 3.3  A Taxonomy of Outcomes\n",
            "\u001b[35mSection_title: 3.4  Human Strategies\n",
            "\u001b[35mSection_title: 4  Machines as meta-answerer\n",
            "\u001b[35mSection_title: 4.1  System architecture\n",
            "\u001b[35mSection_title: 4.2  Semi-structured Embeddings\n",
            "\u001b[35mSection_title: 4.3  Answer Candidate Selector\n",
            "\u001b[35mSection_title: 4.4  Evidence Selector\n",
            "\u001b[35mSection_title: 4.5  Auxiliary impossibility loss\n",
            "\u001b[35mSection_title: 4.6  Training\n",
            "\u001b[35mSection_title: 5  Machine Experiments\n",
            "\u001b[35mSection_title: 5.1  Machine Meta Answering systems\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Comparing Computers and Humans\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: meta-answering is a framework for qa that attempts to simulate real-world-imperfectinformation-seeking tasks, where humans look for answers in settings mediated by machines, using natural language. human meta-answerers can compete with a bert-based single system with access to full documents, by only looking at a five token window around candidates. a machine meta-answerer built on bert can improve the environment's qa system, thus proving that it is possible to investigate mr in imperfect information settings in high-performance regimes. further, the task brings to the surface, yet again but from a novel perspective, limitations of the current nlu paradigm. mma cannot use the contextual information that is effortlessly exploited by humans. thus, it might prove a suitable framework to advance on these challenges.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  MATERIALS AND METHODS\n",
            "\u001b[35mSection_title: 2.1  Samples Isolation\n",
            "\u001b[35mSection_title: 2.3  Molecular Identification\n",
            "\u001b[35mSection_title: 3.  RESULTS AND DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Methods\n",
            "\u001b[35mSection_title: 2.1.  Reasoning Chain Formulation\n",
            "\u001b[35mSection_title: 2.2.  Hop Spans and Reasoning Type Prediction\n",
            "\u001b[35mSection_title: 2.3.  Adversary Construction\n",
            "\u001b[35mSection_title: 2.3.1.  Bridging and Intersection\n",
            "\u001b[35mSection_title: 2.3.2.  Comparatives and Yes/No\n",
            "\u001b[35mSection_title: 3.  Experiments\n",
            "\u001b[35mSection_title: 3.1.  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2.  Implementation Details\n",
            "\u001b[35mSection_title: 3.3.  Main Results\n",
            "\u001b[35mSection_title: 3.4.  Comparison with other attack methods\n",
            "\u001b[35mSection_title: 3.5.  Ablation Studies\n",
            "\u001b[35mSection_title: 3.5.1.  Hop Spans\n",
            "\u001b[35mSection_title: 3.5.2.  Entities\n",
            "\u001b[35mSection_title: 3.5.3.  Adversary Placement\n",
            "\u001b[35mSection_title: 3.6.  Adversarial Retraining\n",
            "\u001b[35mSection_title: 4.  Related Work\n",
            "\u001b[35mSection_title: 4.1.  Multi-hop QA\n",
            "\u001b[35mSection_title: 4.2.  Adversarial Attack\n",
            "\u001b[32mconclusion: in this work, we propose a reasoning chain based adversarial attack for multi-hop qa. by formulating the multihop reasoning process with a reasoning chain, we can identify different reasoning types and customize adversary design for each type. our method allows to attack any certain hop by identifying different hop spans of the question, and making modification on relational words. the hop-targeted attack can inspect models' error-prone parts during the reasoning process specifically. three qa models under evaluation both exhibit poor performance in face of adversaries, suggesting that they are not robust enough and have limited interpretability of conducting multi-hop reasoning. our adversarial evaluation can be utilized to improve models' performance by adversarial retraining as well as motivate new model development according to the weakness detected.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Knowledgeable Path Generator\n",
            "\u001b[35mSection_title: 3.1  Knowledge Path Sampling\n",
            "\u001b[35mSection_title: 3.2  Generating Paths to Connect Entities\n",
            "\u001b[35mSection_title: 3.3  Adapted Commonsense QA Framework\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  KG and Path Data Preparation\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 4.4  Model Variations\n",
            "\u001b[35mSection_title: 4.5  Results\n",
            "\u001b[35mSection_title: 4.6  Study of Path Quality & Interpretability\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we propose a generator of multi-hop knowledge paths, which provides structured evidence for answering commonsense questions. the generator, learned by fine-tuning gpt-2 on random walks sampled from conceptnet, produces a path between each pair of question and answer entities. all generated paths are aggregated into a knowledge embedding and fused with a context embedding given by a text encoder for classification. our qa framework enhanced with this generator outperformes both pre-trained language models and prior kg-augmented methods on two commonsense qa benchmarks. the accuracy gain increases with less training data. furthermore, automatic-and human-based evaluations of the generated paths yield high scores for their validity, novelty, and relevance. future research should investigate how to optimally fuse the knowledge and the context embeddings. it should also address the ambiguity of the entity mentions in the questions, the answers, and the lexical nodes in conceptnet.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Dataset Collection\n",
            "\u001b[35mSection_title: 2.1  Riddle Collection\n",
            "\u001b[35mSection_title: 2.2  Distractor Generation\n",
            "\u001b[35mSection_title: 2.3  Candidate Introduction Collection\n",
            "\u001b[35mSection_title: 3  Dataset Analysis\n",
            "\u001b[35mSection_title: 4  Methods\n",
            "\u001b[35mSection_title: 4.1  Baseline Models\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metric\n",
            "\u001b[35mSection_title: 4.3  Experimental Setup\n",
            "\u001b[35mSection_title: 4.4  Human Performance Evaluation\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this paper, we introduce birdqa, a large-scale, bilingual multiple-choice question answering dataset to facilitate the development of qa systems capable of solving tricky riddles. the huge gap between the human and machine leaves much room for improvement. in future work, we plan to extend birdqa with riddles in other languages and incorporate figurative language understanding into riddle solving. we hope that birdqa will stir more research for question answering on riddles.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Decomposition\n",
            "\u001b[35mSection_title: 3.3  Single-hop Reading Comprehension\n",
            "\u001b[35mSection_title: 3.4  Decomposition Scorer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  HOTPOTQA\n",
            "\u001b[35mSection_title: 4.3  Baseline Models\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 4.5  Evaluating Robustness\n",
            "\u001b[35mSection_title: 4.6  Ablations\n",
            "\u001b[32mconclusion: we proposed decomprc, a system for multihop rc that decomposes a multi-hop question into simpler, single-hop sub-questions. we recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions. moreover, decomprc achieved further gains from the decomposition scoring step. decomprc achieved the state-of-the-art on hotpotqa distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines. in this section, we describe span annotation collection procedure for bridging and intersection questions.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Datasets\n",
            "\u001b[35mSection_title: 4  Models\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Data Preprocessing\n",
            "\u001b[35mSection_title: 6  Results and Error Analysis\n",
            "\u001b[35mSection_title: 6.2  HotpotQA\n",
            "\u001b[35mSection_title: 6.3  MSMARCO\n",
            "\u001b[35mSection_title: 6.4  SearchQA\n",
            "\u001b[32mconclusion: we conclude our discussion by presenting suggestions for good future practices when building and presenting new models and datasets. we constructively offer these points and have no intent to criticize authors whose prior work we reference.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Retrieval and memory augmented models\n",
            "\u001b[35mSection_title: 2.1.1  Fusion-in-Decoder\n",
            "\u001b[35mSection_title: 2.2  Vector quantization\n",
            "\u001b[35mSection_title: 2.2.1  Product quantization\n",
            "\u001b[35mSection_title: 2.2.2  VQ-VAE\n",
            "\u001b[35mSection_title: 3  MEMORY-VQ\n",
            "\u001b[35mSection_title: 4.2  Main results\n",
            "\u001b[35mSection_title: 4.3  Quality-Compression rate trade-off\n",
            "\u001b[35mSection_title: 4.4  Ablations\n",
            "\u001b[35mSection_title: 5  Related work\n",
            "\u001b[32mconclusion: we introduced memory-vq, a novel approach for reducing the storage requirements of memoryaugmented language models without compromising performance. by employing vq-vae to compress token representations, we obtain a lumen model with 16x compression, denoted as lumen-vq. remarkably, lumen-vq maintains performance close to lumen and fid and benefits from lumen inference speed-ups with sharply reduced storage cost. using memory-vq, memory augmentation is a practical solution for drastic inference speedups with extensive retrieval corpora. fine-tuning during fine-tuning, we utilize the adafactor optimizer (shazeer and stern, 2018) with a constant learning rate of 0.0001, a batch size of 128, and a dropout rate of 0.1 for all tasks. when performing multi-task training, we uniformly sample from the tasks. we allocate 48 and 304 tokens for question and passage inputs, respectively. lumen-vq is using 0.999 as an ema factor for code updates.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  VGaokao: Verification Style Reading Comprehension Dataset\n",
            "\u001b[35mSection_title: 2.1  Dataset Construction\n",
            "\u001b[35mSection_title: 2.2  Dataset Analysis\n",
            "\u001b[35mSection_title: 3  Our Extract-Integrate-Compete Approach\n",
            "\u001b[35mSection_title: 3.1  Iterative Evidence Extraction\n",
            "\u001b[35mSection_title: 3.2  Adaptive Evidence Integration\n",
            "\u001b[35mSection_title: 3.3  Pairwise Option Competition\n",
            "\u001b[35mSection_title: 3.4  Implementation Details\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Main Results\n",
            "\u001b[35mSection_title: 4.2  Ablation Study\n",
            "\u001b[35mSection_title: 5  Related Works\n",
            "\u001b[32mconclusion: in this paper, we present a novel verification style reading comprehension dataset named vgaokao from the chinese language tests of gaokao for chinese native speakers, which embed multiple advanced language understanding skills. to address the challenges in vgaokao, we propose a new extract-integrate-compare approach for complementary evidence retrieval/integration and option discrimination. experiments show that our approach outperforms several strong baselines, with additional merits of efficiency and explainability. we believe vgaokao is a challenging test-bed for natural language understanding in chinese and encourage further research in verification style reading comprehensionn.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  TAC\n",
            "\u001b[35mSection_title: 3.  ATTac-2000\n",
            "\u001b[35mSection_title: 3.1.1  Flights\n",
            "\u001b[35mSection_title: 3.1.2  Hotels\n",
            "\u001b[35mSection_title: 3.1.3  Entertainment Tickets\n",
            "\u001b[35mSection_title: 3.2  Allocation Strategy\n",
            "\u001b[35mSection_title: 3.3  Adaptivity\n",
            "\u001b[35mSection_title: 3.3.1  Timing of Bidding Modes\n",
            "\u001b[35mSection_title: 3.3.2  Allocation\n",
            "\u001b[35mSection_title: 3.3.3  Hotel Bidding\n",
            "\u001b[35mSection_title: 4.  Results\n",
            "\u001b[35mSection_title: 4.1  The Competition\n",
            "\u001b[35mSection_title: 4.2  Controlled Testing\n",
            "\u001b[35mSection_title: 5.  Related Work\n",
            "\u001b[32mconclusion: tac-2000 was the rst autonomous bidding agent competition. while it was a very successful event, some minor improvements would increase its interest from a multiagent learning perspective.\n",
            "\u001b[35mSection_title: 6.  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Building IIRC\n",
            "\u001b[35mSection_title: 2.1  Seed Paragraphs\n",
            "\u001b[35mSection_title: 2.2  Collecting Questions\n",
            "\u001b[35mSection_title: 2.3  Collecting Answers\n",
            "\u001b[35mSection_title: 2.4  Dataset Analysis\n",
            "\u001b[35mSection_title: 3.2  Baseline Model\n",
            "\u001b[35mSection_title: 3.2.1  Identifying Links\n",
            "\u001b[35mSection_title: 3.2.2  Selecting Context\n",
            "\u001b[35mSection_title: 3.2.3  QA Model\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Results and Discussion\n",
            "\u001b[35mSection_title: 4.4  Error Analysis\n",
            "\u001b[35mSection_title: 5  Combined Evaluation\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we introduced iirc, a new dataset of incompleteinformation reading comprehension questions. these questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways. our baseline model, built on top of state-ofthe-art models for the most closely related existing datasets, performs quite poorly in this setting, even when given oracle retrieval results, and especially when combined with other reading comprehension datasets. iirc both provides a promising new avenue for studying complex reading and retrieval problems and demonstrates that much more research is needed in this area.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  MODEL COMPRESSION METHODS: OVERVIEW\n",
            "\u001b[35mSection_title: 2  PRUNING\n",
            "\u001b[35mSection_title: 2.1  Pruning Weights\n",
            "\u001b[35mSection_title: 2.1.1  Hessian based Methods.\n",
            "\u001b[35mSection_title: 2.1.2  Magnitude Pruning Methods.\n",
            "\u001b[35mSection_title: 2.1.3  Iterative Magnitude Pruning\n",
            "\u001b[35mSection_title: 2.1.4  Iterative Magnitude\n",
            "\u001b[35mSection_title: 2.2  Pruning Neurons\n",
            "\u001b[35mSection_title: 2.2.2  Removing Redundant Neurons.\n",
            "\u001b[35mSection_title: 2.3  Pruning Blocks\n",
            "\u001b[35mSection_title: 2.4  Pruning Heads and Layers\n",
            "\u001b[35mSection_title: 2.4.2  Pruning\n",
            "\u001b[35mSection_title: 2.5  Summary\n",
            "\u001b[35mSection_title: 3  QUANTIZATION\n",
            "\u001b[35mSection_title: 3.1  Binarized Networks\n",
            "\u001b[35mSection_title: 3.1.1  Deterministic Binarization.\n",
            "\u001b[35mSection_title: 3.1.2  Stochastic Binarization.\n",
            "\u001b[35mSection_title: 3.1.3  Loss Aware Binarization (LAB).\n",
            "\u001b[35mSection_title: 3.2  Ternarized Networks\n",
            "\u001b[35mSection_title: 3.2.1  Ternary Weight Networks.\n",
            "\u001b[35mSection_title: 3.2.2  Trained Ternary Quantization.\n",
            "\u001b[35mSection_title: 3.2.3  Hybrid Ternary Quantization.\n",
            "\u001b[35mSection_title: 3.3  General Quantized Networks\n",
            "\u001b[35mSection_title: 3.3.2  Balanced Quantization.\n",
            "\u001b[35mSection_title: 3.3.3  KMeans based Quantization\n",
            "\u001b[35mSection_title: 3.3.4  Loss Aware Quantization (LAQ).\n",
            "\u001b[35mSection_title: 3.4  Summary\n",
            "\u001b[35mSection_title: 4  KNOWLEDGE DISTILLATION (KD)\n",
            "\u001b[35mSection_title: 4.1  Various Distillation Architectures\n",
            "\u001b[35mSection_title: 4.2  Collaborative Learning\n",
            "\u001b[35mSection_title: 4.3  Multiple Teachers\n",
            "\u001b[35mSection_title: 4.4  Distilling Transformers\n",
            "\u001b[35mSection_title: 4.5  Summary\n",
            "\u001b[35mSection_title: 5  PARAMETER SHARING\n",
            "\u001b[35mSection_title: 5.1  Character-aware Language Models\n",
            "\u001b[35mSection_title: 5.3  Parameter Sharing in Transformers\n",
            "\u001b[35mSection_title: 5.4  Summary\n",
            "\u001b[35mSection_title: 6  TENSOR DECOMPOSITION\n",
            "\u001b[35mSection_title: 6.1  Two Low-Rank Factors\n",
            "\u001b[35mSection_title: 6.2  Factorizing into Block Diagonal Matrices\n",
            "\u001b[35mSection_title: 6.3  Tensor Train and Block Term Decomposition\n",
            "\u001b[35mSection_title: 6.4  Summary\n",
            "\u001b[35mSection_title: 7.1  Transformers with Super-Linear Complexity\n",
            "\u001b[35mSection_title: 7.2  Transformers with Linear Complexity\n",
            "\u001b[35mSection_title: 7.3  Summary\n",
            "\u001b[35mSection_title: 8  SUMMARY AND FUTURE DIRECTIONS\n",
            "\u001b[35mSection_title: 8.1  Comparison across model compression method types.\n",
            "\u001b[35mSection_title: 8.2  Summary of Applications\n",
            "\u001b[35mSection_title: 8.3  Future Trends\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Baseline Experiment\n",
            "\u001b[35mSection_title: 2.1  Baseline Procedure\n",
            "\u001b[35mSection_title: 2.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 2.3  Baseline Performance\n",
            "\u001b[35mSection_title: 3  Incorporating Alternate Prompts\n",
            "\u001b[35mSection_title: 3.1  Investigating Initial Prompt Variations\n",
            "\u001b[35mSection_title: 3.2  Investigating Move Prompt Variations\n",
            "\u001b[35mSection_title: 3.3  Reasoning in Natural Language\n",
            "\u001b[35mSection_title: 3.4  Describing State in Natural Language\n",
            "\u001b[35mSection_title: 4  Analyzing ChatGPT's Strategic Behavior\n",
            "\u001b[35mSection_title: 4.1  Illegal Move Diversity\n",
            "\u001b[35mSection_title: 4.2  Game Level Performance Evaluation\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  ChatGPT's Performance Overview\n",
            "\u001b[32mlimitation: chatgpt's self-attention mechanism plays a crucial role in its performance, especially in chess gameplay. our experiments reveal two critical limitations of transformer-based lms like chatgpt when trained on natural language. the first limitation is related to the increase in imr and rblm over the course of a game. as highlighted by [stöckl, 2021], gpt-2 models are found to devote less attention to san notation tokens that are farther away from the latest input. since a complete game memory is paramount for models to accurately track the board state [toshniwal et al., 2022], we postulate that chatgpt's disproportionate attention allocation might be the cause of a significant portion of its mistakes. this limitation is evident across all variations that depend on formal language but don't actively reinforce the game state, which presents a challenge to the effectiveness of llms in tasks requiring extended conversation memory.the second limitation pertains to the tendency of natural language trained llms to neglect formal language where tokens are used in an unconventional manner. maynez et al.[2020] noted that lms typically remain indifferent to noises or artifacts in training data, which we argue may also apply to formal languages like chess notations. this issue is particularly evident in the int-rules variation, where despite the introduction of helpful data, chatgpt's performance dropped substantially. we hypothesize that this may be due to the model shifting its focus towards the rules, thereby reducing the attention allocated to the game board.these identified limitations, while challenging, also provide valuable insights for future research. for instance, addressing the second limitation might involve frequent repe-tition of formal language sequences, potentially leading to more substantial improvements in game performance. our findings is a first step towards investigating techniques such as token repetition's impact on model performance, laying the ground work for future work to explore how we can mitigate the impact of disproportionate attention allocation.\n",
            "\u001b[35mSection_title: 5.2  Limitations of ChatGPT's Self-Attention Mechanism in Chess Gameplay\n",
            "\u001b[35mSection_title: 5.3  Intent Behind LLMs' Decisions\n",
            "\u001b[32mconclusion: in summary, our investigation reveals that despite its exceptional capabilities in natural language processing, chatgpt faces considerable challenges with complex reasoning tasks involving formal language, as evidenced by its chess gameplay performance. the model's attention mechanism exhibits limitations in adequately recognizing tokens used in formal language, resulting in a suboptimal understanding of the game board. interestingly, our findings indicate that consistent repetition of relevant information throughout a conversation can partially alleviate this limitation. yet, despite chat-gpt's capacity to learn and internalize rules, the model struggles with self-regulation, which neither in-prompt instructions nor improved board comprehension appear to enhance. additionally, we find that the model's decision-making focus, or \"intent,\" can be strengthened by allowing nl reasoning, providing nl chessboard descriptions or enabling a clearer representation of the game board. future research could examine how this disproportionate attention allocation impacts other tasks that involve formal language and necessitate complex cognitive processing. in conclusion, while chatgpt stands as a remarkable advancement in artificial intelligence, it continues to face significant limitations, especially in nonlinguistic contexts. these findings highlight the necessity for further refinement before chatgpt, and models of its kind, can be considered reliable tools for practical applications requiring complex cognition akin to human abilities.   \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  DNA Constructs, Cell Culture, and Transfections\n",
            "\u001b[35mSection_title: 2.2.  Reporter Gene Assays\n",
            "\u001b[35mSection_title: 2.3.  Co-Immunoprecipitation (IP)\n",
            "\u001b[35mSection_title: 2.4.  Confocal Laser Scanning Microscopy (CLSM)\n",
            "\u001b[35mSection_title: 2.5.  Statistical Analysis\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\u001b[35mSection_title: 3.1.  Antagonism of IFN Signaling Differs between ABLVi and ABLVf P Proteins\n",
            "\u001b[35mSection_title: 3.2.  Nuclear Trafficking Differs between ABLVf and ABLVi P Protein\n",
            "\u001b[35mSection_title: 3.3.  Differing Nuclear Trafficking and IFN Antagonism Depend on Distinct Regions of ABLV P Protein\n",
            "\u001b[35mSection_title: 3.4.  Differing Antagonistic Functions of ABLV P Protein Are Due to Altered Interaction with STAT1\n",
            "\u001b[35mSection_title: 3.4.  Differing Antagonistic Functions of ABLV P protein are due to Altered Interaction with STAT1\n",
            "\u001b[32mconclusion: taken together, our data indicate that the p proteins of the two established lineages of ablv, which are associated with frugivorous and insectivorous bats, differ in immune evasion and nuclear trafficking functions. the effects on immune antagonism and nuclear import are independent, resulting from altered sequences in different regions of the proteins, indicative of distinct mechanisms including altered interactions with stats (see summary in table 1). these data suggest that adaptation to the different bat hosts includes changes in p protein, which has multiple roles in viral infection including in replication and immune evasion. these data are consistent with potential differences in the virus-host interface of different bat species.  figure s1. cos7 cells were transfected to express the indicated proteins or empty vector (ev) and with plasmids for the ifnα/stat1 reporter assay before treatment with or without ifnα and calculation of luciferase activity, as described in the legend of figure 2. **** p < 0.0001. supplementary figure s2. hela cells expressing the indicated proteins were treated with or without lmb before analysis by clsm and calculation of fn/c (mean ± sem), as described in the legend of figure 2. **** p < 0.0001, n ≥ 40 cells for each condition.  \n",
            "\u001b[35mSection_title: 4.  Conclusions\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  DNA Constructs, Cell Culture, and Transfections\n",
            "\u001b[35mSection_title: 2.2.  Reporter Gene Assays\n",
            "\u001b[35mSection_title: 2.3.  Co-Immunoprecipitation (IP)\n",
            "\u001b[35mSection_title: 2.4.  Confocal Laser Scanning Microscopy (CLSM)\n",
            "\u001b[35mSection_title: 2.5.  Statistical Analysis\n",
            "\u001b[35mSection_title: 3.  Results and Discussion\n",
            "\u001b[35mSection_title: 3.1.  Antagonism of IFN Signaling Differs between ABLVi and ABLVf P Proteins\n",
            "\u001b[35mSection_title: 3.2.  Nuclear Trafficking Differs between ABLVf and ABLVi P Protein\n",
            "\u001b[35mSection_title: 3.3.  Differing Nuclear Trafficking and IFN Antagonism Depend on Distinct Regions of ABLV P Protein\n",
            "\u001b[35mSection_title: 3.4.  Differing Antagonistic Functions of ABLV P Protein Are Due to Altered Interaction with STAT1\n",
            "\u001b[35mSection_title: 3.4.  Differing Antagonistic Functions of ABLV P protein are due to Altered Interaction with STAT1\n",
            "\u001b[32mconclusion: taken together, our data indicate that the p proteins of the two established lineages of ablv, which are associated with frugivorous and insectivorous bats, differ in immune evasion and nuclear trafficking functions. the effects on immune antagonism and nuclear import are independent, resulting from altered sequences in different regions of the proteins, indicative of distinct mechanisms including altered interactions with stats (see summary in table 1). these data suggest that adaptation to the different bat hosts includes changes in p protein, which has multiple roles in viral infection including in replication and immune evasion. these data are consistent with potential differences in the virus-host interface of different bat species.  figure s1. cos7 cells were transfected to express the indicated proteins or empty vector (ev) and with plasmids for the ifnα/stat1 reporter assay before treatment with or without ifnα and calculation of luciferase activity, as described in the legend of figure 2. **** p < 0.0001. supplementary figure s2. hela cells expressing the indicated proteins were treated with or without lmb before analysis by clsm and calculation of fn/c (mean ± sem), as described in the legend of figure 2. **** p < 0.0001, n ≥ 40 cells for each condition.  \n",
            "\u001b[35mSection_title: 4.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Embedding Models\n",
            "\u001b[35mSection_title: 3  The MTEB Benchmark\n",
            "\u001b[35mSection_title: 3.1  Desiderata\n",
            "\u001b[35mSection_title: 3.2  Tasks and Evaluation\n",
            "\u001b[35mSection_title: 3.3  Datasets\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Analysis\n",
            "\u001b[35mSection_title: 4.3  Efficiency\n",
            "\u001b[35mSection_title: 4.4  Multilinguality\n",
            "\u001b[32mconclusion: in this work, we presented the massive text embedding benchmark (mteb). consisting of 8 text embedding tasks with up to 15 datasets each and covering 112 languages, mteb aims to provide reliable embedding performance estimates. by opensourcing mteb alongside a leaderboard, we provide a foundation for further pushing the state-ofthe-art of available text embeddings.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[32mlimitation: while mteb aims to be a diverse benchmark to provide holistic performance reviews, the benchmark has its limitations. we list them here:1. long document datasets mteb covers multiple text lengths (s2s, p2p, s2p), but very long documents are still missing. the longest datasets in mteb have a few hundred words, and longer text sizes could be relevant for use cases like retrieval.2. task imbalance tasks in mteb have a different amount of datasets with summarization consisting of only a single dataset. this means mteb average scores, which are computed over all datasets, are biased towards tasks with many datasets, notably retrieval, classification and clustering. as mteb grows, we hope to add more datasets to currently underrepresented tasks like summarization or pair classification.3. multinguality mteb contains multilingual classification, sts and bitext mining datasets. however, retrieval and clustering are english-only. sgpt-bloom-7b1-msmarco is geared towards multilingual retrieval datasets and due to the lack thereof cannot be comprehensively benchmarked in mteb. further, mteb does not contain any code datasets that could be used to benchmark code models (neelakantan et al., 2022;allal et al., 2023). it should be easy to extend mteb with datasets, such as codesearchnet \n",
            "\u001b[35mSection_title: 6  Limitations of MTEB\n",
            "\u001b[35mSection_title: 4.  Additional modalities\n",
            "\u001b[35mSection_title: 1  STSBenchmark\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Formulation\n",
            "\u001b[35mSection_title: 3.1  Relevance Estimator\n",
            "\u001b[35mSection_title: 3.2  Evidence Estimator\n",
            "\u001b[35mSection_title: 3.3  Centrality Estimator\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[32mconclusion: in this work, we proposed a coarse-to-fine estimation framework for query focused multi-document summarization. we explored the potential of leveraging distant supervision signals from question answering to better capture the semantic relations between queries and document segments. experimental results across datasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more  gpus with 11gb memory. for the answer sentence selection model, bert was fine-tuned with a learning rate of 3 × 10 −6 and a batch size of 16 for 3 epochs . for span selection, we adopted a learning rate of 3 × 10 −5 and a batch size of 64 for 5 epochs. during inference, the confidence threshold for the relevance estimator was set to θ = 0.75 (kratzwald and feuerriegel, 2018) for both sentence and passage retrieval. for the evidence estimator, k qa was tuned on the development set. we obtained 90 and 110 evidence sentences from the sentence selection and span selection models, respectively. for the centrality estimator, the influence of the query was set to φ = 0.15 (wan, 2008;wan and zhang, 2014 \n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Machine Reading Comprehension Task\n",
            "\u001b[35mSection_title: 2.2  Definitions and Terminologies\n",
            "\u001b[35mSection_title: 3  Measuring Shortcuts\n",
            "\u001b[35mSection_title: 3.1  Adversarial Data Evaluation\n",
            "\u001b[35mSection_title: 3.2  Artifact-Based Models\n",
            "\u001b[35mSection_title: 3.3  Intermediate Reasoning Task Evaluation\n",
            "\u001b[35mSection_title: 3.4  Language Understanding Skills Evaluation\n",
            "\u001b[35mSection_title: 3.5  Summary & Discussion\n",
            "\u001b[35mSection_title: 4  Mitigating Shortcuts\n",
            "\u001b[35mSection_title: 4.1  Training on Adversarial Data\n",
            "\u001b[35mSection_title: 4.2  Altering the Training Process\n",
            "\u001b[35mSection_title: 4.3  Utilizing Intermediate Reasoning Tasks\n",
            "\u001b[35mSection_title: 4.4  Other Approaches\n",
            "\u001b[35mSection_title: 4.5  Summary & Discussion\n",
            "\u001b[35mSection_title: 5  Future Directions\n",
            "\u001b[32mconclusion: we have covered the shortcut identification and mitigation landscape in mrc. the presence of shortcuts can be made clear through a variety of methods, and most researchers are aware of this issue. mitigation methods are varied and have some degree of success, but a lot more work is needed before we can achieve models mostly free of shortcut biases. efforts should be made to improve mrc shortcut debiasing techniques by incorporating those found in other fields such as computer vision and nli, as well as finding methods with lower human and/or computation costs.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data\n",
            "\u001b[35mSection_title: 3.1  Task Formulation\n",
            "\u001b[35mSection_title: 3.2  Data Collection\n",
            "\u001b[35mSection_title: 3.3  Data Analysis\n",
            "\u001b[35mSection_title: 4  Approaches\n",
            "\u001b[35mSection_title: 4.1  Rule-based Methods\n",
            "\u001b[35mSection_title: 4.2  Neural Models\n",
            "\u001b[35mSection_title: 5.1  Experimental Settings\n",
            "\u001b[35mSection_title: 5.2  Baseline Results\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[32mconclusion: we present the first open-domain multiple-choice question answering dataset for solving medical problems, medqa, collected from the real-world professional examinations, requiring extensive and advanced domain knowledge to answer questions. this dataset covers three languages: english, simplified chinese, and traditional chinese. together with the question data, we also collect and release a largescale corpus from medical textbooks from which the reading comprehension models can obtain necessary knowledge for answering the questions. we implement several state-of-theart methods as baselines to this dataset by cascading two components: document retrieval and reading comprehension. and experimental results demonstrate that even current best approach cannot achieve good performance on these data. we anticipate more research efforts from the community can be devoted to this dataset so that future openqa models can be strong enough to solve such real-world complex problems.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Results\n",
            "\u001b[35mSection_title: 2.1.  TP73 Is Preferentially Constitutively Expressed in TP53 wt HMCLs\n",
            "\u001b[35mSection_title: 2.2.  Decitabine Decreased TP73 Methylation and Induced TP73 Expression\n",
            "\u001b[35mSection_title: 2.2.  Decitabine Decreased TP73 Methylation and Induced TP73 Expression\n",
            "\u001b[35mSection_title: 2.3.  Melphalan and Cisplatin Induced TP73 Expression in TP53 wt HMCLs but Not in Decitabine-Treated TP53 mut HMCLs\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[35mSection_title: 4.  Materials and Methods\n",
            "\u001b[35mSection_title: 4.1.  HMCLs and Reagents\n",
            "\u001b[35mSection_title: 4.2.  Bisulfite Treatment, Methylation Specific PCRs and RT-PCR\n",
            "\u001b[35mSection_title: 4.3.  Western Blotting\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Observations and Data Reductions\n",
            "\u001b[35mSection_title: 2.1.  Monitoring with MAXI\n",
            "\u001b[35mSection_title: 2.2.  RXTE Observations\n",
            "\u001b[35mSection_title: 2.3.  Suzaku Observation\n",
            "\u001b[35mSection_title: 3.  Analysis and Results\n",
            "\u001b[35mSection_title: 3.1.  CRSF in X-ray Spectra by RXTE and Suzaku\n",
            "\u001b[35mSection_title: 3.2.  CRSF energy variation\n",
            "\u001b[35mSection_title: 4.  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Image-Text Multi-modality\n",
            "\u001b[35mSection_title: 2.2  Multi-Hop Reasoning\n",
            "\u001b[35mSection_title: 2.3  Visual Question Answering (VQA)\n",
            "\u001b[35mSection_title: 3  VLQA Dataset\n",
            "\u001b[35mSection_title: 3.1  Task Overview\n",
            "\u001b[35mSection_title: 3.2  Constructing VLQA\n",
            "\u001b[35mSection_title: 3.2.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2.2  Ensuring dataset integrity\n",
            "\u001b[35mSection_title: 3.3  VLQA Dataset Analysis\n",
            "\u001b[35mSection_title: 4  Benchmarking\n",
            "\u001b[35mSection_title: 5  Fusion of HOpping and Logical Entailment (HOLE) to solve VLQA\n",
            "\u001b[35mSection_title: 5.1  Modality Hopping based Solver\n",
            "\u001b[35mSection_title: 5.2  Logical Entailment based Reasoner\n",
            "\u001b[35mSection_title: 6  Results & Discussion\n",
            "\u001b[32mconclusion: in this work, we introduced the visuo-linguistic question answering (vlqa) challenge that we believe has the potential to open new research avenues in areas of joint vision & language. our experiments show that a system equipped with state-of-the-art vision-language pre-training does not perform well on the task that requires joint image-text inference. there is a room for significant improvement in capability of these models to tackle multi-modal contexts. our future work would include further expansion of this dataset and building generic ai models that can learn novel visual concepts from a small set of examples.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.2  A NEW TRAINING PIPELINE: SMOE-DROPOUT\n",
            "\u001b[35mSection_title: 4  EXPERIMENT\n",
            "\u001b[35mSection_title: 4.1  IMPLEMENTATION DETAILS\n",
            "\u001b[35mSection_title: 4.2  SUPERIOR PERFORMANCE OF SMOE-DROPOUT\n",
            "\u001b[35mSection_title: 4.4  EXTRA INVESTIGATION AND ABLATION STUDY\n",
            "\u001b[32mconclusion: in this paper, we present a novel plug-and-play smoe-dropout strategy for training overparameterized transformers in full-capacity settings without collapse. we design a fixed and randomly initialized router to assign experts and gradually increase their number along with the training. as a result, our proposal provides an appealing \"self-slimmable\" property to large transformers during inference and downstream fine-tuning, depending on available resources. it implies alleviated representation collapse and delivers an in-situ trade-off between efficiency and performance. extensive experiments across various combinations of network backbone and dataset, consistently demonstrate the significantly improved performance and training time savings from our algorithm. future work includes the extension of other network architectures and tasks like vision recognition. smoe-dropout (k = n 2 ) 82.03 ± 0.26 smoe-dropout (k = n) 82.32 ± 0.14 to evaluate the stability of the improvement obtained by our smoe-dropout, we carry out further experiments of transformer-xl on sst-2. the results are reported in table a4, from which we can observe that our smoe-dropout achieves a statistically significant improvement of 0.93% ∼ 1.17% accuracy gains compared with other smoe-variants and the dense network, where there is no overlap between the error bars (one standard deviation). table a5 demonstrates that both random routing policy and progressively increasing the number of activated experts are beneficial for alleviating representation collapse and providing \"selfslimmable\" property, yet not as good as combining both. to be specific, when applying the strategy of progressively enlarging the number of activated experts, the learnable smoes suffer less representation collapse and achieve better performance, i.e., 0.31% higher accuracy. meanwhile, we find that learnable smoe with curriculum learning has the \"self-slimmable\" property only when activating experts from k = 1 to k = 8. however, the performance starts to degrade if using more experts like k = 16. as for our smoe-dropout with a random routing, it enjoys a better \"self-slimmable\" property from k = 1 to k = 16 (full model capacity), with up to 0.87% higher accuracy on sst-2 across all scenarios, compared to its learnable variants. we conduct a further transfer study of the pre-trained bert networks on a multi-hop questionanswering dataset, hotpotqa yang et al. (2018). and we use exact match (em) accuracy to assess networks' performance. following the same metric in press et al. (2022), we calculate the compositionality gap, i.e., the gap of em accuracy between multi-hop question answering and its all single-hop sub-questions , of each network. as shown in table a6, our smoe-dropout is beneficial for reducing the compositionality gap, which achieves the best performance with up to 0.30% higher em score and 0.30% narrower compositionality gap, compared with the learnable smoe and its dense counterpart. \n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Defining Compatibility\n",
            "\u001b[35mSection_title: 3.2  Mining Silver Labels\n",
            "\u001b[35mSection_title: 3.3  Matching Retrieved and LLM-generated Passages into Pairs\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[32mconclusion: in this work, we study the problem of merging retrieved and llm-generated knowledge for opendomain qa.we tackle the challenge of knowledge conflicts caused by llm's hallucination with a compatibility-oriented knowledge merging framework (combo).specifically, we match llmgenerated and retrieved passages for a given question into pairs based on their compatibility and perform information fusion on the encoder side of the fid-based reader by feeding matched pairs as input.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 4  Efficacy of Fusion in Decoder\n",
            "\u001b[35mSection_title: 4.1  Needle in a haystack\n",
            "\u001b[35mSection_title: 4.2  Piecing a puzzle\n",
            "\u001b[35mSection_title: 4.3  Ablations of design choices\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Settings\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Datasets analysis\n",
            "\u001b[35mSection_title: 5.4  Effect of context padding\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mlimitation: we present sled as a simple and effective method to extend the capabilities of pretrained short-text models to long-text tasks. despite its impressive empirical performance on scrolls, sled suffers from two disadvantages which may limit its applicability to some long-range tasks.long output to obtain linear complexity, sled assumes the output length k is constant. this is since the decoder uses quadratic selfattention over the output, on top of o(nk) crossattention between the output and input. while most current long-text tasks follow this assumption, future tasks, such as academic reports or script writing, may require long text generation. this limitation is not unique to sled and affects other long-range transformers including longt5 and led. aside from finetuning, this also affects pretraining models on long inputs with selfsupervised losses such as span-corruption (raffel et al., 2020b) or denoising (lewis et al., 2020), which require the decoder to process an output that is linear in the length of the input.co-reference resolution and fact retention an assumption at the heart of sled is the locality of information assumption. when the input text is long, this assumption may break if distant entity resolution or factual knowledge are required. for example, a chapter in a book may mention \"they were walking into the room\" when knowledge of what room or who walked is located a few chapters back. in such cases, the encoder used by sled will not be able to access this information, moving more responsibility to the decoder and reducing the effectiveness of the contextual encoding. similarly, in multi-hop questions (yang et al., 2018), attending to one part of the context is necessary in order to fully understand the question and encode a second piece of information correctly. as the encoder will not have access to the first context that leads to better question understanding, here as well more responsibility is delegated to the decoder.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\u001b[32mconclusion: in this work we present sled, a simple approach for modeling long texts which slides a pretrained short-range encoder over a long input document and then generates an output by attending to the encoded tokens. we show sled can perform core operations that are important for long text understanding, such as finding relevant pieces of information and fusing them at decoding time, and demonstrate competitive performance on the scrolls benchmark compared to larger models and models that employ a dedicated and expensive pretraining step. one of sled's most attractive features is that it can be readily used with any short-range pretrained lm. thus, any future encoder-decoder model can be flexibly plugged into it to achieve further gains in performance on scrolls, some of its tasks, or any other long-range task.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 4  Efficacy of Fusion in Decoder\n",
            "\u001b[35mSection_title: 4.1  Needle in a haystack\n",
            "\u001b[35mSection_title: 4.2  Piecing a puzzle\n",
            "\u001b[35mSection_title: 4.3  Ablations of design choices\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Settings\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Datasets analysis\n",
            "\u001b[35mSection_title: 5.4  Effect of context padding\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mlimitation: we present sled as a simple and effective method to extend the capabilities of pretrained short-text models to long-text tasks. despite its impressive empirical performance on scrolls, sled suffers from two disadvantages which may limit its applicability to some long-range tasks.long output to obtain linear complexity, sled assumes the output length k is constant. this is since the decoder uses quadratic selfattention over the output, on top of o(nk) crossattention between the output and input. while most current long-text tasks follow this assumption, future tasks, such as academic reports or script writing, may require long text generation. this limitation is not unique to sled and affects other long-range transformers including longt5 and led. aside from finetuning, this also affects pretraining models on long inputs with selfsupervised losses such as span-corruption (raffel et al., 2020b) or denoising (lewis et al., 2020), which require the decoder to process an output that is linear in the length of the input.co-reference resolution and fact retention an assumption at the heart of sled is the locality of information assumption. when the input text is long, this assumption may break if distant entity resolution or factual knowledge are required. for example, a chapter in a book may mention \"they were walking into the room\" when knowledge of what room or who walked is located a few chapters back. in such cases, the encoder used by sled will not be able to access this information, moving more responsibility to the decoder and reducing the effectiveness of the contextual encoding. similarly, in multi-hop questions (yang et al., 2018), attending to one part of the context is necessary in order to fully understand the question and encode a second piece of information correctly. as the encoder will not have access to the first context that leads to better question understanding, here as well more responsibility is delegated to the decoder.\n",
            "\u001b[35mSection_title: 7  Limitations\n",
            "\u001b[32mconclusion: in this work we present sled, a simple approach for modeling long texts which slides a pretrained short-range encoder over a long input document and then generates an output by attending to the encoded tokens. we show sled can perform core operations that are important for long text understanding, such as finding relevant pieces of information and fusing them at decoding time, and demonstrate competitive performance on the scrolls benchmark compared to larger models and models that employ a dedicated and expensive pretraining step. one of sled's most attractive features is that it can be readily used with any short-range pretrained lm. thus, any future encoder-decoder model can be flexibly plugged into it to achieve further gains in performance on scrolls, some of its tasks, or any other long-range task.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Extractive Question Answering\n",
            "\u001b[35mSection_title: 2.2  Prompt-tuning for PLMs\n",
            "\u001b[35mSection_title: 3  The KECP Framework\n",
            "\u001b[35mSection_title: 3.1  Task Overview\n",
            "\u001b[35mSection_title: 3.2  Query Prompt Construction\n",
            "\u001b[35mSection_title: 3.3  Knowledge-aware Prompt Encoder (KPE)\n",
            "\u001b[35mSection_title: 3.3.1  Passage Knowledge Injection (PKI)\n",
            "\u001b[35mSection_title: 3.3.2  Passage-to-Prompt Injection (PPI)\n",
            "\u001b[35mSection_title: 3.4  Span-level Contrastive Learning (SCL)\n",
            "\u001b[35mSection_title: 3.5  Model-free Prefix-tree Decoder\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Baselines\n",
            "\u001b[35mSection_title: 4.2  Benchmarks\n",
            "\u001b[35mSection_title: 4.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.4  Main Results\n",
            "\u001b[35mSection_title: 4.5  Detailed Analysis and Discussions\n",
            "\u001b[32mconclusion: to bridge the gap between the pre-training and finetuning objectives, kecp views eqa as an answer generation task. in kecp, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query. the span-level contrastive learning objective is proposed to improve the performance of eqa. experiments on multiple benchmarks in both instance-level and task-level few-shot scenarios show that our framework consistently outperforms the state-of-the-art methods.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  OVERVIEW\n",
            "\u001b[35mSection_title: 2.1  Task Setting and Terminology\n",
            "\u001b[35mSection_title: 2.2  Formulation for Dense Retrieval\n",
            "\u001b[35mSection_title: 2.3  Key Aspects\n",
            "\u001b[35mSection_title: 3  DATASETS, EVALUATION AND RESOURCES\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.3  Code Library Resource\n",
            "\u001b[35mSection_title: 4  ARCHITECTURE\n",
            "\u001b[35mSection_title: 4.1  Background\n",
            "\u001b[35mSection_title: 4.1.1  Transformer and Pretrained Language Models\n",
            "\u001b[35mSection_title: 4.1.2  PLMs for Sparse Retrieval\n",
            "\u001b[35mSection_title: 4.2  Neural Architecture for Dense Retrieval\n",
            "\u001b[35mSection_title: 4.2.1  The Cross-Encoder Architecture\n",
            "\u001b[35mSection_title: 4.2.2  The Bi-Encoder Architecture\n",
            "\u001b[35mSection_title: 4.2.3  Comparison between Cross-and Bi-Encoder\n",
            "\u001b[35mSection_title: 4.3  Sparse Retrieval v.s. Dense Retrieval\n",
            "\u001b[35mSection_title: 4.3.1  Discussions\n",
            "\u001b[35mSection_title: 4.3.2  Combining Sparse and Dense Retrieval models\n",
            "\u001b[35mSection_title: 5  TRAINING\n",
            "\u001b[35mSection_title: 5.1  Formulation and Training Issues\n",
            "\u001b[35mSection_title: 5.1.1  Loss Function\n",
            "\u001b[35mSection_title: 5.1.2  Incorporating Optimization Constraints\n",
            "\u001b[35mSection_title: 5.1.3  Major Training Issues\n",
            "\u001b[35mSection_title: 5.2  Negative Selection\n",
            "\u001b[35mSection_title: 5.2.1  In-batch Negatives\n",
            "\u001b[35mSection_title: 5.2.2  Cross-batch Negatives\n",
            "\u001b[35mSection_title: 5.2.3  Hard Negatives\n",
            "\u001b[35mSection_title: 5.2.4  Discussions on the Effect of Negative Sampling\n",
            "\u001b[35mSection_title: 5.3  Data Augmentation\n",
            "\u001b[35mSection_title: 5.3.1  Auxiliary Labeled Datasets\n",
            "\u001b[35mSection_title: 5.3.2  Knowledge Distillation\n",
            "\u001b[35mSection_title: 5.4  Pretraining for Dense Retrieval Models\n",
            "\u001b[35mSection_title: 5.4.1  Task Adaptive Pretraining\n",
            "\u001b[35mSection_title: 5.4.2  Generation-Augmented Pretraining\n",
            "\u001b[35mSection_title: 5.4.3  Retrieval-Augmented Pretraining\n",
            "\u001b[35mSection_title: 5.4.4  Representation Enhanced Pretraining\n",
            "\u001b[35mSection_title: 5.5  Empirical Performance Analysis with RocketQA\n",
            "\u001b[35mSection_title: 5.5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.5.2  Results and Analysis\n",
            "\u001b[35mSection_title: 6  INDEXING FOR DENSE RETRIEVAL\n",
            "\u001b[35mSection_title: 6.1  Traditional Inverted Index for Sparse Retrieval\n",
            "\u001b[35mSection_title: 6.2  Approximate Nearest Neighbor Search for Dense Retrieval\n",
            "\u001b[35mSection_title: 6.2.1  Formulation and Overview\n",
            "\u001b[35mSection_title: 6.2.2  Improving Search Efficiency by Index Structures\n",
            "\u001b[35mSection_title: 6.2.3  Improving Search Efficiency by Product Quantization\n",
            "\u001b[35mSection_title: 6.3  Implementation for ANNS Algorithms\n",
            "\u001b[35mSection_title: 7  INTEGRATION WITH RERANKING\n",
            "\u001b[35mSection_title: 7.1  The Retrieval Pipeline\n",
            "\u001b[35mSection_title: 7.1.1  The General Retrieval Pipeline\n",
            "\u001b[35mSection_title: 7.1.2  PLM-based Rerankers and Multi-stage Ranking\n",
            "\u001b[35mSection_title: 7.2  Retrieval Pipeline Training\n",
            "\u001b[35mSection_title: 7.2.1  Separate Training\n",
            "\u001b[35mSection_title: 7.2.2  Adaptive Training\n",
            "\u001b[35mSection_title: 7.2.3  Joint Training\n",
            "\u001b[35mSection_title: 7.3  Integration and Optimization in Other Applications\n",
            "\u001b[35mSection_title: 8  ADVANCED TOPICS\n",
            "\u001b[35mSection_title: 8.1  Zero-shot Dense Retrieval\n",
            "\u001b[35mSection_title: 8.2  Improving the Robustness to Query Variations\n",
            "\u001b[35mSection_title: 8.3  Model based Retrieval\n",
            "\u001b[35mSection_title: 8.4  Retrieval-Augmented Language Model\n",
            "\u001b[35mSection_title: 9  APPLICATIONS\n",
            "\u001b[35mSection_title: 9.1  Information Retrieval Applications\n",
            "\u001b[35mSection_title: 9.1.1  Temporal Retrieval\n",
            "\u001b[35mSection_title: 9.1.2  Structured Data Retrieval\n",
            "\u001b[35mSection_title: 9.1.3  Multilingual Retrieval\n",
            "\u001b[35mSection_title: 9.1.4  Other Retrieval Tasks\n",
            "\u001b[35mSection_title: 9.2  Natural Language Processing Applications\n",
            "\u001b[35mSection_title: 9.2.1  Question Answering\n",
            "\u001b[35mSection_title: 9.2.2  Entity Linking\n",
            "\u001b[35mSection_title: 9.2.3  Dialog System\n",
            "\u001b[35mSection_title: 9.3  Industrial Practice\n",
            "\u001b[32mconclusion: in this survey, we thoroughly review the recent progress of dense retrieval based on pretrained language models (plm). as an important evolution of language intelligence techniques, plms empower dense retrieval models with excellent modeling capacities to capture and represent text semantics for relevance matching. our survey has extensively discussed the key issues and the mainstream solutions in four major aspects to develop dense retrieval systems, including architecture, training, indexing and integration. next, we briefly summarize the discussions of this survey and introduce some remaining issues for dense retrieval.\n",
            "\u001b[35mSection_title: 10  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Multilingual Extractive RC Datasets\n",
            "\u001b[35mSection_title: 2.2  Multilingual Extractive RC Modeling\n",
            "\u001b[35mSection_title: 3  Problem Definition\n",
            "\u001b[35mSection_title: 3.1  Cross-Lingual Generalization (XLG)\n",
            "\u001b[35mSection_title: 4  Model\n",
            "\u001b[35mSection_title: 4.1  Building Multilingual Parallel Corpora\n",
            "\u001b[35mSection_title: 4.2  Multilingual Adaptive Attention Reader\n",
            "\u001b[35mSection_title: 4.3  Task-level Similarity\n",
            "\u001b[35mSection_title: 5  Experimental Details\n",
            "\u001b[35mSection_title: 5.1  Implementation Details\n",
            "\u001b[35mSection_title: 5.2  Datasets\n",
            "\u001b[35mSection_title: 5.3  Baselines\n",
            "\u001b[35mSection_title: 5.4  Evaluation\n",
            "\u001b[35mSection_title: 5.5  Results and Analysis\n",
            "\u001b[35mSection_title: 5.6  Ablation Study\n",
            "\u001b[32mconclusion: in this paper we propose a multilingual extractive rc modeling approach named xltt, by a crosslingual transposition rethinking approach for lowresource extractive reading comprehension using multilingual adaptive attention to model existing erc training datasets in a multilingual context.experimental results demonstrate the effectiveness of our multilingual erc modeling approach.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.2  REPURPOSING EXISTING DATASET FOR MINT\n",
            "\u001b[35mSection_title: 3.2  MEASURING LLM'S TOOL-AUGMENTED TASK-SOLVING IN MULTI-TURN INTERACTION\n",
            "\u001b[35mSection_title: 3.3  MEASURING LLM'S ABILITY TO LEVERAGE NATURAL LANGUAGE FEEDBACK\n",
            "\u001b[35mSection_title: 3.4  MEASURING THE EFFICACY OF DIFFERENT LLM'S ABILITY TO PROVIDE FEEDBACK\n",
            "\u001b[35mSection_title: 3.5  MINT CAN HELP DETECT FAILURE PATTERNS OF EVALUATED LLMS\n",
            "\u001b[35mSection_title: 3.6  CAN GPT-4 GENERATE HUMAN-LEVEL NATURAL LANGUAGE FEEDBACK?\n",
            "\u001b[35mSection_title: 4.2  EVALUATING INTERACTION\n",
            "\u001b[32mconclusion: in this work, we present mint, an evaluation benchmark designed to evaluate llm's task-solving ability in multi-turn interaction by using tools and leveraging natural language feedback, which we\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Proposed Method\n",
            "\u001b[35mSection_title: 2.1  Task Definition\n",
            "\u001b[35mSection_title: 2.2  Model and Training\n",
            "\u001b[35mSection_title: 2.3  Inference via Beam Search\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Settings\n",
            "\u001b[35mSection_title: 3.2  Settings\n",
            "\u001b[35mSection_title: 3.3  Results\n",
            "\u001b[35mSection_title: 3.4  Future Work\n",
            "\u001b[32mconclusion: in the paper, we propose a new problem of complementary evidence identification and define the criterion of complementary evidence in vector space. we further design an algorithm and a loss function to support efficient training and inference for complementary evidence selection. compared to the baseline, our approach improves more than 20% and remains to scale well to the computationally complex cases.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Single-paragraph QA\n",
            "\u001b[35mSection_title: 3.1  Model Description\n",
            "\u001b[35mSection_title: 3.2  Model Results\n",
            "\u001b[35mSection_title: 4.1  Categorizing Bridge Questions\n",
            "\u001b[35mSection_title: 4.2  Categorizing Comparison Questions\n",
            "\u001b[35mSection_title: 5  Can We Find Better Distractors?\n",
            "\u001b[32mconclusion: in summary, we demonstrate that question compositionality is not a sufficient condition for multi-hop reasoning. instead, future datasets must carefully consider what evidence they provide in order to ensure multi-hop reasoning is required. there are at least two different ways to achieve this.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Datasets\n",
            "\u001b[35mSection_title: 4  Approach Overview\n",
            "\u001b[35mSection_title: 6.1  Task Definition\n",
            "\u001b[35mSection_title: 6.2  Dataset Preparation\n",
            "\u001b[35mSection_title: 6.3  Model Description\n",
            "\u001b[35mSection_title: 7  Knowledge Fusion Question Answering Model\n",
            "\u001b[35mSection_title: 8  Experimental Results and Analysis\n",
            "\u001b[32mconclusion: in this work, we have pushed the current state-ofthe-art by 2.2% on openbookqa and 7.28% on qasc, two tasks that need external knowledge and knowledge composition for question answering. our semantic knowledge ranking and knowledge fusion question answering model over the bertbased language model demonstrably improves the performance on openbookqa and qasc. we also provide a dataset to learn semantic knowledge ranking using the annotations present in qasc, openbookqa, and scitail. we have analyzed the performance of the components in our qa system. our analysis shows the need to further improve knowledge ranking and knowledge composition.\n",
            "\u001b[35mSection_title: 9  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Preliminary\n",
            "\u001b[35mSection_title: 4  Analysis Pipeline\n",
            "\u001b[35mSection_title: 5  Experiment\n",
            "\u001b[35mSection_title: 5.1  Experimental Setups\n",
            "\u001b[35mSection_title: 5.2  Experimental Results\n",
            "\u001b[35mSection_title: 5.2.1  Single-task Setting\n",
            "\u001b[35mSection_title: 5.2.2  Multi-task Setting\n",
            "\u001b[35mSection_title: 5.2.3  Performance Landscape Visualization\n",
            "\u001b[35mSection_title: 5.3  Extension to Fine-tuning\n",
            "\u001b[32mconclusion: in this work, we explore the hypothesis that the adaptations of different delta tuning methods could all be re-parameterized as low-dimensional optimizations in a unified optimization subspace. the empirical results provide strong evidence for our hypothesis. we also extend our analysis to find the connection between fine-tuning and delta tuning.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  OBSERVATIONS AND DATA ANALYSIS\n",
            "\u001b[35mSection_title: 2.1  The sample\n",
            "\u001b[35mSection_title: 3  THE DUST CONTENT OF GALAXIES\n",
            "\u001b[35mSection_title: 3.1  The dust model\n",
            "\u001b[35mSection_title: 3.2  The dust emissivity\n",
            "\u001b[35mSection_title: 3.3  Dust distribution in galaxy discs\n",
            "\u001b[35mSection_title: 3.4  Best-fit dust models to the observed spectra\n",
            "\u001b[35mSection_title: 3.5  Effects of dust extinction on the overall spectrum\n",
            "\u001b[35mSection_title: 3.6  The dust-to-gas mass ratio\n",
            "\u001b[35mSection_title: 4  DISCUSSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Complex question decomposition\n",
            "\u001b[35mSection_title: 3  Proposed method\n",
            "\u001b[35mSection_title: 3.1  Relation Extractor\n",
            "\u001b[35mSection_title: 3.2  Sub-Question Reader\n",
            "\u001b[35mSection_title: 3.3  Comparator\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Dataset\n",
            "\u001b[35mSection_title: 4.2  Experimental Details\n",
            "\u001b[35mSection_title: 4.3  Baseline\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Impact of different question types\n",
            "\u001b[35mSection_title: 5.2  Impact of QETPS\n",
            "\u001b[35mSection_title: 5.3  Results of Evidence Path Generation : Manual Evaluation\n",
            "\u001b[32mconclusion: we propose a three-stage framework of relation extractor-reader and comparator (rerc), which solves the multi-hop qa task through the idea of complex question decomposition, and obtains the state-of-the-art results in the 2wikimultihopqa dataset, which is close to human performance. our rerc framework can also provide faithful evidence with excellent interpretability.\n",
            "\u001b[35mSection_title: 6  Conclusion and future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Few-shot Rationale Generation\n",
            "\u001b[35mSection_title: 3.1  Bridge Entity Questions\n",
            "\u001b[35mSection_title: 3.2  Comparison Questions\n",
            "\u001b[35mSection_title: 4  Rationale Steps as a Mixture of Tasks\n",
            "\u001b[35mSection_title: 4.1  Question Type Task\n",
            "\u001b[35mSection_title: 4.2  Bridge Rationale Steps\n",
            "\u001b[35mSection_title: 4.3  Comparison Rationale Tasks\n",
            "\u001b[35mSection_title: 4.4  Rationale Annotation\n",
            "\u001b[35mSection_title: 4.5  Reasoning Circuit for Multi-hop Question Generation\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 6.1  Automatic Evaluation\n",
            "\u001b[35mSection_title: 6.2  Human Evaluation\n",
            "\u001b[35mSection_title: 7  Discussion and Future Work\n",
            "\u001b[32mconclusion: in this paper, we propose reasoning circuits, a new framework suited to real-world scenarios where the nlp task at hand requires multiple steps of structured reasoning, with only a limited number of available labelled examples, and a small annotation budget, also only a modest deep learning computational infrastructure/budget is accessible. in this work, we apply this framework to the task of fewshot multi-hop question generation which fits all these criteria. we identify structured multi-step rationales that break down this problem into many discrete reasoning steps. each step in these rationales is treated as a single \"task\" within a mixture of similar \"tasks\". the individual tasks can be categorized into control tasks, which control the flow of information between tasks, and generative tasks, that generate free-form text for successive tasks in the reasoning circuit. the framework is relatively easy to implement, since only a single generative model is fine-tuned with a mixture of all reasoning steps; at inference time, the same model can generate all reasoning steps sequentially. we show that fine-tuning with only around 64 to 128 labelled rationale examples with our approach is enough to improve automatic evaluation metrics compared to a baseline trained without rationales on the hot-potqa dataset. more importantly, with human evaluation, we find that this framework can strongly improve the central objective of multi-hop qg, to generate challenging questions which cannot be answered from reading only a single passage.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: the proposed reasoning circuits framework intends to replace the need for thousands of annotated examples with a strong inductive bias of structured rationales. there is two issues with this approach at a conceptual level: 1. it may not always be possible to break down a multi-step reasoning problem cleanly into discrete reasoning steps, and another related issue it increasing complexity of the circuit with the complexity of the task. 2. for the design of these reasoning circuits a researcher must develop a thorough understanding of this reasoning task, so that the final circuit design broadly covers all possible types of reasoning problems expected to be solved. an under-or illdesigned reasoning circuit may cause the system to either not support a certain portion of problems or produce non-sensical outputs.essentially, there is trade off between a tighter control over reasoning by investing in a deep understanding of the problem leading to a comprehensive reasoning circuit design and lower annotations budget, versus, less control over logic and depending on a large number of annotations which allow the model to discover this logic on its own at much higher cost of large scale annotations budget. at the implementation and operations level one of the the key limitations our proposed system is the number of inference steps to solve the problem. the number of times model inference may be needed to solve a single example is equal the length of the longest task sequence chain in the reasoning circuit. one possible solution for this could be by training the model to solve the entire problem by generating all the steps of reasoning and the target string in a single inference step and could massively reduce inference time and costs. \n",
            "\u001b[35mSection_title: 9  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  RealFormer\n",
            "\u001b[35mSection_title: 3.1  Standard Transformer\n",
            "\u001b[35mSection_title: 3.2  Residual Attention Layer Transformer\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  BERT\n",
            "\u001b[35mSection_title: 4.1.1  Pre-training Results\n",
            "\u001b[35mSection_title: 4.1.2  Downstream Results\n",
            "\u001b[35mSection_title: 4.1.3  Research Questions\n",
            "\u001b[35mSection_title: 4.2  ADMIN\n",
            "\u001b[35mSection_title: 4.3  ETC\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Related Work\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Rationale Generation\n",
            "\u001b[35mSection_title: 2.2  Retrieval\n",
            "\u001b[35mSection_title: 2.3  Rationale Ranker\n",
            "\u001b[35mSection_title: 2.3.1  Rationale Ranker Evaluation\n",
            "\u001b[35mSection_title: 2.4  Reasoning Models\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Models\n",
            "\u001b[35mSection_title: 3.2  Unseen Evaluation Datasets\n",
            "\u001b[35mSection_title: 3.3  Context Combination Methods and Experimental Nomenclature\n",
            "\u001b[35mSection_title: 3.4  Experimental Results\n",
            "\u001b[35mSection_title: 3.4.1  Summary\n",
            "\u001b[35mSection_title: 3.4.2  GR+RATD Model Versus Baseline And LLM Direct Prompts\n",
            "\u001b[35mSection_title: 3.4.3  RR Model Scoring And RATD Training Efficacy\n",
            "\u001b[32mconclusion: we have implemented methods for combining explanatory context from two knowledge sources: llmgenerated rationales and retrieved paragraphs from wikipedia.the first method involves training our smaller reasoning model on ratd datasets such that it becomes proficient at reasoning over long, noisy contexts which contain information from both knowledge sources.the second method is to use rationale ranking model scores for each knowledge source as guidance in constructing contexts that may contain information from both, or either knowledge source.we have shown that both methods are individually effective in significantly improving unseen question-answering performance both versus the baselines established by hartill et al. (2023) and versus a baseline that ablates both rr and ratd methods (section 3.4.3).\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Tasks, Datasets, and Models\n",
            "\u001b[35mSection_title: 3  Shortcomings of Free-Text Pipelines\n",
            "\u001b[35mSection_title: 3.1  Joint Model Rationales are More Indicative of Labels\n",
            "\u001b[35mSection_title: 3.2  Sufficiency is not Universally Valid\n",
            "\u001b[35mSection_title: 4  Analyzing Necessary Properties of Joint Models\n",
            "\u001b[35mSection_title: 4.1  Robustness Equivalence\n",
            "\u001b[35mSection_title: 4.2  Feature Importance Agreement\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: after demonstrating the weaknesses that pipeline models exhibit for free-text rationalization tasks, we propose two measurements of label-rationale association in self-rationalizing models. we find that on three free-text rationalization datasets for commonsenseqa and snli, models based on t5 exhibit high robustness equivalence and feature importance agreement, demonstrating that they pass a necessary sanity check for generating faithful free-text rationales.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  1\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS\n",
            "\u001b[35mSection_title: 2.1  Natural Language Interface for Data Visualization\n",
            "\u001b[35mSection_title: 2.2  Question Answering System\n",
            "\u001b[35mSection_title: 2.3  Question Answering Corpus\n",
            "\u001b[35mSection_title: 3  OVERVIEW AND SYSTEM DESIGN\n",
            "\u001b[35mSection_title: 3.1  Design Requirements\n",
            "\u001b[35mSection_title: 3.2  System Architecture and Formulation\n",
            "\u001b[35mSection_title: 4  QUESTION DECOMPOSITION\n",
            "\u001b[35mSection_title: 4.1  Question Types and Decomposition Strategies\n",
            "\u001b[35mSection_title: 4.2  Decomposition Algorithm\n",
            "\u001b[35mSection_title: 4.3  Decomposition Model\n",
            "\u001b[35mSection_title: 4.4  Training Corpus\n",
            "\u001b[35mSection_title: 5  ANSWER EXTRACTION\n",
            "\u001b[35mSection_title: 5.1  Fact Classification\n",
            "\u001b[35mSection_title: 5.2  Fact Search\n",
            "\u001b[35mSection_title: 5.3  Fact Ranking\n",
            "\u001b[35mSection_title: 6  USER INTERFACE AND VISUALIZATION\n",
            "\u001b[35mSection_title: 6.1  User Interface and Interactions\n",
            "\u001b[35mSection_title: 6.2  Annotated Chart Library for Tabular Data\n",
            "\u001b[35mSection_title: 6.3  Answer Facts Layout\n",
            "\u001b[35mSection_title: 7  EVALUATION\n",
            "\u001b[35mSection_title: 7.1  Case Study\n",
            "\u001b[35mSection_title: 7.2  Quantitative Evaluation\n",
            "\u001b[35mSection_title: 7.3  User Study\n",
            "\u001b[32mlimitation: here, we would like to report and discuss several limitations that was found during our system implementation and evaluation. scalability issue. the current implementation of the prototype system still cannot handle large datasets that contain tens of thousands of data records, where the answer extraction algorithm is the primary bottleneck. it will be more difficult to find out accurate answers from a large dataset within a fixed period of time. there are several approaches that could be applied to address the issue, which will be our future work. first, using parallel searching algorithms [45] will greatly improve the algorithm efficiency. second, using a pre-trained model such as tabert [64], to built a table-based q&a system, will also improve the system's performance. although such a system doesn't exist yet, we believe it is a promising direction, which will be our next plan.accuracy issue. although showing the relevant context is helpful for the answer interpretation, when mistake happens, the irrelevant charts could also be a distraction, which will affect users' judgments. we believe there are two methods that could be used to improve the accuracy of the system. first, we can employ knowledge bases such as wolfra-malpha 5 and knowledge graphs to guide the searching directions so that the answers could be more directly found without checking too many irrelevant candidates in the space. second, again, training a qa system based on tabert [64] could also help improve the accuracy.generalization issue. our training corpus is generated based on 26 tabular data that primarily contain marketing data records such as car sales values, and best selling books. as a result, our model could better handle high-level questions in the marking domain, but may have a lower question decomposition quality when facing a question from other domains. to overcome the issue, more datasets in various domains should be collected and more questions should be prepared to train the model and improve the generalization of the system.\n",
            "\u001b[35mSection_title: 8  LIMITATIONS AND FUTURE WORK\n",
            "\u001b[32mconclusion: we present talk2data, a data-oriented online question and answering system that supports answering both low-level and high-level questions. the system employs a novel deep-learning based question decomposition model to resolve a high-level question into a series of relevant low-level questions, and a search algorithm to extract the data facts that are most relevant to each of low level questions. to visualize the data facts, we designed a set of annotated and captioned visualization charts to support interpretation and narration. the proposed technique was evaluated via case studies, performance validation, and a controlled user study. the evaluation showed the power of the talk2data system and revealed several limitations of the current system, which will be addressed in the future.\n",
            "\u001b[35mSection_title: 9  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methods for XQA\n",
            "\u001b[32mlimitation: we manually analyze outputs of the models by qi et al. (2019) and  and identify the following two problems. silent facts. the models make use of facts without including them into their explanations (cf., figure 1). as a result, the predicted answer does not occur in the explanation, leaving the user uninformed about where it came from.unused facts. the models predict facts to be relevant without any relation to the predicted answer. the second fact of the explanation in figure  1 is an example for this. we also found examples where the facts predicted to be relevant do not even contain the entities from the question.\n",
            "\u001b[35mSection_title: 3.1  Limitations of Current Models\n",
            "\u001b[35mSection_title: 3.2  Select & Forget Architecture\n",
            "\u001b[35mSection_title: 3.3  Answer-Fact Coupling Regularizer\n",
            "\u001b[35mSection_title: 4  New Evaluation Scores for XQA\n",
            "\u001b[32mlimitation: current evaluation of xqa is focused on three scores: (i) answer-f 1 , which is based on the token overlap between the predicted and the ground truth answer, (ii) sp-f 1 , which calculates f 1 based on the overlap of predicted and ground truth relevant (\"supporting\") facts and (iii) joint-f 1 , which is based on the definitions of joint precision and joint recall as the products of answer and sp precision and recall as described in . for hotpotqa, models are ranked based on joint-f 1 . we argue that this creates a false incentive that potentially hinders the development of truly usable models for the following reasons.no empirical evidence. there is no empirical evidence that joint-f 1 is related to user performance or experience regarding xqa.rewarding poor explanations. figure 1 shows an example prediction that is rewarded with a joint-f 1 of 0.5 although its explanation provides no value to the user. the reward stems from the overlap of the explanation with the ground truth but does not consider that the predicted answer is not contained in any of the predicted relevant facts.punishing good explanations. consider a model output in which the predicted answer is wrong but the explanation perfectly explains this wrong answer, showing to the user why the model has selected it. standard f 1 -scores compare the model output to the ground truth annotations and will, therefore, score both the answer and the explanation with an f 1 of 0. however, we argue that an explanation should be evaluated with a score higher than 0 if it is able to explain the reasoning process of the model to the user and, thus, lets the user identify the failure of the model.\n",
            "\u001b[35mSection_title: 4.1  Limitations of Current Evaluation Scores\n",
            "\u001b[35mSection_title: 4.2  Fact-Removal Score (FARM)\n",
            "\u001b[35mSection_title: 4.3  Answer-Location Score (LOCA)\n",
            "\u001b[35mSection_title: 5  Experiments and Results\n",
            "\u001b[35mSection_title: 5.1  Dataset\n",
            "\u001b[35mSection_title: 5.2  Experimental Results\n",
            "\u001b[35mSection_title: 6  Human Evaluation\n",
            "\u001b[35mSection_title: 6.1  Choice of Models\n",
            "\u001b[35mSection_title: 6.2  Study Design\n",
            "\u001b[35mSection_title: 6.3  Dependent Variables\n",
            "\u001b[35mSection_title: 6.4  Participants and Data Cleaning\n",
            "\u001b[35mSection_title: 6.5  Results\n",
            "\u001b[35mSection_title: 6.6  Correlation with Evaluation Scores\n",
            "\u001b[32mconclusion: in this paper, we investigated explainable question answering, revealing that existing models lack an explicit coupling of answers and explanations and that evaluation scores used in related work fail to quantify that. this highly impairs their applicability in real-life scenarios with human users. as a remedy, we addressed both modeling and evaluation, proposing a hierarchical neural architecture, a regularization term, as well as two new evaluation scores. our user study showed that our models help the users assess their correctness and that our proposed evaluation scores are better correlated with user experience than standard measures like f 1 .  \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Problem Definition\n",
            "\u001b[35mSection_title: 2.2  Approach Overview\n",
            "\u001b[35mSection_title: 2.3  Embedding Layer\n",
            "\u001b[35mSection_title: 2.4  Primal-Dual Encoder\n",
            "\u001b[35mSection_title: 2.5  Question Decoder\n",
            "\u001b[35mSection_title: 2.6  Output Layer\n",
            "\u001b[35mSection_title: 2.7  Knowledge Distillation\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.4  Main Results\n",
            "\u001b[35mSection_title: 4.2  Impact of Knowledge Distillation on Uncommon Words Generation\n",
            "\u001b[35mSection_title: 4.3  Human Evaluation\n",
            "\u001b[35mSection_title: 4.4  Different Model Configurations\n",
            "\u001b[35mSection_title: 4.5  Case Study\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: automatic question generation is an important task in the improvement of artificial intelligent systems. in this work, we propose a novel primal-dual approach for question generation. it integrates question generation with its dual problem question answering into a unified framework. a knowledge distillation module is introduced into the framework to improve model generalization on uncommon word generation. experimental results on two benchmarks demonstrate the effectiveness of the primal-dual modeling.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3.1  Architecture\n",
            "\u001b[35mSection_title: 4.1  Supervised training with summarization and explanation generation\n",
            "\u001b[35mSection_title: 4.2  Semi-supervised training\n",
            "\u001b[35mSection_title: 4.3  Reward function\n",
            "\u001b[35mSection_title: 5  Evaluation\n",
            "\u001b[35mSection_title: 5.1  Dataset\n",
            "\u001b[35mSection_title: 5.2  Relevant paragraph prediction\n",
            "\u001b[35mSection_title: 5.3  Setup\n",
            "\u001b[35mSection_title: 5.4  Evaluation measures\n",
            "\u001b[35mSection_title: 5.5  Results and discussion\n",
            "\u001b[35mSection_title: 5.6  Analysis\n",
            "\u001b[35mSection_title: 5.6.1  Role of explanation supervision\n",
            "\u001b[35mSection_title: 5.6.2  Training strategy\n",
            "\u001b[35mSection_title: 5.6.3  Error analysis\n",
            "\u001b[32mconclusion: we have proposed suqa, an rc system augmented with an abstractive explainer component. our experiments have demonstrated that the abstractive explainer can generate more concise explanations than an extractive explainer with limited supervison, while keeping explanations sufficient for qa.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  Models\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[32mconclusion: we have performed a grid search hyperparameter benchmarking on three models on the duorc dataset. the models we evaluated are the bert, albert, and longformer models which are transformer-based neural network models. the duorc dataset contained two main components for each unique plot. the selfrc has shorter plot lines while the paraphrase has longer plot lines. the dataset was reduced into subsets called the \"span\" and \"full\" where the \"span\" subset is the set of plots where only the relevant sentences to the questions are extracted. the best performing model is the albert model which was pretrained using the squad1 and fine-tuned on the selfrc. the best performing model fine-tuned on the paraphraserc is the longformer model which was pretrained using the selfrc.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Chain-of-query Generation\n",
            "\u001b[35mSection_title: 3.2  Interaction with Information Retrieval\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[32mconclusion: in this paper, we propose a novel method to improve the accuracy, credibility and traceability of large language models (llm) for complex knowledge-intensive tasks called searchain, which is a framework for deep interaction between llm and information retrieval (ir). in searchain, when llm faces complex questions, it constructs a chain called chain-of-query (coq), each node of the coq is an ir-oriented query and llm-generated answer. ir interacts with each node on the chain, judges whether the answer is correct, and provides llm with its unknown knowledge. llm generates a new coq according to the feedback of ir. ir interacts with llm for multiple rounds, gradually helping llm generate the correct coq, and finally solve the complex question. besides, the contents returned to the user include not only the final answer but also the reasoning process for the question, that is, the coq and the supporting documents retrieved by ir for each node of the coq, which improves the credibility and traceability of the contents generated by llm. experiments on multi-hop question answering datasets show that searchain not only has strong knowledge-reasoning ability when faced with complex questions but also can effectively exploit the interaction with ir to supplement and correct its knowledge, so as to improve the accuracy and credibility of the generated contents. besides, searchain effectively decouples the knowledge of llm and ir, which avoids the misleading of llm and can also accurately explain whether the knowledge involved in solving knowledge-intensive tasks comes from parameters or external ir.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4  RESULTS\n",
            "\u001b[35mSection_title: 4.1  Latency\n",
            "\u001b[32mconclusion: in this work we studied how distillation and parameter count influence the zero-shot effectiveness of neural retrievers. we begin by showing that in-domain effectiveness, i.e., when retrievers are finetuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al. [14] and gupta et al. [9] and zhan et al [46]. furthermore, we show that a distilled reranker has better zeroshot effectiveness than much larger non-distilled rerankers, which is an important and desirable feature of deployed models. however, our largest reranker significantly outperforms smaller rerankers and achieves a new state of the art across almost all datasets used in our zero-shot experiments. this suggests that a large number of parameters may play a significant role in the generalization capability of pretrained language models.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4  RESULTS\n",
            "\u001b[35mSection_title: 4.1  Latency\n",
            "\u001b[32mconclusion: in this work we studied how distillation and parameter count influence the zero-shot effectiveness of neural retrievers. we begin by showing that in-domain effectiveness, i.e., when retrievers are finetuned and evaluated on the same dataset such as ms marco, is not a good proxy for zero-shot effectiveness, which corroborates recent claims by lin et al. [14] and gupta et al. [9] and zhan et al [46]. furthermore, we show that a distilled reranker has better zeroshot effectiveness than much larger non-distilled rerankers, which is an important and desirable feature of deployed models. however, our largest reranker significantly outperforms smaller rerankers and achieves a new state of the art across almost all datasets used in our zero-shot experiments. this suggests that a large number of parameters may play a significant role in the generalization capability of pretrained language models.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Formulation\n",
            "\u001b[35mSection_title: 2.1  Preliminaries\n",
            "\u001b[35mSection_title: 2.2  Derivation of the PBDW statement\n",
            "\u001b[35mSection_title: 2.3  Choice of U: variational and user-defined update spaces\n",
            "\u001b[35mSection_title: 2.4  Variational interpretation of the user-defined space, and well-posedness analysis\n",
            "\u001b[35mSection_title: 2.  Given the orthonormal basis\n",
            "\u001b[35mSection_title: 2.5  Algebraic formulation\n",
            "\u001b[35mSection_title: 2.8  Extension to vector-valued problems\n",
            "\u001b[35mSection_title: 3  Error analysis\n",
            "\u001b[35mSection_title: 4  Application to Acoustics\n",
            "\u001b[35mSection_title: 4.1  A two-dimensional model problem\n",
            "\u001b[35mSection_title: 4.1.1  Problem definition\n",
            "\u001b[35mSection_title: 4.1.2  PBDW spaces\n",
            "\u001b[35mSection_title: 4.1.3  Numerical results\n",
            "\u001b[35mSection_title: 4.2  A three-dimensional acoustic problem\n",
            "\u001b[35mSection_title: 4.2.1  Problem definition\n",
            "\u001b[35mSection_title: 4.2.2  PBDW spaces\n",
            "\u001b[35mSection_title: 4.2.3  Numerical results\n",
            "\u001b[35mSection_title: 5  Application to Fluid Mechanics\n",
            "\u001b[35mSection_title: 5.1  Problem statement\n",
            "\u001b[35mSection_title: 5.2  PBDW spaces\n",
            "\u001b[35mSection_title: 5.3  Numerical results\n",
            "\u001b[32mconclusion: in this paper, we presented a number of extensions to the pbdw formulation for state estimation. first, we proposed a tikhonov regularization of the original pbdw statement for general linear functionals, which relies on holdout validation, to systematically deal with noisy measurements. second, we proposed user-defined update spaces, which guarantee rapid convergence with respect to the number of measurements m and also might not require the solution to m riesz problems. third, we presented an a priori error analysis that provides insights into the role of the regularization hyper-parameter ξ associated with the penalized formulation.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Data\n",
            "\u001b[35mSection_title: 3.1  Data Construction\n",
            "\u001b[35mSection_title: 3.1.1  Data Sources\n",
            "\u001b[35mSection_title: 3.1.2  Composing Valid and Invalid QA Pairs\n",
            "\u001b[35mSection_title: 3.1.3  Transforming QA pairs to Claims\n",
            "\u001b[35mSection_title: 3.1.4  Obtaining silver evidence passages\n",
            "\u001b[35mSection_title: 3.2  Data Validation\n",
            "\u001b[35mSection_title: 3.3  Data Analysis\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Baseline Experiments on FAVIQ\n",
            "\u001b[35mSection_title: 4.1.1  Models\n",
            "\u001b[35mSection_title: 4.1.2  Results\n",
            "\u001b[35mSection_title: 4.2  Professional Fact Checking Experiments\n",
            "\u001b[35mSection_title: 4.2.1  Models\n",
            "\u001b[35mSection_title: 4.2.2  Results\n",
            "\u001b[32mconclusion: we introduced faviq, a new fact verification dataset derived from ambiguous information-seeking questions. we incorporate facts that real users were unaware of when posing the question, leading to false claims that are more realistic and challenging to identify without fully understanding the context. our extensive analysis shows that our data contains significantly less lexical bias than previous fact checking datasets, and include refute claims that are challenging and realistic. our experiments showed that the state-of-the-art models are far from solving faviq, and models trained on faviq lead to improvements in professional fact checking. altogether, we believe faviq will serve as a challenging benchmark as well as support future progress in professional fact-checking.\n",
            "\u001b[35mSection_title: 5  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Question Answering via Chain Extraction\n",
            "\u001b[35mSection_title: 3  Learning to Extract Chains\n",
            "\u001b[35mSection_title: 3.1  Heuristic oracle chain construction\n",
            "\u001b[35mSection_title: 3.2  Chain extraction model\n",
            "\u001b[35mSection_title: 3.3  Answer prediction\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Comparison of Chain Extraction Methods\n",
            "\u001b[35mSection_title: 5.2  Results compared to other systems\n",
            "\u001b[35mSection_title: 5.3  Evaluation of chains\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work, we learn to extract reasoning chains to answer multi-hop reasoning questions. experimental results show that the chains are as effective as human annotations, and achieve strong performance on two large datasets. however, as remarked in past work (chen and durrett, 2019;min et al., 2019a), there are several aspects of hot-potqa and wikihop which make them require multi-hop reasoning less strongly than they otherwise might. as more challenging qa datasets are built based on lessons learned from these, we feel that reasoning in a more explicit way and properties of chain-like representations will be critical. this work represents a first step towards this goal of improving qa systems in such settings.\n",
            "\u001b[35mSection_title: 7  Discussion and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Neural Module Networks\n",
            "\u001b[35mSection_title: 2.1  Modules for visual reasoning\n",
            "\u001b[35mSection_title: 2.2  Modules for textual reasoning\n",
            "\u001b[35mSection_title: 3  Module-wise Faithfulness\n",
            "\u001b[35mSection_title: 3.1  Measuring faithfulness in Visual-NMN\n",
            "\u001b[35mSection_title: 3.2  Measuring faithfulness in Text-NMN\n",
            "\u001b[35mSection_title: 4  Improving Faithfulness in NMNs\n",
            "\u001b[35mSection_title: 4.1  Choice of modules\n",
            "\u001b[35mSection_title: 4.2  Supervising module output\n",
            "\u001b[35mSection_title: 4.3  Decontextualized word representations\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental setup\n",
            "\u001b[35mSection_title: 5.2  Faithfulness evaluation\n",
            "\u001b[35mSection_title: 5.3  Qualitative analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we introduce the concept of module-wise faithfulness, a systematic evaluation of faithfulness in neural module networks (nmns) for visual and textual reasoning. we show that naïve training of nmns does not produce faithful modules and propose several techniques to improve module-wise faithfulness in nmns. we show how our approach leads to much higher module-wise faithfulness at a low cost to performance. we encourage future work to judge model interpretability using the proposed evaluation and publicly published annotations, and explore techniques for improving faithfulness and interpretability in compositional models. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Reasoning Like Program Executors\n",
            "\u001b[35mSection_title: 4  POET with Singleton Executors\n",
            "\u001b[35mSection_title: 4.1  Learning from Math Calculators\n",
            "\u001b[35mSection_title: 4.2  Learning from Logic Solvers\n",
            "\u001b[35mSection_title: 4.3  Preliminary Observation\n",
            "\u001b[35mSection_title: 5  POET with Integrated Executors\n",
            "\u001b[35mSection_title: 6  Experiments and Analysis\n",
            "\u001b[35mSection_title: 6.1  Experimental Results\n",
            "\u001b[35mSection_title: 6.2  Pre-training Analysis\n",
            "\u001b[35mSection_title: 7  Discussion and Open Questions\n",
            "\u001b[32mconclusion: we introduce poet, a new pre-training paradigm for boosting reasoning capability of language models via imitating program executors. experimental results on six datasets demonstrate that poet can significantly boost existing language models on several reasoning skills, including numerical, logical and multi-hop reasoning. our best language model under poet can reach highly competitive performance with previous specialized models. in the future, we hope our work could inspire more transference of reasoning knowledge from program executors to models. and we will also investigate the causes of the reasoning transfer with more insightful experiments, since we still do not know how the reasoning transfer occurs.\n",
            "\u001b[35mSection_title: 8  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Question, Answer, and Passage Types\n",
            "\u001b[35mSection_title: 2.1  Answer Type\n",
            "\u001b[35mSection_title: 2.2  Question Type\n",
            "\u001b[35mSection_title: 2.3  Passage Type\n",
            "\u001b[35mSection_title: 3  Datasets\n",
            "\u001b[35mSection_title: 3.1  Data Sources\n",
            "\u001b[35mSection_title: 3.2  Dataset Creation\n",
            "\u001b[35mSection_title: 3.3  Quantitative Analysis\n",
            "\u001b[35mSection_title: 4  Evaluation of MRC Datasets\n",
            "\u001b[35mSection_title: 4.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.2  Human Performance\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[35mSection_title: 6  Concluding Remarks and Recommendations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Works\n",
            "\u001b[35mSection_title: 3.  Methods\n",
            "\u001b[35mSection_title: 3.1.  Question Splitting\n",
            "\u001b[35mSection_title: 3.2.  Memory Check\n",
            "\u001b[35mSection_title: 3.3.  Tool Picker\n",
            "\u001b[35mSection_title: 3.4.  Tool Input Generation\n",
            "\u001b[35mSection_title: 3.5.  Tool Usage\n",
            "\u001b[35mSection_title: 4.  Evaluation\n",
            "\u001b[35mSection_title: 4.1.  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2.  Results\n",
            "\u001b[35mSection_title: 4.2.1.  MULTI-HOP FACT RETRIEVAL\n",
            "\u001b[35mSection_title: 4.2.2.  FACT VERIFICATION\n",
            "\u001b[35mSection_title: 4.3.  Ablation Study\n",
            "\u001b[35mSection_title: 5.  Cost Analysis\n",
            "\u001b[32mconclusion: we have introduced rebel, a recursive reasoning algorithm designed to use any arbitrary api as an external tool.rebel outperforms the state-of-the-art on questions that require the collection of many facts and those that benefit from the ability to make highly specific queries to outside sources of data, which may be unstructured.rebel also has a demonstrable improvement over the gpt3 llm when answering questions that require multi-step information processing.however, the rebel algorithm tends to over-complicate simple problems, leading to a reduction in accuracy when compared to baseline gpt3 on questions that require minimal compositionality.\n",
            "\u001b[35mSection_title: 6.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  AMR Parsing\n",
            "\u001b[35mSection_title: 3.3  AMR Graph Segmentation\n",
            "\u001b[35mSection_title: 3.4  Sub-question Generation\n",
            "\u001b[35mSection_title: 3.5  Single-hop Question Answering\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Data and Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Quality of Sub-questions\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[32mconclusion: we have demonstrated use of question decomposition based on the amr semantic representation for multi-hop qa, using an intrinsically interpretable framework to incorporate interpretability directly into the system structure. the complex task of multi-hop question interpretation is delegated to amr parsers. these parsers produce amr graphs to which two segmentation methods are applied, i.e., unknownbased and path-based graph segmentation, to achieve question decomposition. to generate a well-formed sub-question, we perform both amr parsing and amr-to-text generation with the same architecture, which uses a fully graphisomorphic linearization technique to complete the transformation from graph to a sequence of symbols without losing adjacency information. experimental results demonstrate that our qdamr system outperforms baseline question decomposition methods, both in performance of multi-hop qa and in the quality of generated sub-questions. since our proposed graph segmentation methods are based on predicateargument relations and parallel conditions/entities respectively, they could in principle be generalized to an unknown number of hops by identifying multiple predicate nodes or capturing multiple parallel conditions/entities. while, as noted, an aim of the qdamr design is to provide inherent interpretability, the effectiveness with which its outputs serve as explanations for human users remains to be evaluated in future work.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  AMR Parsing\n",
            "\u001b[35mSection_title: 3.3  AMR Graph Segmentation\n",
            "\u001b[35mSection_title: 3.4  Sub-question Generation\n",
            "\u001b[35mSection_title: 3.5  Single-hop Question Answering\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Data and Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Quality of Sub-questions\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[32mconclusion: we have demonstrated use of question decomposition based on the amr semantic representation for multi-hop qa, using an intrinsically interpretable framework to incorporate interpretability directly into the system structure. the complex task of multi-hop question interpretation is delegated to amr parsers. these parsers produce amr graphs to which two segmentation methods are applied, i.e., unknownbased and path-based graph segmentation, to achieve question decomposition. to generate a well-formed sub-question, we perform both amr parsing and amr-to-text generation with the same architecture, which uses a fully graphisomorphic linearization technique to complete the transformation from graph to a sequence of symbols without losing adjacency information. experimental results demonstrate that our qdamr system outperforms baseline question decomposition methods, both in performance of multi-hop qa and in the quality of generated sub-questions. since our proposed graph segmentation methods are based on predicateargument relations and parallel conditions/entities respectively, they could in principle be generalized to an unknown number of hops by identifying multiple predicate nodes or capturing multiple parallel conditions/entities. while, as noted, an aim of the qdamr design is to provide inherent interpretability, the effectiveness with which its outputs serve as explanations for human users remains to be evaluated in future work.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Knowledge Retrieval\n",
            "\u001b[35mSection_title: 3.2.1  Code Implementation\n",
            "\u001b[35mSection_title: 3.2.2  Entity Linking\n",
            "\u001b[35mSection_title: 3.2.3  Get Entity Information\n",
            "\u001b[35mSection_title: 3.2.4  Find Entity or Value\n",
            "\u001b[35mSection_title: 3.2.5  Find Relationship\n",
            "\u001b[35mSection_title: 3.3  Knowledge Storage\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setups\n",
            "\u001b[35mSection_title: 4.2  Queries on Popular KBs\n",
            "\u001b[35mSection_title: 4.3  Knowledge-Based Question Answering\n",
            "\u001b[35mSection_title: 4.4  KB as Memory\n",
            "\u001b[32mlimitation: while knowledgpt enables llms to effectively perform kb operations on external knowledge bases, there remain several limitations in its current form. first, the retrieval process entails a singleround of code generation and execution for efficiency concerns. however, a multi-round mecha-nism may better allow llms to autonomously explore kbs. as llms are not aware of the contents within kbs, they might generate search that appear logical but yield no results. for example, a query like \"who is the voice actor for the heroine in ...\" may require a two-hop searching for the relations heroine and voice actor subsequently in certain kbs, or just a single relation main voice actor in others. in these scenarios, a multi-round mechanism empowers llms to probe and revisit the kbs autonomously, which might yield better results but with increased costs. second, we experiment with knowledgpt on representative yet small datasets, constrained by the expenses of accessing gpt-4 via api. while the results validate the effectiveness of knowledgpt, more comprehensive evaluations on full benchmarks are expected to better compare knowledgpt to related methods. we plan to study fine-tuning llms like llama for knowledgpt in our future work to improve the efficiency and conduct more thorough experiments. finally, it remains a practical issue when llms need to access external kgs, rather than solving problems independently. in this work, we simply let llms make this decision, while better approaches remain to be explored.\n",
            "\u001b[35mSection_title: 5  Limitations\n",
            "\u001b[32mconclusion: in this paper, we introduce knowledgpt, a comprehensive framework to integrate llms with external knowledge bases, facilitating llms' retrieval and storage on kbs. for retrieval, knowledgpt adopts \"program of thought\" prompting, which retrieves knowledge via code generation and execution. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 2.1.  Multitask Prompted Fine-tuning of Language Models\n",
            "\u001b[35mSection_title: 2.2.  Retrieving task-specific embeddings\n",
            "\u001b[35mSection_title: 2.3.  Distributed Training of Language Models\n",
            "\u001b[35mSection_title: 3.  Expert Language Models\n",
            "\u001b[35mSection_title: 3.1.  Training Experts\n",
            "\u001b[35mSection_title: 3.2.  Retrieval-of-Experts (RoE)\n",
            "\u001b[35mSection_title: 3.3.  Merging of Experts\n",
            "\u001b[35mSection_title: 4.  Experimental Setup\n",
            "\u001b[35mSection_title: 5.  Expert LMs Can Generalize to Unseen Tasks\n",
            "\u001b[35mSection_title: 6.  Benefits of Expert LMs over MT LMs\n",
            "\u001b[32mlimitation: while we highlight some of the major drawbacks of instruction tuning and propose an alternative approach of instead training and retrieving experts in this paper, we do not perform experimental results over mt lms that have more than >11b parameters. for example, mt lms with >11b parameters may be less susceptible to negative task transfer because of increased model capacity. also, during the inference of unseen tasks, our retrieval mechanism assumes batch inference (i.e. having access to 32 samples of the target tasks without labels). finally, when showing the compositional instruction experiments, we assume the two optimal experts could be retrieved from the compositional instruction (concatenation of the two seen instructions) given as the input along with the evaluation instance. this might not necessarily be the case with more complex, compositional instructions, which might require a separate decomposition stage. we instead focus on showing the possibility merging experts can bring and leave developing novel methods of retrieving the optimal experts during inference for future work.\n",
            "\u001b[35mSection_title: 7.  Limitations and Discussions\n",
            "\u001b[32mconclusion: in this work, we provide an interesting finding that expert lms trained on single tasks show strong generalization capability to unseen tasks, even surpassing mt lms trained on multiple tasks (300+) by a non-trivial margin. we leverage this capability and show three main benefits of training and retrieving experts for inference over mt lms, demonstrating that our proposed distributed approach is more robust against negative task transfer, more adapt at learning new tasks, and can perform compositional instructions. to this end, we urge the research community to further explore distributed and collaborative training of experts which may have other future benefits including efficiency, privacy, and personalization not explicitly explored in this paper.\n",
            "\u001b[35mSection_title: 8.  Conclusion\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 2.1.  Multitask Prompted Fine-tuning of Language Models\n",
            "\u001b[35mSection_title: 2.2.  Retrieving task-specific embeddings\n",
            "\u001b[35mSection_title: 2.3.  Distributed Training of Language Models\n",
            "\u001b[35mSection_title: 3.  Expert Language Models\n",
            "\u001b[35mSection_title: 3.1.  Training Experts\n",
            "\u001b[35mSection_title: 3.2.  Retrieval-of-Experts (RoE)\n",
            "\u001b[35mSection_title: 3.3.  Merging of Experts\n",
            "\u001b[35mSection_title: 4.  Experimental Setup\n",
            "\u001b[35mSection_title: 5.  Expert LMs Can Generalize to Unseen Tasks\n",
            "\u001b[35mSection_title: 6.  Benefits of Expert LMs over MT LMs\n",
            "\u001b[32mlimitation: while we highlight some of the major drawbacks of instruction tuning and propose an alternative approach of instead training and retrieving experts in this paper, we do not perform experimental results over mt lms that have more than >11b parameters. for example, mt lms with >11b parameters may be less susceptible to negative task transfer because of increased model capacity. also, during the inference of unseen tasks, our retrieval mechanism assumes batch inference (i.e. having access to 32 samples of the target tasks without labels). finally, when showing the compositional instruction experiments, we assume the two optimal experts could be retrieved from the compositional instruction (concatenation of the two seen instructions) given as the input along with the evaluation instance. this might not necessarily be the case with more complex, compositional instructions, which might require a separate decomposition stage. we instead focus on showing the possibility merging experts can bring and leave developing novel methods of retrieving the optimal experts during inference for future work.\n",
            "\u001b[35mSection_title: 7.  Limitations and Discussions\n",
            "\u001b[32mconclusion: in this work, we provide an interesting finding that expert lms trained on single tasks show strong generalization capability to unseen tasks, even surpassing mt lms trained on multiple tasks (300+) by a non-trivial margin. we leverage this capability and show three main benefits of training and retrieving experts for inference over mt lms, demonstrating that our proposed distributed approach is more robust against negative task transfer, more adapt at learning new tasks, and can perform compositional instructions. to this end, we urge the research community to further explore distributed and collaborative training of experts which may have other future benefits including efficiency, privacy, and personalization not explicitly explored in this paper.\n",
            "\u001b[35mSection_title: 8.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  Background\n",
            "\u001b[35mSection_title: 1.2  Contributions\n",
            "\u001b[35mSection_title: 2  Datasets\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  SOTA Models\n",
            "\u001b[35mSection_title: 3.2  Model Training\n",
            "\u001b[35mSection_title: 3.3  Comparing Results\n",
            "\u001b[35mSection_title: 3.4  Comparing Normal and Biased Models\n",
            "\u001b[35mSection_title: 3.5  Cluster-based Bias Evaluation\n",
            "\u001b[35mSection_title: 4  Results and Discussion\n",
            "\u001b[35mSection_title: 4.1  Relation Leakage Remains a Problem\n",
            "\u001b[35mSection_title: 4.2  Model Capacity and ANLI\n",
            "\u001b[35mSection_title: 4.3  XNLI and Semantic vs Lexical\n",
            "\u001b[35mSection_title: 4.4  SICK, CAugNLI and Small Data\n",
            "\u001b[35mSection_title: 4.5  PECO for Understanding and Improving\n",
            "\u001b[35mSection_title: 4.6  Other Impressions\n",
            "\u001b[32mconclusion: we have introduced useful tools and techniques for analyzing elicited sentence relation leakage bias in nli datasets, and applied them to a large, representative set of popular current datasets.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Rationale-Augmented Ensembles in Language Models\n",
            "\u001b[35mSection_title: 2.1  Optimality of the rationales in few-shot learning.\n",
            "\u001b[35mSection_title: 2.2  Rationale-augmented ensembles\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experiment setup\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 3.3  Results on GPT-3\n",
            "\u001b[35mSection_title: 3.4  Additional Studies\n",
            "\u001b[35mSection_title: 4  Related work\n",
            "\u001b[32mconclusion: in this paper, we have presented a unified framework for rationale-augmented ensembles, and found that rationale sampling in the output space is a key component for achieving improved performance in natural language processing tasks. by sampling diverse rationales and ensembling the results, we have shown that rational-ensembling methods in the proposed framework can reliably outperform standard prompting and rationale-based few-shot prompting, across a wide range of natural language tasks and alternative language models. overall, rationale-augmented ensembling appears to be a reliable way to shift from the paradigm of (input → output) pairs to (input, rationale → output) pairs to achieve more accurate and interpretable natural language processing.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Data\n",
            "\u001b[35mSection_title: 2.1  Dataset Description\n",
            "\u001b[35mSection_title: 2.2  Dataset Preprocessing\n",
            "\u001b[35mSection_title: 3  Models\n",
            "\u001b[35mSection_title: 3.1  RoBERTa\n",
            "\u001b[35mSection_title: 3.2  ALBERT\n",
            "\u001b[35mSection_title: 3.3  ELECTRA\n",
            "\u001b[35mSection_title: 4  Ensemble Models\n",
            "\u001b[35mSection_title: 5  Model Fine-tuning\n",
            "\u001b[35mSection_title: 6  Experimental Results and Discussion\n",
            "\u001b[32mconclusion: in this paper, the application of transfer learning by utilizing transformer models like roberta, al-bert and electra for question answering span prediction task was explored. we also experimented with pretrained models on several other datasets prior to fine-tuning it on the dialdoc21 dataset, provided as part of the dialdoc21 shared task. maximum confidence score based ensemble techniques were employed to combine various base transformer models to further boost the per-formance. we plan to extend our approach and experiment with other ensembling techniques for further enhancing the performance and also explore avenues for improved scalability when applied to larger datasets.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The HELMA Benchmark\n",
            "\u001b[35mSection_title: 2.1  Automatic Generation\n",
            "\u001b[35mSection_title: 2.2  Human Annotation\n",
            "\u001b[35mSection_title: 2.3  Benchmark Analysis and Usage\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 3.2.1  Hallucination Recognition\n",
            "\u001b[35mSection_title: 3.2.2  Improvement Strategies\n",
            "\u001b[35mSection_title: 3.3  Case Study\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we introduce helma, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing and improving hallucinations. to automatically generate large-scale samples, we propose a chatgpt-based two-step approach, i.e., samplingthen-filtering. we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The HELMA Benchmark\n",
            "\u001b[35mSection_title: 2.1  Automatic Generation\n",
            "\u001b[35mSection_title: 2.2  Human Annotation\n",
            "\u001b[35mSection_title: 2.3  Benchmark Analysis and Usage\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[35mSection_title: 3.2.1  Hallucination Recognition\n",
            "\u001b[35mSection_title: 3.2.2  Improvement Strategies\n",
            "\u001b[35mSection_title: 3.3  Case Study\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: we introduce helma, a large-scale collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing and improving hallucinations. to automatically generate large-scale samples, we propose a chatgpt-based two-step approach, i.e., samplingthen-filtering. we first introduce two different sampling methods to generate diverse samples using instructions and then filter and select the difficult one. besides, we invite qualified human labelers to annotate the hallucinations of chatgpt responses given user queries.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 15.  Division of Radiation\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Open-Domain Passage Retrieval\n",
            "\u001b[35mSection_title: 2.2  Contrastive Learning in NLP\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Contrastive Conflicts in DPR\n",
            "\u001b[35mSection_title: 3.2  Passage Representation Decoupling\n",
            "\u001b[35mSection_title: 3.3  Retrieval\n",
            "\u001b[35mSection_title: 4.2  Main Results on Passage Retrieval\n",
            "\u001b[35mSection_title: 4.3  Ablation Study\n",
            "\u001b[35mSection_title: 4.4  Similarity Analysis\n",
            "\u001b[35mSection_title: 4.5  Transferability\n",
            "\u001b[35mSection_title: 5  Pros and Cons\n",
            "\u001b[32mconclusion: in this paper, we analyze the contrastive conflicts problem in current contrastive learning framework adopted in the open-domain passage retrieval area. to solve such problem, we propose dense contextual sentence representation (dscr). concretely, we first decouple the original passage representations into contextual sentence-level ones, and then refine the original contrastive learning framework by creating sentence-aware positive and negative samples. our dcsr achieves significant performance gain compared to original dpr baseline, especially on datasets with severe conflicting problem. extensive experiments shows that our dcsr also enjoys better transferability, indicating that dcsr well captures the universality in different datasets.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Task family setup\n",
            "\u001b[35mSection_title: 3.2  Training strategies\n",
            "\u001b[35mSection_title: 4.1  Experimental results and discussion\n",
            "\u001b[32mconclusion: in this work, we studied the influence of multi-task learning combinations of task families during the pre-finetuning stage for english abstractive text summarization. we trained three different training strategies, six task families composed of 18 tasks, and evaluated two downstream tasks.\n",
            "\u001b[35mSection_title: 5  Conclusion & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS\n",
            "\u001b[35mSection_title: 3  FLASK: FINE-GRAINED LANGUAGE MODEL EVALUATION PROTOCOL\n",
            "\u001b[35mSection_title: 3.1  SKILL SET CATEGORIZATION\n",
            "\u001b[35mSection_title: 3.2  EVALUATION DATA CONSTRUCTION\n",
            "\u001b[35mSection_title: 3.3  EVALUATION PROCESS\n",
            "\u001b[35mSection_title: 3.4  FLASK-HARD\n",
            "\u001b[35mSection_title: 4  RELIABILITY OF FLASK\n",
            "\u001b[35mSection_title: 5  ANALYSIS BASED ON AUTOMATIC EVALUATION OF FLASK\n",
            "\u001b[35mSection_title: 6  APPLICATION OF FLASK\n",
            "\u001b[32mconclusion: in this paper, we introduce flask, a fine-grained language skill set evaluation setting for the alignment of language models. we categorize 12 fine-grained skills to evaluate llms and annotate necessary skills, the target domain, and the difficulty level for each instance. flask provides a comprehensive and interpretable analysis of the capabilities of llms by allowing the analysis of the performance depending on different skills, domains, and difficulty levels. also, we observe that applying fine-grained evaluation results in better reliability in terms of correlation between humanbased and model-based evaluation and the robustness of model-based evaluation to stylistic changes.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  The Use of Nonparametric Methods\n",
            "\u001b[35mSection_title: 3.  Proposed Methodology\n",
            "\u001b[35mSection_title: 3.1.  Tree Building.\n",
            "\u001b[35mSection_title: 3.2.  Stopping Tree Building.\n",
            "\u001b[35mSection_title: 3.3.  Tree Pruning.\n",
            "\u001b[35mSection_title: 4.  Methodology Application\n",
            "\u001b[35mSection_title: 4.3.  Preliminary Analysis of Bridge Deck Condition Deterioration.\n",
            "\u001b[35mSection_title: 4.4.  Selection of Bridge Deck Deterioration\n",
            "\u001b[35mSection_title: 4.5.  BRP Method\n",
            "\u001b[35mSection_title: 5.  Comparisons of the Proposed BRP Method with Some Notable Nonparametric Methods\n",
            "\u001b[32mconclusion: nonparametric methods possess a number of advantages over the commonly used parametric statistical methods, such as no requirements of specific distribution assumptions, interactions of explanatory variables, and randomness of sample observations.as a convention, bridge deck condition is assessed using a weighted scale from 0 to 9 representing poorest to best conditions.with ranked categorical data in place, the nonparametric deck deterioration modeling can be treated as a classification and decision problem.this paper introduced the brp method for solving the classification and decision problem.the proposed brp method involves four basic steps.the first step consists of tree building, during which a tree is built using recursive splitting of nodes.each resulting node is assigned a predicted class based on the distribution of classes in the learning dataset which would occur in that node and the loss matrix.the assignment of a predicted class to each node occurs whether or not that node is subsequently split into child nodes.the second step consists of stopping the tree building process.at this point a maximal tree has been produced.the third step consists of tree pruning, which results in the creation of a sequence of continuously simpler trees through \"cutting off \" increasingly important nodes.the fourth step consists of optimal tree selection, during which the tree which fits the information in the learning dataset, but does not overfit the information, is selected from among the sequence of pruned trees.the proposed brp method was applied through the cart software program to the indiana bridge inventory data containing information on function adequacy and structural conditions of approximately 5,500 indiana state-maintained highway bridges.the response variable was defined as number of units of deck condition drops after the last inspection.the explanatory variables were consistent with those in [5,6], which mainly included deck age, geographical region, highway class, deck geometric design, deck structure type, average daily traffic, and deck condition rating at the time of inspection.two sets of classification trees were created.the first set used 4 predicted classes for the response variable, including class 1 for no deterioration, class 2 for one unit of deterioration, class 3 for two units of deterioration, and class 4 for three units of deterioration.the significant explanatory variables corresponding to the optimal classification tree were current deck condition rating and deck age.additional variables that appeared to be influential included highway class, traffic, deck width, and wearing surface protection system.the success in correctly predicting deck condition deterioration for the four predicted classes were 91, 62, 69, and 92 percent, respectively, a noted improvement over previous research utilizing parametric methods and the same dataset.wherein, the degrees of successful prediction for classes 2 and 3 were lower than those for classes 1 and 4. thus, the second set of special 2-class tree that only considered no deterioration and one to three units of deterioration was created.the degrees of success in correctly predicting deck condition for the two predicted classes increased to 95 and 84 percentages, correspondingly.with fewer classes used, the special classification tree presented a more general view of deck condition deterioration prediction.the proposed brp method could outperform other known nonparametric methods developed so far for bridge deck condition prediction from 73-75 percent to 92-95 percent, a margin of 20 percent higher in accuracy.\n",
            "\u001b[35mSection_title: 6.  Summary and Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Introducing Nested Boolean Logic\n",
            "\u001b[35mSection_title: 2.1  From Simple Boolean Logic to Nested Boolean Logic\n",
            "\u001b[35mSection_title: 2.2  Quantify Boolean Logic\n",
            "\u001b[35mSection_title: 3  Benchmark\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Data Bias\n",
            "\u001b[35mSection_title: 3.3  Evaluation Results\n",
            "\u001b[35mSection_title: 4  Empower Nested Boolean Logic\n",
            "\u001b[35mSection_title: 4.1  Self-Supervised Learning\n",
            "\u001b[35mSection_title: 4.2  Curriculum Logical Reasoning\n",
            "\u001b[35mSection_title: 5  Empirical Results\n",
            "\u001b[35mSection_title: 5.1  Nested Boolean Logic\n",
            "\u001b[35mSection_title: 5.2  Boolean Benefits Complex Logic\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.4  Fine-tuning Large Language Models\n",
            "\u001b[35mSection_title: 6.1  Chain-of-Thought\n",
            "\u001b[35mSection_title: 6.2  True or False\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: this paper provides a quantified analysis on the multi-nested boolean logic.we flag the deficiency in the state-of-the-art language models in terms of such basic capability, which will inevitably cause pitfalls in dealing with more complex reasoning tasks.for this, we propose curriculum logical reasoning, a new self-supervised learning method to empower language models with foundational logical capability.we also show that our idea can act as a cornerstone learning method for general logical reasoning.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminaries\n",
            "\u001b[35mSection_title: 3  Approximate Nearest Neighbor Noise Contrastive Estimation\n",
            "\u001b[35mSection_title: 4  Experimental Methodologies\n",
            "\u001b[35mSection_title: 5  Evaluation Results\n",
            "\u001b[35mSection_title: 5.1  Effectiveness of ANCE Dense Retrieval\n",
            "\u001b[35mSection_title: 5.2  Efficiency of ANCE Retrieval and Learning\n",
            "\u001b[35mSection_title: 5.3  Representation Learning with ANCE\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: ance fundamentally eliminates the discrepancy between the representation learning of texts and their usages in dense retrieval. our ance trained dense retrieval model, the vanilla bert-siamese, convincingly outperforms all dense retrieval and sparse retrieval baselines in our large scale document retrieval and passage retrieval experiments. it nearly matches the ranking accuracy of the state-of-theart cascade sparse retrieval and bert reranking pipeline. more importantly, all these advantages are achieved with a standard transformer encoder at a 1% online inference latency, using a simple dot-product in the ance-learned representation space.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Question Generation\n",
            "\u001b[35mSection_title: 3.1  Base Model\n",
            "\u001b[35mSection_title: 3.2  Semantics-Reinforced Model\n",
            "\u001b[35mSection_title: 3.3  QA-Based QG Evaluation\n",
            "\u001b[35mSection_title: 4  Semi-Supervised Question Answering\n",
            "\u001b[35mSection_title: 4.1  Synthetic Data Generation\n",
            "\u001b[35mSection_title: 4.2  Synthetic Data Usage\n",
            "\u001b[35mSection_title: 5  Experiment Setup\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Question Generation\n",
            "\u001b[35mSection_title: 6.2  Semi-Supervised Question Answering\n",
            "\u001b[35mSection_title: 6.3  QG and QA Results with BERT\n",
            "\u001b[32mconclusion: we proposed two semantics-enhanced rewards to regularize a qg model to generate semantically valid questions, and introduced a qa-based evaluation method that directly evaluates a qg model's ability to mimic human annotators in generating qa training data. experiments showed that our qg model achieves new state-of-the-art performances. further, we investigated how to use our qg system to augment qa datasets and conduct semi-supervised qa via two synthetic data generation methods along with a data filter and mixing mini-batch training. experiments showed that our approach improves both bidaf and bert qa baselines even without introducing new articles. qa for qa, we use squadv1.1 (rajpurkar et al., 2016). previous semi-supervised qa works sampled 10% from training set as the testing set dhingra et al., 2018). since we want to use the full training set in semi-supervised qa setup without any data size reduction, we instead split the original development set in half for validation and testing respectively. for semi-supervised qa, first, without introducing new articles, we generate new questions for squad training set by keeping all beam search outputs. second, with introducing new articles, we obtain new paragraphs with pre-extracted answer spans from harvestingqa . without using their provided questions, we use our best qg model to label questions. meanwhile, we investigate the influence of synthetic data size, so we sample 10% to 100% examples from harvest-ingqa, which are denoted as h1-h10 in our experiments.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  5% on averag\n",
            "\u001b[35mSection_title: 2   (textda\n",
            "\u001b[35mSection_title: 2.1  ci-003) \n",
            "\u001b[35mSection_title: 2.2  hnique with (*) and pro\n",
            "\u001b[35mSection_title: 2.2.1  e our justification.\n",
            "\n",
            "\u001b[35mSection_title: 2.2.2  e entity extraction model mis\n",
            "\u001b[35mSection_title: 2.2.3  h respect to the uncertain\n",
            "\u001b[35mSection_title: 2.2.4  is our preferred techni\n",
            "\u001b[35mSection_title: 2.2.5  is not required, i.e., whe\n",
            "\u001b[35mSection_title: 2.3  reads which may not be s\n",
            "\u001b[35mSection_title: 2.4   we show that ou\n",
            "\u001b[35mSection_title: 3   developed which have e\n",
            "\u001b[35mSection_title: 2  relia\n",
            "\u001b[35mSection_title: 3  ere i\n",
            "\u001b[35mSection_title: 3.1  rectness.Since the \n",
            "\u001b[35mSection_title: 3.1.1  tences are sequentially generated, we inve\n",
            "\u001b[35mSection_title: 3.1.2   the logit output values.Then, we demonstrate the bene\n",
            "\u001b[35mSection_title: 3.2  ch: Table 1a and 1b show the halluc\n",
            "\u001b[35mSection_title: 3.3  eshold plot for Self Inquiry and web\n",
            "\u001b[35mSection_title: 3.4   output of the mitigation step \n",
            "\u001b[35mSection_title: 4.2  tes the generality \n",
            "\u001b[35mSection_title: 4.3   approach in improving \n",
            "\u001b[35mSection_title: 4.4  they first sample \n",
            "\u001b[35mSection_title: 5  ing that dyn\n",
            "\u001b[35mSection_title: 6  ns as it i\n",
            "\u001b[35mSection_title: 1  5% on averag\n",
            "\u001b[35mSection_title: 2   (textda\n",
            "\u001b[35mSection_title: 2.1  ci-003) \n",
            "\u001b[35mSection_title: 2.2  hnique with (*) and pro\n",
            "\u001b[35mSection_title: 2.2.1  e our justification.\n",
            "\n",
            "\u001b[35mSection_title: 2.2.2  e entity extraction model mis\n",
            "\u001b[35mSection_title: 2.2.3  h respect to the uncertain\n",
            "\u001b[35mSection_title: 2.2.4  is our preferred techni\n",
            "\u001b[35mSection_title: 2.2.5  is not required, i.e., whe\n",
            "\u001b[35mSection_title: 2.3  reads which may not be s\n",
            "\u001b[35mSection_title: 2.4   we show that ou\n",
            "\u001b[35mSection_title: 3   developed which have e\n",
            "\u001b[35mSection_title: 2  relia\n",
            "\u001b[35mSection_title: 3  ere i\n",
            "\u001b[35mSection_title: 3.1  rectness.Since the \n",
            "\u001b[35mSection_title: 3.1.1  tences are sequentially generated, we inve\n",
            "\u001b[35mSection_title: 3.1.2   the logit output values.Then, we demonstrate the bene\n",
            "\u001b[35mSection_title: 3.2  ch: Table 1a and 1b show the halluc\n",
            "\u001b[35mSection_title: 3.3  eshold plot for Self Inquiry and web\n",
            "\u001b[35mSection_title: 3.4   output of the mitigation step \n",
            "\u001b[35mSection_title: 4.2  tes the generality \n",
            "\u001b[35mSection_title: 4.3   approach in improving \n",
            "\u001b[35mSection_title: 4.4  they first sample \n",
            "\u001b[35mSection_title: 5  ing that dyn\n",
            "\u001b[35mSection_title: 6  ns as it i\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Overview\n",
            "\u001b[35mSection_title: 2.2  Hallucination Detection\n",
            "\u001b[35mSection_title: 2.2.1  Identify Key Concepts\n",
            "\u001b[35mSection_title: 2.2.2  Calculate Model's Uncertainty\n",
            "\u001b[35mSection_title: 2.2.3  Create Validation Question\n",
            "\u001b[35mSection_title: 2.2.4  Find Relevant Knowledge *Web Search*:\n",
            "\u001b[35mSection_title: 2.2.5  Answer Validation Question\n",
            "\u001b[35mSection_title: 2.3  Hallucination Mitigation\n",
            "\u001b[35mSection_title: 2.4  Design Decisions\n",
            "\u001b[35mSection_title: 3  Experiments and Results\n",
            "\u001b[35mSection_title: 2  D > C\n",
            "\u001b[35mSection_title: 3  D > B\n",
            "\u001b[35mSection_title: 3.1  Motivating Findings\n",
            "\u001b[35mSection_title: 3.1.1  Hallucination Causes Further Hallucination\n",
            "\u001b[35mSection_title: 3.1.2  Logit Output Values Provide a Signal for Hallucination\n",
            "\u001b[35mSection_title: 3.2  Hallucination Detection Performance\n",
            "\u001b[35mSection_title: 3.3  Hallucination Mitigation Performance\n",
            "\u001b[35mSection_title: 3.4  Active Detection and Mitigation\n",
            "\u001b[35mSection_title: 4.1  Efficacy with LLM from Another Family\n",
            "\u001b[35mSection_title: 4.2  Multi-hop Questions\n",
            "\u001b[35mSection_title: 4.3  False Premise Questions\n",
            "\u001b[35mSection_title: 4.4  Other Applications\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we proposed an approach that actively 'detects' and 'mitigates' hallucinations of the large language models.through systematic and extensive experiments with the article generation task, we showed that our approach successfully reduces the hallucinations of the gpt-3.5 (text-davinci-003) from 47.5% to 14.5% on average.we also demonstrated the individual efficacy of our dete\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Two-stage Approach\n",
            "\u001b[35mSection_title: 2.2  End-to-End Approach\n",
            "\u001b[35mSection_title: 2.3  Retrieval-free Approach\n",
            "\u001b[35mSection_title: 3  The Proposed Method\n",
            "\u001b[35mSection_title: 3.1  Background\n",
            "\u001b[35mSection_title: 3.2  The Proposed Library Overview\n",
            "\u001b[35mSection_title: 3.3  Ranker Service\n",
            "\u001b[35mSection_title: 3.3.1  Cached Ranking Results\n",
            "\u001b[35mSection_title: 3.3.2  Ranking APIs\n",
            "\u001b[35mSection_title: 3.4  Reader Hub\n",
            "\u001b[35mSection_title: 3.5  Evaluation\n",
            "\u001b[35mSection_title: 3.6  Pipeline Configurations\n",
            "\u001b[35mSection_title: 4  Use Cases\n",
            "\u001b[35mSection_title: 4.1  Efficient Reader Comparison\n",
            "\u001b[35mSection_title: 4.2  Reproducible Research\n",
            "\u001b[35mSection_title: 4.3  Knowledge-empowered Applications\n",
            "\u001b[35mSection_title: 5  Experiment Results\n",
            "\u001b[35mSection_title: 5.1  Reproducing Prior Art\n",
            "\u001b[35mSection_title: 5.2  Time saved by SF-QA\n",
            "\u001b[35mSection_title: 5.3  Model Accuracy v.s. Corpus release year\n",
            "\u001b[32mconclusion: in conclusion, this paper presents sf-qa, a novel evaluation framework to make open-domain qa research simple and fair. this framework fixes the gap among researchers from different fields, and make the open-domain qa more accessible. we show the robustness of this framework by successfully reproducing several existing models in opendomain qa research. we hope that sf-qa can make the open-domain qa research more accessible and make the evaluation easier. we expect to further improve our framework by including more models in both ranker and reader side, and encourage community contributions to the project as well.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  OPEN-DOMAIN QUESTION ANSWERING OVER WIKIPEDIA GRAPH\n",
            "\u001b[35mSection_title: 3.1  LEARNING TO RETRIEVE REASONING PATHS\n",
            "\u001b[35mSection_title: 3.1.1  THE GRAPH-BASED RECURRENT RETRIEVER\n",
            "\u001b[35mSection_title: 3.2  READING AND ANSWERING GIVEN REASONING PATHS\n",
            "\u001b[35mSection_title: 4.3  PERFORMANCE OF REASONING PATH RETRIEVAL\n",
            "\u001b[35mSection_title: 4.4  ANALYSIS\n",
            "\u001b[32mconclusion: this paper introduces a new graph-based recurrent retrieval approach, which retrieves reasoning paths over the wikipedia graph to answer multi-hop open-domain questions. our retriever model learns to sequentially retrieve evidence paragraphs to form the reasoning path. subsequently, our reader model re-ranks the reasoning paths, and it determines the final answer as the one extracted from the best reasoning path. our experimental results significantly advance the state of the art on hotpotqa by more than 14 points absolute gain on the full wiki setting. our approach also achieves the state-of-the-art performance on squad open and natural questions open without any architectural changes, demonstrating the robustness of our method. our method provides insights into the underlying entity relationships, and the discrete reasoning paths are helpful in interpreting our framework's reasoning process. future work involves end-to-end training of our graph-based recurrent retriever and reader for improving upon our current two-stage training.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Document Graph Network\n",
            "\u001b[35mSection_title: 2.1  Graph-structured Representation\n",
            "\u001b[35mSection_title: 2.2  Architectural Overview\n",
            "\u001b[35mSection_title: 2.3  Prefiltering Step\n",
            "\u001b[35mSection_title: 2.4  Identifying Supporting Facts\n",
            "\u001b[35mSection_title: 3  Evaluation\n",
            "\u001b[35mSection_title: 3.1  State-of-the-Art Machine Comprehension Performance\n",
            "\u001b[35mSection_title: 3.2  Supporting Facts Identification\n",
            "\u001b[35mSection_title: 3.3  Analysis\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we investigated the role played by interlinked sentence representation for complex, multi-hop question answering under the focus of supporting facts identification, i.e. retrieving the minimum set of facts required to answer a given question. we emphasise that this problem is worth pursuing, showing that the performance of stateof-the-art models substantially deteriorates as the size of the accompanying context increases.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  CONTINUAL KNOWLEDGE LEARNING (CKL)\n",
            "\u001b[35mSection_title: 3.1  TASK FORMULATION\n",
            "\u001b[35mSection_title: 3.2  COMBINED METRIC FOR CKL\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 5.1  MAIN RESULTS\n",
            "\u001b[35mSection_title: 5.2  EXPLORING MULTIPLE PHASES OF CKL\n",
            "\u001b[32mconclusion: in this paper, we propose continual knowledge learning (ckl), where we establish benchmark datasets and metrics, and explore methodologies towards continual knowledge learning of an ever-changing lm. we find that parameter-expansion methods show the most robust performance throughout all of the experimental settings, which nevertheless has severe memory inefficiency and that seeing the same data often is a critical cause of forgetting. we also discuss several other interesting results of which we leave further exploration to future studies. to this end, we suggest the community to explore ckl for the better design of an ever-changing lm. \n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Annotation and optimization of STAR parameters related to non-continuous alignments\n",
            "\u001b[35mSection_title: 3.  Choice of genome references and annotations for STAR mapping\n",
            "\u001b[35mSection_title: 4.  Filtering, classification, and rearrangement of alignments\n",
            "\u001b[35mSection_title: 5.  CRSSANT method details (clustering gap1 and trans alignments to DGs and NGs)\n",
            "\u001b[35mSection_title: 5.1  CRSSANT data pre-processing\n",
            "\u001b[35mSection_title: 5.2  Network representation of non-continuous alignments\n",
            "\u001b[35mSection_title: 5.3  Network clustering of non-continuous alignments\n",
            "\u001b[35mSection_title: 5.4  DG analysis\n",
            "\u001b[35mSection_title: 5.5  NG clustering\n",
            "\u001b[35mSection_title: 5.6  Output files from CRSSANT.py\n",
            "\u001b[35mSection_title: 7.  Identification and structure analysis of U3 genes\n",
            "\u001b[35mSection_title: 8.  Identification and structure analysis of U8 genes\n",
            "\u001b[35mSection_title: 9.  Identification and structure analysis of U13 genes\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  MAJOR APPLICATIONS OF SEMANTIC RETRIEVAL MODELS\n",
            "\u001b[35mSection_title: 3  BACKGROUND\n",
            "\u001b[35mSection_title: 3.1  Problem Formalization\n",
            "\u001b[35mSection_title: 3.2  Indexing Methods\n",
            "\u001b[35mSection_title: 3.3  Classical Term-based Retrieval\n",
            "\u001b[35mSection_title: 4  EARLY METHODS FOR SEMANTIC RETRIEVAL\n",
            "\u001b[35mSection_title: 4.1  Query Expansion\n",
            "\u001b[35mSection_title: 4.2  Document Expansion\n",
            "\u001b[35mSection_title: 4.3  Term Dependency Models\n",
            "\u001b[35mSection_title: 4.4  Topic Models\n",
            "\u001b[35mSection_title: 4.5  Translation Models\n",
            "\u001b[35mSection_title: 5  NEURAL METHODS FOR SEMANTIC RETRIEVAL\n",
            "\u001b[35mSection_title: 5.1  Sparse Retrieval Methods\n",
            "\u001b[35mSection_title: 5.1.2  Sparse Representation\n",
            "\u001b[35mSection_title: 5.2  Dense Retrieval Methods\n",
            "\u001b[35mSection_title: 5.2.1  Term-level Representation\n",
            "\u001b[35mSection_title: 5.2.2  Document-level Representation Learning.\n",
            "\u001b[35mSection_title: 5.3  Hybrid Retrieval Methods\n",
            "\u001b[35mSection_title: 5.3  Hybrid Retrieval Methods\n",
            "\u001b[35mSection_title: 5.4  Model Learning\n",
            "\u001b[35mSection_title: 5.4.1  Loss Functions.\n",
            "\u001b[35mSection_title: 5.4.2  Negative Sampling Strategies.\n",
            "\u001b[35mSection_title: 6  CHALLENGES AND FUTURE DIRECTIONS\n",
            "\u001b[35mSection_title: 6.1  Pre-training Objectives for the Retrieval Stage\n",
            "\u001b[35mSection_title: 6.2  More Effective Learning Strategies\n",
            "\u001b[35mSection_title: 6.3  Benchmark Testbed for Efficiency Comparison\n",
            "\u001b[35mSection_title: 6.4  Advanced Indexing Schemes\n",
            "\u001b[32mconclusion: the purpose of this survey is to summarize the current research status on semantic retrieval models, analyze existing methodologies, and gain some insights for future development. it includes a brief review of early semantic retrieval methods, a detailed description of recent neural semantic retrieval methods and the connection between them. specially, we pay attention to neural semantic retrieval methods, and review them from three major paradigms, including sparse retrieval methods, dense retrieval methods and hybrid retrieval methods. we also refer to key topics about neural semantic retrieval models learning, such as loss functions and negative sampling strategies. in addition, we discuss several challenges and promising directions that are important for future researches. we look forward to working with the community on these issues.\n",
            "\u001b[35mSection_title: 7  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Generation\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Theory Generation\n",
            "\u001b[35mSection_title: 3.3  Forward Inference\n",
            "\u001b[35mSection_title: 3.4  Question Generation and English Synthesis\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Can RoBERTa Answer Reasoning Questions?\n",
            "\u001b[35mSection_title: 4.3  Performance on Hand-Authored Problems\n",
            "\u001b[35mSection_title: 2.  The MMax model (trained on DMax) solves all but one\n",
            "\u001b[35mSection_title: 4.4  Reasoning with Paraphrased Rules\n",
            "\u001b[35mSection_title: 4.5  Generating Explanations\n",
            "\u001b[35mSection_title: 4.6  Other Architectures\n",
            "\u001b[35mSection_title: 5  Discussion and Future Work\n",
            "\u001b[35mSection_title: 5.1  Extending The Theory Language\n",
            "\u001b[35mSection_title: 5.2  Generating Training Data\n",
            "\u001b[35mSection_title: 5.3  Formal Theorem Proving\n",
            "\u001b[35mSection_title: 5.4  Natural Language Inference (NLI)\n",
            "\u001b[32mconclusion: just as mccarthy advocated 60 years ago for machines reasoning (\"taking advice\") in logic, we have shown (in a restricted setting) that machines can by trained to reason over language. while we have assumed a particular semantics of inference, the methodology we have used is general: characterize the desired behavior in a formal way, synthesize examples, generate linguistic equivalents, and train a model. the result, at least within our experiments, appears to be both nat-ural and robust, in a way distinct from working with the original formalization.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Analysis\n",
            "\u001b[35mSection_title: 2.1  Position Bias on Synthetic Datasets\n",
            "\u001b[35mSection_title: 2.2  Visualization of Position Bias\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Baselines\n",
            "\u001b[35mSection_title: 3.2  Bias Ensemble with Answer Prior\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Effect of De-biasing Methods\n",
            "\u001b[35mSection_title: 4.2  Generalizing to Different Positions\n",
            "\u001b[35mSection_title: 4.3  NewsQA and NaturalQuestions\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: most qa studies frequently utilize start and end positions of answers as training targets without much considerations. our study shows that most qa models fail to generalize over different positions when trained on datasets having answers in a specific position. we introduce several de-biasing methods to make models to ignore the spurious positional cues, and find out that the sentence-level answer prior is very useful. our findings also generalize to different positions and different datasets. one limitation of our approach is that our method and analysis are based on a single paragraph setting which should be extended to a multiple paragraph setting to be more practically useful.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.2  Synthetic Question Selection\n",
            "\u001b[35mSection_title: 4  Question Value Estimator (QVE)\n",
            "\u001b[35mSection_title: 4.1  QVE Training: Two Baselines\n",
            "\u001b[35mSection_title: 4.2  QVE Training: Direct Feedback from QA\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Datasets\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 5.3  Compared Baselines\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Overall Results\n",
            "\u001b[35mSection_title: 6.3  Experiments with Larger Models\n",
            "\u001b[32mconclusion: we propose a question value estimator to estimate the usefulness of synthetic questions and select useful ones for improving target-domain qa train- 7 we treat it as a binary classification problem here: if a question is selected, the prediction is 1; 0 otherwise.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 4  Method\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 2.2  PrLMs and Chinese PrLMs\n",
            "\u001b[35mSection_title: 2.3  Multi-grained and Hierarchical Models\n",
            "\u001b[35mSection_title: 3  Our Proposed Model\n",
            "\u001b[35mSection_title: 4.2  Setup\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  For Different Types Chinese MRC Tasks\n",
            "\u001b[35mSection_title: 4.5  For Other Languages\n",
            "\u001b[35mSection_title: 5  Ablation Study\n",
            "\u001b[35mSection_title: 5.1  Effect of Each Module\n",
            "\u001b[35mSection_title: 5.2  Scoring Function\n",
            "\u001b[35mSection_title: 5.3  Error Analysis\n",
            "\u001b[32mconclusion: this paper aims at addressing the newly discovered difficulty of the boundary ambiguity between sentences and sub-sentences, which exists in many languages to different extents and essentially limits the performance of span extraction mrc models, especially in chinese environment. we apply explicit span-sentence predication (esp) to enhance model's ability of precisely locating sentences containing the target span. our proposed model design is evaluated on chinese span extraction mrc benchmark, cmrc 2018. the experimental results show that our model significantly improves both em and f1 scores compared with strong baselines and helps achieve a new state-of-the-art performance. our method also shows generality and potential in dealing with other languages. this work highlights the research line of further improving challenging mrc by analyzing specific linguistics phenomena.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  QA Graph Model Overview\n",
            "\u001b[35mSection_title: 3.2  Graph Construction\n",
            "\u001b[35mSection_title: 3.2.1  EQAG node construction procedure\n",
            "\u001b[35mSection_title: 3.2.2  EQAG edge construction procedure\n",
            "\u001b[35mSection_title: 3.2.3  Training Graph Construction Analysis\n",
            "\u001b[35mSection_title: 4  Learning from QA graphs with GNNs\n",
            "\u001b[35mSection_title: 5  Training and Inference\n",
            "\u001b[35mSection_title: 5.1  Training\n",
            "\u001b[35mSection_title: 5.2  Inference\n",
            "\u001b[35mSection_title: 6  Experiments\n",
            "\u001b[35mSection_title: 6.1  Datasets\n",
            "\u001b[35mSection_title: 6.2  Setup\n",
            "\u001b[35mSection_title: 6.3  Experiments with EQAG\n",
            "\u001b[35mSection_title: 7  Case Study\n",
            "\u001b[35mSection_title: 7.1  Error Analysis\n",
            "\u001b[32mconclusion: to our knowledge, our model is the first graphbased approach for jointly modeling sentence-level semantics of question-answer pairs for as2 as an offline processing application, such as those required by community qa, forums, etc. this is different from previous methods using graphs, e.g., multihop or graph-based qa, which mainly model semantics via entities. our approach builds query-specific small-scale training graphs for offline learning, through (q, a) pairs as nodes, and edges encoding relations between members of pairs to capture both supporting question-question, and answer-answer dependencies. further, we demonstrate that our approach achieves significant performance gains over existing sota models on as2 for metrics of p@1, map, and mrr.\n",
            "\u001b[35mSection_title: 8  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Synthetic Dataset Construction\n",
            "\u001b[35mSection_title: 3  How the Shortcut Questions Affect\n",
            "\u001b[35mSection_title: 5  How do Models Learn Shortcuts?\n",
            "\u001b[35mSection_title: 6  Related Works\n",
            "\u001b[32mconclusion: in this work, we try to answer why many mrc models learn shortcut tricks while ignoring the pre-designed comprehension challenges that are purposely embedded in many benchmark datasets. we argue that large proportions of shortcut questions in training data push mrc models to rely on shortcut tricks excessively. to properly investigate, we first design two synthetic datasets where each instance has a shortcut version paired with a challenging one which requires paraphrasing, a complex reasoning skill, to answer, rather than performing question word matching or simple matching. with these datasets, we are able to adjust the proportion of shortcut questions in both training and testing, while maintaining other factors relatively steady. we propose two methods to examine the model training process regarding the shortcut questions, which enable us to take a closer look at the learning mechanisms of bidaf and bert under different training settings. we find that learning shortcut questions generally requires less computational resources, and mrc models usually learn the shortcut questions at their early stage of training. our findings reveal that, with larger proportions of shortcut questions for training, mrc models will learn the shortcut tricks quickly while ignoring the designed comprehension challenges, since the remaining truly challenging questions, usually limited in size, may not motivate models to explore sophisticated solutions in the later training stage. \n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Attractor manipulations\n",
            "\u001b[35mSection_title: 3.2  Dataset construction\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Models\n",
            "\u001b[35mSection_title: 4.2  Input representation\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Impact on probabilities\n",
            "\u001b[35mSection_title: 5.2  Semantically unrelated attractors\n",
            "\u001b[35mSection_title: 6  Varying position of information\n",
            "\u001b[35mSection_title: 6.1  Separating key entity and critical fact\n",
            "\u001b[35mSection_title: 6.2  Varying key entity position\n",
            "\u001b[35mSection_title: 7  Discussion\n",
            "\u001b[32mconclusion: we have presented results manipulating inputs of pre-trained lms, to test the ability of such models to represent and retain information conveyed by input text. our results show that though models may appear to handle information correctly in simple settings, these correct predictions are easily broken by insertion of distracting material in the context. systematic manipulation of the distracting content further indicates key roles for semantic similarity and relative word position in models' selection of relevant contextual cues for prediction. overall, the results suggest that lm predictions are driven more by coarse-grained superficial cues than by extraction of robust meaning information from context. the results serve as a reality check for considerations of the extent to which lms \"understand\" their input, and lay groundwork to understand the mechanisms that do drive predictions in these models.   \n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORKS 2.1 Contextual Embedding\n",
            "\u001b[35mSection_title: 2.2  Machine Comprehension Dataset\n",
            "\u001b[35mSection_title: 2.3  Films Related Works\n",
            "\u001b[35mSection_title: 3  NEW DATASET: TIMOS 3.1 Overview\n",
            "\u001b[35mSection_title: 3.2  Data Collection\n",
            "\u001b[35mSection_title: 3.3  Data Analysis\n",
            "\u001b[35mSection_title: 3.4  Challenges\n",
            "\u001b[35mSection_title: 4  METHOD\n",
            "\u001b[35mSection_title: 4.1  Task Formulation\n",
            "\u001b[35mSection_title: 4.2  Multi-Level Comprehension Network\n",
            "\u001b[35mSection_title: 4.3  Multi-Step Recurrent Relational Network\n",
            "\u001b[35mSection_title: 5  EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Streams\n",
            "\u001b[35mSection_title: 6  RESULTS AND ANALYSIS 6.1 Baseline Comparison\n",
            "\u001b[35mSection_title: 6.2  Trope Difficulty\n",
            "\u001b[35mSection_title: 6.3  Knowledge Adaption\n",
            "\u001b[35mSection_title: 6.4  Qualitative Analysis\n",
            "\u001b[35mSection_title: 7  HUMAN EVALUATION AND DISCUSSION\n",
            "\u001b[35mSection_title: 7.1  Human Inference Process\n",
            "\u001b[35mSection_title: 7.2  Setup\n",
            "\u001b[35mSection_title: 7.3  Human Performance\n",
            "\u001b[35mSection_title: 7.4  Potential Directions\n",
            "\u001b[32mconclusion: in this work, we presented a brand new task with a new dataset timos for situation and behavior understanding in films. unlike previous tasks and datasets, trope detection requires deep cognitive skills, including consciousness, systematic generalization, and casual and motivational comprehension. modern learning systems including contextual embedding bert, movie tag prediction system, and recurrent relational network reach at most 23.97 f1 score, which is far behind human performance (64.87) and suggests that trope detection is a challenging task. we proposed a multi-level comprehension framework with multi-stream attention and multistep reasoning to tackle this problem and boost performance to 25.00 f1. we carefully analyze the task along with the network's behavior. we adopt human evaluation to verify the answerability of our dataset and discuss potential directions. we believe that our research could pave a new path for future research on deep cognition.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Results\n",
            "\u001b[35mSection_title: 2.1.  Differential Expression Profiles in PCa Samples\n",
            "\u001b[35mSection_title: 2.2.  Correlation Analysis for mRNAs and lncRNA Genes Using Gene Expression Data\n",
            "\u001b[35mSection_title: 2.3.  Prediction of Potential Triplex-Forming lncRNAs and Triplex Structure in the Promoter Region of Co-Expression Genes\n",
            "\u001b[35mSection_title: 2.4.  AD000684.2 and ASMTL-AS1 Are Highly Expressed in PCa\n",
            "\u001b[35mSection_title: 2.5.  AD000684.2 Positively Regulates the Expression of Correlated Genes\n",
            "\u001b[35mSection_title: 2.6.  AD000684.2 Forms RNA-DNA Triplexes with the Promoter Region of Regulated Genes\n",
            "\u001b[35mSection_title: 2.7.  AD000684.2 Promotes Cell Proliferation and Motility, as Well as Inhibits Cell Apoptosis, in PCa Cell Lines\n",
            "\u001b[35mSection_title: 3.  Discussion\n",
            "\u001b[35mSection_title: 4.  Materials and Methods\n",
            "\u001b[35mSection_title: 4.1.  Computational Analysis\n",
            "\u001b[35mSection_title: 4.1.1.  RNA-Seq Data Acquisition of PCa\n",
            "\u001b[35mSection_title: 4.1.2.  Significantly Differential Gene Expression Analysis\n",
            "\u001b[35mSection_title: 4.1.3.  COX Proportional Hazards Regression Analysis\n",
            "\u001b[35mSection_title: 4.1.4.  Pearson Correlation Coefficient between Differentially Expressed lncRNAs and mRNAs in PCa\n",
            "\u001b[35mSection_title: 4.1.5.  Acquisition of the lncRNA Sequences and Promoter Sequences of Genes\n",
            "\u001b[35mSection_title: 4.1.6.  In Silico Prediction of RNA-DNA Triplexes\n",
            "\u001b[35mSection_title: 4.2.  Experimental Validation\n",
            "\u001b[35mSection_title: 4.2.1.  In Vitro Transcription\n",
            "\u001b[35mSection_title: 4.2.2.  Electrophoretic Mobility Shift Assay (EMSA)\n",
            "\u001b[35mSection_title: 4.2.3.  Cell Culture\n",
            "\u001b[35mSection_title: 4.2.4.  RNA Interference and Transfection\n",
            "\u001b[35mSection_title: 4.2.5.  Cell Proliferation Assay\n",
            "\u001b[35mSection_title: 4.2.6.  Annexin V-FITC Apoptosis Assay\n",
            "\u001b[35mSection_title: 4.2.7.  Transwell Migration Assay\n",
            "\u001b[35mSection_title: 4.2.8.  Wound Healing Assay\n",
            "\u001b[35mSection_title: 4.2.9.  Statistical Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Datasets\n",
            "\u001b[35mSection_title: 2.2  Language Modeling\n",
            "\u001b[35mSection_title: 3  Experimental Results\n",
            "\u001b[35mSection_title: 3.1  Automatic Metrics\n",
            "\u001b[35mSection_title: 3.2  In-Domain Analysis\n",
            "\u001b[35mSection_title: 3.3  Out-of-Domain Analysis\n",
            "\u001b[35mSection_title: 3.4  Human Evaluation\n",
            "\u001b[35mSection_title: 3.5  Qualitative Analysis\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we present mixqg, a question generation model pre-trained on a collection of qa datasets with a mix of answer types. we show through experiments that the resulting model is a strong starting point for further fine-tuning which achieves state-of-the-art results on target datasets in commonly-used similarity metrics as well as our designed human evaluation. we release our code and the model checkpoints to facilitate qg research and downstream applications.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 6  Ethical Considerations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  The DCQA dataset\n",
            "\u001b[35mSection_title: 3.1  Annotation paradigm\n",
            "\u001b[35mSection_title: 3.2  Annotators and corpus\n",
            "\u001b[35mSection_title: 4  Analyzing DCQA\n",
            "\u001b[35mSection_title: 5  Question answering on DCQA\n",
            "\u001b[35mSection_title: 6  Question answering on external datasets\n",
            "\u001b[35mSection_title: 6.1  INQUISITIVE answer collection\n",
            "\u001b[35mSection_title: 6.2  Answering answerable questions\n",
            "\u001b[35mSection_title: 6.3  Handling question answerability\n",
            "\u001b[32mconclusion: we present dcqa that connects pieces in a document via open-ended questions and full-sentence answers. dcqa is collected via a new paradigm that regards the main purpose of a new sentence as an answer to a free-form question evoked earlier in the context. consequently, this paradigm yields both discourse and semantic links across all sentences in a document. dcqa is introduced with the goal of providing a more scalable data collection paradigm, also as initial resource, for answering open-ended questions for discourse comprehension. our experiments showed that dcqa provides valuable supervision for such tasks.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Model-Written Evaluations\n",
            "\u001b[35mSection_title: 3  Evaluating Persona\n",
            "\u001b[35mSection_title: 3.1  Experimental Setup\n",
            "\u001b[35mSection_title: 3.2  Qualitative Evaluation of Generated Data\n",
            "\u001b[35mSection_title: 3.3  Data Quality: Quantitative Analysis\n",
            "\u001b[35mSection_title: 3.4  Data Diversity\n",
            "\u001b[35mSection_title: 3.5  Model Evaluation Results\n",
            "\u001b[35mSection_title: 4  Evaluating Sycophancy\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Model Evaluation Results\n",
            "\u001b[35mSection_title: 5  Evaluating Advanced AI Risks with Few-shot Multiple Choice Generation\n",
            "\u001b[35mSection_title: 5.1  Behaviors Tested\n",
            "\u001b[35mSection_title: 5.2  Dataset Generation Procedure\n",
            "\u001b[35mSection_title: 5.3  Data Quality Analysis\n",
            "\u001b[35mSection_title: 5.4  Model Evaluation Results\n",
            "\u001b[35mSection_title: 6  Evaluating Gender Bias with\n",
            "\u001b[32mlimitation: model capabilities we observed that current lms struggle to generate several categories of examples. lms struggled to generate examples related to concepts they do not understand well (e.g. cryptography and steganography). as discussed in §6, we also found that lms struggled to generate examples with many constraints, in particular, those in the bbq dataset (parrish et al., 2022). we expect these limitations to wane as lms grow more capable with scale. lastly, many evaluations related to lm capabilities require the dataset creator to know how to solve the evaluation. we expect that lms will not be able to generate high-quality evaluations of this kind (e.g., to test for factual knowledge they do not yet know). our approach is thus differentially useful for evaluating other properties of models aside from capabilities (e.g., safety-related behaviors).model biases lms learn biases from their training data (sheng et al., 2019;gehman et al., 2020;brown et al., 2020), impacting the generator p g and discriminator p d . for example, generated evaluations may exhibit gender or racial biases and be lower quality for languages under-represented in the lm training data. lms will also be systematically worse at generating evaluations for tasks that are omitted from their training data (e.g., due to copyright, licensing, or privacy issues).example diversity we found limited example diversity for some kinds of evaluations ( §3.4) though not all ( §5). diversity appears to depends on the kind of evaluation generated, the generation hyperparameters, and the prompt used, and thus sometimes requires e.g. hyperparameter tuning to get right.we found data visualizations to be powerful tools for understanding and debugging data diversity, such as those at evals.anthropic.com/model-written/. qualitatively, we also found that using p d to rank/filter examples limited the diversity, since p d sometimes selected for prototypical examples for testing some behavior (observed qualitatively by workers in §3.2). we are excited for future work to explore other methods that achieve similar example quality with higher diversity than our method, such as generating many examples and subsampling for diversity.instructions may be misunderstood lms, similar to crowdworkers, may generate evaluations that are testing something different than intended, especially if the generation instructions are underspecified. for example, using the method in §3.1, we generated statements that a person who \"shares beliefs with derek parfit\" (the influential analytic philosopher) would agree or disagree with. the \"disagree\" statements were often ones that many people, not just derek parfit, would disagree with (\"i support slavery\" or \"i believe evolution never happend\"). in this case, we should have provided more specific instructions to the lm, to have it generate examples that derek parfit would disagree with but that another philosopher would agree with. when feasible, we recommend briefly examining the generated data, to catch salient issues such as the above.sensitivity to instructions our approach allows the dataset developer fairly fine-grained control over the evaluation by using instructions to guide p g . however, the quality of lm outputs is sensitive to text inputs in unintuitive ways (perez et al., 2021;lu et al., 2022), adding hard-to-predict variance to the quality of the resulting evaluation; see appendix §a.4 for a possible example of this effect we found. we hope that lm advances such as instruction-tuning (wei et al., 2021;sanh et al., 2022;ouyang et al., 2022) mitigate this issue in the future. for now, it may be possible to use prompt sensitivity to generate more diverse datasets, by generating similar datasets with distinct prompts and combining the results, as we did in §3. where prompt sensitivity caused issues, we found it helpful to be able to view example generated outputs in seconds, to quickly iterate and catch salient failures. for the 133 datasets in §3, we found a general instruction template that worked well; we did not do dataset-specific tuning to obtain samples rated as high-quality by human evaluators.\n",
            "\u001b[35mSection_title: 8  Limitations & Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Question Answering Systems\n",
            "\u001b[35mSection_title: 2.1  Question Answering using Knowledge Bases\n",
            "\u001b[35mSection_title: 2.1.2  Benchmarks\n",
            "\u001b[35mSection_title: 2.1.3  Methods for QA\n",
            "\u001b[35mSection_title: 3.  KG embedding based methods\n",
            "\u001b[35mSection_title: 2.1.3.2  Methods for Query Construction for Question Answering over Knowledge Graphs\n",
            "\u001b[35mSection_title: 2.1.3.3  Embedding Based Methods for Question Answering over Knowledge Graphs\n",
            "\u001b[35mSection_title: 2.2  Question Answering using Text\n",
            "\u001b[35mSection_title: 2.  Reader:\n",
            "\u001b[35mSection_title: 2.2.1  Benchmarks\n",
            "\u001b[35mSection_title: 2.2.2  Systems\n",
            "\u001b[35mSection_title: 2.2.3  Retriever Methods for Factoid ODQA\n",
            "\u001b[35mSection_title: 2.2.4  Reader Methods in ODQA\n",
            "\u001b[35mSection_title: 2.2.4.1  Neural Models for Predicting Factoid Answers\n",
            "\u001b[35mSection_title: 2.2.4.2  Information Retrieval Based Methods\n",
            "\u001b[35mSection_title: 2.2.4.3  Quasi Knowledge Graph Based Methods\n",
            "\u001b[35mSection_title: 2.3  Question Answering over Heterogenous Sources\n",
            "\u001b[35mSection_title: 3  Biomedical Question Answering Overview\n",
            "\u001b[35mSection_title: 3.1  Difference with general domain QA\n",
            "\u001b[35mSection_title: 3.2  Biomedical Question Answering Benchmark Datasets\n",
            "\u001b[35mSection_title: 3.2.1  Classification by Task\n",
            "\u001b[35mSection_title: 3.2.2  Classification by Answer Type\n",
            "\u001b[35mSection_title: 3.2.3  Classification by Sub-Domain\n",
            "\u001b[35mSection_title: 3.3  Some Important BQA Datasets\n",
            "\u001b[35mSection_title: 3.3.1  BioASQ Task B\n",
            "\u001b[35mSection_title: 3.3.2  PubMedQA\n",
            "\u001b[35mSection_title: 3.3.3  Cloze Type QA datasets\n",
            "\u001b[35mSection_title: 3.3.4  Re-purposing Medical Examination Questions\n",
            "\u001b[35mSection_title: 3.3.5  Other Datasets\n",
            "\u001b[35mSection_title: 3.4  Methodologies\n",
            "\u001b[35mSection_title: 2.  Biomedical Question Answering using Text\n",
            "\u001b[35mSection_title: 4  Biomedical Question Answering using Structured Knowledge\n",
            "\u001b[35mSection_title: 4.1  Biomedical Structured Knowledge Resources\n",
            "\u001b[35mSection_title: 4.2  Methods\n",
            "\u001b[35mSection_title: 5  Biomedical Question Answering using Text\n",
            "\u001b[35mSection_title: 5.1  Methods for Retrieval and Answering Summary Type Questions\n",
            "\u001b[35mSection_title: 1.  Classical Lexical or Rule Based Methods\n",
            "\u001b[35mSection_title: 2.  Neural Methods\n",
            "\u001b[35mSection_title: 5.1.1  Classical (Lexical or Rule Based) Methods\n",
            "\u001b[35mSection_title: 5.1.2  Neural Methods\n",
            "\u001b[35mSection_title: 5.2  Methods for Machine Reading\n",
            "\u001b[35mSection_title: 2.  Neural Methods\n",
            "\u001b[35mSection_title: 5.2.1  Non Neural Methods\n",
            "\u001b[35mSection_title: 5.2.2  Neural Methods\n",
            "\u001b[35mSection_title: 6.1.2  Limited use of Knowledge Bases\n",
            "\u001b[35mSection_title: 6.1.3  Lack of Explainability and Interpretability\n",
            "\u001b[35mSection_title: 6.1.4  Fairness and Bias\n",
            "\u001b[35mSection_title: 6.1.5  Focus on Machine Comprehension\n",
            "\u001b[35mSection_title: 6.1.6  Weakness of Evaluation Techniques\n",
            "\u001b[35mSection_title: 6.2  Future Directions\n",
            "\u001b[35mSection_title: 6.2.1  Integration of Structured Knowledge and Text\n",
            "\u001b[35mSection_title: 6.2.2  More focus on End-to-End QA Pipelines\n",
            "\u001b[35mSection_title: 6.2.3  Transfer Learning, MultiTask Learning and Meta Learning\n",
            "\u001b[35mSection_title: 6.2.4  Explanation and Interpretation of Answers\n",
            "\u001b[35mSection_title: 6.2.5  More Datasets\n",
            "\u001b[35mSection_title: 6.2.6  Conversational Systems\n",
            "\u001b[35mSection_title: 6.2.7  Inference Based Systems\n",
            "\u001b[35mSection_title: 6.2.8  Regulation\n",
            "\u001b[32mconclusion: in this review, we focused on the recent advances in biomedical question answering. we provided a review of general domain question answering using knowledge bases, texts or both, before moving on to biomedical question answering systems. we explored current state of the art biomedical qa systems and discussed their limitations. we finally provided an overall analysis of limitations of bqa systems along with ways to overcome them. we finally explore the potential areas of focus for further research.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Preliminaries\n",
            "\u001b[35mSection_title: 4  LinkBERT\n",
            "\u001b[35mSection_title: 4.1  Document graph\n",
            "\u001b[35mSection_title: 4.2  Pretraining tasks\n",
            "\u001b[35mSection_title: 4.3  Strategy to obtain linked documents\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Pretraining setup\n",
            "\u001b[35mSection_title: 5.2  Evaluation tasks\n",
            "\u001b[35mSection_title: 5.3  Results\n",
            "\u001b[35mSection_title: 5.4  Analysis\n",
            "\u001b[35mSection_title: 5.5  Ablation studies\n",
            "\u001b[35mSection_title: 6  Biomedical LinkBERT (BioLinkBERT)\n",
            "\u001b[35mSection_title: 6.1  Pretraining setup\n",
            "\u001b[35mSection_title: 6.2  Evaluation tasks\n",
            "\u001b[35mSection_title: 6.3  Results\n",
            "\u001b[32mconclusion: we presented linkbert, a new language model (lm) pretraining method that incorporates document link knowledge such as hyperlinks. in both the general domain (pretrained on wikipedia with hyperlinks) and biomedical domain (pretrained on pubmed with citation links), linkbert outperforms previous bert models across a wide range of downstream tasks. the gains are notably large for multi-hop reasoning, multi-document understanding and few-shot question answering, suggesting that linkbert effectively internalizes salient knowledge through document links. our results suggest that linkbert can be a strong pretrained lm to be applied to various knowledge-intensive tasks. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Single-hop QA\n",
            "\u001b[35mSection_title: 3.3  Multi-hop QA with Conservation Learning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Dataset and Metrics\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Main Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Studies\n",
            "\u001b[35mSection_title: 4.5  Evaluation across Reasoning Types\n",
            "\u001b[35mSection_title: 4.6  Evaluation of Robustness\n",
            "\u001b[35mSection_title: 4.7  Case Study\n",
            "\u001b[32mconclusion: in this paper, we introduce a novel prompt-based conservation learning framework for multi-hop qa -a framework that retains knowledge from previous component tasks -able to answer questions in a principled way that matches human expectations by answering sub-questions and integrating the answers. by developing soft prompts related to reasoning types during training, we also show that we can condition plms to stimulate and apply the reasoning knowledge required for specific multihop questions. experimental results on multiple multi-hop qa datasets demonstrate the improved performance of pcl over previous multi-hop qa models in multi-hop qa.\n",
            "\u001b[35mSection_title: 5  Conclusions and Future Work\n",
            "\u001b[35mSection_title: 6  Acknowledgments\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 1.1.  Description of the teacher training\n",
            "\u001b[35mSection_title: 2.  The study\n",
            "\u001b[35mSection_title: 2.1.  The research group\n",
            "\u001b[35mSection_title: 2.2.  Research methodology\n",
            "\u001b[35mSection_title: 2.3  Results\n",
            "\u001b[35mSection_title: 2.3.  Results analysis\n",
            "\u001b[32mconclusion: the fact that, after training, polish teachers still have the opinion that curricula are not adjusted to ibse is significant, and informs us that the introduction of ibse into school practice will be a major challenge. on the other hand, the fact that the teachers, after training, reinforced their attitudes that existing curricula can be realized with the use of ibse methods and that carrying out experiments according to the ibse rules does not require more advanced equipment in school laboratories in comparison to the traditional methods, allows for an optimistic look at the future. changes in attitudes are in this case less expensive than changes in external conditions. the stronger agreement that ibse might have a positive influence on the examination results can be the result of a session on new forms of external examinations. the increase in the number of statements saying that the school system and parents are in favor of the traditional/lecturing methods gives teachers some kind of excuse for such rare application of the use of strategy that they found valuable but at the same time very demanding.\n",
            "\u001b[35mSection_title: 3.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  PRELIMINARIES ABOUT NEURAL INFORMATION RETRIEVAL\n",
            "\u001b[35mSection_title: 2.1  Dense Retrieval\n",
            "\u001b[35mSection_title: 2.2  Reranking\n",
            "\u001b[35mSection_title: 3  NIR-PROMPT\n",
            "\u001b[35mSection_title: 3.1  Basic Idea\n",
            "\u001b[35mSection_title: 3.2  Overall Framework\n",
            "\u001b[35mSection_title: 3.3  Matching Description Module\n",
            "\u001b[35mSection_title: 3.3.1  Manual\n",
            "\u001b[35mSection_title: 3.3.2  Continuous\n",
            "\u001b[35mSection_title: 3.3.3  Hybrid Prompt for\n",
            "\u001b[35mSection_title: 3.4  Essential Matching Module\n",
            "\u001b[35mSection_title: 3.5  Training\n",
            "\u001b[35mSection_title: 4  EXPERIMENTS\n",
            "\u001b[35mSection_title: 4.1  Experimental Se ings\n",
            "\u001b[35mSection_title: 4.1.1  Research estions.\n",
            "\u001b[35mSection_title: 4.1.3  Implementation Details.\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.2.2  Out-of-domain\n",
            "\u001b[35mSection_title: 4.2.3  New Task\n",
            "\u001b[35mSection_title: 4.2.4  Performance on BEIR.\n",
            "\u001b[35mSection_title: 4.3  Model Analysis\n",
            "\u001b[35mSection_title: 4.3.2  Relationships between\n",
            "\u001b[35mSection_title: 4.3.5  Affect of Arrangement of Samples.\n",
            "\u001b[35mSection_title: 5  RELATED WORK\n",
            "\u001b[35mSection_title: 5.1  Neural Information Retrieval Based on Text Matching\n",
            "\u001b[35mSection_title: 5.1.1  Neural Retrieval\n",
            "\u001b[35mSection_title: 5.1.2  Neural Reranking\n",
            "\u001b[35mSection_title: 5.2  Multi-Task Learning for Natural Language Processing Tasks\n",
            "\u001b[35mSection_title: 5.2.1  Traditional\n",
            "\u001b[35mSection_title: 5.2.2  Multi-task Prompt\n",
            "\u001b[35mSection_title: 5.3  Prompt Engineering\n",
            "\u001b[35mSection_title: 5.4  Out-of-domain Generalization of Neural Information Retrieval\n",
            "\u001b[32mconclusion: in this paper, we point out that although there are some differences among the various information retrieval tasks, there are still essential matching signals shared by the various tasks, such as exact matching, semantic matching, and inference matching.if the model can capture and exploit these signals, the generalization ability of the model across tasks and domains will be improved.with this intuition, we propose a neural information retrieval training framework called nir-prompt consisting of essential matching module (emm) and matching description module (mdm) based on the idea of decoupling the process of signal capturing and signal combination.mdm uses the method of prompt learning to obtain the description of different tasks in the pre-trained language model.emm is trained on diverse mixed datasets and combined with the guidance from the task descriptions in mdm to capture essential matching signals and adapt these signals to different tasks.based on this, a generalized neural information retrieval pipeline consisting of retrieval and reranking is constructed.the experimental results on eighteen public datasets and a heterogeneous benchmark for testing the generalization ability of retrieval models show that our method yields better in-domain multi-task, out-of-domain multi-task, and new task adaptation performance for dense retrieval, reranking, and the entire neural information retrieval pipeline compared to the traditional fine-tuning paradigm.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background: BERT\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Span Masking\n",
            "\u001b[35mSection_title: 3.2  Span Boundary Objective\n",
            "\u001b[35mSection_title: 3.3  Single-Sequence Training\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Tasks\n",
            "\u001b[35mSection_title: 4.2  Implementation\n",
            "\u001b[35mSection_title: 4.3  Baselines\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Per-Task Results\n",
            "\u001b[35mSection_title: 5.2  Overall Trends\n",
            "\u001b[35mSection_title: 6  Ablation Studies\n",
            "\u001b[35mSection_title: 6.1  Masking Schemes\n",
            "\u001b[35mSection_title: 6.2  Auxiliary Objectives\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: we presented a new method for span-based pretraining which extends bert by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. together, our pretraining process yields models that outperform all bert baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  LONG CONTEXT LARGE LANGUAGE MODELS\n",
            "\u001b[35mSection_title: 2.2  EFFICIENT ATTENTION METHODS\n",
            "\u001b[35mSection_title: 2.3  RETRIEVAL-AUGMENTED LANGUAGE MODELS\n",
            "\u001b[35mSection_title: 2.4  CONCURRENT WORK\n",
            "\u001b[35mSection_title: 3  EXPERIMENTAL SETUP\n",
            "\u001b[35mSection_title: 3.1  LARGE LANGUAGE MODELS\n",
            "\u001b[35mSection_title: 3.2  DATASETS AND METRICS\n",
            "\u001b[35mSection_title: 3.3  CONTEXT WINDOW EXTENSION\n",
            "\u001b[35mSection_title: 3.4  RETRIEVAL\n",
            "\u001b[35mSection_title: 3.5  INSTRUCTION TUNING\n",
            "\u001b[35mSection_title: 4  RESULTS\n",
            "\u001b[35mSection_title: 4.1  MAIN RESULTS\n",
            "\u001b[35mSection_title: 4.2  COMPARING TO OPENAI MODELS\n",
            "\u001b[35mSection_title: 4.3  ABLATION ON DIFFERENT RETRIEVERS\n",
            "\u001b[35mSection_title: 4.4  INCREASING THE NUMBER OF RETRIEVED CHUNKS\n",
            "\u001b[32mconclusion: in this work, we systematically study the retrieval-augmentation versus long context extension using the state-of-the-art llms after instruction tuning for various long context qa and querybased summarization tasks.after study, we have the following interesting findings: i) retrieval largely boosts the performance of both 4k short context llm and 16k/32k long context llms.\n",
            "\u001b[35mSection_title: 5  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Evaluation Data Construction\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Perturbed Data Creation\n",
            "\u001b[35mSection_title: 3.3  Iterative Rationale Annotation Process\n",
            "\u001b[35mSection_title: 3.4  Data Statistics\n",
            "\u001b[35mSection_title: 4  Metrics\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experiment Settings\n",
            "\u001b[35mSection_title: 5.2  Evaluation Results\n",
            "\u001b[32mlimitation: we provide a new interpretability evaluation benchmark which contains three tasks with both english and chinese annotated data. there are three limitations in our work.• how to evaluate the quality of human-annotated rationales is still open. we have several annotators to perform quality control based on human intuitions and experiences. meanwhile, we compare model behaviors on full inputs and humanannotated rationales to evaluate the sufficiency and comprehensiveness of rationales, as shown in table 4 and table 7. however, this manner has damaged the original input distribution and brings uncontrollable factors on model behaviors. therefore, how to automatically and effectively evaluate the quality of human-annotated rationales should be studied in the future.• we find that the interpretability of model architectures and saliency methods vary with tasks, especially with the input form of the task. thus our benchmark should contain more datasets of each task type ( e.g., single-sentence task, sentencepair similarity task and sentence-pair inference task) to further verify these findings. and we will build evaluation datasets for more tasks in the future.• due to space limitation, there is no analysis of the relationships between metrics, e.g., the relationship between plausibility and accuracy, and the relationship between faithfulness and robustness. we will take these analyses in our future work.finally, we hope more evaluation metrics and analyses are proposed based on our benchmark. and we hope our benchmark can facilitate the research progress of interpertability.\n",
            "\u001b[35mSection_title: 6  Limitation Discussion\n",
            "\u001b[32mconclusion: we propose a new fine-grained interpretability evaluation benchmark, containing token-level rationales, a new evaluation metric and corresponding perturbed examples for three typical nlp tasks, i.e., sentiment analysis, textual similarity and machine reading comprehension. the rationales in this benchmark meet primary properties that a rationale should satisfy, i.e., sufficiency, compactness and comprehensiveness. the experimental results on three models and three saliency methods prove that our benchmark can be used to evaluate interpretability of both models and saliency methods. we will release this benchmark and hope it can facilitate progress on several directions, such as better interpretability evaluation metrics and causal analysis of nlp models. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Open-domain Question Answering\n",
            "\u001b[35mSection_title: 2.2  Counterfactual Thinking and Causality\n",
            "\u001b[35mSection_title: 3.1  Dataset Collection\n",
            "\u001b[35mSection_title: 3.1.1  Question and Answer Annotation\n",
            "\u001b[35mSection_title: 3.1.2  Question and Answer Verification\n",
            "\u001b[35mSection_title: 3.1.3  Answer Post-processing\n",
            "\u001b[35mSection_title: 3.2  Dataset Analysis\n",
            "\u001b[35mSection_title: 3.3  Dataset Splits\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Retrieval Corpus\n",
            "\u001b[35mSection_title: 4.2  Comparison Systems\n",
            "\u001b[35mSection_title: 4.4  Results and Discussion\n",
            "\u001b[35mSection_title: 4.5  Case Study\n",
            "\u001b[32mlimitation: the main limitation of ifqa dataset is that it only covers event-based questions, due to the nature of creating counterfactual presuppositions. therefore, our dataset is not intended for training general opendomain qa models or evaluate their capabilities.for data collection, we relied heavily on human annotators, both for question annotation and verification. despite our efforts to mitigate annotator bias by providing explicit instructions and examples and by sampling annotators from diverse populations, it is not possible to completely remove this bias. in addition, we use heuristic rules to select only a small portion of wikipedia passages and then present them to human annotators (as mentioned in section 3.1.1), which might lead to pattern-oriented bias in the annotated data.for evaluated models, large language models performance on our dataset may preserve biases learned from the web text during pre-training or and make biased judgments as a result.\n",
            "\u001b[35mSection_title: 6  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Bi-partite structure of scientific theories\n",
            "\u001b[35mSection_title: 2.  Integers and their Kolmogorov complexity\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Problem Formulation\n",
            "\u001b[35mSection_title: 3.2  Method Overview\n",
            "\u001b[35mSection_title: 3.3  Pre-trained Language Model based MRC Model\n",
            "\u001b[35mSection_title: 3.4  Transformer-based Autoencoder\n",
            "\u001b[35mSection_title: 3.5  Rewriting Question with Gradient-based Optimization\n",
            "\u001b[35mSection_title: 4.1  SQuAD\n",
            "\u001b[35mSection_title: 4.2  Ablation and Analysis\n",
            "\u001b[35mSection_title: 4.3  Question Generation\n",
            "\u001b[35mSection_title: 4.4  QNLI\n",
            "\u001b[32mconclusion: in this work, we present a novel question data augmentation method, called crqda, for contextrelevant answerable and unanswerable question generation. crqda treats the question data augmentation task as a constrained question rewriting problem. under the guidance of a pre-trained mrc model, the original question is revised in a continuous embedding space with gradient-based optimization and then decoded back to the discrete space as a new question data sample. the experimental results demonstrate that crqda outperforms other strong baselines on squad 2.0. the crqda augmented datasets can improve multiple reading comprehension models. furthermore, crqda can be used to improve the model performance on question generation and question-answering language inference tasks, which achieves a new state-of-theart on the squad 1.1 question generation task. yukun zhu, ryan kiros, rich zemel, ruslan salakhutdinov, raquel urtasun, antonio torralba, and sanja fidler. 2015. aligning books and movies: towards story-like visual explanations by watching movies and reading books. in proceedings of the ieee international conference on computer vision, pages 19-27. figure 3 and figure 4 provide some augmented data samples of each baseline on squad 2.0. we can see that the baseline eda tends to introduce noise which destroys the original sentence structure. the baselines of text vae, backtranslation and ae+noised often change some important words of the original question. this can cause the augmented question to miss the original key information and not to able to infer the original answer. in contrast, it can be observed that the generated answerable questions of crqda still maintain the key information for the original answer inference.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Formulation\n",
            "\u001b[35mSection_title: 3  Proposed Model\n",
            "\u001b[35mSection_title: 3.1  Question-Passages Reader\n",
            "\u001b[35mSection_title: 3.1.1  Word Embedding Layer\n",
            "\u001b[35mSection_title: 3.1.2  Shared Encoder Layer\n",
            "\u001b[35mSection_title: 3.1.3  Dual Attention Layer\n",
            "\u001b[35mSection_title: 3.1.4  Modeling Encoder Layer\n",
            "\u001b[35mSection_title: 3.2  Passage Ranker\n",
            "\u001b[35mSection_title: 3.3  Answer Possibility Classifier\n",
            "\u001b[35mSection_title: 3.4  Answer Sentence Decoder\n",
            "\u001b[35mSection_title: 3.4.1  Word Embedding Layer\n",
            "\u001b[35mSection_title: 3.4.2  Attentional Decoder Layer\n",
            "\u001b[35mSection_title: 3.4.3  Multi-source Pointer-Generator\n",
            "\u001b[35mSection_title: 3.4.4  Combined Attention\n",
            "\u001b[35mSection_title: 3.5  Loss Function\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 5  Experiments on NarrativeQA\n",
            "\u001b[35mSection_title: 5.1  Setup\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 6  Related Work and Discussion\n",
            "\u001b[32mconclusion: this study sheds light on multi-style generative rc. our proposed model, masque, is based on multi-source abstractive summarization and learns multi-style answers together. it achieved stateof-the-art performance on the q&a task and the q&a + nlg task of ms marco 2.1 and the summary task of narrativeqa. the key to its success is transferring the style-independent nlg capability to the target style by use of the question-passages reader and the conditional pointer-generator decoder. in particular, the capability of copying words from the question and passages can be shared among the styles, while the capability of controlling the mixture weights for the generative and copy distributions can be acquired for each style. our future work will involve exploring the potential of our multi-style learning towards natural language understanding.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Experiments on TREC and BEIR\n",
            "\u001b[35mSection_title: 4  Demonstrations for Ranking\n",
            "\u001b[35mSection_title: 4.1  The Impact of Demonstrations\n",
            "\u001b[35mSection_title: 4.2  Difficulty-based Selection (DBS)\n",
            "\u001b[35mSection_title: 4.2.1  Difficult Demonstrations are Beneficial\n",
            "\u001b[35mSection_title: 5  Main Results and Discussion\n",
            "\u001b[35mSection_title: 5.1  DBS for TREC and BEIR\n",
            "\u001b[35mSection_title: 5.2  Comparison to Random Selection\n",
            "\u001b[35mSection_title: 5.3  DBS with Conditional DQL (CDQL)\n",
            "\u001b[35mSection_title: 5.4  DBS for Question Generation\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work we present passage ranking with demonstrations (parade), an extensive study on the topic of using demonstrations to improve reranking performance of llms.we show the challenges of applying demonstrations effectively, and that performance heavily relies on selecting \"good\" demonstrations.we propose a simple yet effective selection method, named difficulty-based selection (dbs), and confirm its effectiveness in both reranking using query likelihood scoring and query generation tasks.for future work, we plan to combine difficulty-based selection with similaritybased selection as an effort to further improve the robustness and effectiveness of the selected demonstrations, and extend dbs to other ranking paradigms (qin et al., 2023;sun et al., 2023).\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Setting\n",
            "\u001b[35mSection_title: 4  Method\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 6  Experimental Result\n",
            "\u001b[35mSection_title: 6.1  Second-Stage Reranking\n",
            "\u001b[35mSection_title: 6.2  First-Stage Retrieval\n",
            "\u001b[32mconclusion: in this work, we introduce neural pg-rank, a novel training algorithm designed to address challenges associated with training llm-based retrieval models.as a rigorous approach that reduces the dependence on intricate heuristics and directly optimizes relevant ranking metrics, neural pg-rank has demonstrated its effectiveness when training objective aligns with evaluation setup -specifically, in the context of second-stage reranking -by exhibiting remarkable in-domain performance improvement and presenting subtantial out-of-domain generalization to some critical datasets employed in downstream question answering.our work establishes a principled bridge between training objectives and practical utility of the collective set of retrieved results, thereby paving the way for future research endeavors aimed at constructing highly effective retrieval-based llm pipelines that are tailored for practical applications.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Definition\n",
            "\u001b[35mSection_title: 2.2  Module\n",
            "\u001b[35mSection_title: 2.2.1  Atomic and Compound Modules\n",
            "\u001b[35mSection_title: 2.2.2  Module Arithmetic\n",
            "\u001b[35mSection_title: 2.3  Task Decomposition and Module Composition\n",
            "\u001b[35mSection_title: 3  REMOP: Retrieval with Modular Prompt Tuning\n",
            "\u001b[35mSection_title: 3.1  Basic Architecture\n",
            "\u001b[35mSection_title: 3.2  Separating Specificity from Commonness\n",
            "\u001b[35mSection_title: 3.3  Instruction Decomposition\n",
            "\u001b[35mSection_title: 3.4  Modular Prompt Composition\n",
            "\u001b[35mSection_title: 3.5  Modular Prompt Training\n",
            "\u001b[35mSection_title: 4  Exploration on Modularity\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Module Scaling\n",
            "\u001b[35mSection_title: 4.3  Module Addition\n",
            "\u001b[35mSection_title: 4.4  Module Subtraction\n",
            "\u001b[35mSection_title: 4.5  Module Arithmetic Combination\n",
            "\u001b[35mSection_title: 5  Generalizing to New Tasks: Zero-shot Retrieval\n",
            "\u001b[35mSection_title: 5.1  Experimental Settings\n",
            "\u001b[35mSection_title: 5.2  Experimental Results\n",
            "\u001b[35mSection_title: 5.3  Analysis\n",
            "\u001b[35mSection_title: 6  Related Works on Modularity\n",
            "\u001b[32mconclusion: this paper formally defines a new information retrieval paradigm, modular retrieval, which aims to bring the benefits of modular learning to information retrieval tasks and increase the interpretability of the retrieval process. based on the proposed retrieval paradigm, we propose a novel modular retrieval method remop, which utilizes three modular operations to flexibly combine retrieval modules to perform zero-shot retrieval tasks, with high interpretability and generalization ability. we explore and verify the effectiveness of the proposed module arithmetic, and experiments on a zero-shot retrieval benchmark show that remop has comparable performance to the fine-tuned model in zero-shot retrieval.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Problem Formulation\n",
            "\u001b[35mSection_title: 3.1  Named Entity Recognition\n",
            "\u001b[35mSection_title: 3.2  Few-shot Named Entity Recognition\n",
            "\u001b[35mSection_title: 4.1  Schema of Entity Types\n",
            "\u001b[35mSection_title: 4.2  Paragraph Selection\n",
            "\u001b[35mSection_title: 4.3  Human Annotation\n",
            "\u001b[35mSection_title: 5  Data Analysis\n",
            "\u001b[35mSection_title: 5.1  Size and Distribution of FEW-NERD\n",
            "\u001b[35mSection_title: 5.2  Knowledge Correlations among Types\n",
            "\u001b[35mSection_title: 6  Benchmark Settings\n",
            "\u001b[35mSection_title: 6.1  Standard Supervised NER\n",
            "\u001b[35mSection_title: 7  Experiments\n",
            "\u001b[35mSection_title: 7.1  Models\n",
            "\u001b[35mSection_title: 7.2  The Overall Results\n",
            "\u001b[35mSection_title: 7.3  Error Analysis\n",
            "\u001b[32mconclusion: we propose few-nerd, a large-scale few-shot ner dataset with fine-grained entity types. this is the first few-shot ner dataset and also one of the largest human-annotated ner dataset. few-nerd provides three unified benchmarks to assess approaches of few-shot ner and could facilitate future research in this area. by implementing state-of-the-art methods, we carry out a series of experiments on few-nerd, demonstrating that few-shot ner remains a challenging problem and worth exploring. in the future, we will extend few-nerd by adding cross-domain annotations, distant annotations, and finer-grained entity types. few-nerd also has the potential to advance the construction of continual knowledge graphs.\n",
            "\u001b[35mSection_title: 8  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Constructing Multilingual Parallel Corpora\n",
            "\u001b[35mSection_title: 3.2  Multilingual Modeling\n",
            "\u001b[35mSection_title: 3.2.1  Multilingual Encoder. Bidirectional Encoder Representations\n",
            "\u001b[35mSection_title: 3.2.2  Multilingual\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Comparisons Between XLRC And Baselines\n",
            "\u001b[35mSection_title: 5.2  XLRC Pre-trained on SQuAD\n",
            "\u001b[35mSection_title: 5.3  XLRC Pre-trained on DRCD\n",
            "\u001b[35mSection_title: 5.4  XLRC Pre-trained on Multiple Datasets\n",
            "\u001b[35mSection_title: 5.5  Comparisons between XLRC and SOTA\n",
            "\u001b[32mconclusion: despite tremendous advances have been made in the field of spanextraction reading comprehension in recent years, large-scale highquality extractive qa datasets in languages other than english remain scarce, and collecting such a sufficient amount of training data for each language is costly, and even impossible, making training reading comprehension systems in other languages challenging. to make full use of all existing extractive training datasets in various languages and to learn the rich hidden semantic knowledge from different language families, we propose xlrc, a multilingual extractive reading comprehension method, to simultaneously model the existing extractive reading comprehension training data in a multilingual environment by using multilingual bert and multilingual attention. experimental results demonstrate the effectiveness of our proposed multilingual modelling by transferring the semantic knowledge learned from various existing datasets in different languages.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Taxonomy of Disagreement\n",
            "\u001b[35mSection_title: 3.1  Task Conceptualization\n",
            "\u001b[35mSection_title: 3.2  Perceptions of Benchmark Validity\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Task Conceptualization\n",
            "\u001b[35mSection_title: 5.2  Perceptions of Benchmark Validity\n",
            "\u001b[35mSection_title: 5.3  Progress\n",
            "\u001b[35mSection_title: 6  A Framework for NLP Benchmarks\n",
            "\u001b[32mconclusion: we develop a taxonomy of disagreement (based on measurement modeling) which distinguishes between how tasks are conceptualized and how measurements of model performance are operationalized. to provide evidence for our taxonomy, we conduct a survey of practitioners and meta-analysis of relevant literature. based on our taxonomy, we propose a framework for the creation of benchmarks and the documentation of their limitations. future work includes studying task conceptualization via benchmark inter-annotator disagreement.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Description\n",
            "\u001b[35mSection_title: 3  Methods\n",
            "\u001b[35mSection_title: 3.1  Step Models\n",
            "\u001b[35mSection_title: 3.2  Search\n",
            "\u001b[35mSection_title: 3.3  Premise Recovery Scoring\n",
            "\u001b[35mSection_title: 3.4  Re-Ranking Proofs\n",
            "\u001b[35mSection_title: 4  Everyday Norms: Why Not?\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Implementation Details\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Coverage Results\n",
            "\u001b[35mSection_title: 6.2  Human Step Validity Evaluation\n",
            "\u001b[35mSection_title: 6.3  Human Coverage Evaluation\n",
            "\u001b[35mSection_title: 6.4  Error Analysis: ADGV\n",
            "\u001b[35mSection_title: 7  Related work\n",
            "\u001b[32mconclusion: in this work, we tackle the generation of missing premise statements in textual reasoning through the use of abduction. we introduce a new system capable of abductive and deductive step generation, which yields inferred missing premises while building a proof showing its reasoning. furthermore, we propose a novel validation method that reduces hallucination and other common failure modes in end-to-end and stepwise searches. future work can improve our system by scaling up the models used, plus using additional notions of validation as discussed in the error analysis. we believe our overall framework can be a promising foundation for future reasoning systems.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\u001b[32mlimitation: end-to-end models are able to produce a single generation per example reducing the time complexity for sufficiently small sets of premises.step-by-step models like our search procedure in this work are capable of handling sets of any size of premises for the search, but do increase the execution time per example, especially when using validators that require doing generation themselves. nevertheless, validators do reduce the total time required for running a set of examples due to their ability of pruning the search space and thus removing numerous heuristic and generation calls. with better heuristics and validators it may be possible to reduce the time complexity further, but that is left for future work.both the entailmentbank and enwn dataset were written in english and capture relatively limited domains of textual reasoning. different languages might introduce easier lexical patterns for abstraction though and could be a promising path forward. we believe adgv and its variants should work on non-english languages, but testing this was left to future work.enwn draws on everyday ethical scenarios because this was a domain we found fruitful to exhibit the kind of reasoning our system can do. however, we do not follow in the steps of delphi (jiang et al., 2021) in making any claims about its ability to make systems ethical or say anything about \"values\" encoded in pre-trained models. we do not support its use as part of any user-facing system at this time.step signaturestep typex, x → y d deductive x, y d → y d deductive g, x → ya abductive g, y d → ya abductive ya, x → ya abductive ya, y d → ya abductive table 6: a list of possible input statement types each step model can take. x refers to a premise, y d refers to an intermediate deductive conclusion, g refers to the goal, and y a refers to an abductive hypothesis. note that the deductive model can accept inputs in any order but the abductive model cannot, as the abduction operation is not commutative. also note that deductive outputs can be used as inputs to abductive steps, but not the other way around; allowing deductive steps to accept abductive generations could result in vacuous proofs.\n",
            "\u001b[35mSection_title: 9  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Related Work\n",
            "\u001b[35mSection_title: 3.  Methodology\n",
            "\u001b[35mSection_title: 3.1.  Context Understanding\n",
            "\u001b[35mSection_title: 3.2.  Question Generation\n",
            "\u001b[35mSection_title: 4.  Results\n",
            "\u001b[35mSection_title: 4.1.  Dataset and Metrics\n",
            "\u001b[35mSection_title: 4.2.  Baselines and Ablation Settings\n",
            "\u001b[35mSection_title: 4.3.  Comparison with Baselines\n",
            "\u001b[35mSection_title: 4.4.  Ablation Tests Analysis\n",
            "\u001b[35mSection_title: 4.5.  Case Study\n",
            "\u001b[32mconclusion: the present study proposes a novel approach to question generation, which leverages the benefits of a knowledge graph and mimics human reasoning. the proposed model, referred to as kgel, incorporates an answer-aware graph reasoning module to improve the ability to identify key information from the context. empirical evaluations on the hotpotqa dataset demonstrate the superiority of kgel over baseline models, particularly in terms of the completeness and answerability of generated questions.\n",
            "\u001b[35mSection_title: 5.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 3  METHODOLOGY\n",
            "\u001b[35mSection_title: 3.1  Preliminary of Dense Retrieval\n",
            "\u001b[35mSection_title: 3.2  Web Graph based Document Clustering\n",
            "\u001b[35mSection_title: 3.3  Robustly Unsupervised Dense Retrieval\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL METHODOLOGY\n",
            "\u001b[35mSection_title: 5  EVALUATION RESULTS\n",
            "\u001b[35mSection_title: 5.1  Overall Performance\n",
            "\u001b[35mSection_title: 5.2  Ablation Studies\n",
            "\u001b[35mSection_title: 5.3  Effectiveness of the Group Weights Learned by Web-DRO\n",
            "\u001b[35mSection_title: 5.4  Retriever Characteristics using the Group Weighting Mechanism\n",
            "\u001b[35mSection_title: 5.5  Case Study\n",
            "\u001b[32mconclusion: in this paper, we propose web-dro, which is an efficient grouplevel data clustering and reweighting strategy for unsupervised dense retrieval model training.we use structures of the web graph to train a link prediction style embedding model for clustering and utilize groupdro to reweight the clusters during training.experiments on ms marco and beir show that web-dro achieves significant improvements over baseline models, especially on difficult tasks.further analysis shows that group weights learned by\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Dynamically Fused Graph Network\n",
            "\u001b[35mSection_title: 3.1  Paragraph Selection\n",
            "\u001b[35mSection_title: 3.2  Constructing Entity Graph\n",
            "\u001b[35mSection_title: 3.3  Encoding Query and Context\n",
            "\u001b[35mSection_title: 3.4  Reasoning with the Fusion Block\n",
            "\u001b[35mSection_title: 3.5  Prediction\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Implementation Details\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Evaluation on Graph Construction and Reasoning Chain\n",
            "\u001b[35mSection_title: 4.4  Case Study\n",
            "\u001b[32mconclusion: we introduce dynamically fused graph network (dfgn) to address multi-hop reasoning. specifically, we propose a dynamic fusion reasoning block based on graph neural networks. different from previous approaches in qa, dfgn is capable of predicting the sub-graphs dynamically at each reasoning step, and the entity-level reasoning is fused with token-level context. we evaluate dfgn on hotpotqa and achieve leading results. besides, our analysis shows dfgn can produce reliable and explainable reasoning chains. in the future, we may incorporate new advances in building entity graphs from texts, and solve harder reasoning problems, e.g. \"comparison\" in hot-potqa.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  H-parser\n",
            "\u001b[35mSection_title: 2.2  H-executor\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Supervised Experiments\n",
            "\u001b[35mSection_title: 3.1.1  Datasets\n",
            "\u001b[35mSection_title: 3.1.2  Baselines\n",
            "\u001b[35mSection_title: 3.1.3  Implementation Details\n",
            "\u001b[35mSection_title: 3.1.4  Fine-tuning Results\n",
            "\u001b[35mSection_title: 3.2  Few-shot Results\n",
            "\u001b[35mSection_title: 3.3  Zero-shot Results\n",
            "\u001b[35mSection_title: 3.3.1  Dataset\n",
            "\u001b[35mSection_title: 3.3.2  Global Question Parser\n",
            "\u001b[35mSection_title: 3.3.3  Zero-Shot Results on HotpotQA\n",
            "\u001b[35mSection_title: 3.3.4  Results on NQ\n",
            "\u001b[35mSection_title: 3.4  Ablation Study\n",
            "\u001b[35mSection_title: 3.5  Case Study\n",
            "\u001b[35mSection_title: 4.2  Explainable QA\n",
            "\u001b[32mconclusion: we propose hpe for answering complex questions over text, which combines the strengths of neural network approaches and symbolic approaches. we parse the question into h-expressions followed by the hybrid execution to get the final answer. our extensive empirical results demonstrate that hpe has a strong performance on various datasets under supervised, few-shot, and zero-shot settings. moreover, our model has a strong interpretability exposing its underlying reasoning process, which facilitates understanding and possibly fixing its errors. by replacing our text reader with kb or \n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  The Co-Evolution of Seed and Target Sequences\n",
            "\u001b[35mSection_title: 3.  The Evolution of Imprinted miRNAs\n",
            "\u001b[35mSection_title: 4.  Unimprinted miRNAs with an AAGUGC Motif\n",
            "\u001b[35mSection_title: 4.1.  miR-17, miR-20, miR-93, and miR-106\n",
            "\u001b[35mSection_title: 4.2.  miR-302, miR-427, and miR-430\n",
            "\u001b[35mSection_title: 4.3.  miR-290~295 and miR-371~373\n",
            "\u001b[35mSection_title: 5.  Large Clusters of Imprinted miRNAs\n",
            "\u001b[35mSection_title: 5.1.  C19MC miRNAs\n",
            "\u001b[35mSection_title: 5.2.  C2MC miRNAs\n",
            "\u001b[35mSection_title: 5.3.  Targeting of Igf2 and PTEN by C2MC and C19MC Imprinted miRNAs\n",
            "\u001b[35mSection_title: 5.3.1.  Complementarity between C2MC miRNAs and Igf2 mRNA\n",
            "\u001b[35mSection_title: 5.3.1.  Complementarity between C2MC miRNAs and Igf2 mRNA\n",
            "\u001b[35mSection_title: 5.3.2.  Complementarity of C2MC and C19MC miRNAs with PTEN mRNA\n",
            "\u001b[32mconclusion: oviparous mothers commit resources to offspring before fertilization. therefore, genes expressed in embryos are unable to influence how much an embryo receives because the quantity of yolk in an egg is a fait accompli. by contrast, how much an embryo receives is \"up for grabs\" in placental development because mothers invest substantial resources after fertilization. embryos have evolved to actively solicit or seize maternal resources and to compete with other (contemporary or future) embryos for maternal investment. this fundamental difference between prezygotic and postzygotic provisioning accounts for many distinctive features of mammalian development.\n",
            "\u001b[35mSection_title: 6.  Conclusions\n",
            "\u001b[32mconclusion: oviparous mothers commit resources to offspring before fertilization. therefore, genes expressed in embryos are unable to influence how much an embryo receives because the quantity of yolk in an egg is a fait accompli. by contrast, how much an embryo receives is \"up for grabs\" in placental development because mothers invest substantial resources after fertilization. embryos have evolved to actively solicit or seize maternal resources and to compete with other (contemporary or future) embryos for maternal investment. this fundamental difference between prezygotic and postzygotic provisioning accounts for many distinctive features of mammalian development.\n",
            "\u001b[35mSection_title: 6.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  A Unified Token Routing View of Multi-Vector Retrievers\n",
            "\u001b[35mSection_title: 2.1  Single-Vector Retrieval\n",
            "\u001b[35mSection_title: 2.2  Multi-Vector Retrieval\n",
            "\u001b[35mSection_title: 3  The CITADEL Method\n",
            "\u001b[35mSection_title: 3.1  Dynamic Lexical Routing\n",
            "\u001b[35mSection_title: 3.2  Router Optimization\n",
            "\u001b[35mSection_title: 3.3  Sparsely Activated Router Design\n",
            "\u001b[35mSection_title: 3.4  Regularization for Routing\n",
            "\u001b[35mSection_title: 3.5  Inverted Index Retrieval\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  MS MARCO Passages Retrieval\n",
            "\u001b[35mSection_title: 4.2  BEIR: Out-of-Domain Evaluation\n",
            "\u001b[35mSection_title: 5.1  Number of Token Interactions\n",
            "\u001b[35mSection_title: 5.2  Latency-Memory-Accuracy Tradeoff\n",
            "\u001b[35mSection_title: 5.3  Combination with Product Quantization\n",
            "\u001b[35mSection_title: 7  Ablation Studies\n",
            "\u001b[35mSection_title: 8  Related Works\n",
            "\u001b[32mconclusion: this paper proposes a novel multi-vector retrieval method that achieves state-of-the-art performance on several benchmark datasets while being 40× faster than colbert-v2 and 17× faster than the most efficient multi-vector retrieval library to date, plaid, on gpus. by jointly optimizing for the token index size and load balancing, our new dynamic lexical routing scheme greatly reduces the redundancy in the all-to-all token interaction of col-bert while bridging the word-mismatch problem in coil. experiments on both in-domain and outof-domain datasets demonstrate the effectiveness and efficiency of our model.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\u001b[32mlimitation: the limitation of citadel mainly shows in two aspects. first, at the beginning of training, the model needs to route each token vector to multiple activated keys for token interaction, which increases the computation cost compared to coil and col-bert. this results in slower training speed but it gets better when training approaches the end as more tokens are pruned by the ℓ 1 regularization. another drawback lies in the implementation of citadel, or more generally speaking, most multivector retrieval methods. the token-level retrieval and aggregation make them not compatible with established search libraries such as faiss or pyserini. moreover, for time and space efficiency, multi-vector retrieval also requires more engineering efforts and low-level optimization. recently, xtr (lee et al., 2023) provides a solution that constrains the document-level retrieval to be consistent with the token-level retrieval during training, which can be used for streamlining citadel. \n",
            "\u001b[35mSection_title: 10  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Problem Setting\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[32mconclusion: followup queries are essential to solving the difficult cases of multi-hop qa, and real followup questions are an advance in making this process interpretable. we have shown that pointer generator networks can effectively learn to read partial information and produce a fluent, relevant question about what is not known, which is a complement to their typical role in summarizing what is known. our task poses a novel challenge that tests semantic properties of the generated output.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  CONTEXT ENRICHMENT\n",
            "\u001b[35mSection_title: 2.1  Problem Statement\n",
            "\u001b[35mSection_title: 2.2  Motivating Applications\n",
            "\u001b[35mSection_title: 3  LEARNED KEYLESS JOINS FOR CONTEXT ENRICHMENT\n",
            "\u001b[35mSection_title: 3.1  Keyless Joins: Definition and Objective\n",
            "\u001b[35mSection_title: 3.2  Join Specification for Context Enrichment\n",
            "\u001b[35mSection_title: 3.3  Background: Transformer-Based Encoders\n",
            "\u001b[35mSection_title: 4  EMBER\n",
            "\u001b[35mSection_title: 4.1  Usage\n",
            "\u001b[35mSection_title: 4.2  API and Architecture\n",
            "\u001b[35mSection_title: 5  PREPROCESSING\n",
            "\u001b[35mSection_title: 6  REPRESENTATION LEARNING\n",
            "\u001b[35mSection_title: 7  JOINING\n",
            "\u001b[35mSection_title: 8  EVALUATION\n",
            "\u001b[35mSection_title: 8.1  Evaluation Metric and Applications\n",
            "\u001b[35mSection_title: 8.1.2  Entity Matching (EM).\n",
            "\u001b[35mSection_title: 8.1.3  Search (S).\n",
            "\u001b[35mSection_title: 8.1.4  Question Answering (QA). We modify the Stanford Question\n",
            "\u001b[35mSection_title: 8.1.5  Recommendation (R).\n",
            "\u001b[35mSection_title: 8.2  Experimental Setup\n",
            "\u001b[35mSection_title: 8.3  Generalizability\n",
            "\u001b[35mSection_title: 8.4  Extensibility: Architecture Lesion\n",
            "\u001b[35mSection_title: 8.4.2  Removing Representation Learning (-rl).\n",
            "\u001b[35mSection_title: 8.4.3  Remove\n",
            "\u001b[35mSection_title: 8.4.4  Remove MLM Pretraining (-mlm).\n",
            "\u001b[35mSection_title: 8.5  Extensibility: End-to-End Workloads\n",
            "\u001b[35mSection_title: 8.6  Low Development Effort\n",
            "\u001b[35mSection_title: 9  RELATED WORK\n",
            "\u001b[32mconclusion: we demonstrate how seemingly unrelated tasks spanning data integration, search, and recommendation can all be viewed as instantiations of context enrichment. we propose keyless joins as a unifying abstraction that can power a system for general context enrichment, which allows us to view context enrichment as a data management problem. consequently, we developed and applied ember, a first-of-its kind system that performs no-code context enrichment via keyless joins. we evaluate how developing a keyless join enrichment layer empowers a single system to generalize to five downstream applications, with no ml code written by the user.\n",
            "\u001b[35mSection_title: 10  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Language Description\n",
            "\u001b[35mSection_title: 2.1  Declarative: Constraints over tables\n",
            "\u001b[35mSection_title: 3  Solver and Interpreter\n",
            "\u001b[35mSection_title: 4  Example Solutions\n",
            "\u001b[35mSection_title: 4.1  Explanation Regeneration\n",
            "\u001b[35mSection_title: 4.2  Micro-models and Interpreter\n",
            "\u001b[32mconclusion: cosata is an open-source constraint satisfaction solver for easily expressing and evaluating multifact compositional patterns in semi-structured tables of text, paired with an interpreted language that allows expressing micro-models. the tool, source, examples, and documentation are available at http://www.github.com/clulab/cosata/ .\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Three Levels of Tasks\n",
            "\u001b[35mSection_title: 4  Annotations and Dataset\n",
            "\u001b[35mSection_title: 4.1  Corpus selection\n",
            "\u001b[35mSection_title: 4.2  Annotations via natural language\n",
            "\u001b[35mSection_title: 4.3  Annotation uncertainty explanations\n",
            "\u001b[35mSection_title: 5  Baseline Models\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Task 1: Sentence classification.\n",
            "\u001b[35mSection_title: 6.4  Qualitative error analysis.\n",
            "\u001b[35mSection_title: 7  Discussion and Future Work\n",
            "\u001b[35mSection_title: 8  Acknowledgments\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Approach\n",
            "\u001b[35mSection_title: 2.1  Problem Formulation\n",
            "\u001b[35mSection_title: 2.2  Graph-based Iterative Retrieval\n",
            "\u001b[35mSection_title: 2.3  Pre-trained Language Model Based Encoding\n",
            "\u001b[35mSection_title: 2.4  Answer Choice-aware Attention Module\n",
            "\u001b[35mSection_title: 2.5  Prediction\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Knowledge Sources\n",
            "\u001b[35mSection_title: 3.3  Baselines\n",
            "\u001b[35mSection_title: 3.4  Experimental Setting\n",
            "\u001b[35mSection_title: 3.5  Experimental Results and Analysis\n",
            "\u001b[35mSection_title: 3.6  Ablation Study\n",
            "\u001b[35mSection_title: 3.7  Case Study\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[35mSection_title: 4.1  Benchmarks\n",
            "\u001b[35mSection_title: 4.2  Information Retrieval for Knowledge Facts\n",
            "\u001b[32mconclusion: in this paper, we deal with the multi-choice question answering task which requires background knowledge or commonsense. we propose a novel question-answering method by exploring how to efficiently integrate multiple knowledge sources, i.e. conceptnet, wikipedia and the cambridge dictionary. firstly, we propose a novel graph-based iterative knowledge retrieval module to iteratively retrieve concepts and entities related to a given question and its choices. in addition, we propose an answer choice-aware attention mechanism to fuse all hidden representations encoded by a pre-trained language model. we conducted experiments on the commonsenseqa dataset and the experimental results show that our method significantly outperforms other competitive methods in accuracy. further ablation studies show the effectiveness of graph-based iterative knowledge retrieval module and answer choice-aware attention module in retrieving and synthesizing background knowledge from multiple knowledge sources. in the future, we will extend our method to deal with the open-domain question answering tasks that require the external background knowledge.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Materials and Methods\n",
            "\u001b[35mSection_title: 2.1.  Data Retrieval\n",
            "\u001b[35mSection_title: 2.2.  Hairpin-structured miRNA Precursors Prediction -Pre-miRNA Extraction\n",
            "\u001b[35mSection_title: 2.3.  Human miRNAs Sequence Prediction\n",
            "\u001b[35mSection_title: 2.4.  Hybridization Prediction between Target miRNA and Viral miRNA\n",
            "\u001b[35mSection_title: 2.5.  Secondary Structure Prediction of miRNA\n",
            "\u001b[35mSection_title: 3.  Results\n",
            "\u001b[35mSection_title: 3.1.  miRNA Precursor (Pre-miRNA) Hairpins Prediction\n",
            "\u001b[35mSection_title: 3.2.  Human miRNAs Prediction from miRNAs Precursor (Hairpin)\n",
            "\u001b[35mSection_title: 3.4.  Secondary Structure miRNA Precursor\n",
            "\u001b[32mconclusion: in our current investigation, we identified mirnas for sars-cov-2 in human beings using computational tools.this study was based on an interesting hypothesis of the utilization of host mirna as a potential post-exposure therapy because the current evidence suggests that host mirnas may downregulate the viral gene expression.\n",
            "\u001b[35mSection_title: 5.  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Introduction\n",
            "\u001b[35mSection_title: 2.  Problem Setting\n",
            "\u001b[35mSection_title: 3.  The Exact-Extract Algorithm\n",
            "\u001b[35mSection_title: 4.  Experimental Setup\n",
            "\u001b[35mSection_title: 5.  Results\n",
            "\u001b[35mSection_title: 5.1  Performance\n",
            "\u001b[35mSection_title: 5.2  How Extractive and Exact is Greedy?\n",
            "\u001b[35mSection_title: 6.  Pretraining Models to Extract\n",
            "\u001b[35mSection_title: 7.  Error Analysis\n",
            "\u001b[35mSection_title: 7.1  Exact-Extract\n",
            "\u001b[35mSection_title: 7.2  Greedy Decoding\n",
            "\u001b[32mconclusion: we investigate the optimality of greedy decoding for extractive question answering by comparing it to exact-extract, an optimal decoding algorithm that guarantees both extractiveness and exactness. while the greedy algorithm lags behind exact-extract in the zero-shot setting, training the model on as few as 16 labeled examples shrinks the performance gap substantially. this gap continues to narrow as more examples are available, typically converging to less than 1 point (f1) when training on 1024 examples. overall, our results showcase the impressive ability of pretrained language models to adapt to extractive question answering while relying only on a naive decoding algorithm. \n",
            "\u001b[35mSection_title: 8.  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Framework Description\n",
            "\u001b[35mSection_title: 3.1  Fact Retrieval\n",
            "\u001b[35mSection_title: 3.2  Semantic Graph Construction & Analytics\n",
            "\u001b[35mSection_title: 3.2.1  AMR-SG Construction\n",
            "\u001b[35mSection_title: 3.2.2  Path-based Analytics\n",
            "\u001b[35mSection_title: 3.3  Hypothesis Assessment with Fact-level Reasoning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Implementation\n",
            "\u001b[35mSection_title: 4.3  Comparison Methods\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Main Results\n",
            "\u001b[35mSection_title: 5.2  Ablation Study\n",
            "\u001b[35mSection_title: 6.2  Case Study\n",
            "\u001b[35mSection_title: 6.3  Analysis of Fact-level Reasoning\n",
            "\u001b[32mconclusion: we propose to dynamically construct amr-sg that can reflect the intrinsic relations of relevant facts leveraging amr, a graph annotation. amr-sg combines the advantages of rich textual corpus and graph structure, where we can select useful facts that completely form the reasoning chain and make fact-level modeling. experimental results show that amr-sg can maintain high explainability, and successfully couple with strong pretrained models to achieve significant improvement on openbookqa and arc-challenge over approaches leveraging additional kgs. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Fusion-in-Decoder\n",
            "\u001b[35mSection_title: 2.2  LUMEN\n",
            "\u001b[35mSection_title: 3.1  Architecture\n",
            "\u001b[35mSection_title: 3.2  Training\n",
            "\u001b[35mSection_title: 3.3  Computational analysis\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental setup\n",
            "\u001b[35mSection_title: 4.2  Main results\n",
            "\u001b[35mSection_title: 4.3  Retrieval and reranking\n",
            "\u001b[35mSection_title: 4.4  Multi-task training\n",
            "\u001b[35mSection_title: 4.5  Other ablations\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: retrieval-augmented language models are powerful but slow in inference, while pre-computed memory-augmented models are fast at the cost of quality. hybrid late-interaction models such as lu-men present a good quality-compute trade-off. we introduce glimmer, an improved late-interaction model that also incorporates learned end-to-end reranking and multi-task training to achieve an even better trade-off. glimmer achieves strong gains in quality at faster speeds compared to lumen and fid on the kilt benchmark of knowledgeintensive tasks.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Logical Reasoning\n",
            "\u001b[35mSection_title: 2.2  Current Machine Reading Comprehension Settings\n",
            "\u001b[35mSection_title: 3  THE BENCHMARK OF GENERALIZED ZSLR\n",
            "\u001b[35mSection_title: 3.1  Zero-shot Data Construction\n",
            "\u001b[35mSection_title: 3.2  Generalized Zero-shot Setting\n",
            "\u001b[35mSection_title: 3.3  Pilot Experiments\n",
            "\u001b[35mSection_title: 4  METHODS\n",
            "\u001b[35mSection_title: 4.1  Heuristic Input Reconstruction\n",
            "\u001b[35mSection_title: 4.2  Text Graph Construction and Reasoning\n",
            "\u001b[35mSection_title: 4.3  Type-aware Contrastive Learning\n",
            "\u001b[35mSection_title: 5  MAIN EXPERIMENTS\n",
            "\u001b[35mSection_title: 5.1  Current Full-data Benchmark\n",
            "\u001b[35mSection_title: 5.2  Baselines\n",
            "\u001b[35mSection_title: 5.3  Implementation Details\n",
            "\u001b[35mSection_title: 5.4  Main Results\n",
            "\u001b[35mSection_title: 5.5  Ablation Studies\n",
            "\u001b[35mSection_title: 5.6  Parameter Analysis\n",
            "\u001b[35mSection_title: 6  ANALYSIS OF MODEL GENERALIZATION\n",
            "\u001b[35mSection_title: 6.1  Generalization on Full-data Setting of ReClor\n",
            "\u001b[35mSection_title: 6.2  Generalization on Other Dataset\n",
            "\u001b[35mSection_title: 7  CASE STUDY\n",
            "\u001b[32mconclusion: to study the zero-shot capability of the logical reasoning models, we propose the first benchmark for the generalized zero-shot logical reasoning, named zslr. it includes six splits sampled with three strategies and two metrics to comprehensively evaluate the performances. also, we propose a model taco to enhance the reasoning type perception through the heuristic input reconstruction and the type-aware contrastive learning. also, we conduct extensive experiments on the zero-shot splits, full-data setting as well as other dataset. superior results illustrate the effectiveness and generalization capability of the proposed modules.\n",
            "\u001b[35mSection_title: 8  CONCLUSION AND FUTURE WORK\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  GİRİŞ\n",
            "\u001b[35mSection_title: 2.  KULLANILAN VERİLER VE YÖNTEMLER\n",
            "\u001b[35mSection_title: 2.1.  Çalışma Alanı\n",
            "\u001b[35mSection_title: 2.2.  Çok Değişkenli Uyarlanabilir Regresyon Eğrileri (MARS)\n",
            "\u001b[35mSection_title: 2.3.  Regresyon Analizi\n",
            "\u001b[35mSection_title: 2.4.  Model Uygulamaları\n",
            "\u001b[35mSection_title: 3.  BULGULAR VE İRDELEME\n",
            "\u001b[35mSection_title: 4.  SONUÇLAR\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Multi-vector Retrieval\n",
            "\u001b[35mSection_title: 2.2  Three-stage inference of Multi-vector Retrieval\n",
            "\u001b[35mSection_title: 3  XTR: Contextualized Token Retriever\n",
            "\u001b[35mSection_title: 3.1  In-Batch Token Retrieval\n",
            "\u001b[35mSection_title: 3.2  Scoring Documents using Retrieved Tokens\n",
            "\u001b[35mSection_title: 4.2  Zero-shot Document Retrieval\n",
            "\u001b[35mSection_title: 4.3  Multilingual Document Retrieval\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 5.1  Towards Better Token Retrieval\n",
            "\u001b[35mSection_title: 5.2  Efficient Scoring\n",
            "\u001b[35mSection_title: 5.3  Qualitative Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: multi-vector retrieval leverages query and document token representations for effective information retrieval. in this paper, we propose xtr that simplifies the existing three-stage inference of multivector models by improving the initial token retrieval stage. specifically, xtr scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens. as a result, xtr achieves state-of-the-art performances on zero-shot information retrieval benchmarks while greatly reducing the flops of the scoring stage. we further show that indeed our objective encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 3  Dataset Construction\n",
            "\u001b[35mSection_title: 3.2  Context Retrieval\n",
            "\u001b[35mSection_title: 4.4  Results\n",
            "\u001b[35mSection_title: 2  Building the QASPER Dataset\n",
            "\u001b[35mSection_title: 2.1  Papers\n",
            "\u001b[35mSection_title: 2.2  Decoupled Data Collection\n",
            "\u001b[35mSection_title: 3  QASPER Analysis\n",
            "\u001b[35mSection_title: 4  Modeling QASPER\n",
            "\u001b[35mSection_title: 4.1  Task Setup\n",
            "\u001b[35mSection_title: 4.2  QASPER Model\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Training Details\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[32mconclusion: we presented qasper, an information-seeking qa dataset over nlp research papers. with natural questions asked as follow-up to titles and abstracts, the task presented by qasper requires evidence from multiple paragraphs and/or figures and tables within the full text of the papers. our empirical quire workers to write questions grounded in those snippets.  table 5: error analysis of our best model (led from row 5 from table 2) on 55 test examples with low f 1 score (excluding those with \"yes,\" \"no,\" or \"unanswerable\" gold answers). \"quotations\" denote extractive gold answers. we note lacks domain knowledge errors are not always solved by better entity type resolution (see †).\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  The Problem\n",
            "\u001b[35mSection_title: 2.2  Natural Logic\n",
            "\u001b[35mSection_title: 2.3  Natural Logic Inference\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Candidate Premises Retrieval\n",
            "\u001b[35mSection_title: 3.2  Contextualized Neural Natural Logic Prover\n",
            "\u001b[35mSection_title: 3.3  Gumbel-Softmax Training\n",
            "\u001b[35mSection_title: 3.4  Objective Function\n",
            "\u001b[35mSection_title: 4  Experiment Set-Up\n",
            "\u001b[35mSection_title: 5  Experiment Results\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: in this work, we explore the feasibility of combining natural logic with neural networks for interpretable question answering. we present an end-toend differentiable method for learning the parameters as well as the structure of natural logical rules, which is capable of considering the contextual information while conducting natural logic-based reasoning. experimental results on the regents science exam of the aristo dataset show that our proposed model could bring improvements over baseline methods.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 1.1  System Components and Related Work\n",
            "\u001b[35mSection_title: 2  Method\n",
            "\u001b[35mSection_title: 2.1  Retrieval\n",
            "\u001b[35mSection_title: 2.2  Reranking and Evidence Set Scoring\n",
            "\u001b[35mSection_title: 2.3  Iterator In-domain Evaluation\n",
            "\u001b[35mSection_title: 2.4  QA Models\n",
            "\u001b[35mSection_title: 2.4.1  Stage 1 Pretraining\n",
            "\u001b[35mSection_title: 2.4.2  Stage 2 Pretraining\n",
            "\u001b[35mSection_title: 2.4.3  Base and Base+RATD Models\n",
            "\u001b[35mSection_title: 2.4.4  QA Model In-domain Evaluation\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Unseen Evaluation Datasets\n",
            "\u001b[35mSection_title: 3.2  Models\n",
            "\u001b[35mSection_title: 3.3  Experimental Results\n",
            "\u001b[35mSection_title: 3.3.1  StrategyQA and CommonsenseQA\n",
            "\u001b[35mSection_title: 3.3.2  DROP and IIRC\n",
            "\u001b[35mSection_title: 3.3.3  ARC-DA and Musique\n",
            "\u001b[32mconclusion: we have argued that an ability to reason over imperfect and incomplete information is a critical skill with which question-answering models must be endowed. to facilitate such ability we create ratd datasets that are designed to impart heuristic reasoning strategies with context of a form similar to that which retrieved contexts for downstream tasks will have. we show that training on ratd datasets improves performance on all unseen evaluation datasets with retrieved contexts. this sometimes comes at a small cost in situations where questions come with gold contexts that are in a form that our model is already good at utilizing (sqa gf , drop, and iirc g ) although we suggest that in practice such gold contexts are the less common case. (r1)\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Methodology\n",
            "\u001b[35mSection_title: 3.1  Parameter-Efficient Tuning\n",
            "\u001b[35mSection_title: 3.2  Tuned Parameters as Task Embeddings\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.3  Training Details\n",
            "\u001b[35mSection_title: 4.4  Experimental Results\n",
            "\u001b[35mSection_title: 4.5  Analysis\n",
            "\u001b[32mconclusion: in this paper, we show that efficiently tuned parameters are highly predictive for inter-task transferability and thus can be used as off-the-shelf task embeddings for source task selection in intermediatetask transfer learning. our empirical investigation with three parameter-efficient tuning methods on 22 nlp tasks demonstrates that our approach outperforms prior works on inter-task transferability prediction despite being more efficient.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  Machine Reading Comprehension\n",
            "\u001b[35mSection_title: 2.2  Temporal Ordering Reasoning\n",
            "\u001b[35mSection_title: 3  Our Approach\n",
            "\u001b[35mSection_title: 3.1  Task Definition\n",
            "\u001b[35mSection_title: 3.2  Model Architecture\n",
            "\u001b[35mSection_title: 3.3  Learning Objectives\n",
            "\u001b[35mSection_title: 3.4  Inference\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Settings\n",
            "\u001b[35mSection_title: 4.2  Implementation Details\n",
            "\u001b[35mSection_title: 4.3  Main Results\n",
            "\u001b[35mSection_title: 4.4  Ablation Study\n",
            "\u001b[35mSection_title: 4.5  Question Representation Analysis\n",
            "\u001b[35mSection_title: 4.6  Case Study\n",
            "\u001b[35mSection_title: 4.7  Error Analysis\n",
            "\u001b[32mconclusion: temporal reading comprehension plays a critical role in natural language understanding. in this paper, we propose a precise question understanding method to tackle the trc problem. specifically, we encode temporal ordering questions into repre-sentations of referred events and concerned temporal relations, based on which candidate answers are evaluated in terms of their temporal relations to the referred events. in addition, a contrastive loss is employed to empower the model to capture essential differences among temporal relations. experimental results based on four pre-trained models verify the effectiveness of our proposed approach. in the future, we will investigate general approaches to handle more diverse temporal relation understanding problems and improve the passage understanding capability for temporal reading comprehension.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Improving PROMPTTUNING with SPOT\n",
            "\u001b[35mSection_title: 2.1  Experimental setup\n",
            "\u001b[35mSection_title: 2.1.1  Baselines\n",
            "\u001b[35mSection_title: 2.1.2  Evaluation datasets\n",
            "\u001b[35mSection_title: 2.1.3  Data for source prompt tuning\n",
            "\u001b[35mSection_title: 2.1.4  Training details\n",
            "\u001b[35mSection_title: 2.2  Effect of SPOT\n",
            "\u001b[35mSection_title: 3  Predicting task transferability\n",
            "\u001b[35mSection_title: 3.1  Measuring transferability\n",
            "\u001b[35mSection_title: 3.2  Defining task similarity through prompts\n",
            "\u001b[35mSection_title: 3.3  Predicting transferability via similarity\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mlimitation: as other parameter-efficient adaptation methods (see §4) may outperform prompttuning in specific situations, it would be interesting to test whether an approach similar to spot could extend successfully to these methods. at the same time, we believe that prompttuning has its own merit. as pre-trained language models become larger and larger, some advantages of prompttuning over other methods are: (1) among current methods with learnable parameters, prompttuning is the most parameter efficient, requiring less than 0.01% task-specific parameters for most model sizes.(2) prompttuning is simpler than other methods, as it does not modify the internal model architecture (cf. the prefix-tuning method of li and liang (2021), which adds a prefix to each layer of both the transformer encoder and decoder); as such, prompttuning allows mixed-task inference and facilitates transfer learning between tasks.(3) as model capacity increases, prompttuning becomes more competitive with modeltuning; to the best of our knowledge, this has not been shown for other methods. (4) soft prompts could possibly be interpreted as natural language instructions.additionally, since our prompt-based task embedding approach does not capture all of the factors that influence task transferability, we leave further exploration of other task embedding methods to future work.\n",
            "\u001b[35mSection_title: 5  Limitations & Future work\n",
            "\u001b[32mconclusion: in this paper, we study transfer learning in the context of prompt tuning. we show that scale is not necessary for prompttuning to match the performance of modeltuning. on superglue, our spot approach matches or even exceeds the performance of modeltuning by a large margin across model sizes while being more parameter-efficient. our large-scale study on task transferability indicates that tasks can benefit each other via prompt transfer in various scenarios. finally, we demonstrate that task prompts can be interpreted as task embeddings to formalize the similarity between tasks. we propose a simple yet efficient retrieval approach that measures task similarity to identify which source tasks could confer benefits to a novel target task. taken as a whole, we hope that our work will spur more research into prompt-based transfer learning.   (raffel et al., 2020), i.e., learning rate 0.001, adafactor optimizer with pre-training parameter states restored, and dropout probability 0.1. to improve the model tuning baselines, we perform a sweep over the batch size hyperparameter and select 2 16 tokens per batch, following lester et al. (2021). table 5 shows the performance of our spot xxl superglue submission, along with several strong competitors from the public superglue leaderboard. apart from the human baseline, the top-7 submissions all tune >3b parameters directly on the final tasks. only three previous superglue submissions use parameter efficient adaptation, in the sense of tuning <1m parameters on the final tasks; all other submissions tune >50m parameters. 17 17 the \"ailabs team, transformers\" submission is listed as tuning 3m parameters, but we suspect this is in error, as the our spot submission achieves a score of 89.2, which far exceeds all other parameter-efficient adaptation methods, including gpt-3, which benefits from over 10× more frozen parameters (although it uses no tuned parameters). compared to warp (hambardzumyan et al., 2021), our spot approach tunes 16× more parameters (410k vs. 25k), and benefits from 50× more frozen parameters.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  3\n",
            "\u001b[35mSection_title: 2.2  Cell culture and cell transfection\n",
            "\u001b[35mSection_title: 2.3  Quantitative real-time polymerase chain reaction (qRT-PCR)\n",
            "\u001b[35mSection_title: 2.4  Cell counting kit-8 (CCK-8) assay\n",
            "\u001b[35mSection_title: 2.5  Cell migration and invasion assay\n",
            "\u001b[35mSection_title: 2.6  Reactive oxygen species (ROS), proline and ATP assays\n",
            "\u001b[35mSection_title: 2.7  Pull-down assay with biotinylated miR-621\n",
            "\u001b[35mSection_title: 2.8  Luciferase reporter assay\n",
            "\u001b[35mSection_title: 2.9  RNA immunoprecipitation (RIP) assay\n",
            "\u001b[35mSection_title: 2.10  Western blotting\n",
            "\u001b[35mSection_title: 2.11  Tissue array\n",
            "\u001b[35mSection_title: 2.12  Statistical analysis\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 3.1  Circ_0000705 is highly expressed in ESCC tissues and cell lines\n",
            "\u001b[35mSection_title: 3.2  Circ_0000705 promotes cell proliferation, invasion and migration of ESCC cells\n",
            "\u001b[35mSection_title: 3.3  Circ_0000705 promotes proline metabolism in ESCC cells\n",
            "\u001b[35mSection_title: 3.4  Circ_0000705 acts as a ceRNA by sponging miR-621 in ESCC cells\n",
            "\u001b[35mSection_title: 3.5  miR-621 directly targets PYCR1 mRNA in ESCC cells\n",
            "\u001b[35mSection_title: 3.6  miR-621 inhibition or PYCR1 over-expression partly reverses the functions of circ_0000705 knockdown in ESCC cells\n",
            "\u001b[35mSection_title: 3.7  PYCR1 expression is negatively correlated with miR-621 expression, and positively correlated with circ_0000705 in ESCC tissues\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[32mconclusion: these findings indicated that circ_0000705 could promote proline metabolism and escc progression by targeting mir-621/pycr1 axis, suggest a key role for circ_0000705 in proline metabolism in escc, and imply that circ_0000705 may be a potential biomarker or/and possible therapeutic target for escc. however, whether circ_0000705 is also highly expressed and has a similar mechanism of regulating proline metabolism in other types of tumors deserve further research in the future.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\u001b[35mSection_title: 1  3\n",
            "\u001b[35mSection_title: 2.2  Cell culture and cell transfection\n",
            "\u001b[35mSection_title: 2.3  Quantitative real-time polymerase chain reaction (qRT-PCR)\n",
            "\u001b[35mSection_title: 2.4  Cell counting kit-8 (CCK-8) assay\n",
            "\u001b[35mSection_title: 2.5  Cell migration and invasion assay\n",
            "\u001b[35mSection_title: 2.6  Reactive oxygen species (ROS), proline and ATP assays\n",
            "\u001b[35mSection_title: 2.7  Pull-down assay with biotinylated miR-621\n",
            "\u001b[35mSection_title: 2.8  Luciferase reporter assay\n",
            "\u001b[35mSection_title: 2.9  RNA immunoprecipitation (RIP) assay\n",
            "\u001b[35mSection_title: 2.10  Western blotting\n",
            "\u001b[35mSection_title: 2.11  Tissue array\n",
            "\u001b[35mSection_title: 2.12  Statistical analysis\n",
            "\u001b[35mSection_title: 3  Results\n",
            "\u001b[35mSection_title: 3.1  Circ_0000705 is highly expressed in ESCC tissues and cell lines\n",
            "\u001b[35mSection_title: 3.2  Circ_0000705 promotes cell proliferation, invasion and migration of ESCC cells\n",
            "\u001b[35mSection_title: 3.3  Circ_0000705 promotes proline metabolism in ESCC cells\n",
            "\u001b[35mSection_title: 3.4  Circ_0000705 acts as a ceRNA by sponging miR-621 in ESCC cells\n",
            "\u001b[35mSection_title: 3.5  miR-621 directly targets PYCR1 mRNA in ESCC cells\n",
            "\u001b[35mSection_title: 3.6  miR-621 inhibition or PYCR1 over-expression partly reverses the functions of circ_0000705 knockdown in ESCC cells\n",
            "\u001b[35mSection_title: 3.7  PYCR1 expression is negatively correlated with miR-621 expression, and positively correlated with circ_0000705 in ESCC tissues\n",
            "\u001b[35mSection_title: 4  Discussion\n",
            "\u001b[32mconclusion: these findings indicated that circ_0000705 could promote proline metabolism and escc progression by targeting mir-621/pycr1 axis, suggest a key role for circ_0000705 in proline metabolism in escc, and imply that circ_0000705 may be a potential biomarker or/and possible therapeutic target for escc. however, whether circ_0000705 is also highly expressed and has a similar mechanism of regulating proline metabolism in other types of tumors deserve further research in the future.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Global-Memory Augmented Transformers\n",
            "\u001b[35mSection_title: 2.1  Sequence Compression\n",
            "\u001b[35mSection_title: 3  Global Reasoning on Synthetic Data\n",
            "\u001b[35mSection_title: 3.1  Majority Tagging\n",
            "\u001b[35mSection_title: 3.2  Numerical Reasoning Over Text\n",
            "\u001b[35mSection_title: 4  Masked Language Modeling\n",
            "\u001b[35mSection_title: 4.1  Random Initialization\n",
            "\u001b[35mSection_title: 4.2  BERT Initialization\n",
            "\u001b[35mSection_title: 4.3  Sequence Compression\n",
            "\u001b[35mSection_title: 4.4  Reading Comprehension Performance\n",
            "\u001b[32mconclusion: in this work, we proposed gmat, a simple extension to the transformer architecture that allows a better trade-off between compute and performance and can be naturally used for sequence compression. our approach can be seamlessly integrated with the increasingly-popular sparse transformer variants.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  MATERIAL AND METHODS\n",
            "\u001b[35mSection_title: 2.1.  Data Collection\n",
            "\u001b[35mSection_title: 2.2.  Random Forests Classification\n",
            "\u001b[35mSection_title: 3.  RESULTS AND DISCUSSION\n",
            "\u001b[32mconclusion: results indicated that seasonal apricot workers suffered from food insecurity. food security was largely dependent on the purchasing power of food. the auc value of the rf classification model (0.846) indicated its utility in the detection of the driving forces and causation patterns behind household food (in)security of the seasonal agricultural workers in turkey and in the world. the findings of this study showed how the families of agricultural workers perceiving food security and necessity of developing different intervention strategies for different populations. thus, public services addressing the chronic food insecurity status of\n",
            "\u001b[35mSection_title: 4.  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 3.1  Explanation Types and Conditions\n",
            "\u001b[35mSection_title: 3.2  Hypotheses\n",
            "\u001b[35mSection_title: 3.3  Implementation Details for Conditions\n",
            "\u001b[35mSection_title: 3.4  User Study & Interface\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Quantitative Results\n",
            "\u001b[35mSection_title: 4.2  Qualitative Results\n",
            "\u001b[35mSection_title: 4.3  What Misleads Users?\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 5.1  Why Explanations Worked for ODQA\n",
            "\u001b[35mSection_title: 5.2  Implications and Recommendations\n",
            "\u001b[32mconclusion: we conducted user studies to understand whether explanations from a state-of-the-art open-domain qa system help improve error-detectability for endusers. our study showed that for odqa, simple explanations based on evidence snippets can significantly improve error-detectability and beat strong baselines such as communicating model's confidence. we observed this for multiple modalities of interaction: spoken and visual modalities. however, results also indicated that not every explanation type is guaranteed to improve performance over confidence and the best explanation strategy may change with the modality, e.g., due to differences in users' cognitive abilities across modalities. thus, developers and researchers of explainable odqa systems should not take the effectiveness of explanations for granted and should evaluate and tune them on the tasks and modalities where these models will be eventually deployed.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 7  Ethical Impact Statement\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Model\n",
            "\u001b[35mSection_title: 2.1  State Tracking\n",
            "\u001b[35mSection_title: 2.2  Location Tracking\n",
            "\u001b[35mSection_title: 2.3  Learning and Inference\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Task 1: Sentence Level\n",
            "\u001b[35mSection_title: 3.2  Task 2: Document Level\n",
            "\u001b[35mSection_title: 3.3  Model Ablations\n",
            "\u001b[32mconclusion: in this paper, we present a structured architecture for entity tracking which leverages both the discrete and continuous characterization of the entity evolution. we use a neural crf approach to model our discrete constraints while tracking entities and locations recurrently. our model achieves state of the art results on the propara dataset.\n",
            "\u001b[35mSection_title: 4  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 340  C A A T A A T T T C C C C A T A T G T G G A A T T T G A T A A G T T A G A G C A C C C A C T T C A A T T C A A G A A G A T T\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2  KRAS, NRAS, HRAS-canonical isoforms-the 'p21' proteins and their mutations\n",
            "\u001b[35mSection_title: 4  Alternative splicing of HRAS\n",
            "\u001b[35mSection_title: 5  Alternative splicing of KRAS\n",
            "\u001b[35mSection_title: 6  Summary\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  N-CRITICS: Ensemble of Critics\n",
            "\u001b[35mSection_title: 4  Experiments and Results\n",
            "\u001b[32mconclusion: we introduced n-critics, an innovative method leveraging feedback from open-sourced llms to iteratively refine model outputs, setting it apart from current self-refinement approaches (also, their underlying models are not free to use).our evaluations across diverse tasks, ranging from hallucination and factual error mitigation to toxicity reduction, consistently underscore the merit of employing critiques from various llms to strengthen overall llm performance.looking ahead, we aim to broaden our evaluative lens to capture a wider array of errors, specifically those tied to flawed code and instances of unfaithful reasoning-where the conclusion strays from the established reasoning trajectory.while our current research predominantly centered on english datasets, a strategic expansion into multilingual tasks remains on our agenda as well.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Instruction Bias in NLU Benchmarks\n",
            "\u001b[35mSection_title: 2.1  Patterns in Crowdsourcing Instructions\n",
            "\u001b[35mSection_title: 2.2  Instruction Bias Propagation to Datasets\n",
            "\u001b[35mSection_title: 3  Effect on Model Learning\n",
            "\u001b[35mSection_title: 3.1  Experimental Setting\n",
            "\u001b[35mSection_title: 3.2  Results\n",
            "\u001b[32mconclusion: we identify a prominent source of bias in crowdsourced nlu datasets, called instruction bias, which originates in annotation instructions written by dataset creators. we study this bias in 14 nlu benchmarks, showing that instruction examples used to create nlu benchmarks often exhibit clear patterns that are propagated by annotators to the collected data. in addition, we investigate the effect of instruction bias on model performance, showing that instruction patterns can lead to overestimated performance as well as limit the ability of models to generalize to other task examples.\n",
            "\u001b[35mSection_title: 4  Conclusions and Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  The Proposed Model: PPBERT\n",
            "\u001b[35mSection_title: 2.1  Pre-training\n",
            "\u001b[35mSection_title: 2.2  Post-training\n",
            "\u001b[35mSection_title: 2.2.1  Training Details\n",
            "\u001b[35mSection_title: 2.2.2  Post-training Datasets\n",
            "\u001b[35mSection_title: 2.3  Fine-tuning\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Tasks\n",
            "\u001b[35mSection_title: 3.2  Datasets\n",
            "\u001b[35mSection_title: 3.2.1  GLUE\n",
            "\u001b[35mSection_title: 3.2.2  SuperGLUE\n",
            "\u001b[35mSection_title: 3.2.3  SQuAD\n",
            "\u001b[35mSection_title: 3.2.4  Financial datasets\n",
            "\u001b[35mSection_title: 3.2.5  Additional benchmarks\n",
            "\u001b[35mSection_title: 3.3  Experimental Results\n",
            "\u001b[35mSection_title: 3.3.1  GLUE Results\n",
            "\u001b[35mSection_title: 3.3.2  SuperGLUE Results\n",
            "\u001b[35mSection_title: 3.3.3  Overall Trends\n",
            "\u001b[35mSection_title: 4  Ablation Study and Analyses\n",
            "\u001b[35mSection_title: 4.1  Cooperation with other Pre-trained LMs\n",
            "\u001b[35mSection_title: 4.1.1  Comparisons to SOTA models\n",
            "\u001b[32mconclusion: in the paper, we present a 'pre-training'+'post-training'+'fine-tuning' three-stage paradigm and a language model named ppbert based on the three-stage paradigm, which is a supplementary framework for the standard 'pre-training'+'fine-tuning' two-stage architecture. our proposed three-stage paradigm helps to incorporate task-awareness knowledge and domain knowledge within pre-trained model, also reduce the bias in the training corpus. ppbert can benefits from the regularization effect since it leverages cross-domain or cross-task data, which helps model generalize better with limited data and adapt to new domains or tasks better. with the latest plms as baseline and encoder backbone, ppbert is evaluated on 24 well-known benchmarks, which outperforms strong baseline models and obtains new sota results. we hope this work can encourage further research into the language models training, and the future works involve the choice of other transfer learning sources such as cv etc.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Preliminary\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 3.1  Overview\n",
            "\u001b[35mSection_title: 3.2  Initializing Conversational Knowledge Memory\n",
            "\u001b[35mSection_title: 3.3  Iterative Tool-augmented Reasoning\n",
            "\u001b[35mSection_title: 3.3.1  Tool-augmented Reasoning Step\n",
            "\u001b[35mSection_title: 3.3.2  Iteration for Step-by-Step Reasoning\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Experimental settings\n",
            "\u001b[35mSection_title: 4.1.1  Datasets\n",
            "\u001b[35mSection_title: 4.1.2  Baselines\n",
            "\u001b[35mSection_title: 4.1.3  Implementation Details\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Detailed Analysis\n",
            "\u001b[35mSection_title: 4.3.1  Ablation Study\n",
            "\u001b[35mSection_title: 4.3.2  Combining CoT Improvement Strategies\n",
            "\u001b[35mSection_title: 4.3.3  Computational Cost Analysis\n",
            "\u001b[35mSection_title: 4.3.4  Case study\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.1  Estimating IRT Parameters\n",
            "\u001b[35mSection_title: 2.2  Analysis of Estimated Parameters\n",
            "\u001b[35mSection_title: 3  Predicting IRT Parameters\n",
            "\u001b[35mSection_title: 3.1  Feature Design\n",
            "\u001b[35mSection_title: 3.2  Quantitative Analysis and Ablation\n",
            "\u001b[35mSection_title: 3.3  Feature Importance Study\n",
            "\u001b[32mconclusion: in this paper, we explored qa datasets through the lens of item response theory. we have demonstrated a way to build regression models that can describe the difficulty and discrimination of a question. we note that our work is limited in two important ways: firstly, we only use the dfgn model in our artificial crowd, which may have introduced a bias in which some factors that make questions difficult/discriminatory are only applicable to this model. secondly, we only explore the hotpotqa dataset, which may further limit our analysis to only be applicable to hotpotqa or similar datasets. future work could incorporate multiple models and datasets to explore a more easily generalizable difficulty/discrimination prediction pipeline. we also note that our analysis here focused on qa. however, there are many nlp tasks in which the difficulty or discrimination of an item may be important. our work here could naturally extend to these domains. finally, automatically predicting these traits without relying on user responses can engender a host of creative educational applications. future work can also leverage such predictive models to explore more efficient strategies for learning and evaluation.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Task Description\n",
            "\u001b[35mSection_title: 3  Dataset Curation\n",
            "\u001b[35mSection_title: 3.1  Sub-domain Splits\n",
            "\u001b[35mSection_title: 3.2  Common Preprocessing\n",
            "\u001b[35mSection_title: 3.3  Sub-domain Datasets\n",
            "\u001b[35mSection_title: 4  Baseline Model\n",
            "\u001b[35mSection_title: 5  Shared Task Submissions\n",
            "\u001b[35mSection_title: 5.1  D-Net (Li et al., 2019)\n",
            "\u001b[35mSection_title: 5.2  Delphi (Longpre et al., 2019)\n",
            "\u001b[35mSection_title: 5.3  HLTC (Su et al., 2019)\n",
            "\u001b[35mSection_title: 5.4  CLER\n",
            "\u001b[35mSection_title: 5.6  HierAtt (Osama et al., 2019)\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Main Results\n",
            "\u001b[35mSection_title: 6.2  Summary of Findings\n",
            "\u001b[35mSection_title: 6.3  Comparison to In-domain Fine-tuning\n",
            "\u001b[32mconclusion: we have presented the mrqa 2019 shared task, which focused on testing whether reading comprehension systems can generalize to examples outside of their training domain. many submissions improved significantly over our baseline, and investigated a wide range of techniques.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  The Task\n",
            "\u001b[35mSection_title: 4  Dataset Collection\n",
            "\u001b[35mSection_title: 5  Models\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 7.1  Qualitative Reasoning\n",
            "\u001b[35mSection_title: 7.2  Linguistic Phenomena\n",
            "\u001b[35mSection_title: 7.3  Use of the Annotations\n",
            "\u001b[32mconclusion: understanding and applying textual qualitative knowledge is an important skill for questionanswering, but has received limited attention, in part due the lack of a broad-coverage dataset to study the task. quartz aims to fill this gap by providing the first open-domain dataset of qualitative relationship questions, along with the requisite qualitative knowledge and a rich set of annotations. specifically, quartz removes the requirement, present in all previous qualitative reasoning work, that a fixed set of qualitative relationships be formally pre-specified. instead, quartz tests the ability of a system to find and apply an arbitrary relationship on the fly to answer a question, including when simple reasoning (arguments, polarities) is required.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Model architecture\n",
            "\u001b[35mSection_title: 2.1  Encoder module\n",
            "\u001b[35mSection_title: 2.2  Reasoning module\n",
            "\u001b[35mSection_title: 2.2.1  Build entity graph\n",
            "\u001b[35mSection_title: 2.2.2  Relational Reasoning\n",
            "\u001b[35mSection_title: 2.3  Prediction module\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Baseline Methods\n",
            "\u001b[35mSection_title: 3.3  Overall Result\n",
            "\u001b[35mSection_title: 3.4  Effect of Layer Number\n",
            "\u001b[35mSection_title: 3.5  Effect of Paragraph Number\n",
            "\u001b[32mconclusion: multi-paragraph reasoning is crucial for answering open-domain questions in practice, while it is still not considered in most existing openqa systems. in this work, we propose a novel openqa model kgnn, which performs reasoning over paragraphs via a knowledge enhanced graph neural network. experimental results show that kgnn outperforms strong baselines with a large margin on the hotpotqa dataset, and also has the ability to tackle more informative texts. we hope our work can shed some lights to the combination of knowledge graph and text for openqa.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  Baseline foundation models\n",
            "\u001b[35mSection_title: 2.  Extractive (\"quotes only\")\n",
            "\u001b[35mSection_title: 3.  Retrieval-augmented\n",
            "\u001b[35mSection_title: 4.  Learning from human feedback\n",
            "\u001b[35mSection_title: 2.  Volition, the mens rea problem, and who is liable\n",
            "\u001b[35mSection_title: 3.  AI's freedom of speech\n",
            "\u001b[35mSection_title: 1.  Section 230\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Learning to Talk with Agents\n",
            "\u001b[35mSection_title: 4  COMMAQA Benchmark\n",
            "\u001b[35mSection_title: 4.1  Complex Task: Multi-hop QA\n",
            "\u001b[35mSection_title: 4.2  Agents: QA models\n",
            "\u001b[35mSection_title: 4.3  Composition Operators\n",
            "\u001b[35mSection_title: 4.4  Building Examples\n",
            "\u001b[35mSection_title: 4.4.1  Details of Example Construction\n",
            "\u001b[35mSection_title: 4.5  COMMAQA Dataset\n",
            "\u001b[35mSection_title: 4.6  Final COMMAQA dataset\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Models\n",
            "\u001b[35mSection_title: 5.2  Results\n",
            "\u001b[35mSection_title: 5.3  Compositional Generalization\n",
            "\u001b[35mSection_title: 6  Future Work\n",
            "\u001b[32mconclusion: in this work we motivate a new challenge task of solving complex task by communicating with existing ai agents. developing approaches for this challenge, we argue, could allow for more generalizable, privacy-preserving and efficient models. towards this goal, we introduce a new benchmark dataset commaqa which involves multihop questions with three multi-hop reasoning challenges, all solvable by composing four qa agents. each agent has an internal knowledge base (similar to ai assistants or large lms) that can be queried via natural language queries. experiments with state-of-art language models indicated that they struggle to solve commaqa, even when provided with agents' internal knowledge. in contrast, a model that is able to learn to communicate with the agents, albeit using annotated decompositions, is able to solve this task. this indicates the need and potential of such approaches to solve complex tasks. we hope this dataset will enable future work on learning to communicate with agents without relying on this additional supervision. theory 1: what movies have people from the country $1 acted in? a1:select(textqa, _, \"who are from $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"which movies has {} been an actor in?\") theory 2: what movies have the directors from $1 directed? a1:select(textqa, _, \"who is from the country $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"which movies has {} directed?\") theory 3: what awards have movies produced by people born in $1 won? a1:select(textqa, _, \"who were born in the year $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"for which movies was {} the producer?\") a3:project_keys_flat_unique(tableqa, a2, \"which awards did the movie {} win?\") theory 4: what awards have movies written by people born in $1 won? a1:select(textqa, _, \"who were born in the year $1?\") a2:project_keys_flat_unique(textqa/tableqa, a1, \"what movies has {} written?\") a3:project_keys_flat_unique(tableqa, a2, \"which awards were given to {}?\")\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Question Answering using Entailment\n",
            "\u001b[35mSection_title: 3  Our Approach: Multee\n",
            "\u001b[35mSection_title: 3.1  Sentence Relevance Model\n",
            "\u001b[35mSection_title: 3.2  Multi-level Aggregation\n",
            "\u001b[35mSection_title: 3.2.1  Join Operations\n",
            "\u001b[35mSection_title: 3.3  Implementation Details\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Results\n",
            "\u001b[35mSection_title: 4.2  Ablations\n",
            "\u001b[35mSection_title: 4.3  Effect of Pre-training\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: using entailment for question answering has seen limited success. neural entailment models are designed and trained on tasks defined over sentence pairs, whereas qa often requires reasoning over longer texts spanning multiple sentences. we propose multee, a novel qa model that addresses this mismatch. it uses an existing entailment model to both focus on relevant sentences and aggregate information from these sentences. results on two challenging qa datasets, as well as our ablation study, indicate that entailment based qa can achieve state-of-the-art performance and is a promising direction for further research.\n",
            "\u001b[35mSection_title: 7  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2.2  Training objectives for the retriever\n",
            "\u001b[35mSection_title: 2.3  Pretext tasks\n",
            "\u001b[35mSection_title: 2.4  Efficient retriever fine-tuning\n",
            "\u001b[35mSection_title: 3  Related work\n",
            "\u001b[35mSection_title: 3.1  Retrieval in natural language processing\n",
            "\u001b[35mSection_title: 3.2  Few-shot learning\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Benchmarks\n",
            "\u001b[35mSection_title: 4.2  Technical details\n",
            "\u001b[35mSection_title: 4.3  Pre-training loss and tasks\n",
            "\u001b[35mSection_title: 4.4  Fine-tuning\n",
            "\u001b[35mSection_title: 4.5  Training and evaluating Atlas\n",
            "\u001b[35mSection_title: 4.5.1  MMLU Results\n",
            "\u001b[35mSection_title: 4.5.2  Open-domain Question Answering Results\n",
            "\u001b[35mSection_title: 4.5.3  FEVER Results\n",
            "\u001b[35mSection_title: 4.5.4  KILT Results\n",
            "\u001b[35mSection_title: 5.1  Interpretability and Leakage\n",
            "\u001b[35mSection_title: 5.2  Temporal Sensitivity and Updateability\n",
            "\u001b[35mSection_title: 5.2.1  Index Compression\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Analysis Setup\n",
            "\u001b[35mSection_title: 2.1  Extractive MRC\n",
            "\u001b[35mSection_title: 2.2  Evaluation Protocol\n",
            "\u001b[35mSection_title: 2.3  Datasets\n",
            "\u001b[35mSection_title: 2.4  MRC Models\n",
            "\u001b[35mSection_title: 3  Entity Name Substitution\n",
            "\u001b[35mSection_title: 3.1  Perturbation Pipeline\n",
            "\u001b[35mSection_title: 3.2  Candidate Name Collection\n",
            "\u001b[35mSection_title: 3.3  Perturbation Quality\n",
            "\u001b[35mSection_title: 4  Results and Analysis\n",
            "\u001b[35mSection_title: 4.1  Which datasets lead to less robustness?\n",
            "\u001b[35mSection_title: 4.5  Bias Exhibited by Person Names\n",
            "\u001b[35mSection_title: 4.6  Improving Robustness with Continual Pretraining\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this paper, we systematically study the robustness of mrc models to entity name substitution. specifically, we propose a substitution framework along with candidate names of different implications. we experiment with three pretrained language models on five mrc datasets. we find that models trained on distantly-supervised datasets are susceptible to entity name substitution, while models trained on human-annotated datasets are relatively robust, with gpe renaming harder than per and org renaming. the lack of robustness can be further attributed to model's overreliance on entity knowledge and name clues. we also find that spanbert, which is pretrained using span-level objectives, shows better robustness than bert and roberta. leveraging these insights, we study defense approaches based on continual pretraining and demonstrate that entity-based masking policies are beneficial to model's robustness. future works include systematically studying the effect of background knowledge in mrc, and developing more effective methods to improve the robustness of mrc models.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  MOTIVATION OF SELF-SUPERVISED LEARNING\n",
            "\u001b[35mSection_title: 3  GENERATIVE SELF-SUPERVISED LEARNING\n",
            "\u001b[35mSection_title: 3.1  Auto-regressive (AR) Model\n",
            "\u001b[35mSection_title: 3.2  Flow-based Model\n",
            "\u001b[35mSection_title: 3.3  Auto-encoding (AE) Model\n",
            "\u001b[35mSection_title: 3.3.1  Basic AE Model\n",
            "\u001b[35mSection_title: 3.3.2  Context Prediction Model (CPM)\n",
            "\u001b[35mSection_title: 3.3.3  Denoising AE Model\n",
            "\u001b[35mSection_title: 3.3.4  Variational AE Model\n",
            "\u001b[35mSection_title: 3.4  Hybrid Generative Models\n",
            "\u001b[35mSection_title: 3.4.1  Combining AR and AE Model.\n",
            "\u001b[35mSection_title: 3.4.2  Combining AE and Flow-based Models\n",
            "\u001b[35mSection_title: 3.5  Pros and Cons\n",
            "\u001b[35mSection_title: 4  CONTRASTIVE SELF-SUPERVISED LEARNING\n",
            "\u001b[35mSection_title: 4.1  Context-Instance Contrast\n",
            "\u001b[35mSection_title: 4.1.1  Predict Relative Position\n",
            "\u001b[35mSection_title: 4.1.2  Maximize Mutual Information\n",
            "\u001b[35mSection_title: 4.2  Instance-Instance Contrast\n",
            "\u001b[35mSection_title: 4.2.1  Cluster Discrimination\n",
            "\u001b[35mSection_title: 4.2.2  Instance Discrimination\n",
            "\u001b[35mSection_title: 4.3  Self-supervised Contrastive Pre-training for Semisupervised Self-training\n",
            "\u001b[35mSection_title: 4.4  Pros and Cons\n",
            "\u001b[35mSection_title: 5  GENERATIVE-CONTRASTIVE (ADVERSARIAL) SELF-SUPERVISED LEARNING\n",
            "\u001b[35mSection_title: 5.1  Generate with Complete Input\n",
            "\u001b[35mSection_title: 5.2  Recover with Partial Input\n",
            "\u001b[35mSection_title: 5.3  Pre-trained Language Model\n",
            "\u001b[35mSection_title: 5.4  Graph Learning\n",
            "\u001b[35mSection_title: 5.5  Domain Adaptation and Multi-modality Representation\n",
            "\u001b[35mSection_title: 5.6  Pros and Cons\n",
            "\u001b[35mSection_title: 6  THEORY BEHIND SELF-SUPERVISED LEARNING\n",
            "\u001b[35mSection_title: 6.1  GAN\n",
            "\u001b[35mSection_title: 6.1.1  Divergence Matching\n",
            "\u001b[35mSection_title: 2  Disentangled Representation\n",
            "\u001b[35mSection_title: 6.2  Maximizing Lower Bound\n",
            "\u001b[35mSection_title: 6.2.2  Mutual Information\n",
            "\u001b[35mSection_title: 6.3  Contrastive Self-supervised Representation Learning\n",
            "\u001b[35mSection_title: 6.3.1  Relationship with Supervised Learning\n",
            "\u001b[35mSection_title: 6.3.2  Understand Contrastive Loss\n",
            "\u001b[35mSection_title: 6.3.3  Generalization\n",
            "\u001b[35mSection_title: 7  DISCUSSIONS AND FUTURE DIRECTIONS\n",
            "\u001b[32mconclusion: this survey comprehensively reviews the existing selfsupervised representation learning approaches in natural language processing (nlp), computer vision (cv), graph learning, and beyond. self-supervised learning is the present and future of deep learning due to its supreme ability to utilize web-scale unlabeled data to train feature extractors and context generators efficiently. despite the diversity of algorithms, we categorize all self-supervised methods into three classes: generative, contrastive, and generative contrastive according to their essential training objectives. we introduce typical and representative methods in each category and sub-categories. moreover, we discuss the pros and cons of each category and their unique application scenarios. finally, fundamental problems and future directions of self-supervised learning are listed.\n",
            "\u001b[35mSection_title: 8  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction and Motivation\n",
            "\u001b[35mSection_title: 2  Knowledge Repositories and their creation\n",
            "\u001b[35mSection_title: 3  Reasoning with external knowledge: Models and Architectures\n",
            "\u001b[35mSection_title: 3.1  Extracting the External Knowledge\n",
            "\u001b[35mSection_title: 3.2  Models and architecture for NLQA with external knowledge\n",
            "\u001b[32mconclusion: in this paper we have surveyed 2 recent research on nlqa when external knowledge -beyond what is given in the test part -is needed in correctly answering the questions. we gave several motivating examples, mentioned several datasets, discussed available knowledge repositories and methods used in selecting needed knowledge from larger repositories, and analyzed and grouped several models and architectures of nlqa systems based on how the knowledge is expressed in them and the type of reasoning module used in them. although there have been some related recent surveys, such as [67], none of them focus on how exactly knowledge and reasoning is done in the nlqa systems dealing with datasets that require external knowledge. our survey touched upon knowledge types that include structured knowledge, textual knowledge, knowledge embedded in a neural network, knowledge provided via specially constructed examples and combinations of them. we explored how symbolic, neural and mixed models process and reason with such knowledge. based on our observations for various models following are some questions and future directions. following up on the lifecycleqa dataset where some concepts, such as \"indicates\", were manually defined, several questions -with partial answers -may come to mind. (i) how big is the list of such concepts? if a list of them is made and they are defined then these definitions can be directly used or compiled into neural models. can cyc and the book [27] be starting points in this direction? (ii) can these definitions be learned from data? how? unless one only focuses on specific datasets, the challenge in learning these definitions would be that for each of them specialized examples would have to be created. (iii) when is it easier to just write the definitions in a logical language? when is it easier to learn from data sets? some concepts are easy to define logically but may require lot of examples to teach a system. there are concepts whose definitions took decades for researchers to formalize. an example is the solutions of frame problem. so it is easier to use that formalization rather than learning from scratch. on the other hand the notion of \"cause\" is still being fine tuned. another direction of future work centers around the question of how well neural network can do reasoning; what kind of reasoning they can do well and what are the challenges ?\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Directions\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Dataset Diagnosis by Input Ablation\n",
            "\u001b[35mSection_title: 3.1  Formulation\n",
            "\u001b[35mSection_title: 3.2  Example Set of Requisite Skills\n",
            "\u001b[35mSection_title: 4  Experiments and Further Analyses\n",
            "\u001b[35mSection_title: 4.1  Experimental Settings\n",
            "\u001b[35mSection_title: 4.2  Results of Reading and Reasoning Skills\n",
            "\u001b[35mSection_title: 4.3  Further Analyses\n",
            "\u001b[35mSection_title: 5  Qualitative Evaluation\n",
            "\u001b[35mSection_title: 5.1  Human Solvability after the Ablation\n",
            "\u001b[35mSection_title: 5.2  Reconstructability of Ablated Features\n",
            "\u001b[35mSection_title: 6  Discussion\n",
            "\u001b[32mconclusion: existing analysis work in mrc is largely concerned with evaluating the capabilities of systems.by contrast, in this work, we proposed an analysis methodology for the benchmarking capacity of datasets.our methodology consists of input-ablation tests, in which each ablation method is associated with a skill requisite for mrc.we exemplified 12 skills and analyzed 10 datasets.the experimental results suggest that for benchmarking sophisticated nlu, datasets should be more carefully designed to ensure that questions correctly evaluate the intended skills.in future work, we will develop a skill-oriented method for crowdsourcing questions.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Methodology\n",
            "\u001b[35mSection_title: 2.1  Multi-hop Encoder\n",
            "\u001b[35mSection_title: 2.1.1  Answer-aware Context Encoder\n",
            "\u001b[35mSection_title: 2.1.2  GCN-based Entity-aware Answer Encoder\n",
            "\u001b[35mSection_title: 2.1.3  Encoder Reasoning Gate\n",
            "\u001b[35mSection_title: 2  Maxout Pointer Decoder\n",
            "\u001b[35mSection_title: 2.3  Breadth-First Search Loss\n",
            "\u001b[35mSection_title: 3  Experiment\n",
            "\u001b[35mSection_title: 3.1  Dataset\n",
            "\u001b[35mSection_title: 3.2  Baselines\n",
            "\u001b[35mSection_title: 3.3  Implementation Details\n",
            "\u001b[35mSection_title: 3.4  Automatic Evaluation\n",
            "\u001b[35mSection_title: 3.4.1  Metrics\n",
            "\u001b[35mSection_title: 3.4.2  Results and Analysis\n",
            "\u001b[35mSection_title: 3.4.3  Ablation Study\n",
            "\u001b[35mSection_title: 3.5  Human Evaluation\n",
            "\u001b[35mSection_title: 3.6  Case Study\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: multi-hop qg task is more challenging and worthy of exploration compared to conventional singlehop qg. to address the additional challenges in multi-hop qg, we propose mulqg, which does multi-hop context encoding with graph convolutional network and encoding fusion via a gated reasoning module. to the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentencelevel information. the model performance on hot-potqa dataset demonstrates its effectiveness on aggregating scattered pieces of evidence across the paragraphs and fusing information effectively to generate multi-hop questions. the strong reasoning ability of the multi-hop encoder in the mulqa model can potentially be leveraged in complex generation tasks for the future work.  table a1: performance comparison between our multqg model and fine-tuning state-of-the-art large pre-trained models on hotpotqa test set.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  The DomainInv Framework\n",
            "\u001b[35mSection_title: 4.1  Overview\n",
            "\u001b[35mSection_title: 4.2  Domain Invariant Fine Tuning\n",
            "\u001b[35mSection_title: 4.3  Adversarial Label Correction\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Experimental Results\n",
            "\u001b[35mSection_title: 5.3  Ablation Studies\n",
            "\u001b[32mconclusion: in this paper we proposed a novel qa domain adaptation framework called domaininv, an unsupervised algorithm which does not require the use of labeled target domain, neither it depends on the synthetic data or pseudo labeled target domain. domaininv uses 1) domain invariant fine tuning which fine tunes the qa model using the target style on the source domain and 2) adversarial label correction which identifies the target distributions which are still far apart from source domain and optimize the feature generator to bring them closer near to source support class wisely. evaluation of domaininv showed that it outperforms all the baselines and achieves the superior performance establishing the new benchmark on qa domain adaptation.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 4  The DomainInv Framework\n",
            "\u001b[35mSection_title: 4.1  Overview\n",
            "\u001b[35mSection_title: 4.2  Domain Invariant Fine Tuning\n",
            "\u001b[35mSection_title: 4.3  Adversarial Label Correction\n",
            "\u001b[35mSection_title: 5.1  Experimental Setup\n",
            "\u001b[35mSection_title: 5.2  Experimental Results\n",
            "\u001b[35mSection_title: 5.3  Ablation Studies\n",
            "\u001b[32mconclusion: in this paper we proposed a novel qa domain adaptation framework called domaininv, an unsupervised algorithm which does not require the use of labeled target domain, neither it depends on the synthetic data or pseudo labeled target domain. domaininv uses 1) domain invariant fine tuning which fine tunes the qa model using the target style on the source domain and 2) adversarial label correction which identifies the target distributions which are still far apart from source domain and optimize the feature generator to bring them closer near to source support class wisely. evaluation of domaininv showed that it outperforms all the baselines and achieves the superior performance establishing the new benchmark on qa domain adaptation.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Refine Networks (RefNet) Model\n",
            "\u001b[35mSection_title: 2.1  Passage and Answer Encoder\n",
            "\u001b[35mSection_title: 2.2  Preliminary and Refinement Decoders\n",
            "\u001b[35mSection_title: 2.3  Reward-RefNet\n",
            "\u001b[35mSection_title: 2.4  Copy Module\n",
            "\u001b[35mSection_title: 3  Experimental Details\n",
            "\u001b[35mSection_title: 3.1  Datasets\n",
            "\u001b[35mSection_title: 3.2  Implementation Details\n",
            "\u001b[35mSection_title: 3.3  Evaluation\n",
            "\u001b[35mSection_title: 4  Results and Discussions\n",
            "\u001b[35mSection_title: 4.1  RefNet's performance across datasets\n",
            "\u001b[35mSection_title: 4.2  Human Evaluations\n",
            "\u001b[35mSection_title: 4.3  Analysis of Refinement Decoder and Preliminary Decoder\n",
            "\u001b[35mSection_title: 4.4  Analysis of Reward-RefNet\n",
            "\u001b[35mSection_title: 5  Related Work\n",
            "\u001b[32mconclusion: in this work, we proposed refine networks (refnet) for question generation to focus on refining and improving the initial version of the generated question. our proposed refnet model consisting of a preliminary decoder and a refinement decoder with dual attention network outperforms the existing state-of-the-art models on the squad, hotpot-qa and drop datasets. along with automated evaluations, we also conducted human evaluations to validate our findings. we further showed that using reward-refnet improves the initial draft on specific aspects like fluency, answerability and originality. as a future work, we would like to extend refnet to have the ability to decide whether a refinement is needed on the generated initial draft.\n",
            "\u001b[35mSection_title: 6  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Notations\n",
            "\u001b[35mSection_title: 3.2  Soft-labeling for latent context C\n",
            "\u001b[35mSection_title: 3.3  Block Attention\n",
            "\u001b[35mSection_title: 3.4  Answer-span Prediction\n",
            "\u001b[35mSection_title: 3.5  Property of Block Attention\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Hyper-parameter Settings\n",
            "\u001b[35mSection_title: 5  Results & Discussion\n",
            "\u001b[35mSection_title: 5.1  Reading Comprehension\n",
            "\u001b[35mSection_title: 5.2  Performance on Passages with Multi-mentioned Answers\n",
            "\u001b[35mSection_title: 5.3  Supporting Facts Prediction\n",
            "\u001b[35mSection_title: 5.4  Analysis on λ\n",
            "\u001b[35mSection_title: 5.5  Space and Time Complexity\n",
            "\u001b[32mconclusion: in this paper, we showed the importance of predicting an answer with the correct context of a given question. we proposed blanc with two novel ideas: context word prediction task and a block attention method that identifies an answer within the context of a given question. the context words prediction task labels latent context words with the labeled answer-span and is used in a multi-task learning manner. block attention models the latent context words with negligible extra parameters and training/inference time. we showed that blanc increases reading comprehension performance, and we verify that the performance gain increases for complex examples (i.e., when the answer occurs two or more times in the passage). also, we showed the generalizability of blanc and its contextaware performance with the zero-shot supporting fact prediction task on the hotpotqa dataset. \n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 4  Case Studies\n",
            "\u001b[35mSection_title: 4.1  Overnight\n",
            "\u001b[35mSection_title: 4.2  Break\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[35mSection_title: 6  Further Related Work\n",
            "\u001b[32mconclusion: we wish to rapidly develop semantic parsers in new domains. to this end, we have demonstrated that constrained decoding of powerful language models can enable the paraphrasing of user utterances into a controlled sublanguage, which may then be mapped to a task-specific representation. with small hundreds of examples we are able to quickly bootstrap models for a variety of datasets, enabling future work that explores human in the loop interactions for iterative model refinement. \n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Briefs for Fact Checking\n",
            "\u001b[35mSection_title: 2.1  Passage Briefs\n",
            "\u001b[35mSection_title: 2.2  Entity Briefs\n",
            "\u001b[35mSection_title: 2.3  Question Answering Briefs\n",
            "\u001b[35mSection_title: 3  QABrief Dataset\n",
            "\u001b[35mSection_title: 3.1  Question Generation\n",
            "\u001b[35mSection_title: 3.2  Question Answering\n",
            "\u001b[35mSection_title: 3.3  QABrief Dataset Statistics\n",
            "\u001b[35mSection_title: 4  QABrief Model\n",
            "\u001b[35mSection_title: 4.1  Question Generation\n",
            "\u001b[35mSection_title: 4.2  Question Answering\n",
            "\u001b[35mSection_title: 5  Experimental Setup\n",
            "\u001b[35mSection_title: 5.1  Human Evaluation\n",
            "\u001b[35mSection_title: 5.2  Automatic Evaluation of Model Quality\n",
            "\u001b[35mSection_title: 5.3  Model Details\n",
            "\u001b[35mSection_title: 6  Results\n",
            "\u001b[35mSection_title: 6.1  Briefs Increase Fact Checking Quality\n",
            "\u001b[35mSection_title: 6.2  QABriefs are Preferred\n",
            "\u001b[35mSection_title: 6.3  Generating QABriefs with QABRIEFER\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[35mSection_title: 8  Discussion\n",
            "\u001b[32mconclusion: we propose the concept of fact checking briefs, to be read before performing a fact check. crucially, we develop qabriefer and release the accompanying qabriefdataset, to create qabriefs. we show in extensive empirical studies with crowdworkers and volunteers that qabriefs can improve accuracy and efficiency of fact checking. \n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\u001b[35mSection_title: 10.1  Dataset Analysis\n",
            "\u001b[35mSection_title: 10.2  Additional Human Evaluation Results\n",
            "\u001b[35mSection_title: 10.3  Model Training Details\n",
            "\u001b[35mSection_title: 10.4  Dataset Collection Details\n",
            "\u001b[35mSection_title: 10.4.1  Recruitment for the Task\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Base Model\n",
            "\u001b[35mSection_title: 3.2  Adaptive Learning\n",
            "\u001b[35mSection_title: 3.2.1  Sensitivity normalization\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Setup\n",
            "\u001b[35mSection_title: 4.2  Main Results\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[35mSection_title: 4.3.1  Ablation Study\n",
            "\u001b[35mSection_title: 4.3.2  Task Specialization\n",
            "\u001b[35mSection_title: 4.3.3  Additional Benchmark\n",
            "\u001b[32mconclusion: multitask retrieval has compelling practical advantages such as model simplicity and memory efficiency, but it lags behind task-specific retrieval in the existing literature. we have shown that it is possible to significantly improve the performance of multitask retrieval by promoting task specialization. the key steps are the use of a base model optimized for multitasking with appropriate prompting and a per-parameter adaptive learning technique that upweights the task gradients by the parameters' sensitivity to the task losses. we have achieved strong results on the kilt retrieval benchmark. \n",
            "\u001b[35mSection_title: 5  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 2.2.  Understanding the properties of language representation models through experimental studies\n",
            "\u001b[35mSection_title: 3.  Research questions\n",
            "\u001b[35mSection_title: 4.1.  Language representation models\n",
            "\u001b[35mSection_title: 4.2.  Multiple-choice question answering using LRMs\n",
            "\u001b[35mSection_title: 4.2.2.  Fine-tuning language representation models for multiple-choice question answering\n",
            "\u001b[35mSection_title: 4.3.  Benchmark construction\n",
            "\u001b[35mSection_title: 4.3.1.  High-value and low-value sets of items\n",
            "\u001b[35mSection_title: 4.3.2.  Value questions\n",
            "\u001b[35mSection_title: 4.3.3.  Bet questions\n",
            "\u001b[35mSection_title: 4.4.  Experimental set-up 4.4.1. RQ1: preference elicitation\n",
            "\u001b[35mSection_title: 4.4.2.  RQ2: thinking in bets without task-specific fine-tuning\n",
            "\u001b[35mSection_title: 4.4.3.  RQ3: thinking in bets after task-specific fine-tuning\n",
            "\u001b[35mSection_title: 5.  Results\n",
            "\u001b[35mSection_title: 5.1.  RQ1: preference elicitation\n",
            "\u001b[35mSection_title: 5.2.  RQ2: thinking in bets without task-specific fine-tuning\n",
            "\u001b[35mSection_title: 5.3.  RQ3: thinking in bets after task-specific fine-tuning\n",
            "\u001b[35mSection_title: 5.4.  Summary of results\n",
            "\u001b[35mSection_title: 6.  Discussion\n",
            "\u001b[32mconclusion: modern lrms, based on transformer neural networks, have rapidly exceeded the previous state-of-theart on a range of natural language understanding tasks, including question answering, text summarization and information extraction [104][105][106]. in this article, we addressed the question of whether such lrms can be adapted for (approximately) rational decision-making and preference elicitation. in the cognitive science literature, such decision-making is often evaluated using bets. given the near human-like performance of lrms on language-based problems, we formulated a set of rqs to specifically test whether: (i) lrms have a distinct preference for high-value items over lowvalue items, especially when the items were not seen during training, and after stratifying by the format of the questions, (ii) lrms can make, or be taught to make, (approximately rational) bets in a generalizable manner, including when an lrm has been fine-tuned on one 'modality' of bet but is evaluated on another modality.\n",
            "\u001b[35mSection_title: 7.  Conclusion and future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Background\n",
            "\u001b[35mSection_title: 2.1  Dual Encoder and dense retrieval\n",
            "\u001b[35mSection_title: 2.2  BEIR generalization task\n",
            "\u001b[35mSection_title: 3.1  T5 dual encoder\n",
            "\u001b[35mSection_title: 3.2  Multi-stage training\n",
            "\u001b[35mSection_title: 4.2  Configurations\n",
            "\u001b[35mSection_title: 4.3  Models for comparison\n",
            "\u001b[35mSection_title: 5  Evaluation Results\n",
            "\u001b[35mSection_title: 5.1  Results on MS Marco\n",
            "\u001b[35mSection_title: 5.2  Results on BEIR generalization tasks\n",
            "\u001b[35mSection_title: 5.3  Data efficiency for large retrievers\n",
            "\u001b[35mSection_title: 6  Ablation Study and Analysis\n",
            "\u001b[35mSection_title: 6.1  Scaling up in different training stages\n",
            "\u001b[35mSection_title: 6.2  Importance of the fine-tuning dataset\n",
            "\u001b[35mSection_title: 6.3  Different pre-training strategies\n",
            "\u001b[35mSection_title: 6.4  Document length vs. model capacity\n",
            "\u001b[35mSection_title: 6.5  Scaling up with different bottleneck size\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[35mSection_title: 8  Inference latency\n",
            "\u001b[32mconclusion: this paper presents the generalizable t5 retriever (gtr), a scaled-up dual encoder model with a fixed-size dot-product bottleneck layer. we show that scaling up the model size brings significant improvement on retrieval performance across the board on the beir zero-shot retrieval benchmark, especially for out-of-domain generalization. the gtr-xxl model performs at the level of state-ofthe-art performance on beir, outperforming many models that use earlier interactions between queries and documents. this sheds light on the research direction to continue enhancing the single vector representation model through better backbone encoders. moreover, our in-depth analysis reveals the impact of scaling up under the scenarios of different training stages, pre-training strategies, fine-tuning datasets, and bottleneck sizes, as well as how scaling up influences the retrieved document lengths. our findings can inform future work and is an integral part of the joint effort to improve dual encoder models.\n",
            "\u001b[35mSection_title: 9  Conclusion\n",
            "\u001b[32mlimitation: in our work, we focus on standard dual encoder training and have not investigated other techniques such as distillation. there have been shown distillation is a strong recipe to improve the dense retrieval models on out-of-domain performance (santhanam et al., 2021;formal et al., 2021). we hope to investigate whether the scaling effect could also benefit distillation if we scale up the student dual encoders. in addition, we only focus on english-only corpus and we leave the exploration of scaling up dense retrievers for multi-lingual corpus to future work. table 7 shows the comparisons of gtr models and the baselines. note that the best rocketqa model used additional augmented data other than ms marco to improve the model performance while all others do not. our best gtr-xxl models outperforms rocketqa on both mrr and recall.  a.2 recall on beir  \n",
            "\u001b[35mSection_title: 10  Limitations\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Method\n",
            "\u001b[35mSection_title: 3.1  Open-Domain QA as a POMDP\n",
            "\u001b[35mSection_title: 3.2  Agent\n",
            "\u001b[35mSection_title: 3.2.1  Belief Module\n",
            "\u001b[35mSection_title: 3.2.2  Policy Module\n",
            "\u001b[35mSection_title: 3.3  Training\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setup\n",
            "\u001b[35mSection_title: 4.2  Results\n",
            "\u001b[35mSection_title: 4.3  Analysis\n",
            "\u001b[32mconclusion: this work presents an adaptive informationseeking approach for open-domain question answering, called aiso. it models the open-domain qa task as a pomdp, where the environment contains a large corpus and the agent is asked to sequentially select retrieval function and reformulate query to collect the evidence. aiso achieves stateof-the-art results on two public datasets, which demonstrates the necessity of different retrieval functions for different questions. in the future, we will explore other adaptive retrieval strategies, like directly optimizing various informationseeking metrics by using reinforcement learning techniques.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Global BERT-based Transformer Architecture\n",
            "\u001b[35mSection_title: 2.1  Stacked Propagation Layers\n",
            "\u001b[35mSection_title: 2.2  Output Layer\n",
            "\u001b[35mSection_title: 3  Experiments\n",
            "\u001b[35mSection_title: 3.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 3.2  Label Generation\n",
            "\u001b[35mSection_title: 3.3  Baseline Models\n",
            "\u001b[35mSection_title: 3.4  Implementation details\n",
            "\u001b[35mSection_title: 3.5  Results\n",
            "\u001b[35mSection_title: 4  Related Work\n",
            "\u001b[32mconclusion: in this paper, we have introduced a novel transformer-based model for long document summarization based on propagation layers that spread information between multiple transformer windows. this model preserves the architecture of commonly used pre-trained language models, thus allowing the transfer of parameters. an evaluation, conducted on top of the bert model in the context of an extractive summarization task, further revealed its effectiveness in dealing with long documents compared to other adaptations of bert and previously proposed models. in the future, we plan to adapt our model to other tasks that require understanding long documents, as question-answering and document-scale machine translation. a baselines: implementation details bertsumext:\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3.1  Dual-Tower Architecture\n",
            "\u001b[35mSection_title: 3.2  Constructing Positive Instances\n",
            "\u001b[35mSection_title: 3.3  Iterative Contrastive Learning\n",
            "\u001b[35mSection_title: 3.4  Lexicon-Enhanced Dense Retrieval\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 4.1  Experimental Setting\n",
            "\u001b[35mSection_title: 4.2  Experimental Results\n",
            "\u001b[35mSection_title: 4.2.1  Effect of Iterative Contrastive Learning\n",
            "\u001b[35mSection_title: 4.2.2  Effect of Pretraining and Lexicon-Enhanced Dense Retrieval\n",
            "\u001b[35mSection_title: 4.3  Case Study\n",
            "\u001b[32mconclusion: in this paper, we introduce laprador, an unsupervised pretrained dense retriever that achieves state-of-the-art performance on the zero-shot text retrieval benchmark beir.we propose iterative contrastive learning (icol) for efficiently training laprador and lexicon-enhanced dense retrieval (ledr) to combine lexical matching with laprador.our experiments verify the effectiveness of both icol and ledr, shedding light on a new paradigm for unsupervised text retrieval.for future work, we plan to extend unsupervised laprador to multilingual and multi-modal retrieval.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Approach\n",
            "\u001b[35mSection_title: 4  Proof of Concept\n",
            "\u001b[35mSection_title: 4.1  Keyword Mapping\n",
            "\u001b[35mSection_title: 4.2  Question Selection\n",
            "\u001b[35mSection_title: 4.3  Question Classification\n",
            "\u001b[35mSection_title: 4.4  Evaluation\n",
            "\u001b[32mconclusion: we have shown that bloom's revised taxonomy can be transferred from pedagogy to qa systems. the diagonal of the matrix is a determinant for defining complex questions, ranging from simple questions in the upper left to complex questions on the bottom right. for the proof of concept, we added pos tags to the questions as syntactic information to train a domain-independent classifier for question complexity. we argued that question words also contribute to complexity, so they were not transformed. although the unequal distribution of the training data only allowed a binary classification for two representative classes a1 and d3, the classifier already provides good results for computing question complexity.\n",
            "\u001b[35mSection_title: 5  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  QA Graphs and Architecture\n",
            "\u001b[35mSection_title: 4  Experiments\n",
            "\u001b[35mSection_title: 5  Analysis\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Global slot memory augmented Transformer with hierarchical attention\n",
            "\u001b[35mSection_title: 3  Masked language modeling\n",
            "\u001b[35mSection_title: 3.1  Experiments setup\n",
            "\u001b[35mSection_title: 3.2  Experimental MLM task results\n",
            "\u001b[35mSection_title: 4  Fine tuning -question answering task\n",
            "\u001b[35mSection_title: 5  Ablation study\n",
            "\u001b[35mSection_title: 5.1  Ablation study results\n",
            "\u001b[32mconclusion: this paper presented a study of our proposed model on two new tasks; masked language modeling as pretraining task and question answering task using hot-potqa as finetuning task. experimental results showed good performance of the model on masked language modeling using linear learning rate. the prposed model with chunked input outperformed t5 as baseline. this approves previous results on translation task where the proposed model overcomes the base line.\n",
            "\u001b[35mSection_title: 6  Conclusion and future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  INTRODUCTION\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  Traditional Pipeline IR Framework\n",
            "\u001b[35mSection_title: 2.2  Model-based IR Approaches\n",
            "\u001b[35mSection_title: 2.3  Knowledge-Intensive Language Tasks\n",
            "\u001b[35mSection_title: 3  OUR APPROACH\n",
            "\u001b[35mSection_title: 3.1  Motivation\n",
            "\u001b[35mSection_title: 3.2  Model Architecture\n",
            "\u001b[35mSection_title: 3.2.1  Query Encoder.\n",
            "\u001b[35mSection_title: 3.2.2  Identifier\n",
            "\u001b[35mSection_title: 3.3  Pre-training Tasks\n",
            "\u001b[35mSection_title: 3.3.1  Inner Sentence Selection (ISS)\n",
            "\u001b[35mSection_title: 3.3.2  Lead Paragraph Selection (LPS)\n",
            "\u001b[35mSection_title: 3.3.3  Hyperlink Identifier Prediction (HIP).\n",
            "\u001b[35mSection_title: 3.3.4  Learning\n",
            "\u001b[35mSection_title: 4  EXPERIMENTAL SETTINGS\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Baselines\n",
            "\u001b[35mSection_title: 4.2.1  Traditional IR Models.\n",
            "\u001b[35mSection_title: 4.2.2  Generative Retrieval\n",
            "\u001b[35mSection_title: 4.3  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.4  Implementation Details\n",
            "\u001b[35mSection_title: 4.4.1  Model Architecture.\n",
            "\u001b[35mSection_title: 4.4.2  Pre-training Process.\n",
            "\u001b[35mSection_title: 4.4.3  Fine-tuning Process.\n",
            "\u001b[35mSection_title: 5  EXPERIMENTAL RESULTS\n",
            "\u001b[35mSection_title: 5.1  Baseline Comparison\n",
            "\u001b[35mSection_title: 5.2  Impact of Fine-tuning Strategies\n",
            "\u001b[35mSection_title: 5.3  Impact of Pre-training Tasks\n",
            "\u001b[35mSection_title: 5.4  Zero-and Low-Resource Settings\n",
            "\u001b[35mSection_title: 5.5  Memory and Inference Efficiency\n",
            "\u001b[35mSection_title: 5.6  Case Study\n",
            "\u001b[32mconclusion: in this paper, we have proposed corpusbrain, a novel pre-trained generative retrieval model to encode all information about the corpus into its parameters. to train such a strong generative model, we delicately devised a set of pre-training tasks to emphasize different aspects of semantics between queries and documents. the key idea is to sample a context from one document as a pseudo query and generate the document identifiers of source or destination documents based on hyperlinks. corpusbrain just needs to pre-train one model and could be then adapted to improve a diversity of downstream kilt tasks without the need of constructing additional index. through experiments on the kilt benchmark in terms of the retrieval task, corpusbrain achieved significant improvements over strong baseline approaches. we also showed that corpusbrain can achieve strong performance under both the zero-and low-resource settings.\n",
            "\u001b[35mSection_title: 6  CONCLUSION\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related work\n",
            "\u001b[35mSection_title: 3  Dataset collection & analysis 3.1 Annotation process\n",
            "\u001b[35mSection_title: 3.2  Statistics\n",
            "\u001b[35mSection_title: 3.3  Challenges raised by adversarial questions\n",
            "\u001b[35mSection_title: 4  Evaluation metrics\n",
            "\u001b[35mSection_title: 5  French monolingual experiments\n",
            "\u001b[35mSection_title: 5.1  Baselines\n",
            "\u001b[35mSection_title: 5.2  Comparison with FQuAD1.1 scores\n",
            "\u001b[35mSection_title: 5.3  Learning curves\n",
            "\u001b[35mSection_title: 5.4  Baseline performances by question category\n",
            "\u001b[35mSection_title: 6  Multilingual experiments\n",
            "\u001b[32mconclusion: in this paper, we introduced fquad2.0, a qa dataset with both answerable questions (coming from fquad1.1) and 17,000+ newly annotated unanswerable questions, for a total of almost 80,000 questions. to the best of our knowledge, this is the first french (and, perhaps most importantly, non-english) adversarial question answering dataset. we trained various baseline models using camembert architectures. our best model, a finetuned camembert large , reaches 83% f1 score and 82.3% f1 no ans , the latter measuring its ability to distinguish answerable questions from unanswerable ones. the study of learning curves with respect to the number of samples used for training such models show that our baseline models would benefit from additional unanswerable questions. in the future, we plan to collect additional samples to expand fquad2.0. for comparison, its english cousin squad2.0 (rajpurkar et al., 2018) contains 53,775 unanswerable questions. such a large-scale dataset would of course enable the acquisition of even better models as the ones presented in sec-tions 5 and 6. as far as data collection is concerned, we could also collect additional answers for each unanswerable question. by following the same procedure as in d' hoffschmidt et al. (2020), this would allow for the computation of human performance, measuring the inherent difficulty of the challenge provided by fquad2.0.\n",
            "\u001b[35mSection_title: 7  Conclusion & future work\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Verify-and-Edit Framework\n",
            "\u001b[35mSection_title: 3.1  Deciding when to edit\n",
            "\u001b[35mSection_title: 3.2  How to edit a specific rationale\n",
            "\u001b[35mSection_title: 3.3  Answering again\n",
            "\u001b[35mSection_title: 4  Experiment Setup\n",
            "\u001b[35mSection_title: 4.1  Reasoning tasks\n",
            "\u001b[35mSection_title: 4.2  Compared methods\n",
            "\u001b[35mSection_title: 5.2  Results on HotpotQA\n",
            "\u001b[35mSection_title: 5.3  Results on 2WikiMultiHop\n",
            "\u001b[35mSection_title: 5.4  Results on fact verification\n",
            "\u001b[35mSection_title: 5.5  Cost considerations\n",
            "\u001b[35mSection_title: 5.6  Evaluating the reasoning chains with human study\n",
            "\u001b[35mSection_title: 5.7  Ablation study: editing at different consistency thresholds\n",
            "\u001b[32mconclusion: in this paper, we introduce a verify-and-edit framework for open-domain question-answering. it is a first attempt to post-edit cot-style reasoning chains for better end-task performance. by combining knowledge retrieval with reasoning, the framework edits cots in a natural and conversational way, which enhances prediction factuality. combined with google search, the framework also shows a promising direction that combines the open-generation ability of state-of-art llms with the updated facts provided by search engines.\n",
            "\u001b[35mSection_title: 6  Conclusions\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Distribution Shifts in Dense Retrieval\n",
            "\u001b[35mSection_title: 3.1  Preliminaries on Dense Retrieval\n",
            "\u001b[35mSection_title: 3.2  ZeroDR and Distribution Shifts\n",
            "\u001b[35mSection_title: 4  COCO-DR Method\n",
            "\u001b[35mSection_title: 4.1  Continuous Contrastive Pretraining\n",
            "\u001b[35mSection_title: 4.2  Distributionally Robust Optimization\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Experimental Setups\n",
            "\u001b[35mSection_title: 5.2  Overall Results\n",
            "\u001b[35mSection_title: 5.3  Ablation Study\n",
            "\u001b[35mSection_title: 5.4  Influence of COCO Pretraining\n",
            "\u001b[35mSection_title: 5.5  Influence of Implicit DRO\n",
            "\u001b[32mconclusion: coco-dr improves zerodr accuracy by combating the distribution shifts using continuous contrastive learning and implicit distributionally robust optimization. coco helps models better capture the sequence representations of target corpora in pretraining. implicit dro improves model robustness by reweighting query clusters in fine-tuning.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Framework\n",
            "\u001b[35mSection_title: 2.1  Retriever\n",
            "\u001b[35mSection_title: 2.2  Reasoning Chain Generator\n",
            "\u001b[35mSection_title: 2.3  Reader\n",
            "\u001b[35mSection_title: 2.4  Chain-Aware Loss\n",
            "\u001b[35mSection_title: 2.4.1  Reasoning Chain Modeling\n",
            "\u001b[35mSection_title: 2.4.2  Supervised Training\n",
            "\u001b[35mSection_title: 2.4.3  Distantly Supervised Training\n",
            "\u001b[35mSection_title: 2.5  Training & Inference\n",
            "\u001b[35mSection_title: 3  Experimental Setup\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  QA Performance\n",
            "\u001b[35mSection_title: 4.2  Iterative Retriever Performance\n",
            "\u001b[35mSection_title: 5  Discussions on Explainability\n",
            "\u001b[35mSection_title: 5.1  Effect of Chain Modeling\n",
            "\u001b[35mSection_title: 5.2  Scalability on Chain length\n",
            "\u001b[35mSection_title: 5.3  Case Study on Reasoning\n",
            "\u001b[35mSection_title: 6  Related Work\n",
            "\u001b[32mconclusion: we propose a novel chain guided retriever-reader framework for multi-hop qa. our modeling for the reasoning chains is effective to find both direct and indirect facts and is less likely to introduce noise. moreover, our framework is corpus-independent and is capable of handling the setting without any ground-truth annotations. further analysis and discussions also elucidate some of the inner workings of our framework while maintaining the explainability at the same time.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.1  Encoder/Decoder architecture\n",
            "\u001b[35mSection_title: 1.2  Attention\n",
            "\u001b[35mSection_title: 1.3  Foundation vs Fine-tuned models\n",
            "\u001b[35mSection_title: 1.4  The Impact of Transformers\n",
            "\u001b[35mSection_title: 1.5  A Note on Diffusion models\n",
            "\u001b[35mSection_title: 2  The Transformers Catalog\n",
            "\u001b[35mSection_title: 2.1  Features of a Transformer\n",
            "\u001b[35mSection_title: 2.1.1  Pretraining Architecture\n",
            "\u001b[35mSection_title: 2.1.2  Pretraining or Finetuning Task\n",
            "\u001b[35mSection_title: 2.  Causal Language Modeling (Causality-masked LM):\n",
            "\u001b[35mSection_title: 3.  Prefix Language Modeling (Prefix LM):\n",
            "\u001b[35mSection_title: 4.  Masked Language Modeling (MLM):\n",
            "\u001b[35mSection_title: 5.  Permuted Language Modeling (PLM):\n",
            "\u001b[35mSection_title: 8.  Next Sentence Prediction (NSP):\n",
            "\u001b[35mSection_title: 2.1.3  Application\n",
            "\u001b[35mSection_title: 2.2  Catalog table\n",
            "\u001b[35mSection_title: 2.3  Family Tree\n",
            "\u001b[35mSection_title: 2.4  Chronological timeline\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Semi-structured Explanation\n",
            "\u001b[35mSection_title: 3  Reward Engineering for SEG\n",
            "\u001b[35mSection_title: 3.1  Reward Model\n",
            "\u001b[35mSection_title: 3.2  Reward Metric\n",
            "\u001b[35mSection_title: 3.3  Reward Aggregation\n",
            "\u001b[35mSection_title: 4  Experiment\n",
            "\u001b[35mSection_title: 4.1  Datasets\n",
            "\u001b[35mSection_title: 4.2  Models\n",
            "\u001b[35mSection_title: 4.2.1  Evaluation Metrics\n",
            "\u001b[35mSection_title: 4.3  Results\n",
            "\u001b[35mSection_title: 4.4  Effect of Different Metrics in R m\n",
            "\u001b[35mSection_title: 4.5  Effect of KL Coefficient β\n",
            "\u001b[35mSection_title: 4.6  Effect of Weight Factor α\n",
            "\u001b[35mSection_title: 4.7  Discussion\n",
            "\u001b[32mconclusion: in this work, we focused on the semi-structured explanation generation task and proposed to use train a single model with sft+rl to generate both answers and structured explanations.we highlight the inadequacy of sft in performing this complex task, and proposed a carefully designed reward engineering method in rl to better address this problem.we investigated different reward aggregation methods and conduct extensive experiments under different settings to better highlight the dynamic of the rl objective function and reward model choices.our method achieves the new sota results on two seg benchmarks, explagraph and copa-sse.\n",
            "\u001b[35mSection_title: 5  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1.  INTRODUCTION\n",
            "\u001b[35mSection_title: 2.  Concatenations of HIV/SIV regions \"placed\" in sequence and side by side.\n",
            "\u001b[35mSection_title: 3.  Evidence of the absence of 4 HIV/SIV « Exogenous\n",
            "\u001b[35mSection_title: 4.  Evidence for HIV/SIV sequences in this region, and their compaction in the 225 bases portion of both\n",
            "\u001b[35mSection_title: 6.  The discovery of a new EIE from the HIV1 group «O» differentiating COVID-19 from the Bat RaTG13\n",
            "\u001b[35mSection_title: 8.  First encouraging mutations in the 225 bases, « A » and « B » regions, particularly in USA WA state.\n",
            "\u001b[35mSection_title: 10.  New evidence of increased deletions from region 225 bases in WA State in the USA.\n",
            "\u001b[35mSection_title: 11.  The region 1770 bases of the 2 proteins SPIKE in COVID_19 and Bat RaTG13.\n",
            "\u001b[35mSection_title: 12.  Evidence of a SPIKE significant EIE of Plasmodium Yoelii and of a possible HIV1 EIE with a crucial\n",
            "\u001b[35mSection_title: 13.  The analysis of deletions in the SPIKE critical region of 1770 bases in the USA WA state (Seattle).\n",
            "\u001b[32mconclusion: 1) 18 rna fragments of homology equal or more than 80% with human or simian retroviruses have been found in the covid_19 genome. 2) these fragments are 18 to 30 nucleotides long and therefore have the potential to modify the gene expression of covid19. we have named them external informative elements or eie. 3) these eie are not dispersed randomly, but are concentrated in a small part of the covid_19 genome. 4) among this part, a 225-nucleotide long region is unique to covid_19 and bat ratg13 and can discriminate and formally distinguish these 2 genomes. 5) in the decreasing slope of the epidemic, this 225 bases area and the 1770 bases spike region, exhibits an abnormally high rate of mutations/deletions (cases of 44 patients from wa seattle state, original epicenter in usa). 6) in the comparative analysis of both spikes genes of covid_19 and bat ratg13, we note two abnormal facts: • the insertion of 4 contiguous prra amino acids in the middle of spike (then we show that this site was already an optimal cleavage site before this insertion). • an abnormal ratio of synonymous codons / non synonymous codons in the second half of spike.\n",
            "\u001b[35mSection_title: 4.  CONCLUSIONS\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  RELATED WORK\n",
            "\u001b[35mSection_title: 2.1  QA datasets with questions created by annotators.\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 2.1  QA datasets with questions created by annotators.\n",
            "\u001b[35mSection_title: 3  Dataset Construction\n",
            "\u001b[35mSection_title: 3.1  Data Source\n",
            "\u001b[35mSection_title: 3.2  Annotation Scheme\n",
            "\u001b[35mSection_title: 4  Data Analysis\n",
            "\u001b[35mSection_title: 4.1  Label Properties\n",
            "\u001b[35mSection_title: 4.2  Question Properties\n",
            "\u001b[35mSection_title: 5  Experiments\n",
            "\u001b[35mSection_title: 5.1  Models\n",
            "\u001b[35mSection_title: 5.2  Evaluation Metrics and Experimental Details\n",
            "\u001b[35mSection_title: 5.3  Experimental Results and Analysis\n",
            "\u001b[32mconclusion: in this paper, we propose a new challenge: conditional question answering with hierarchical multispan answers, which might be widespread in multispan qa in real-world scenarios. moreover, we introduce cmqa, which contains conditional and hierarchical samples to study the new proposed task. data analysis and experimental results show the main characteristics and challenges of cmqa, and the poor model performance demonstrates that the proposed task is challenging for the community to solve. we believe cmqa can serve as a benchmark to study the new proposed task and help build more reliable and sophisticated qa systems. figure 4: the amount of labels in cmqa. c-a refers to condition-answer and c-f refers to coarse-fine.\n",
            "\u001b[35mSection_title: 6  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Quantifying NLI Dataset Bias\n",
            "\u001b[35mSection_title: 2.1  Saturation and Bias Scoring\n",
            "\u001b[35mSection_title: 2.2  Relative Dataset Bias Scoring\n",
            "\u001b[35mSection_title: 2.3  Biased Model Results\n",
            "\u001b[35mSection_title: 2.4  Model Training\n",
            "\u001b[35mSection_title: 2.4.1  Replication Training Details\n",
            "\u001b[35mSection_title: 2.4.2  Biased Condition Training\n",
            "\u001b[35mSection_title: 3  Analyzing NLI Dataset Bias\n",
            "\u001b[35mSection_title: 3.1  Sample-level Model Behavior\n",
            "\u001b[35mSection_title: 3.2  Cluster-based Bias Evaluation\n",
            "\u001b[35mSection_title: 4  Results\n",
            "\u001b[35mSection_title: 4.1  Result-Metric Correlations\n",
            "\u001b[35mSection_title: 5  Discussion\n",
            "\u001b[32mconclusion: in the four years since (poliak et al., 2018) single sentence relation leakage bias has proven to remain a difficult issue. efforts to debias nli have led to datasets that merely exhibit different kinds of bias than those shown before, or less saturated benchmarks that continue to exhibit cheating features. future work must prioritize reducing observable bias directly using a model-driven approach.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\u001b[35mSection_title: 9  Acknowledgements\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Work\n",
            "\u001b[35mSection_title: 3  Explanations for CommonsenseQA\n",
            "\u001b[35mSection_title: 3.1  Formatting of the Explanation\n",
            "\u001b[35mSection_title: 2.  Free Flow (FF)\n",
            "\u001b[35mSection_title: 4  ECQA Dataset\n",
            "\u001b[35mSection_title: 4.1  Dataset Analysis\n",
            "\u001b[35mSection_title: 4.2  Human Validation Experiments\n",
            "\u001b[35mSection_title: 5  Explanation Retrieval\n",
            "\u001b[35mSection_title: 5.1  Property Ranker\n",
            "\u001b[35mSection_title: 5.2  Property Selector\n",
            "\u001b[35mSection_title: 5.3  Experiments and Results for XR System\n",
            "\u001b[35mSection_title: 6  Explanation Generation\n",
            "\u001b[35mSection_title: 6.1  Property Generation (XGP)\n",
            "\u001b[35mSection_title: 6.2  Free-Flow Explanation Generation (XGF)\n",
            "\u001b[35mSection_title: 6.2.1  XGF-I\n",
            "\u001b[35mSection_title: 6.2.2  XGF-II\n",
            "\u001b[32mconclusion: we have presented desiderata of what constitutes an explanation in the case of common-sense qa. based on it, we generated a human-annotated explanation dataset ecqa for commonsenseqa. we have also proposed models to retrieve and generate common-sense facts required to justify the answer choice. we have publicly released our crowdsourced ecqa dataset and code/models. in future work, we plan to explore directions to design rlbased schemes for joint training of property ranker and property selector components in the xr system and joint training of xgp and xgf-ii to generate free-flow explanation. another direction is to improve the accuracy and interpretability of the existing models for commonsenseqa using the ecqa dataset.\n",
            "\u001b[35mSection_title: 7  Conclusion and Future Work\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Related Works\n",
            "\u001b[35mSection_title: 3  Model\n",
            "\u001b[35mSection_title: 3.1  Encoding\n",
            "\u001b[35mSection_title: 3.2  Model Layout Controller\n",
            "\u001b[35mSection_title: 3.3  Stack NMN with Soft Program Execution\n",
            "\u001b[35mSection_title: 3.4  NMN Modules\n",
            "\u001b[35mSection_title: 3.5  Bridge Entity Supervision\n",
            "\u001b[35mSection_title: 3.6  Prediction Layer\n",
            "\u001b[35mSection_title: 3.7  Optimization\n",
            "\u001b[35mSection_title: 4  Experimental Setup\n",
            "\u001b[35mSection_title: 5  Results\n",
            "\u001b[35mSection_title: 5.1  Baseline\n",
            "\u001b[35mSection_title: 5.2  Primary NMN Results\n",
            "\u001b[35mSection_title: 5.3  Ablation Studies\n",
            "\u001b[35mSection_title: 5.4  Comparison with Original NMN Modules\n",
            "\u001b[35mSection_title: 5.5  Adversarial Evaluation\n",
            "\u001b[35mSection_title: 6  Analysis\n",
            "\u001b[35mSection_title: 6.1  Controller's Attention on Questions\n",
            "\u001b[35mSection_title: 6.2  Predicting Network Layouts\n",
            "\u001b[35mSection_title: 6.3  Finding Intermediate Bridge-Entities\n",
            "\u001b[32mconclusion: in this work, we proposed a self-assembling neural modular network for multi-hop qa. we designed three modules that reason between the question and text-based context. the resulting model outperforms both the single-hop baseline and the original nmn on hotpotqa . because of the interpretable nature of our model, we presented analyses to show that our model does in fact learn to perform compositional reasoning and can dynamically assemble the modular network based on the question.\n",
            "\u001b[35mSection_title: 7  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\u001b[35mSection_title: 1  Introduction\n",
            "\u001b[35mSection_title: 2  Implicit Relations\n",
            "\u001b[35mSection_title: 3  The IMPLICITRELATIONS Benchmark\n",
            "\u001b[35mSection_title: 3.1  Data Collection\n",
            "\u001b[35mSection_title: 3.2  Data Analysis\n",
            "\u001b[35mSection_title: 4  Experimental Setting\n",
            "\u001b[35mSection_title: 4.1  Task and Model Specification\n",
            "\u001b[35mSection_title: 4.2  Evaluation\n",
            "\u001b[35mSection_title: 5  Large LMs Can Infer Implicit Relations\n",
            "\u001b[35mSection_title: 5.1  Effect of In-Context Examples\n",
            "\u001b[35mSection_title: 5.2  Effect of Model Size\n",
            "\u001b[35mSection_title: 6  Implicit Relations for QA\n",
            "\u001b[35mSection_title: 7  Related Work\n",
            "\u001b[32mconclusion: in this work, we propose the task of implicit relation inference, which decouples the inference of reasoning steps from their execution. we introduce implicitrelations, a benchmark that includes over 600 questions implicit reasoning questions along with more than 2,000 annotated implicit relations. we show that large lms can infer implicit relations well in the in-context setup across multiple types of questions and reasoning skills, but that this success does not transfer to an improvement in the downstream task of answering implicit reasoning questions. our work sheds light on the types of capabilities missing from large lms for addressing implicit reasoning questions, and provides a valuable resource for further improving the ability of models to infer implicit relations.\n",
            "\u001b[35mSection_title: 8  Conclusion\n",
            "\n",
            "\n",
            "#########\n",
            "\n",
            "\n",
            "#########\n"
          ]
        }
      ],
      "source": [
        "#Extract text from Section 'limitation','Disscussion','Conclusion'\n",
        "columns = ['corpusid','paper-title','section-title','Target','Content','ref_paper_id','pdfurl']\n",
        "d_sec=pd.DataFrame(columns=columns)\n",
        "\n",
        "strings=''\n",
        "for i in range(len(df)):\n",
        "    a=eval(df['content'][i])\n",
        "    position=[]\n",
        "    text_end=len(a['text'])-1\n",
        "\n",
        "    #URL\n",
        "    try:\n",
        "        url=a['source']['pdfurls'][0]\n",
        "    except:\n",
        "        url='NaN'\n",
        "    \n",
        "    #section\n",
        "    try:\n",
        "        dic=eval(a['annotations']['sectionheader'])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    for j in range(len(dic)):\n",
        "        if 'attributes' not in list(dic[j].keys()): # section_num strings in section\n",
        "            continue\n",
        "        \n",
        "        start,end = get_offset(dic,j)\n",
        "        try:\n",
        "            start_next,end_next=get_offset(dic,j+1)\n",
        "        except:\n",
        "            print('bottom')\n",
        "        # print('start:',start)\n",
        "        sec_num=dic[j]['attributes']['n']\n",
        "        section_title = a['text'][start:end].lower()\n",
        "            \n",
        "        if 'disscuss' in section_title and 'conclusion' not in section_title:\n",
        "            Total=a['text'][end:start_next].lower()\n",
        "\n",
        "            print(Fore.GREEN +'disscussion:',Total.replace('\\n',''))\n",
        "            strings=Total.replace('\\n','')\n",
        "            # add-data\n",
        "            d_sec_append=pd.DataFrame(data=[[df['corpusid'][i],df['title'][i],section_title,'Insight-tree',strings,df['ref_paper_id'][i],url]], columns=columns)\n",
        "            d_sec=pd.concat([d_sec, d_sec_append], ignore_index=True, axis=0)\n",
        "\n",
        "        if 'limitation' in section_title and 'conclusion' not in section_title:\n",
        "            Total=a['text'][end:start_next].lower()\n",
        "\n",
        "            print(Fore.GREEN +'limitation:',Total.replace('\\n',''))\n",
        "            strings=Total.replace('\\n','')\n",
        "            # add-data\n",
        "            d_sec_append=pd.DataFrame(data=[[df['corpusid'][i],df['title'][i],section_title,'Insight-tree',strings,df['ref_paper_id'][i],url]], columns=columns)\n",
        "            d_sec=pd.concat([d_sec, d_sec_append], ignore_index=True, axis=0)\n",
        "\n",
        "        \n",
        "        if 'conclusion' in section_title:\n",
        "            for p in range(len(eval(a['annotations']['paragraph']))):\n",
        "                ref_flag=eval(a['annotations']['paragraph'])[p]['end']\n",
        "                if ref_flag > end:\n",
        "                    break\n",
        "            \n",
        "            Total=a['text'][end:ref_flag].lower()\n",
        "\n",
        "            print(Fore.GREEN +'conclusion:',Total.replace('\\n',''))\n",
        "            strings=Total.replace('\\n','')\n",
        "            # add-data\n",
        "            d_sec_append=pd.DataFrame(data=[[df['corpusid'][i],df['title'][i],section_title,'Insight-tree',strings,df['ref_paper_id'][i],url]], columns=columns)\n",
        "            d_sec=pd.concat([d_sec, d_sec_append], ignore_index=True, axis=0)\n",
        "        \n",
        "        print(Fore.MAGENTA + 'Section_title:',sec_num + ' ',a['text'][start:end])\n",
        "        end_pos=end\n",
        "        position.append(end_pos)\n",
        "        \n",
        "    print('\\n')\n",
        "    print('#########')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpusid</th>\n",
              "      <th>paper-title</th>\n",
              "      <th>section-title</th>\n",
              "      <th>Target</th>\n",
              "      <th>Content</th>\n",
              "      <th>ref_paper_id</th>\n",
              "      <th>pdfurl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>263908862</td>\n",
              "      <td>Idea2Img: Iterative Self-Refinement with GPT-4...</td>\n",
              "      <td>limitation and discussion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>tasks beyond image generation. idea2img explor...</td>\n",
              "      <td>{}</td>\n",
              "      <td>https://export.arxiv.org/pdf/2310.08541v1.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>263908862</td>\n",
              "      <td>Idea2Img: Iterative Self-Refinement with GPT-4...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>we have presented idea2img, a multimodal itera...</td>\n",
              "      <td>{}</td>\n",
              "      <td>https://export.arxiv.org/pdf/2310.08541v1.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>248665596</td>\n",
              "      <td>From Distillation to Hard Negative Sampling: M...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this paper, we have built on the splade mod...</td>\n",
              "      <td>{220302524: '[30]'}</td>\n",
              "      <td>https://arxiv.org/pdf/2205.04733v2.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264490922</td>\n",
              "      <td>What Else Do I Need to Know? The Effect of Bac...</td>\n",
              "      <td>discussion and conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>large general-purpose language models, such as...</td>\n",
              "      <td>{123758373: 'Chen and Durrett, 2019;', 2466523...</td>\n",
              "      <td>https://export.arxiv.org/pdf/2305.14331v2.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>253098647</td>\n",
              "      <td>Learning to Perform Complex Tasks through Comp...</td>\n",
              "      <td>conclusion &amp; future work</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this work, we proposed cft as an improvemen...</td>\n",
              "      <td>{204915921: 'Khot et al., 2020;', 248666080: '...</td>\n",
              "      <td>https://export.arxiv.org/pdf/2210.12607v1.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>252819325</td>\n",
              "      <td>CMQA: A Dataset of Conditional Question Answer...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this paper, we propose a new challenge: con...</td>\n",
              "      <td>{215785913: 'Chen et al., 2020', 67855846: 'Du...</td>\n",
              "      <td>https://www.aclanthology.org/2022.coling-1.146...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1177</th>\n",
              "      <td>249049412</td>\n",
              "      <td>Examining Single Sentence Label Leakage in Nat...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in the four years since (poliak et al., 2018) ...</td>\n",
              "      <td>{}</td>\n",
              "      <td>https://arxiv.org/pdf/2112.09237v3.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>236459873</td>\n",
              "      <td>Explanations for CommonsenseQA: New Dataset an...</td>\n",
              "      <td>conclusion and future work</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>we have presented desiderata of what constitut...</td>\n",
              "      <td>{218487313: 'Yadav et al., 2020', 52822214: 'Y...</td>\n",
              "      <td>https://aclanthology.org/2021.acl-long.238.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>202565945</td>\n",
              "      <td>Self-Assembling Modular Networks for Interpret...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this work, we proposed a self-assembling ne...</td>\n",
              "      <td>{139103297: 'Chen and Durrett, 2019;', 1899278...</td>\n",
              "      <td>https://www.aclweb.org/anthology/D19-1455.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1180</th>\n",
              "      <td>248476403</td>\n",
              "      <td>Inferring Implicit Relations with Language Models</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this work, we propose the task of implicit ...</td>\n",
              "      <td>{230799347: 'Geva et al., 2021', 225075843: 'L...</td>\n",
              "      <td>https://arxiv.org/pdf/2204.13778v1.pdf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1181 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       corpusid                                        paper-title  \\\n",
              "0     263908862  Idea2Img: Iterative Self-Refinement with GPT-4...   \n",
              "1     263908862  Idea2Img: Iterative Self-Refinement with GPT-4...   \n",
              "2     248665596  From Distillation to Hard Negative Sampling: M...   \n",
              "3     264490922  What Else Do I Need to Know? The Effect of Bac...   \n",
              "4     253098647  Learning to Perform Complex Tasks through Comp...   \n",
              "...         ...                                                ...   \n",
              "1176  252819325  CMQA: A Dataset of Conditional Question Answer...   \n",
              "1177  249049412  Examining Single Sentence Label Leakage in Nat...   \n",
              "1178  236459873  Explanations for CommonsenseQA: New Dataset an...   \n",
              "1179  202565945  Self-Assembling Modular Networks for Interpret...   \n",
              "1180  248476403  Inferring Implicit Relations with Language Models   \n",
              "\n",
              "                   section-title        Target  \\\n",
              "0      limitation and discussion  Insight-tree   \n",
              "1                     conclusion  Insight-tree   \n",
              "2                     conclusion  Insight-tree   \n",
              "3      discussion and conclusion  Insight-tree   \n",
              "4       conclusion & future work  Insight-tree   \n",
              "...                          ...           ...   \n",
              "1176                  conclusion  Insight-tree   \n",
              "1177                  conclusion  Insight-tree   \n",
              "1178  conclusion and future work  Insight-tree   \n",
              "1179                  conclusion  Insight-tree   \n",
              "1180                  conclusion  Insight-tree   \n",
              "\n",
              "                                                Content  \\\n",
              "0     tasks beyond image generation. idea2img explor...   \n",
              "1     we have presented idea2img, a multimodal itera...   \n",
              "2     in this paper, we have built on the splade mod...   \n",
              "3     large general-purpose language models, such as...   \n",
              "4     in this work, we proposed cft as an improvemen...   \n",
              "...                                                 ...   \n",
              "1176  in this paper, we propose a new challenge: con...   \n",
              "1177  in the four years since (poliak et al., 2018) ...   \n",
              "1178  we have presented desiderata of what constitut...   \n",
              "1179  in this work, we proposed a self-assembling ne...   \n",
              "1180  in this work, we propose the task of implicit ...   \n",
              "\n",
              "                                           ref_paper_id  \\\n",
              "0                                                    {}   \n",
              "1                                                    {}   \n",
              "2                                   {220302524: '[30]'}   \n",
              "3     {123758373: 'Chen and Durrett, 2019;', 2466523...   \n",
              "4     {204915921: 'Khot et al., 2020;', 248666080: '...   \n",
              "...                                                 ...   \n",
              "1176  {215785913: 'Chen et al., 2020', 67855846: 'Du...   \n",
              "1177                                                 {}   \n",
              "1178  {218487313: 'Yadav et al., 2020', 52822214: 'Y...   \n",
              "1179  {139103297: 'Chen and Durrett, 2019;', 1899278...   \n",
              "1180  {230799347: 'Geva et al., 2021', 225075843: 'L...   \n",
              "\n",
              "                                                 pdfurl  \n",
              "0         https://export.arxiv.org/pdf/2310.08541v1.pdf  \n",
              "1         https://export.arxiv.org/pdf/2310.08541v1.pdf  \n",
              "2                https://arxiv.org/pdf/2205.04733v2.pdf  \n",
              "3         https://export.arxiv.org/pdf/2305.14331v2.pdf  \n",
              "4         https://export.arxiv.org/pdf/2210.12607v1.pdf  \n",
              "...                                                 ...  \n",
              "1176  https://www.aclanthology.org/2022.coling-1.146...  \n",
              "1177             https://arxiv.org/pdf/2112.09237v3.pdf  \n",
              "1178     https://aclanthology.org/2021.acl-long.238.pdf  \n",
              "1179      https://www.aclweb.org/anthology/D19-1455.pdf  \n",
              "1180             https://arxiv.org/pdf/2204.13778v1.pdf  \n",
              "\n",
              "[1181 rows x 7 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpusid</th>\n",
              "      <th>paper-title</th>\n",
              "      <th>section-title</th>\n",
              "      <th>Target</th>\n",
              "      <th>Content</th>\n",
              "      <th>ref_paper_id</th>\n",
              "      <th>pdfurl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>263908862</td>\n",
              "      <td>Idea2Img: Iterative Self-Refinement with GPT-4...</td>\n",
              "      <td>limitation and discussion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>tasks beyond image generation. idea2img explor...</td>\n",
              "      <td>{}</td>\n",
              "      <td>https://export.arxiv.org/pdf/2310.08541v1.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>263908862</td>\n",
              "      <td>Idea2Img: Iterative Self-Refinement with GPT-4...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>we have presented idea2img, a multimodal itera...</td>\n",
              "      <td>{}</td>\n",
              "      <td>https://export.arxiv.org/pdf/2310.08541v1.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>248665596</td>\n",
              "      <td>From Distillation to Hard Negative Sampling: M...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this paper, we have built on the splade mod...</td>\n",
              "      <td>{220302524: '[30]'}</td>\n",
              "      <td>https://arxiv.org/pdf/2205.04733v2.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264490922</td>\n",
              "      <td>What Else Do I Need to Know? The Effect of Bac...</td>\n",
              "      <td>discussion and conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>large general-purpose language models, such as...</td>\n",
              "      <td>{123758373: 'Chen and Durrett, 2019;', 2466523...</td>\n",
              "      <td>https://export.arxiv.org/pdf/2305.14331v2.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>253098647</td>\n",
              "      <td>Learning to Perform Complex Tasks through Comp...</td>\n",
              "      <td>conclusion &amp; future work</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this work, we proposed cft as an improvemen...</td>\n",
              "      <td>{204915921: 'Khot et al., 2020;', 248666080: '...</td>\n",
              "      <td>https://export.arxiv.org/pdf/2210.12607v1.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>252819325</td>\n",
              "      <td>CMQA: A Dataset of Conditional Question Answer...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this paper, we propose a new challenge: con...</td>\n",
              "      <td>{215785913: 'Chen et al., 2020', 67855846: 'Du...</td>\n",
              "      <td>https://www.aclanthology.org/2022.coling-1.146...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1177</th>\n",
              "      <td>249049412</td>\n",
              "      <td>Examining Single Sentence Label Leakage in Nat...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in the four years since (poliak et al., 2018) ...</td>\n",
              "      <td>{}</td>\n",
              "      <td>https://arxiv.org/pdf/2112.09237v3.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>236459873</td>\n",
              "      <td>Explanations for CommonsenseQA: New Dataset an...</td>\n",
              "      <td>conclusion and future work</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>we have presented desiderata of what constitut...</td>\n",
              "      <td>{218487313: 'Yadav et al., 2020', 52822214: 'Y...</td>\n",
              "      <td>https://aclanthology.org/2021.acl-long.238.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>202565945</td>\n",
              "      <td>Self-Assembling Modular Networks for Interpret...</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this work, we proposed a self-assembling ne...</td>\n",
              "      <td>{139103297: 'Chen and Durrett, 2019;', 1899278...</td>\n",
              "      <td>https://www.aclweb.org/anthology/D19-1455.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1180</th>\n",
              "      <td>248476403</td>\n",
              "      <td>Inferring Implicit Relations with Language Models</td>\n",
              "      <td>conclusion</td>\n",
              "      <td>Insight-tree</td>\n",
              "      <td>in this work, we propose the task of implicit ...</td>\n",
              "      <td>{230799347: 'Geva et al., 2021', 225075843: 'L...</td>\n",
              "      <td>https://arxiv.org/pdf/2204.13778v1.pdf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1181 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       corpusid                                        paper-title  \\\n",
              "0     263908862  Idea2Img: Iterative Self-Refinement with GPT-4...   \n",
              "1     263908862  Idea2Img: Iterative Self-Refinement with GPT-4...   \n",
              "2     248665596  From Distillation to Hard Negative Sampling: M...   \n",
              "3     264490922  What Else Do I Need to Know? The Effect of Bac...   \n",
              "4     253098647  Learning to Perform Complex Tasks through Comp...   \n",
              "...         ...                                                ...   \n",
              "1176  252819325  CMQA: A Dataset of Conditional Question Answer...   \n",
              "1177  249049412  Examining Single Sentence Label Leakage in Nat...   \n",
              "1178  236459873  Explanations for CommonsenseQA: New Dataset an...   \n",
              "1179  202565945  Self-Assembling Modular Networks for Interpret...   \n",
              "1180  248476403  Inferring Implicit Relations with Language Models   \n",
              "\n",
              "                   section-title        Target  \\\n",
              "0      limitation and discussion  Insight-tree   \n",
              "1                     conclusion  Insight-tree   \n",
              "2                     conclusion  Insight-tree   \n",
              "3      discussion and conclusion  Insight-tree   \n",
              "4       conclusion & future work  Insight-tree   \n",
              "...                          ...           ...   \n",
              "1176                  conclusion  Insight-tree   \n",
              "1177                  conclusion  Insight-tree   \n",
              "1178  conclusion and future work  Insight-tree   \n",
              "1179                  conclusion  Insight-tree   \n",
              "1180                  conclusion  Insight-tree   \n",
              "\n",
              "                                                Content  \\\n",
              "0     tasks beyond image generation. idea2img explor...   \n",
              "1     we have presented idea2img, a multimodal itera...   \n",
              "2     in this paper, we have built on the splade mod...   \n",
              "3     large general-purpose language models, such as...   \n",
              "4     in this work, we proposed cft as an improvemen...   \n",
              "...                                                 ...   \n",
              "1176  in this paper, we propose a new challenge: con...   \n",
              "1177  in the four years since (poliak et al., 2018) ...   \n",
              "1178  we have presented desiderata of what constitut...   \n",
              "1179  in this work, we proposed a self-assembling ne...   \n",
              "1180  in this work, we propose the task of implicit ...   \n",
              "\n",
              "                                           ref_paper_id  \\\n",
              "0                                                    {}   \n",
              "1                                                    {}   \n",
              "2                                   {220302524: '[30]'}   \n",
              "3     {123758373: 'Chen and Durrett, 2019;', 2466523...   \n",
              "4     {204915921: 'Khot et al., 2020;', 248666080: '...   \n",
              "...                                                 ...   \n",
              "1176  {215785913: 'Chen et al., 2020', 67855846: 'Du...   \n",
              "1177                                                 {}   \n",
              "1178  {218487313: 'Yadav et al., 2020', 52822214: 'Y...   \n",
              "1179  {139103297: 'Chen and Durrett, 2019;', 1899278...   \n",
              "1180  {230799347: 'Geva et al., 2021', 225075843: 'L...   \n",
              "\n",
              "                                                 pdfurl  \n",
              "0         https://export.arxiv.org/pdf/2310.08541v1.pdf  \n",
              "1         https://export.arxiv.org/pdf/2310.08541v1.pdf  \n",
              "2                https://arxiv.org/pdf/2205.04733v2.pdf  \n",
              "3         https://export.arxiv.org/pdf/2305.14331v2.pdf  \n",
              "4         https://export.arxiv.org/pdf/2210.12607v1.pdf  \n",
              "...                                                 ...  \n",
              "1176  https://www.aclanthology.org/2022.coling-1.146...  \n",
              "1177             https://arxiv.org/pdf/2112.09237v3.pdf  \n",
              "1178     https://aclanthology.org/2021.acl-long.238.pdf  \n",
              "1179      https://www.aclweb.org/anthology/D19-1455.pdf  \n",
              "1180             https://arxiv.org/pdf/2204.13778v1.pdf  \n",
              "\n",
              "[1181 rows x 7 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Insight_dataset=d_sec[d_sec['Target']=='Insight-tree']\n",
        "Insight_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "Insight_dataset.insert(7,'Solved','None')\n",
        "Insight_dataset.insert(8,'Unsolved/FW','None')\n",
        "Insight_dataset=Insight_dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Delete rows with lack of information\n",
        "emp=[]\n",
        "for i in range(len(Insight_dataset)):\n",
        "    text=Insight_dataset['Content'][i]\n",
        "    if len(text)<=10:\n",
        "        emp.append(i)\n",
        "\n",
        "Insight_dataset.drop(index=emp, inplace=True)\n",
        "\n",
        "Insight_dataset=Insight_dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Drop_duplicates\n",
        "Insight_dataset=Insight_dataset.drop_duplicates(\"Content\")\n",
        "Insight_dataset=Insight_dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "Insight_dataset.to_csv('Insight_dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Frequently Used Words - Insight Section(Limitation, Disscussion, Conclusion), Paper-title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Insight Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "strings=''\n",
        "for i in range(len(Insight_dataset)):\n",
        "    a=Insight_dataset['Content'][i]\n",
        "    strings=strings+' '+a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Tokenizer(strings):\n",
        "    #lower & clean punctuation\n",
        "    strings=strings.lower()\n",
        "    strings=strings.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    #Stopwords\n",
        "    token=nltk.word_tokenize(strings)\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens=[]\n",
        "    for i in range(len(token)):\n",
        "        if token[i] not in stop_words:\n",
        "            tokens.append(token[i])  \n",
        "\n",
        "    #clean tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filter_word=['introduction','conclusion','evaluation','figure','table','survey','et','al','research','we','use','work','issue','also','future','however','finally','task','paper','nlp','natural','language','processing','international','conference','meeting','annual','method','example','arxiv']\n",
        "    clean_token=[]\n",
        "    for item in tokens:\n",
        "        item=lemmatizer.lemmatize(item, pos=\"n\")\n",
        "        if item not in filter_word and re.search(r'^\\d+$',item) is None:\n",
        "            clean_token.append(item)\n",
        "    \n",
        "    return clean_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCADIAZADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3R7mUSuqICAcD5Sfqfegz3IAPlDnPRSdvI/OrOEQs+FX1bpXL3nji2juTBZWkl2QcblbaD9OCTXM4SV3KZlVrU6SvN2OhMswjZtnOQAApPYc/Sk82by5GKkEAFVCH0H/16xbbxjaT29wZIXguYY2kEMhxvwM4B/8ArVpaHqv9s6cLvyfJ+crt37unvgU+XmekhQxFObSi7lkyzGTaqYBJGSp4H9c026uHitI5TFnLIHG8oVyQOw9T0rNi8R+Z4mbR/suNpI83zPRd3TH9a17m1iu0CTByoOcLIy89exHpVRi7O0rjVRVIvke2nzK11fyQTTpHbiRYIRM7GTbwd3A4PPy1HO9xNqtvGoHkeWZOJ2QnleTgds9M4OazrnV4IPEX9jmyL/aESJ5jMclcE4xj/aPfvXQCCMSrLt+dUKA56Djj9BTs5dSIy9pdJ7P+uhVTUGeRCYQIJJTEr7/myMjlcdMg96auozG0e6NsoiAOB5vzEhsdMdPxrBvfE+l2l+Vs7WW8mVifkchAx6kDkZ68gU208XWDAWV7Yy2kbHuxYDnPPQ9annezkZfWoJ8rn/XraxurqrYk8yBEKhtv7wkMQ+wj7ueuMcc+lNk1Od7CeWKFVlikEbB2IAztORlQf4h1Aq39htJkDBAytlgyue7bsgg+vOaoapeaXolk63KsRcMT5YYs0hwBnJPsOc02ppas1k5Ri3KWhdup5obVJJIUyZFV1SUjGWABBxz1HHFJcXzxSypFAJBDGJJCX28HPA45PB9K5OXxWk8CodHu3t1OQxunBPOck45/E1raRquneIXaMW1wkkceH3OcMuehIPzdeh96XO27J/19xnHFQnLlhLX+vI0jqb7pGW3BgjlSNnL4J3BSCBj/AGh3qTUdUttLWE3Bb99II0CjJz/hUOqSwafZySmzmuBLKpaOEEksAMH2HyiuH8Qa5cajf2bPYy26wHckb5y5yPb2xTlNwvdixGJ9jF3ev/B/yPS6pamZRbKYXdG3/eUE4GD1wDx+Fc//AMJhf/8AQvXP/fbf/EVqXNw15okN1LahGfloZCuVGDwCw6/hWsZqT0NFiKdRNQf4M1okaOFEZy7KACx71gXdprr+KYZ4J2GnAruXzMLj+IFe5PNX9Y1O601ImttNlvd5O4Rk/Lj6A1kWfjGW41WCwm0p7d5WCnfKcrnvgqKiUo3s2TWqUrqnJtO67/mX5NT1FYyy2xLbn+T7NJwR91M553f3/ujHvVQa3q6yrHLZKGaISIot5MscZ2deOwyeB3rpqawQHewXKg/Mew71upLsdVjDk1DUIWlW3tpZQblgGmifAXAIA4zgnPzcgY+laOn3VzctcC4gMQSQrH8pG9ezc+vp1GPcViXnja0iuDDZW0l4wOMqcKfpwSfyos/G1pLcCG9tpLNjxuY5UfXgEflWbqwehzfW6HNy839epofbtQE8h8hiuwEL5LY3At8v1PHzdKle+ukjdvIZmw4CLC/DD7oz3z6jitIEMAQQQeQRWEfFEEet3VhcRCGO3QsZy+c4AOMY9/WhtLcqbVO3NLclXUNTyAbQchTny2wMgnB/x7VMLq/Lw74wikqXIhY4BDcdeoIXn3rEl8dLlmttMnlhH/LQtt/oa1tF8R2et7kiDRTqMmN/T1B71KnFuyZlTrU5y5VO7NiqS2lzsXddsDjkAHrj1z61yHxW/wCQFpH/AGF4P5PW346/5ETW/wDr0f8AlW3s78r7na4J2uaslrcEEpdEHOQMH06delPtYJojulnZ8jG0nIHvXIiDVbn4O2cOiOyag2mQeUVba33V3AHsSuQK5jR9N8BXk0NncLqWj678ocXVxLFMz+zE7Tk9Oh9qcaC1d9hKmr3PXqyrbSruDxHfanJqtxLbXEaJHYsP3cJAGWHPU49B1PWrt5ZW2oWUtndxCW3lXZIjE4Yelef+ENLstF+KfiOw0+AQWsdrCUjBJAyFJ6knqTRBXT1KR6M+4o2wgNjgn1qp5V+AcTp1yM88Z+lcd8WmnTw7pjWr+XcDVYTE/wDdba+D+dU/FXgLTNL8MXusWtxfDWbSLz/t7XTmR2HJzzjnnoBQqMZJNu1xcifU7/y73B/epntx7H29cU8Jc7mPmDHUD8Bx06ZzXE+J9Z1V/hDDqlpJIl3PaQPPLFwyKwXewx06/gDXJeMtA8GR+HbObQJIbrVZZo1hVLkyyXOTyGXPf6DB44pww9929wVPzPYPLvcD98meM8cdT7fSrdVdO0+10uwisrKBYLeIYSNei5OT+pNc5oNqtrr7i3snkjdZGku57F4ZkJYHa0jACUHJxjpjvWcYLWzFax0em38WqaZbX8CusVxEsqBwAwBGRnGeatVxGg6K+nW3hiWCyeC6aBkvJDGQ3MRIEnfAYDAPToKZY6Q1zHYWgsbiC7NvLFq88kTKJt0RU5c8SZchgQTgA9OlaOmruzC53VZN/rMVq+6KVZDEWWSDBBb6H1GPxzXMaTZanNqen3V3aXCfb5Fmug6ECIwKypn0LEoR64rsLy9+x3Vv5hbyHVw2yMsd3GOg6df0rOrHl6kVG+W97C6bcNc2zStMkrFzkICFTgfKM8n6+/4VM91FHOYnYKQm/LEAYzii3uYrqMvEWKg4+ZCvP0IFUrmGGXWIxcY2eT8oJwCc1xYqrUpU4una7aWu2ppTSa3uaKSJIu6N1ZfVTmqj6rapKYyzcHBYLwDUET21nqbxo6pE0eWG7gNn/CnaWqSaSVfBDbt1cn1ytVkqVNxUvev1Xu29N7l8qWrNEEMAQcg8gilrM062WWyjlZn3EY69gcY/L+dW4rOOFwys5I9Tn1/x/SvQoVpVacalviSf3mbunYyfGF29r4dm2EhpWEWR6Hr+gNM8HWEVroUU4QedcZZ2xzjOAPpgUeNLdp/DsjKM+VIrn6dP61L4RukufDtsqsC0WY3HoQeP0xVf8vNexxb4z3v5dPvIfGVhFc6FLcFB51vhlbHOM4I+nNJ4I/5F0f8AXVv6VP4uuUt/DlyrEBpcIg9Tkf0BqDwR/wAi6P8Arq39KP8Al58hNL67p/KZNt/yUyT/AHm/9F13NcIjCD4lt5h2hmIBPvHxXd06XX1KwW0/8TOG1H/ko9t/vR/+g1ueLr17Lw/MYyVeVhECO2ev6A1zc10l38RYnjYMizKgI74GD+ua6Dxpavc+HnZBkwushA9OQf51CfuysYQk3TrOPdmV4Z1fQdJ0tBLchLuTJlPlOT14GQOmMVJ4j1vQdV0iWJLkPcKN0J8pwQfqR3qXwxp2janosTvaQvcR5SXI5zngn6itn/hG9G/6B8P5U0pONlaw6dOtOgox5eVrz/q5n+CLp7jQNjknyJTGufTAI/nWf40tJ01Cy1MQma3hADrjIGGzz7HpXWWdla2MTR2kKRIzZIToT0/pU/ByOD6iq5Lx5WdDwzlQVKT1XX0MXTvFWlX4VROIJD/yzm+X8j0rXSGFHaSONFeTG5lUAt6ZPesfUfCmlagGbyPIlP8AHD8v6dKxfCtxdWOvXOiyTGaCPcF9FKnqPT6Uc0k0pEqrUpzjCsk77Nf5G7qP/IbtP9YMhRwOvz9B/NvYCue8Zf8AIxab/ur/AOh10txc41lI4pwpwiyKXADfMeANp559R1Fc140ITX9OduFCjJPs1Fb4TLGW9lJ+aO6qhq24W0ZRHdvMACKzAng5+6cnir9UtRiaZEiEUjqxOSgQ7eODhq1R3z+Fl2uG1P8A5KLa/wC9H/Ku5rhtT/5KLa/70f8AKsquy9Tlxvwx/wASO5rnPGt69roJjjJDTyCMkf3cEn+WPxro65nxzavPoSyoM+RKGb6EEfzIp1PhdjXF39hLl7Fvwrp0NjodvIqDzZ0Ejvjk55A+mMU7xPp0N/olwzoPNhQyRvjkYGcfjT/DV3HeeH7MowJjjETD0KjH/wBf8ad4iu47PQbx3YAvG0aD1ZhgUtOQlKn9Wt0t+hn+Cb17rQzFIxLW7lAT/dxkf1Fc9c2Kaj8QJLaQZjaXLD1AXJH6Vu+BLZ4dFkmcYE0pK+4Axn881nW3/JTJP95v/RdZvWMbnFJOVCip9Wjt0RI0VEUKijAVRgAVwkkSWHxGiW3UIjuCVXgfMvNd7XDaj/yUe2/3o/8A0GrqbL1OrGrSD/vIu+OdK/4SXw59nsL23S7t51uYC7gKWQkEE9up/GsHVrvxd4q8P3mkvpdhprPGyvL9uSRpmX+BFH3dxAGWPANd6+l2TSea0PzjJ3b2yOS3HPHJJ/GqX2jRBJ5m878Y3fPn7+//ANC5rWeLo0ElUkl2u7fqdqbM+yk1PS/Aul21hFZyapb2cAa2nmABAUbuQfTPPSuf1638U+ONL/sqfwzY2CNIub6S+SfysEElAoyDxj8a7o6LYsUOx9qoUC+a3QgD1z0GPpSzaVCdMmsbb/R0kHUZP588jjGPSto1Endbgmxt9NeaZpKmxspNSuIwqCLzljZx0LFm4964KwPi2z8Y6p4jbwizQ30EaeX/AGlANgVQM5zz09q7SLQTCwZbhCBC0PltGSnOecbuvOPpxSP4fMybZriNt1mbVz5PLDGAeWOOucDr37URlGN139Rp26GN420zVPEWgaRHbWBW8W9huZrczJmJQG3fNnBwSBx1rc8V2NzqfhPVLK0j8y4ntnSNNwG5iOBk8Clm0Z53TM0CRiEROkcGA2GDf3uBxjHuarWmjz2mvLOEVrdAyocAbFJZhjnIxuC7cYwM5zxS5tFrsK7MXVpZ9D+F9lpLyvbavPZxWUEUY3u020AqNufQjPQVk6bd6p4a0+O4PwzggFtCPNuoLqHzCoHLYC7j69a7TxL4ah8R29tm6ms7u0lE1tcw43Rt9D1Ht7VkT+EPEWoQNaan41uZrOQbZI7exigZ1PUbhnrWkZx5dfnv+n6lJo6bSNUt9a0i11K13eRcxh1DDBHsfcdKmuo5ZIwImwQwJG4rkemRyKbp9hbaXp1vYWkfl28CCONc5wBRc6ha2jhJ5djEZA2k8fgK5KtSnTTlJ2XnoS1fQoR22pyW0IabaQmWzIwZidpweOMYNTyQ3UMDusrvOWUR/MSMYAOR09Tmnf21p/8Az8f+ON/hQNZ08nAuP/HG/wAK5vrmF/5+x+9f5mapJD7i8Wx8mHy5Z5GU4CsoJC4ySWIHcVHqVw0MkCG6FpC+7dOQOCMYXLcDOT19KoeJRC8aeaw/do7BSjkjgfONo5x/dPBzzVvWXmjjiaJXxzudZQoXp1BB3flXXUVoXFUdosn0yd7iCQtKJ0WQrHMAB5i4HPHHUkcelS3cNtNGPtO3aOhY4x+NV9HuDc2Rc3YucORvEezHA49/rSXUS3GqwxzcxeWSq54LZrjxc7UbcqlzNKz21fXcui7xTuKi6XHEYg8BUnJywOaQ2MFyWa3uGSNuHWJvlNWP7Os/+fdPyqWKGOBSsSBFJzgetc8MFKVo1oQ5V2un+hrzdiIXFrbDyA4Xy8LtwTjjP8uas1mSRQy6p+92tuOFAK8/Lzn+LsfwrTr0YaKy2RjGTbdxskaTRPFIoZHBVlPQg1yD+EL+xunl0bUvJR/4XYjA9MjOa37m8uYtRdB5K28VuZWLPgk5IGeOBxVQ+ItsEcogV4mCr5glwNxj388cKADk9vSrdLmM6tKnUtzboxtR8P3EOk3d9qt815cRxERrk7Uzxnn/AOtWl4II/wCEeUZ581/6U/U7p7zwdezvsyfMX5DlcCQgYPfgDms/wvaz3GgQvA4VkklG4uVwTjB4HOMdD1rLl5aljkUI0sSuRfZ/U0fEHhiPWJFuYZvIulGN2Mhh2z7+9Zw8P+IZAIbrXAsDfL8rsWPtyBn863BZ6iWZjMvMocKJnxjnPbOOnGcZFCWV8F/eSpK3nK/zyHAAOSRxwT6dKp04t3NJUISk5crV99TJTwj9k1yxurJ4hbW4XzA7HezDOT0x3FdUyq6lWAKkYII4IrNezvXM2JRHku0e2VupA2546Ag8c9aiuLfUoNG1MpM0l20Mhg2sT8+04IGOOccdOKqMEtEa0oRp3UY2TMDUPDf2PW4ItIvZbWa5SR1XJCqE25G4HP8AEOMGkh0HxFfPNFdatLHHHJ5ZJZ8PwDlRxkc4+oNLpZ0VNf0l9NMYRbGc3LKeA37v7/8At9c559e1XvDE+kSy6zYWkti8T3jMtvEyFWjMUYJCjqucj061UqEUYLDUm+1+iehf0KeOGP8Asy3huZYrVmia7YKELg5Ixu3dTjpj3rH1Cyuda12afS57q3MOYHn4WMsvUAht3fH3ce9XfDWk6dGmpsmn2qsby4hJEKgmPd9zp9326VW0vTYrbSdem0uyt4r8TXUcDxRKrDBO1QQOmQOOlU6cGrG0qanBQa0/yGnQfFLjZJrSBD1KyNn+Q/nWtoPhyDRA8nmGa5kGGkIxx6AVieHre3+0rNbanYlxaMJ7a2t2jdiccy5kb5ge5APJq1pGmrD4JhvbCBf7Vk0sbJ8ZkZjGCBnrjOMDtip9jGLvcilRhGSla783c6GSzMl15xuZgvy/ul2hTtOfTP61S1/QYtctkVn8qaMkxyYz16gj0rkobe3/ALJv5rbU7EuNLnE9tbW7RuxKdZcyN8wPcgHk1tWunWlnrOkxQwIEu7CYXIIz5+PLwXz94/MeT6mqlTTVmaPlqRcZLR+ZTs9I8QywYtdcjNujtECHfIKsVI5XsQR+FdJoVz9p0sZkkkeCWS3keQcs8blGPU8ZU49qyfCUGlWonghgs4tTjmuFlVEVZRH5zbc99uNmO3Ssez05kv18rS7qLVP7bmmN00DAfZjM5b95jG0rkBc8kg45zSjRim7MeHoxgrr87nSara+Iprwtpt9bQW+0YVxznv8AwmsKTwt4im1Bb6S/tDcrgiTceMdP4MVFb6Xff8JIZLpzFdjUWlWdNMld3h3nan2gPsCFMLtI49CeTLBojQxpfpYSDUP7ekYy+WfMELXD556iMqc+nOfelKhF7sVTCQqO8m/vNaxsvFMV3E13qVrJbhh5igcke3yD+da1neW+r2k5WNjEs0ts6yAclHKN+BKn8KwbC2tY9dum1TTJptSa8Z7e7a1aRREfubZACEAHBGRyD61Rg0QwIl+lhIuoHX5HM3lnzBC1wwPPUIUOfTnPvVRppK1zanSUFa7frqWpfCN5Z3Ly6LqLW6v1jdiMfiM5/EUR+Eb69uEk1rU2nROkaEnP4nGPwFYsOgzRmO5XTZxcDypN/lNneLw/N06iM9f7p9Kuyw3U/ii2uo9KFvPHqRWWSOxkDmH5l3tPnaysMHaAcZ56Uvq0O5j9Ro3207X0+47mKKOCFIokCRoAqqOgFcppenTXni671eNkFvDcSQMrE7ywXacDGMZ96t6Ro1u+rate3liGnGoF7aWaPJVfLTlCegznkdfwrMsbG9h1+S5vrWWfTzqM3kxLE2YZC3yzEfxqeQG/h69yQ3Ti+uxrOjGdr9NTtq5u78P3c/iyHVVkhECFSVLHdwMdMY/Wt7zn8zb5LYzjd2pRK5QMYyOnHPf8K5nUhIKlKNRJS6O5I/3G+lcHXdbi8TEqRweDXC183xG7+ya8/wBDVG94o1LU9MsLOTTYrd2lu7eF2mlKYDyouANjcEEgnqM5AJ4p99rV9pyaZFNpsUl7fTtAIobklEIR3B3lASMLz8oxk9ccz65p51PTEtkmEEqyRzxSkbgjxurrkdxkDjjikl0m5uptIuLu8ieewneZzFAUWXdG6YALHbgPnqenvx9XCcGuXqt/u0H01I77V73TtLiubq206CdnKyLPqHlxJ1wfMKZOcdNveszUfEcl98P59as4Ds8uQSiO88t0CMysY5FRgTleDgAjmtXV9GnvtRsNQtLuO3ubMSKvnQeahDhc8blww2jBz3PXNZ03hS7PhGXw/BqsYWZp/OuJ7UuzCSRn4AdQCN3XkH0FaR5NGwVjQ1fWrmw1TT9Os9PW6nvUlZS8/lqnl7fvHaTg7uwPIHHPGZqviC4n8EX+oxWjRSQGaK6jS7MTxGNmVjG4RsnI44HXt0rVXSbqXVNO1C8vIZJrSOaMiG3Mav5mznBdiMbPU5z2rPvfC11PoGoaTb6lDEl9PcSyySWpchZXLYXDjBGcZOc+gojyaXBWL2r61c2Gqafp1np63U96krKXn8tU8vb947ScHd2B5A454px+KZrmGyjt9PT+0Lmee3ME1xsjjeEkPlwpz04wvOe1Xl0q6l1TTtQvbyGSe0jmjIhgMav5mzsXYjGz1Oc9q5rWtA8p7azujdS6a93dXzzWtiLhlldsohjKuCuHf5ip5UfdoioPT+uoKx1Hh7VZta0aHUJ7WO2aUtiNJjIAASPvbV7g9vxp2o6St/MshmKFV242571D4ZN7/Yyreo67ZHWDzIlicwg4QsigBTjsAO3ArYrmxOHpV4unUV0J7nOS+HXjjZxcqwUE8pj+tY6ffX6129x/x7S/7h/lXEJ99frXx+c4OjhZwVFWvf8ArUEdLrE8kMgxfR20Rt5SwaLcTjbkg7gMgHjrUfiGXyIUkzHGUjkZZniDEOAMKMjA3f0qPX7mWImMG4RfKeQOiREArj5hvYHjPp34qzrFxLDNbRpK6LIH3bXRM4xjlvqeO/4V9nV+AwrfAy3Y3BuhNIrBofMxC4HDLtHI9edw/CmXti146nz2RV6KB39ahtbtorIyP5s58zb99JCOB3Xip7nUFtpxF5UkjFd3yDPGcVxYmeGlTcK700vv122NaXM0miL+zbgf8xCf9f8AGrVrC0EZV52mO7O5uo9qq/2sP+fS4/75p2lv5sdw+CN07HB6jgVy4aeDVeMaF23frL9TV81tRyo7ag7bTsRuqhcA7e/Gc8/yq7WaR/xOgR5ec4OFGQNvHOOvXv0rSr1o9TCHX1KWqavp+i2v2nUbpLeIsFBbqx9AByT7Cq2keJtH12WWLTrzzpYhudDG6Mo9cMAag8R6E2smyltr77HqFlKZraQoHGcYIKnqMH8Kq6Hrmov4gn0HWrW1XUIrYXK3FoxMcse7b0PKnPY1sopxutzQ29M1Wy1izF3YTebCWZM7SpDKcEEMAQc+oqO21vTbu2vLmG7QwWcrxTysCqoyfe5IAIHqOK4u/wBTPgjWPEUaj9xfwHULJccfaCQjr7ksUP0rdsfC1vF4BTw/eSFBLBtuJA3PmMcscnr8x/lTcIrXowJ7Pxv4d1C9is7bUg88x2xgxOoc+zFQD+Bqt4t8ZWvhyKOJWLXjzRLtMLsqozDccgYztzgZznHBqomo634XvdL0/WTaahYXU6Wlvdwr5cqOR8u9OQRx1HSpfiKQui6c5OFXVLYsT0A3dapQjzrswN3Sdf07XPO+wSyv5O3f5lvJFjOcY3qM9D0qld+N/DdjfPZ3GqxJMjbHwrMqN6MwG0H6mti9877DcfZ/9f5TeX/vY4/WvNPB9l4iv/AkdpZS6CbKZJI5knhlMu4kht+Gxu/D0pQhFpt7Ad/qXiHSdI8n7deJEJ0aSI7S29V25xgHP3l46nPGaq2/jPw9dXFnBBqkLyXgBgADYbJIAJxgHIPBwa5m00uXS9d8B2F1PHcyWtveqJU5UgIu3GfQECsuKGOHwmpjRVI8UhhgdxPgfpxVKlG39ef+QWPQNW1SewuEjiSMhl3HcD6/Wny6wsOmQ3BUGWUcKOme/wCFZ/iP/j8i/wCuf9TTdGslvXLz5aKEYVT0yea+QnjMU8dUw9J76K+y21+64EX9uX+/d5i4/u7BitvTdTW+ibeAkiDLDtj1qLVNMt2spHiiSOSMbgUGMgda5qOR492xiN67TjuD2rCeJxeW4hKtPni1/W+wGve6/K0pW1wiDgMRkmmW2v3Mcg8/Eqd+AD+latlpNtBbqJYkkkIyxcZ59qxdas0tLweUNqOu4D0PeqxccyoQWKnU+XRfLYDqY5FljWRDlWGQadWZoMpk00Kf4HK/1/rWhNKsELyucKgya+kw9dVaEaz0ur/5iIru9hso98zdeijqawp/ENw5IhRI19+TWdc3Et7dGR8lmOFUdvQCt+x0OCKMNcr5kp5IPQf418+8ZjMxquGFfLBdf6/JDMka3qAOfOB9tg/wq7a+Im3BbqMEf3k7fhWwbCzK4NrDj2QVjaroyQxNcWwIVeWTrgeop1cNmeEj7WNXmS3Wr/MDeilSaNZI2DI3Qin1ymj35tLkRuf3MhwR6H1rq69jLsdHGUufZrdCCqt7fw2MW6Q5Y/dUdTVl2CIzscKoyT7VxV5dPeXTzP3Pyj0HYVjm2Y/U6aUPie3+YF6bX7t2Pl7Il7YGT+tMTXb5DlpFcejIP6VpabosSQrLdJvkYZ2notaDafZsuDaw49kArzaWBzOrFVZVnFvpd/ktBlWw1eK+/dOPLmI6Z4P0rlSMHFbGq6X9hxc2xYRg8jPKntzWOTk5PWvLzStiJONHEr3o317p2/yA7aUZjX5WOF7DP+TU9V5SQikbsbecfh7VOSFUknAAyTX2dJr2kvkN7IjuLmK1iMkzhVH5n6Vg3HiKZmIt41RfVuTVDUb1766L5PljhF9BWxp+hxLEsl0u+Q87Oy14M8bisfWdLBvliuv9fhYRmf23qGc+cPpsH+FWrfxFKrAXEauvqnBrb+wWeMfZYcf7grI1XRo44WuLYFdvLJ1GPUUquEzPCxdWNXmturt/gwNiOSG9hWSKTKc9PpjB/OlS2VDnJIzwDXL6VfGzu1yf3TnDj+tddXpZdiqeOp+0kveW4bEP2ZCMbmx9f8/5NJ9kj5+Z+feqV7bTvcTzQXCp5kKxqTMw2sGPbpzuAz1BqFtM1CSJ4nuyUa3aPBlYkOc4bOAeMgY/HrXo/VqT3RDnLsacsSpBMw7oR/P/ABri0++v1rsyCunsGGD5Z6OX7ep5NcYn31+tfMcQxUZ00vP9C0b/AIgUSSWcZvIbY/Oymc4XIwM+5GeB/hVjVzcEKkCpkRSSbmi35KgYUemc/pTtWsru8VBayqhVW6yMmGONrfL1xzweDmnT3slpLbG5GEdXDiJGfDcY6DOMZ/MV9XO3IjKpbld3YTR5RLbzbZY5lWUhZI0CqwwDwB9cfUVc8hftX2jJ3bNmO2M5pLa5iuoy8JYqDj5kK8/QgVVvDNPex2kUpiUpvZl69cVy4icadNSa5ndWXn0LpLSyZoVDbweR53zZ8yQv06Zqp/Zb/wDP9cf99VbtoDbxlDK8mTnLnJqKcqtSonUpWt1un+RbsloyEW0pvEuGWDOTuKg5xgj8+lXK5NtQ8UDWX003WhLIlutwWeGVchmZQAN/ONpz6ZHrWz4ev59T0WG6uTCZy8iOYARGSjsuVySSPl4PfrxnFd/Jyq5KhyhrHh/S9fSJNTtBOISTGd7IUJ64KkHtSaR4d0nQfN/s2ySFpceY5ZndsdMsxJP51qVzdtqdw+m3Fyt952orbu50/an7ph22gb+OnJ5oTk1a+hE6ijozLuYpPGXiTSHfR7y0sNLka4klvofKaR+NqIDyRkAk9OBXX39ha6pYy2V7Cs1tKMPG3QjOf6VgyandBZEstS+1Rl7ZRc7EOxnlCsvygA/Kc46j8RS6hJeHTdVga+mP2WaPEoVAxQqjENhcYGTyAOlU23a2ljN4hW0X9a/5FjTvBnh/SrxLy005VuI/uPJK8hT/AHdxOPwrVv7C01Syls76BJ7aUYeNxwaxdR1G4hjsltdSia2lWQtfSzRoCwIwu4Rsvduwzt6+rbvU7+GK123Ubs8CtdSQoGSBSR++GR354PpnopynzN3bB4iKv5Glo/h/TNBWUabbtCJcb90rvnGcfeJx1PSs/UfBvhi8vHurzT4lmmOXKytGJD7hWAY/UVNMxj8TxeZqckMUkC+UhMYWRt3KjK8546HPNX7+3tbhozczKix5ypYAEH1z7A/r9Qc0k73NFNyvboQLYaKs9hOiwK+nxtHa7ZcCJXAUgAHByABz+FQto+gpYm3MaG3W5N6VWZifNDbi3Bz15x09qsfY9IYImIiFYkASE4O3nPPotPe30yVE3PGVCGNMTEDBGMDB9G/Wi77sfvGX4j/4/Iv+uf8AU1e8PLjT3PrIf5CqHiIYu4R/0z/qauaRcRWujmaZgsYlwWPQZIGT7c18thVfOJ/P9C3sas4zbyg90P8AKuMtF33kCnoZFH612LSxzWjyRSLIhU4ZTkH8a5Cw/wCQhbf9dV/nRnyvXop/1qgR1banYJc/ZmvrZbgsF8oyqGyegxnOayfEn37Y+zf0pdUSc69a7IZGhYReYVbgkSZG7ngLyf8AaJxzjFJ4k+/bfRv6V6eeJfUZfL80BP4c/wCPOX/rp/QU/wAQSlNPCA/6xwD9Bz/hTPDn/HnL/wBdP6CmeJP9Tb+m41w8zjk112/N2DqZ+iQiXU48jIQF/wDD9a3LPW9P1B4ktp2cyrvjJidVcDrtJABI7gcisnw7/wAf8n/XI/zFW7LR57e20OJ3izYIVl2k85jK/Lx6nvit+H4RWEv1bZnNzTXL/WxaGuacfMxOcRo77vLbDKv3ipxhsf7OakOqWLzS2/nBnRkjddpPLj5R05yKydN8OG0a3hnjWWK3RkjlN5KxIKlf9UflXg44P0pNO8PXVrcafNNNE7RbmuSpPzsAwjxx2DHr6CvcaiYqdbS6/r+vyMq4j8m5li/uOV/I12NhKZ7CCQ8koM/XvXKal/yErjH981emGfDtllzG3nHa+/YFPz8k4OPbjrivk8i93G1KS21/B/8ABOmTsrmtrMhj0ubHVsL+ZrmtPiE2oQRkZBcEj2HNaNxafZPDqL5ok3ur5HT7vrxnOMk4GSScDOKqaL/yFoP+Bf8AoJqc2XPmdOm9vdX3sFsbv9u6ZjP2tccdjx3yeOB71IdQAzmI/LcCA8+uMH9RUa6HpyMGS22sDnKuwPf39/8AOKlOl2p3ZWX5pRMf3z/fHQ9fYcdK+u90n3iW9iE1jPGR1Q4+vauJru3/ANW30NcJXyXEkVz05eT/AE/zLR2shICfMRlcdcDt7VBrEph0yUg8thR+PX9M1PMMomA2cdhVLxD/AMg5f+ug/ka9fFzlCjXa7foN7IwNPEZv4fNZVQNkljgcc11n260/5+oP+/grjoIJLmYRRLuc9BnFXP7E1D/nh/4+v+NfO5Zi8Th6bVGk5Jvez+7QR0v260/5+oP+/gpGvLN1Km6gwRg/vBXNHRr5RloQB6l1/wAaX+xNQ/54f+Pr/jXpPNMe/wDmHf3P/ICgRgkV1clnHq2l2yzSSqMLJmNsHOP8Tn8Kw/7E1D/nh/4+v+NdAnn2mjfLHunihJEfXLAcDioyChXo1Z88XFNdU0DIm0W2kKGRnfYzH5gvIZw5B46ZX8iaZFoUETKRcXBxjIZlO4Bg3PHPIHvjjpVQ6pqoPlNZtu8wqWS3fBXAwwOTjkk856VJbajqjvaRyWZTdxKXjbjkDqOAcc56HOOMGvq7S7kc6NJbZLPS/s0ZJSKLYucZwB7VxyffX6129x/x7S/7h/lXEJ99frXyPEf8Sn8/0LR3dZJvdQgv1gezkmUiVgUZBuG5dvUjGAcH+ta1IGUsVDAsACRnkZr6lq5nKLezsRW8sk0ZaW3kgYHG1ypJ9/lJFVb6K6a5iktY13IPvlv0xWhnnHeqV3PN56WtsQsjjcXP8Irjx0YOjabe623vfS3zNIXRH5uq/wDPvB+f/wBepYbuRGEd3HskY/LsG4Y98Zx9TxUP2K+T5kvyzejLwabbzzXV5C/C7MrMgPcBgP8APv7Vz4RzjV5ajmnbRScWn93VBUdloYfiu4gm1a1srjTdKuoYntmkN/EJGKzTiI+WD0I6k+68c1v6Fdre6SkqRRRRrJLFGsQ+QokjIpX2IUEfWsXxNeWE18dMvNJsb+4EcLWcd0qtvkkd1OAQcBQm5iO1bWhXZvdJjkMEcDRvJA0cRyimN2jO3gfLlePaval8CG9ie6uDBc24LhYm3l89MBc01tQUeSBBMWmDFFCjJAx6njrnmi9sReyQ78eWm7cMkHkcY/GiO3uDcW8kzoxiV1LDq2cYOPXjmuFurzNLa6/S/wCplrcX+0I2jjKRyu7lgIwBuBXg5yccfWmnU4BG77ZDsUErjnO7bjr1zUaWU8MgmjMbSCSU7WJAKu2euODwO1ImnOJYpHZD8zvMBnnJBAH0IFTzV+39afrf7gvIfe3TGzuZLeR0eD+IAEE4BxyD6irM9wsBRdjyO+dqJjJx164FY/mJ9gl0+OeB7gkpISzDByFGPlOT0H5+lJeX1pO1rJOIN4UkJICyEEKcZ2/ewVPTgZp3q6tJ3svzd/wFdmp/aEZSMpHI7uxURqBuBHXOTgYqjm3vVElxDJNK0kkcaqdrbQ2cHkDjHfnkjucsF1bRpDcQG3jMIO9FVlQK+Mc7eCcDHHPakdCsduJLiKMSPJOJ8kLz0GQQed2evbvUTnWg+Z/1t/wQbZKkdg9hbxpaTbCzeXCH5PBVifmxjBI5PepYNN064TzUhcHc2Q0jZDZGc89cqOfaolthc20Mxit2WAuoWQEI6/3uc46dee9WdLurW4twLbyAAociDlRuzgfXjmtqNSrJ3e1v8v1uNS1szK8R/wDH5F/1z/qafbNt8NTEziDMmPMJIAyQOo6eme3WmeI/+PyL/rn/AFNWtKiabRHjUZLOeN7J6d15FeDhP+RxU+f6Fy+Es6fI0ujB3cs5D5JUrg5PGD6dPwrm7D/kIW3/AF1X+ddTbwSW2mtFK+9wHOdxbqSQMnk4zjJrlrD/AJCFt/11X+dGff7xR9f1QR2NbUraY67DNFbNKhMIk/euQPnYg7AwAC4zkg80niT79t9G/pTdWuRbeJLFcbTMEQETlN539CAwBABPUHO7A9C7xJ9+2+jf0r088/3CXy/NDJ/Dn/HnL/10/oKk1+EyaeHA/wBW4J+nT/Co/Dn/AB5y/wDXT+grWljWaJo3GVYYNY4Sh7fLFS7pgcroswi1OPJwHBT/AA/Wutri7u1lsboxtkEHKsO47Gt2x1yCWMJct5co4JPRv8K8/JcZHD82FrvlafX8gZr0hIUEk4A5JqA39mBk3UOPZxWNquspNE1vbElW4Z+mR6CvaxWYUMPTc3JN9FfcRj3EnnXEsv8Afct+ZroZFS1sLGKU3g3cH7MHJHyk87QT1rL0iwN3dB2H7mM5Y+p9K2NckeCCCdZZowkwyYtuMEEZOVPH4dcfWvK4ew87yxE/taL9fxCWwy7gJ8NogWQGOJDiQ5YYA6+9YWnSiHUIHPADgE+x4rq7VVk02FCrhGhUFZPvYx0PvXJXlq9ncvC/Y8H1HY1ln9OVOvDFR6fmndAtjtqKxtO1uJ4Viun2SKMbj0atBtQs1GTdQ49nBr3aOOw9aCnGa+/YB95IIbOaQ/wof5VxFbOqap9uxa2oZkJGTjlj2ArHIIJB6ivlM7xcMRWSpu8Y9fPqNHayKGWPJYcdQM1X1mIy6ZLgcrhvy6/pmpp/uJx29M/5NWNuU2tg5GDx1r6iVNVva0n1VvwG9kcbp0wt9QgkY4UNgn2PH9a6GWzv2knZLrbvP7s7jhRkfw4x0yPfNYGpWD2NyVwTE3KN7elaOna6scaw3efl4Eg54968HKcXHBznhsR7uv4/11InBSLQtb2WKdTjDBlCySsQeRg8jjAB+uama0vHlYmbajNk7ZG5G4HA4+XABHHXNSjVLFhkXMf4nFMfWLCMf68MfRQTX0bxeHSu6i+9EeyQsUU1vcTzzzkwAEgbicD6Y7D0PNPGoWpIHmHJycFGGAMZJ44HI5PqKz21iO+lFpFGwWbKF27ZHpWjHYW8SkBCxKlWLMW3A4Bzk+w/Knh8TSxCbpO6Qcso6RFS+tnOBJhsgbWUg5JwOCM9x+YqJ9Tt440ck/OwGMHgFtuT6c+vpUi2FsjpIEO9CSHZyTkgA8k88AflVVrfSQoV5YyoOcNOTnndzk88881tKcY/E7CftbdCz9phurKaSF9yhWB4Iwcehrjk++v1rqTdafb2ckMNwmNpwN5bt71yqnDg+9fJ8QVITqU+Rp7/AKGsb213NlrnTzcsFN6ZZZCSFx1LYA54HOCPoO3BY11YRREGS9C7dq7QpO3YWz7cS9/6VuDUrI9LmL8WxTxeWrdLmE/RxX1axVF7SX3ojkRgySaaqOHN3uDmNvuFgxIyOO2U7cHJHPStW4glt5Ybi3QyeWnlshPJWtCoHudt7HbBMllLE56CssWoTpqMnbVWfnfQuEVF3SKn9pyt8sdlMX9GGAKjt7VoLyAyDMpJdiAcZIbjOcenarNzeyJceRbw+bIBubnAApkO69uILxSFVMq8Z65wf8f1NceHcZYi06jlKN0tLLpe3Rva+ugVE+VWXYoalcX0+vLY6cNPingtxOZ7yJpCQzMu1ArKR9zk57rxzV3QLwX2jRT+TFEd8iMsPKFldlLL6qSCQfQ1jeJIBq2qx6a1lo7iGAXAl1SEyA7mIKoMjpt+Y5/iXjmtfw/fLfaPA/lQQsu6Py4GymEYplP9g4yPYivYkvdRT2NSq1/cPa2plQKWBA+bpUV7LK1zDaQvsLjczjqBVTUreW3sz+/eWMkAiTkj3BrxcdjZxpVfZRfuq19NHb9OpUY6q5sA5ANLUMyPJAFSUxHjLAZ4rNuCbSPzYdQaR1IyjuGz+FdGJxjw65pRukrt3X4K92TGNzT+y2+7d5EW7dvzsGd2CM/XBI/E1XhtrCR3CWcAMJ2Z8teMY6f57VO6faIVxJJHnByhwazLC0803H+kTptkI+R8Z9z70sRi60K1OFON1K/XyBRTTbNI2VoetrCeMf6sdOOP0H5VMAAAAAAOgFNWMpCIxIxIGA7cn61nyQbAd+puJfdwBn6VrXxE6UU+S/zSt5asEkzTrH0aYyTz5LopRCkbSs/y/N8wyTjPpxjFXbC5aexWWT7wyCQOtZ+hxW6yy7URplVWMo3/ADZ3cjcABnB+7xW9CrGtSVSOzSZlNNTSJNW0ue/uEkieMBV2ncT6/SrWl2kllaeVIVLbicqeKnvLj7LZT3G3d5UbPtzjOBnFcponj2PVtRFtLYraxhGkaZrjIUKMkn5Rx+Nc9PAUoV3iI/EyKmIp05KEnq9jr5FLxso6kEVgW2g3UN1DK0kJVHDHBOeD9K2ob60ufN8i6gl8o4k2SBth98dKSC/s7mVooLuCWRAGZY5AxUHoSAelGJwFLEyjOoneOxrzIxtVnltfEMEiO0cRiTzmDKBtDnJbP3QAW+ucDpza1mya8WGRJYkReCzsQDkjHQUy5t9G1Z2vvtkbNagFpobgYiKksCcHHGSeePUGrASwv9OawtbqGSOHZG4Rw+0KRw2D7Y5roxOHp4ml7Kps9w5l0HaTYy2EDpKyEs2RtJ9K0K5qPSra/Vo7XVbeW2S58544lD7evyZDYAwcYx2/CmWmk2l4wW31e1uVg4dYlViAWJKsQ3T6+lOhh6dCmqcHoied9jobq0hvIvLmXI7EdR9KwZ/D1wjEwOsi+h4Na+lyWht3htLuC4WORs+U4IQFiQuATjAOPwqjY2OoSCF7meRI8hnj899x4bvxjJKfKOPl965MXlmHxTvUWvdaMpS7Gd/Yt/uAMIGfV1/xq3aaAXO6eZdoOCsZzz6ZpZNJ1V5lk+0ruVCqt575Dn+Pp06fJ0496mm07UneTypkgVrhpP3cpBIKgA529RjpyDn2rlpcP4OEru79X/lYOZ9jYihjgiEcShUHQCory0F5GiGWSMK2T5ZxuGCCD7EE1V0qyvbQyfa7kzBkQDMjNhgDuPPTJNYHiey8LtqRn1/U5QWQBbQ3DbV9wi8jNezCCXux2XYicrRv/wAA623iEFvFCGZhGgXc3U4GMmob2whvo9sgww+646iuC8LXllaeNzp2iXs8+kz2pfy5d2I5Ae24A9B19/an3Ony6t8TtSsRdTQWr2sb3Ahba0igLhc9uSPwzRVw0KicKmqa6mar+6ml1sbk2gXaMfL2Sr2wcH9aYmhXznDRqg9Wcf0rH1LSIfB3iDRLnR5JoYbu6FvcQNIWVwSBnn6n9K0viBJdR2mnndcrpfn/AOntbZ3hOMdO3X9K8Z8OYRzTTdn5/wBfmV7ZpNtao6DT9Iisj5jHzJv72OB9KzrjQLmS5leOSEIzEgEnIBP0rM0Pw94TuriG+0O8lEkLCQrDctk4PR1POD+FWUxYfE253HbHfaeJCf8AaQ4/9BFdVXKMLOmqNrKOvYaquybW/mdSUJC4YqQMcU+vN/Bc0kfiZr2ViE1uCa4Absyytgf988/jUEesz6R4DvdYgby7nVtRkMcpH3AxPP4BW/OvRVCzdvIhYlct2u/9fielzwRXERjlQMp7GsO58OuCTbSgj+6/B/OuGmPgcWTyQ69enVQhK3h87eZMcH7uMZ//AF12nhTV59a8GJdXR3XAR45G/vFcjP5YrjxmWUMSr1Y/PZjp11KXL+TuVzol+DxCD7hx/jSf2RcK22V4IjjOHkHT1rnvCmjQ6v4PhuNauLlrC3LpDbwFtoG4kuwXknJI9ABXX2Oh6Romm+ZG1y9qMSqSzuV6kEBeeM/hXnPhzBwk7yk/u/yCFZyV7Jddx9jptpbTiaS9jZ426BgADjP8ua1xd27HCzxk/LwGB6nA/M1WGm6fcxI/khoyoK5ZuhUD19APyqc2NuSx2MCxDHDsOQxYd/Uk16WHwtHDQ5KSsjW8mOM0UuYkkVmZWxjkcYB5/EVzkWhXUyF1khADMvJPYkentXQQWFvbyB4kZWGed7HrjPU/7I/Kq8NrIsynydjCZ3MuRypLYHr3HFceYYOniZw502lfb1Q031Mr/hHbv/npB/30f8KaNBuWj3iaDb1zuP8AhWvZ2bRv++SQtsKuTs2v+XJ/GnQWYFjJbm2VGKkFiFw55weP6158cnw8rPka36v/ACHcxx4eu2UMJYCCMj5j/hS/8I7d/wDPSD/vo/4VqvbM1nEiQGIKwMkahMvxj3B5x19KclnkW6tGxjUsWWQqcZ6DA4x7ULKMO3b2b6dX5eQXI9Wvry0e3jsoFmeXcCu0kjpg8Ecc85wOnI60+6E0N3FdpCZP3ex1U8jvVLWf3WoaWmxBF5qBPu8MHXgAoccehHT6Z1Lq9S1KKUZ5H+6iDJr2saoex5qkuW1nfzvp6lrpYjsIZEEs864lmbJHoOwo0xHWCVnUrvlZgCMHFQ/2udrN9jm2qcMcdDV21nNzD5hjaME8A9x61xYSeGnOEaU23FPo9b7t6f1cJX6nNa/YRaxr4s003Q5LiG1WVrjU7MTsULMAiDIOAVJJzxuHHNW/C1lpX2BNRs9H06yuX8yGR7OBVDbXKnawAJQlMj2xUOt3GgTa0NP8RRaV5K26zWzXpTLMWYOBu6YAT65PpW9Yy2c1lE9hJBJa42xtAQUwOMDHHGMfhXtSb5UhN6FW8P2fUoLl8+UVKMfTr/jUeq3cUtp5cTiQ5BO3nApnii+k07Q5bkWq3MSsolQ3LQsFLAZUqpJOSOOPrVHxBr/9h3ItYdIjuo/JWeX96EwpkVMAbSCcsDyR0PPr5NbL61SNSnSklGpfdap2SfXqXHWzsampn/j1WQkQFv3mPw/+vUWoNZizaO3SJnODmMA7RnqTWYvii9uprOxg0aCS7uPtayI93iKNreRUPzbCSDuyDtz045JDl8SLLY6SbLSFkm1OWWDyHkCLG8YfcGIU5AKMMgfQHpU4jLKtSVSzXvq12rtaW012/wCCNXVjpof9RH/uj+VZ1jPFbzXUczqjeaSNxxkVXudYv4JrHT4tNt5NTuIpJmh+1FYY0QqCfM2ZPLqANvc9MVDaeIbbUZtMMtiEjvRPHvkIJiniOGjPHP3ZDnP8B45rprYWrJ0502rxvvs7qxCWjua99Ix06SSBskrkFfTPP6VVhfT4rRCFieQqPlxli1Ys/jiC20/S5TDZ2z6isstuLy9FvF5KEYYuVOCwZCFAP3j6E1taBqNnrekW+q2sCIswbptOCrFWAYcEZB5HWor4GrKqq2m1tVe3mtf6sh6pak2jjFgAeu41fwM5xXLa/wCMrHQJGtIIRcXQ5ZEO1UJ/vH19q5j/AIWXqvmZ+x2ez0w2fz3f0rfCYd0aEKT1srHBXzGhCbTep6Jq/wDyBb//AK95P/QTXjWh/wCsv/8AsHXP/opq7+x8ZWev6beWbxm2vHt5AqFsq/yn7p9fb+dcj4Fjjm8TxxSorxvFIrIwyGBU5BHcV1R0POxdSFatScHdP/M9AtbO6mujcGy+yolgbZV3qfMYkEYwfujHGcH5jwKmTSpT4OXS0C29w1j5Jx/C5TB5Hv1IraAAGAMCsHX/ABbYaBIIZklluGXcI0XHHqSeOx9aXM2erP2dKLlN6FHVbe5l0/U7ySxNlFFpE8HlllO8kZGNpPyrg4zg/MeBUr6Xe6ilxtthYg6a1mhLqQ7HoRtJ+UY4zg/MeBXMXnxIu7hHij0y18lwVZJ8yBgeoPTIot/iZfow8+wtXQdoyyfzJqrs4Pr2F5tZP7jsYbS4uL83MunG2hSya2MBdMykkEAbSQFABAzj7x4FUoobg2Go6e8zWkDWwt4PtcsRkjdgQBlCfl5GM/NWjoHiWy8QQsbfdHOgy8L9R7j1FUrrS47zxI7TJJErHarLG/7zMWG+bdtGQCuCp4HBBOQRfc76cYVIqUXdE+i6fPDfC4uUvlkS3EA85oNmMg4URgE4xwTjr0qfxTqT6T4durmJ9k2AkZ9GJxn8OT+FQa74usNAuVtriOeSZkDgRqMYJI6kj0NcF4p8Yf8ACQ2sVrFatBEknmEs+SxwQOMcdTS1bucmJxdKjTlCMveO58E6hd6noH2i8mM0vnMu4gDgY9K6OvJvDvjX+wNL+xf2f5/7wvv87b1xxjafStf/AIWh/wBQf/yZ/wDsKTi7k4fMKEaUVOWttdz0KvOLDULLw34w1yXX4pI57iffa3LQs4MeTgKQDjjH5Y7V22har/bWjwah5Pk+bu+TduxhiOuB6Vo04y5bpnc0qqjOL80ef2V7NqnxNtL9rSa3tpLJ0tzMu1nUZ+bHbkn8MVd0/wD5Kxq3/Xgn/slSax8QbDTriW2t7eW5njYq2fkUEcEZPP6Vz0nxL1Qt+6s7NV9GDMf5iq5m+nSxwzxeHpuznd3vsbvjz/j88N/9hOP+Yrb1vxFZ6C0H26K4MUwb97HFvVMY4bHTOf0NchZ/E2YOBe6fGyd2gYgj8DnP513mm6la6tZJd2cm+J/wIPcEdjUt6JNbG9DEU6zk6UtWeeXk+la1r+mT+E7SVb5LlWnuIYTHGI/4t3Qf48jnNafxGkfTzp+qxg7lWe1JHrJGQv5EZru6gvL230+0kurqVYoYxlmP+etV7TVabGro2jK7tf8AQ4TxXat4f8OaFdxKfMsEa2bH+3EVJ/MD86taj4auLv4cadZ2a/6ZaJFcIn958EsP/HjVS/8AiYRIV0+wBQdHnbr/AMBH+NUV+JeqhvmtLIr6BWB/9Cp88rI4ZYzCqTXNo9Njeh+IGjLCqX1rc216Bh7ZrYlg3cCukt7tb7RDdJBLAssTMI5U2sBz1HauZ0f4hRaheQ2lxYSRyyuEVom3gk+oOMD862tRGNctX8uF/wDVjEqbm++eY/cdT0wMHmpdnsrHXRrxqR5oyuvSxgeDJooPhrumlSMMJlBdgMkk4HPeuia9tf8AhFzJ9ph2fZvL3eYMbtnTPr7VsVzOv+NbDRJGtkU3V2OsaHAT/ePr7VMnzSbCfLRhecrK1h11cCfS7D7NdsE8h9zQS4+ZYsgEg9jir2jvKVu0aV5CrIVMjFuWjUn8MknHbNcK/wAS9UL5Szs1X0YMT+e4Vq6V8SLeeVYtStvs+4486M7lH1HUfrUuDOWljcO6ifP+HlY6/F0A5UNknK52+gzn/wCtT1+0hwCCQG68cip0dJY1kjYMjDKspyCPUVHd3MdlZzXU2fLhQu2Bk4Aya51h7faf3nquSSuNT7RxnIwRkHHPPP6Vxl/4jvj8QYNOt7lltFlSJ0AGGPU/zx+FOufiZYpn7LYXEp/6aMEH6ZrhLfVni8QLq0kfmOJzOybsZOc4zW9KlyLdv1PHxuPpvljTl11t2PdKK89/4Wh/1B//ACZ/+wrQ0Tx7/bOsW+n/ANm+T5275/P3YwpPTaPSq5Wdkcww0pKMZavyf+RtaurtqWm+VJMjrJk+WjkMu5MglRgD2Pp7VakBXWYXIJVoioOOhBzVfVZIU1DTkm3/ADyEKFfb82Vxkdx/9ar11eRWigvks3CqoyTXNi1D2alOVlFp39Gd6uVYrqKzuLmGdtm6QyKSOoNSaUd1q5AIjMrGMH+7/nNOt5YdQjZ3txlG24cAkUf2jbrAjhWCHIAAxgjt7Vw4WH7yM1NONpculm03rf09BSaS1MnWLi6l1kWNpHpcbJbrM09+hfeCzDagBHTbknPG5eOav6BfC/0iGUxQwvl1aOFsodrldy/7LY3A+hFZniO3sHuo573w/pmoExEJLdIC/HPljMbcnccDPPNa1joWj6XO0+n6VY2krLsMlvbpGxXIOMgDjIH5V7TtyoNLEeu6KNdsDZSX11awt98W4jy/IIzvRuhHbFQXXhqC+w13eXc0vkiFpWKKWAkEmSFUDOVA4HT35rZeWOLHmOqbjgbjjJ9KYl1BIGKTRkKSGww4x1qFJonntpc5e78LS/8ACQ6fPZ3N3bwRpfSyXMTpvSSaSJgoDKQQcP1U4wO+DT7GPQ47jSrW3luQNOZmiZlb5pJNyMJMrkEliecZLDHpXQtPa3HnW7SIwCHzBu6A5B5/A1TTQNKsx5qwugTDE+dIQdvIJGecYBH0qudW1KU7rcqX0Vvq11ptzDPdWlwfOiiurcqHUdWUq6kEHYOoyCoqlcadol3oFrooe+hhIkmimhVjKpViJGLFThm3tnI/iPStiOx0yBoFUzK0LM8YaeXgtnOcnnqeDSx2mnW2wQu6PD5jIzSyNgsctnJ5GRnBPap9rBdRcy7kc2mWWpi3lsruW1msQYoZ7TblFZVJTDKylSAnY9B3FGrXjaB4annM8k8sUe1JJsFncnAJwAOp7AVZ06CDS7CGyiydilhlSu45JPXpznjtXN/Eac/8Izb7cgS3C5+m1j/hQpKTsmc+KquFGUl0RxvhbRj4j10rcszQpmWds8tz0z6kn+detQ6Vp9vCIYbK3SMDG0RiuM+GMQFtqM2PmZ0X8gT/AFrvqcnqcmWUYqgptas4Hxv4WtYLFtW06IW8kJBlSMYUgnGQB0IOOlcv4OuhZ+IY5yoIVG4LbRyMcnt1r1bXYxN4f1FCOttJ+e015d4EjSXxTAki7l2Oce4GRTT01OTF0Y08ZTcNL2++56WmtM8cki26bI0BZvN4yWZQQcfd+XO705xWNrEWm64yT3ttuNojHas+0yDcRxxkp8ud3HFdfVTUNUstKg86+uUhTtu6n6DqfwpXXY9erBONqj08zDs20y0QJb6RZxHyGlX51LMQTxux04BySOvTrVq70PS/EWkhntoUeRMpNEuCh9jgEj2NZFz8SdLjYrBa3M2P4iAoP65/SqbfE+Mfd0lz9ZwP/Zab8kcbxOES5JSTXp/kcv4Znm0zxdaITtbz/s8g9cnaR+f8q7+Moni6OCT7EZxKXXZGVfYYjzkufXG3HPWvOdPuPtfi61udmzzr9JNuc4zIDivVcXc2sWsssLLEspG37SCEPltg7QOpB6ZPXNV3M8ofuTS2uX7nS9PvZRLdWNtPIBtDSxKxx6ZIrg/iLp1lY29gbSzt7cu7hjFGFzwOuBXo9cD8T/8Aj103/ff+QrOO5vmMI/V5O2v/AASXwJpmm3fh3zLuxtZpTOwDSxKxxgcZIrpv7D0Pj/iV6fz0/cJ/hWH8PVX/AIRcsxwBNIDz2wK6jyEZcK+c4J6cisasqqb5Vc0wVOm6ELroLBDb2kSwW8UcMYztjjUKB3PA+tSB0OMMDnpzTRCAQd7HnPb0xR5K5U8/KABS5qvY7EorRGNP4Q0e71WXUbm3MssmCUZvkyBjOB/WtOLTbCBNkNlbxr6LEo/pVPVvEmlaMdl5cgS4yIkG5/yHT8a5yb4m2KsfJsLhx6uyr/jW9mzilVwtGTu0mS+NvDNlJpE2oWtvHDcwDe3lrtDr3yB3759qxvhxcNJdahppkdY5oN+UbBUghcg9j83X2FO1T4iJqGmXNmumFPPjaPeZ87cjGcbarfDb/kZJv+vVv/QlqtbHmupRljYSovfc6K5szZ3t5DcXPiMqdv2U20ksoYbRnkZAbdu+9gYA96x/Hk15Ho+h2lzuVzFumXcWzIFUHk9cZPPvXQapc3tr4jilkTU5IRNhI7WNmiaIwsOdoxu83H3ugwemak1nw5PrvhmzguZFGpwRKfMboX2gMDjsT/Sqk9Fc78TSlUozhDf/AIJT8C6LpjaBDfNbxT3Mpbc8ihtmGIAGenA/Wupl06xmXbLZ27r6NEpH8q8hhu9e8H3jR4kt8nJjkG6OT3HY/UV1ul/Em1mKx6nbNA3eWL5l/LqP1qGmc+ExdCMFSqLla7r+vxOht/C2j2mpx6hbWginjzgITt5GM46flT7so+oiL/Ti6tE4EYfy/v5OTjHbnJ6VftLy2vrdZ7WdJom6MhyKytTjWXW7UblV1MbAySYBAc8KMHJPGenaiO56KjCMfcSs+weLNZbRNBlniOLiQ+VF7Me/4AE1wvgrw3HrlzNfX+6S3ibG0n/WOeeT6D+tafxPmOdNgB+X945Hv8oH9a57RfGOoaFYfY7WC1aMuXJkRiST9GHpTS00PHxVem8Zat8MfzPW49NsYo/Ljs7dE6bViAH8q4nxz4Ws4tObVLGFYHiI81IxhWUnGcdjkisv/hZWs/8APtYf9+3/APiqrah481TUtPnsprezEcy7WKIwI+mWoSZpiMbhKtJwt6aG/wDDfWJJoZ9KmYt5Q82HPZc4I+mSPzNd3JHHNE0UqK8bgqysMgg9iK8m+HrFfFKAfxQuD+hr1ulLc6ssm54dKXTQzv8AhH9G/wCgTY/+A6f4V5FokMUviu0hkjR4muQpRlBUjPTFe3V4roH/ACONl/19j+dETmzGEVOlZdf8j1n/AIR/Rv8AoE2P/gOn+FSQaPplrMs1vp1pFKv3XjhVWHbggVbeRIygY4LttX3P+RVX+1LMxtIJSUXbkhG7jIA45ODnApHqctKL2SM3XUiOq6SzuFbzxt3s+3IZegHG45I54wTntV65ZYdWhllIEZjKqx6Bs0v2izv7qGJS8m0GVWVmCHaVx04bkj1xirMrQSOLeXazMMhCOtc2Mhz00k7NNNX2vfRfM2jNPYpWVzDCLt5JUA89iOeo9qr252RxyuAoeZpFDemR2wefTpV9dLslbcIBn3JIpL1N0tsPK3x78MMEgdPT/wDVXFhcLXg4us17t7Wv1fW9hVZLldjM8RRpcS20YN35ixyOhggaQHoMZAwrHPBPTn1roK5zxSQj2TNAky/vBtkh81ScDAC/3iRge276Ho69h/CgIbi3W4CZd0KNuBTGfzqCXTo5FaMzSqG3nAK9G+8OnTJq7VSVYxfK0jsoKHB3kAnI461zVqjpxTS6ryF7OMnqQtpSb49rsVDLncRwq5wowOQdxBzWgwDKQc4PocVnzSyxySEM2IieM9d3T8jTpGZPMBkfzlwIl3H5uB2785rleOir+7t/wf1TXqio0lHYe62Nu6RSSRxvKNiI8mC/sATz1pgn0shStzbkOpRcTD5gBz35wKhv9Mnu9VtLpJFWOEYIJ65PORg7uBxyMHJ56VHe+Horqyht1uJ1WCERRpuG0kDgsMdfpivQVKnpoKyLAvNMnvVRbmJppFyu2X73UYGDyeD0rn/iJbf8UxCUB2w3CnrnAww/qK24NAt4meR5ppJZGiZ3YryYypXoB/dA+nvzVjWdOXVtIubFiB5qYUnsw5B/MCnywi7xMMTS9pSlBbtHG/DGdTDqMGfmDI4HqDkf0FegV4toup3PhXXy00LfITFcRdyPb36EV6bB4w0GeESDUYkyOVkyrD8DTktThy7E01R9nN2a7lvX5RB4e1GQnGLaQD6lSB+teZeAP+Rsg/65v/6DWx4r8UprkS6NoyyT+aw8xwpG4DnAB57ZJ9qx/AH/ACNkH/XN/wD0Gmloc+Jrxq4ynyapNfmeukgAk8AV4pd3N34q8SKC53XEojiB6Rrnj8hzXtbKGUqehGDXiFu0vhzxMjTxkvaT/OvdgD2+o6Uom2at+4n8N9f6+89R07wZomnxKps0uZAPmknG/J+h4FaqaZYR/csbZf8AdiUf0qlb+KdDuIRKup2ygjO2RwjD8DzWF4i8dWcVq9rpMhuLuQbBIgO1M9x6n0xS1Z1uphaMLq1vKxw9qyt41hZMbDqKlcdMeZXp/kv/AMJPvcPHFv3IvluVkbysbt27aDjIxg9PXp5XpdvNaeKLCC4jaOVLuIMrdQdwr1hjosd0t8wVZUlYCUq3DBSrY46AE5PSrOPKZWjPm01NmuB+J/8Ax66b/vv/ACFdxbXUd0rtGHwjsh3IV5BIOM9eRXJ/EexkuNDhuY1LfZpcvjspGM/nioW52Y9c2GlYj8FWv2zwY8Gcbrg/xEcAqT0rpZLO5aaYI4SFzFgrK27Cn5h04yPQ1wngXxRZaXbS6ffyeUjSeZHIQSMkAEHHTp/Ou0fxXoKDJ1S3/Bs/ypu9zHB1aLoRvJJpW3JJ7C6ke4CyHy2aNowZ3B+UgkcdAQPc5NJ4h1J9I8PXV4mPNjQKmTnDEgD64zmr1neW+oWiXVrJ5kEmdr4Izg47+4rM8W2MmoeGL2CFS0m0OoHU7SDj8gaR1VNKUpU97Ox5p4Z0VvE2tOt1M/lqDLO+cs3PTPqTXp9r4Z0SzQLFplucfxSIHP5tmvNvBWu2+iatIbslbedNjOBnaQcgn2616U3iXRFj8w6raY64EoJ/LrTlc87LVh/Zc0rc3W5W8SWllbeGdRdLWCMiBgCsYGCeBXE/Db/kZJv+vVv/AEJas+KfE7eIh/ZOjxSSw/fkfbguF54HoMZ5qt8Nv+Rkm/69W/8AQlp20M6tWFTGw9nsjtbr+1bjUrizVrmC1aUSi5iC/LEIx8q9fmMme3T61q6a9zJpdo94u26aBDMMYw+0bv1zWDrEOom7nuGutShtUnVFWyXcfKMX3goBJPmHn2FR3Pi9NC/s611O1m8yWzjkkZW3Mrnggg9eR1zTa0R6sqsKd5TdkdPcW0F3CYbiGOWM9UkUMD+BrkdX+Hen3atJpzm0m6hCS0Z/qPw/Kta38ZaBcKCuoxofSRSpH5iku/Geg2kRf7ekrAcJCCxP9PzqFdGdZ4WrH32n8zzXSNRvfCviAxyFkCSeXcxZ4YZ5/wAQa9N1CJZ9Qhm8i7LRTRqGCjZgPyRk5HXk46CvMCJ/FfipmjiKm6lyQOdiDjJ+gFem6tE76rbeWrbjs2yeVvERD9zn5c9OOvQ4q+pxZa3yzS1inp/X3HN/E+BimnXAHyguhPucEfyNX/h41vc+HXjaONpIZmByoJwQCP6/lW54g0Ya3oktkzDzsBo3xgBx0P0PI/GvMND1i88JaxKk8DbSdlxA3B46Ee47fWpWqsFd/V8Yq0vhkesX0McVo7RQJvyoGEUnkgd+Kqi0ujFI3lIHCfu1KJydx68ddu32zVW38b6BcRhje+Ue6SowI/TH60y58d+H7dSVu3mYfwxRNn8yAP1pWZ3SrYd+9zr70XBDfxfNHEu/yjyFjXDZ/U4+gyPfhxTVWgceYwchlHCDHyHB+u7A/Pj0j8O+IY/EMVxNFbtDHFIEG9sluM59v1rapWLpxhOKlCTsMi3eUu7fuxzvxn8ccV4xoH/I42X/AF9j+de1V4nfJN4f8WyMU+a3ufNQH+Jc5H5iqicGae66c3sn/keyTi3nHlyuMowPDlSD25Bz3quNO05iECgnaFUea2QF6Y54xWZB4m8N30azveRox5KTMVKnHp07dqt2Wu6JdX0drZ3iS3DbtqpuOe556dqj3jvVWlN7p3NCKyt4JvNjQh8EffJABxnAJwM4FV7uSO21GK4d1xs2MM8gZ4OKfNqcUBlDRSkRuI8gD5mIBwOfQ1EbqyurlUkt2LnaFMiAEgjcOCc469utc+Lozq07U7XTTV/I2jOEXYsjULQj/j4j/OoJ5YpZoZE8qQF1Xd5e4ggg9c8darCa0MbSrYq6EQlAoAYiQ4Gc8Vce0CyQtDFtUHJQBcA5HPI9B2qaaxV/3vLbyvf8RSlGUfdMzxO6oLYP9q2OsiEQttB4BA4BJbcBgcdD9D0FYXiG7eye3mjmkidUkJKsoDL8pK8q2WOBgcdDyK2opo54llicOjchgeDXc/hRY+iisObxZpkE95BKZVltnEZTZkyMc/cAPPT2qCJ1IQ+J2JZNb2GcJHHL5LSF9svRUAJ7deenr3qSLUppCqeTCZGuHh2iU/KF3HJ44yF/Wq2neKtO1G9FmFntrg/djuI9hb6cmte5uYLK2e4uJFihQZZ26CnddjOE1Nc0ZXRjXPiEpbymOKNZBGXUtJwDg4U8ff4+5+tTrr0ckscaLGGYLuLy7VQndlScH5vl6e9Uf+E40vIYw3ogJx9oMHyfnnP6V0ME8VzAk0LrJE43KynIIp3XYVOoqj9ydyrp2o/2h537kxiNsDLAk9eCOx46e9XqxH8V6ZELzzWkjNrN5DKVBLtz90A89Pal0rxNZ6tetaQwXcUoQviaPbwO/U+oqWONendR5rtia54W03XsPcI0dwBgTRnDY9D6iuBuPDfh+0u2hm8TAFTgqtsWx7bgSK73W9fTTlntxZ30kvlEiSGHKKSODuz2rl/C+q2NhoqxyaLe3MrMxeWK1Dq3PHJNO7PPxcaE6qjZX6t3/Sx0mhaJo2h2P220JkDx7zcycsUxnjjgfQVQ0/QtB0HUjfw3N55yRu/lSDgDBJGNoOcAnHWunl+zDT284LFbCP5g3yhVx09uK5mXxVoUk8s32W9ljZTHJOkR8sjGOeR29s0X7nTUjRpKKdlbY6b7bDuKZk3BPMI8tsgc9eOOh4rA13StC11ozdSGG6KArLHwcHkA8YPrzzj2rZsmsb60+0WkhlimQJ5gkbJAzxnOQeT71lS6hodleXtpOskRtYFEjOxKsrAAAckk4OOmaNDSs4ygue1n3OYf4f2nmbU1h2JDFVNsRu25yA2cdjXQ6H4U0rR0hvlSW5uGClGlx8pPoOgPvTbDWNEvtShtYrK+ilkXZGzhlBAUjs3pnmr11rumaZLJpssUzNBGpWPbvLg9AuTkn60Sba0OWjRwsP3iS/Hf5lK78Pabda7/AGvN9rjuUkVwiOgRyhA6EZ7DPNaj2doxmnEJZgVmDGRsnI+voOnTpVA+LdEewS4WGWUlmHkLCDIuMEkjoB05zWrY3Wn6lpwvbdEaCRecx88diPasv3nc6aXsXJ8jV3qPjmgt5ZI4o5SWkJIByCx5OMnA5z6UPe280bxvE0kbKAVKghgR0wfb1rDPjPRw7PBZ3c6KeZorf5R+JINamk3Om6tbtdWTCSMnayMoG08cEflj9KTVToy4VoTfLCSZy2seBdKBjntZrm389sLGFDgE88AkH8Mk1mReCrLJaTULqRFBYlLYL8o6tksf8faus1HXtGea4t7ppo3spAmwKCZGYH7g55468EZ680tj4k0W8vhZm0ktZ5sgLcQBPMz1Hfr79a2UmcEsPhJVOhZ0aS007TrfT7RLmVEYojOBls7mzngY4Pv0q4NUjeZYo4ZHYyeXwVx0Y5znp8pqw6WlpG87pDEiZkZyAMepz+JrmW8XaCrOsFjPPGGy0kVsNuR35xSO2U1RSjKSX+RDe+ENI8QTzXNqZrOXOWKqNj5zyB+HqPpzVOP4YRB8yaq7L6LAFP57jXY6Vd2d/ZLeWUeyKUk5KbSTnnP45qhfeLNOsrt7VVuLqdPvpbR79v15FPmZhPDYSyqVEtfxLGjeHtO0OFo7SL5nGHlk5ZvqfT2FUtL8K6d4cuZ9Rtnu3bymBQ4f5cg8Kq5J44HNWLXVbbxFaumnXjwTRsPMVkIdfwz6/UVJ/Z0sgl26gTLs8pyMnBw3Ubuvzg9unGAaa9TphToyUZQSstjDutRF3e3ksy+I40XH2VbW0njGNozkbeW3bvvcYx70zU7XS9ZhtV1hLldQjtk8yaIbOSoYja3TknggHt14ro2067ZpGGpSjexOAvCjsBz2OPwBHfNSTNc6do0rxxvf3EETOse7a0pGSFB55PSrcl0KdJTTjNXTPNpfCenslw8OpyJ5YJRZY1O4bC/UNx0I9uM1o6d8P7G6nmRtSuJFhbaxWEJk+xJPpT/7b+Js8ZuYPDOlwxY3LBNNmUj6hwM/UCtrwR4xXxdp9w0lo1pfWknlXMBOdp7EfXB4PTBqpRaV1+Zl/ZmGWvL+L/zL9tp+leFbQG3t2HmMELgbnY+59OKrXs++/hvZY5oo4t4baVYkRyADHpliM+wrmp/iJqyeMNX8O2ukx313E6pYxx5TPGWaVicADjpikn8deJ/DN9anxfotpDpty/li6snJEZ/2sk5+nHAOM4xQqUl6+p1Kgox5YqyO5TV4JfL2I+15PLySo2naW5BOR0NY2spoWtQRNe20jSsilZIR86hhkcjqPvde6mneO/El14Z8Jy6tp6W8squiqJgWQhjjPBH86w18WeNNcthqHhvw9aHTiP3cl9Jh58dSq7hgZ6Z60o021zdPUJUlUjaSuiWb4YQM+YdUkRfR4Qx/MEVJb/DKyRgbnUJ5R6IgT+ea0fBXjJPFdtdRT2rWWp2T+XdWzHO08jI9sgjHYiqPirx23hfxdplhcRxHTbi3klmYIzS7gG2qmDjkhRyO9LknzcvU5VluH5vg1+Z1OlaPY6NbGCxh8tGOWJYksfUk1dJwCT2rz2XxD8SZB9rtfCdklofmWCacGbb7/OMH2xmuz1XV7bQ9Fm1PUnEcUEYaTbzk9ML6kk4H1qZQat1OtU1BKMQGrJJGrRRM5bZjkAfNt7+28Vka1Z6P4htjLcwThol4njAVlBOAOeoz69Oelc9a+KvH+t2w1LSPDVgmnSDdCt1MfNlXsR8wAz7j866Lwf4wh8UQ3MMtq9lqdm/l3VpIclD0yPUcH6flmpU3FX/UipQc4tT1RyzeB7NmymqTouCwD2wzgZH98dweMZ9q6Hw14Os9NuYNUjuriWRQ20MoUcgg5HP86k8Sax4qtdRjsvD3h+K8DRiRruebbGpyRtIyOeM9e9c/L478T+G9TsofF+i2cVldyCNbqyckIfcEnp6ccZxnFNQlJafmc9LLqMJKSWv9eZ20qXkkUgazhbcrsU2qdzjGw8n0/lSm0L3J3WSGJphu3JGcoE4z34b8avXVzDZWk11cyLHBChkkduiqBkmvObfx34t8TSSzeE/DkDacjFFub58eZj0G5f0zipjFy1R0qlc9EFlaAMBbQgMoVsRjkDoD7Vz+keLhqXjPVvDgsPKGnIGE/m53/d427Rjr6npTvCupeKL6S6i8R6NBYNFt8uSGXcsuc5wMnpj17153Frlzovxc8Srp+nvqGo3e2G3t1O0E4UlmbsAB/nrVwp35k+xpGC1R7TRXmV/458YeFpre58UaFZDTJpBG0tk5LRk+uWOTjPpnHWvS45EliSSNgyOAysOhB6Gs5QcdWJpodXG6BBG/jrXJmUGSM4Qntk8/yrsq4Gzj1EeMdbudMMTTROA0MvCyKffsQQMVBw4t2nTdr6/oy/8AEBRDp9jfRgC5hulCMOvQn+aimePZZCNKtViMqTXGWiDY8wjAC599xqSTTNb8Q6javq9vDZ2Ns/meSkgcyN+FbHiDRV1uwESyeVcROJIZf7rD+lBlOnOrGrKKte1ul7b/AH7GZJrGsy2zW7+EnMLLsKfaVxjGMfdqbwVZ39hoj21/C8TLMxjViPukA/zzUS3njGFRE2l2c7DjzxKAD74yDWzpk9ybXZqMkH2xD+9WLIVdx+Uc+2KDSjHmqKTcrpdUl+iOb8L2sUvi3xBcugaSG4KoSPu7mbOPyFdnjnNc54e066stb12a4i2R3M4eI7gdy5fng8dR1roFmjeWSNWBePG5fTPSg0wkOWlZqzu/zZBqX/ILu/8Ari//AKCaxvAv/IqW3++//oRrcvY3msLiKMZd4mVR6kg1meFLC50zw/Ba3cXlzKzkruBxliRyDigcov6xF20s/wA0Z3xAldNBhiViqTXKo59sE/zA/Kumht4re2S3iRViRdqqBwBVPW9Ji1rS5bKVthbDI+M7WHQ1ix3Pi21h+xfYrO4kjUAXPnAcdAxXOe36UENunWlNptNLZX2voM8JD7Nr2vWEXFtHMGRR0Ukngfp+VRQWsV18T70zIH8m3WRARkbtqAH9TWz4b0Y6VaSyzTCe7u382aUdCTyAPbk/nVe00y8i8dX+pPDi0ltgiSbhy3ycYznse1BkqMlTpxa+1f03/I6LFcrBGr/Eu6YjJSxBHscqP6muqrBg067Txtdai0WLV7QRrJuHLZXjGc9jQdOIi24WXVfqUfDEEa+JfEbhRkTqBx0BLE0ngy5hsvDN3PO4jghuJCSewAFX9E0+6stZ1u4uItkVzMrxNuB3AbvQ8dR1rP0rQLqXwhfabcgQSXMrPGdwYY+UqeCeMig5acJx5XGOq5/z0+8tW/iO8vIg+leH7ia26I7yLCCPbPaqPgov/a+vh4Bbt5yEwhgwQkvkZHFTae3iqysYdPXTLT90ojS5acbcDgEqOTU/hnRr3SdT1Z7tvMW4aNlnyP3h+YscA8cnvQKHtJ1KcnfS97qyWj8r/oUdCtopfHmtTuoZ4jhCe2ep/T9al8eoq2mmzgYlS7UK3cAgn+gq5o+mXlr4m1i8mh2wXBXyn3A7vwByPxpPGOnXWo6faraReYYrlZH+YLhQDzyfegbpS+qzVtW3+ZW8fyuNFt4FYqk9yqOfbBP88H8K6e2tobO2jt4I1jijG1VA6CqWu6PHrmlyWcjbGyGjfGdrDof5j8axYrnxfYQrbyWFpd7MKtwZgu7sM5IyfwoNm3SrynJNppbK+3Q6LUJWttLu5ohh44XdceoBNcR4V1DUrHRl+yeH3uxK7O1wJwu85x0x26V2mni9l08DVEhFw+d6RcqAeg59q56307X/AA40sGlRQX1g7l0jkfa8ee2SQP8APagiupucKqulZ7K7V/LUgsodWufGsGqSaQ9jC0Zjn/eBg3BwT0/2fyrorrRIruR3e5uF3yeZhCowdqrxxnov6mq+mXGvS3Rk1W3tLS1C4CI+5yxIxzkitumm1sb4WPLFtX1d9VYxZfDkM140jSskOFCxx/KRgDjPp1P5dMcyeJPENn4Y0SfVL3cY48BUT7zseiitUMpYqDyOSK5rx14bbxb4Zn0uGZI7lWWaIv8Ad3DOAfYjIqotOSUtjpWr1Miz1z4ga1bR3tjoekWNtMoeIX07s5U9CduMfiBWT8Jzc/8ACU+NvtohF19sTzhBny9++bdtzzjOcZq/YXXxLWwh0w6Nplu8aCL+0JLgMoAGN2xSTn8MZ7VN8PfCWpeFtb8RG9LTQXTwtDdMy5nI3liQCSOW7/rW7aUJLT5epo7WZmeE1jPxt8VMceYIPl+mY8/0rZ+LqRt8ONQLgbleEx5/veYo/kTXHQWWszfF7xJd6DPCl/aYcRXAPlzoQoZCR07Ee4rY1PQ/G/jue2sNes7TSNIilEswhmEjy49ME++M4xnPOBVNJTjJvZIGtUyPxq0j/AvTWlz5htrMtnrnC16D4aWNfCukLFjyxZQ7cemwVynxciSD4bTQxKEjjlhVVHQAMABVDSm8f6DoFnaaXY2GrWPkK1tPLKI3iQjIVwWGcZxx2FRbnp79WK14jfDmI/jx4ljg/wBS1oGcDpu/ck/jkt+tHjSFJ/jB4QSRQy7d2D6hiR+oFanw/wDD/wDZEV/rmq6jbXWqakwkmkjcFI1JJAB9yfpwAOlR+IdKutS+JfhnVrMQy2NurLJKJ4xg5bIALZbqOgNVzL2norfgHMub5HoFebfG55V8D24TOxr6MSY9Njn+YFeh3VylnbPPIGYLgbV6sScAD6kgVzmtDTPF+my6BcpNH9pDqr/KTFIjHqASQQVJ9CO/NYUnyyUnsiY6O50dokUdlAkAAhWNQmOm3HH6V5tp37r9oHVVt+FksQZwOmdsZ/nt/OpbE/EvQLKPSYdM03VIoF8uC8acL8o4G4FgTge34nrWr4M8LzaDf3eqa5eR3OvaoSXZB8qKOdi/p+Qx0rRJQUne9x7XGar45v5fEs3h3wzpA1C+t1zcTTS7IounHv1Hcc+tcT8U5PF7+F7Y6/Bo0Vp9sXYLNpDJv2PjO7jGM/pXR3Xh3xT4Z8bajrvhyzttStdS5mt5ZRGyNnJ5JA65x9cY71neLvDPjbxpov2m8htYZLeQfZtMtpVO4k4ZndjjIHTB9fx0p8kZRatb8So2TR1XxTeVPhtqxiJyViDEf3TIgP6VpeBo4YvAmhrAAENlExx/eKgt/wCPE1q6lp1vq2lXOnXa7oLiMxuB1wR1HvXl9gnjnwMz6JpcdlrdjGzGASEo8YxuxyR25xk9ayh71Plvre5F1y2bset15f4ViRvjd4plZQXSABT6Z8vP8q6Lwjq3iTUZp5fEEVhaxuFW2gtiS2eSS3J7D16giqfh7w9qlj8TvEOsXNrssLuMLBL5inefl7A5HQ9RRFcnMm+gRa11E+L4B+HV5kdJYsf99iuo8OEnwxpJPX7FD/6AKxviPo1/r3gu60/TIPPunkjKx71XIDAnliB0re0S3ltNB062nXZNDaxRuuQcMFAIyPepbXskvMPsl6ub0rSr628Qa3dOnlR3OPIk3A5PPOM5/OiisjGdJTab6f8ADF8WN+d5FzsPy+WPNZwvI3ZyPmyM9fX2q5YwzwW+yeQu+c5LlsfiRmiigcaai7ozf7AdNQ+2w3arLvd8NFkHc2efmGcA4/L6Uo0JzbGCS4ilXZAq74M8xEEE/Nzn04ooqudlcqC80iea4Lo9sY5HjLRtB8oCqwyRu+bqOOOg9KSXQpZYmia9DKViX5oiS2wEfN83zZzntziiijnYcqNeCLyYI4gS2xQuSSScD3rA1fw/cX2qQ3UFxGv71WZmjy0YWNgBncMpu2kr6knNFFJNrUpO2wxfCTG4nmnvlmaZZQwMbKBvLEgYfO35/u+oznpizB4dMDF/MtXlazFq7NaAZwWwQARgfOQR3wKKKfOx8zK7+FZJhAJr5G2CMOwgIZwqhdmd3CHGdv8Ae5z2rT/spo9Fm06GdE3mTYxiDKis5YLtJwQAdv0HaiihybFzMyofB6R25ja6R3JYFjER+7MbJ5fDZC/MD1/hH1qJ/Bkkh3NqrA+WEKpAFQ/Jt+6DjBwMj6/WiinzyHzyNG18OR20cvzxSSNbiBGkiLCMZcnALZwd+MAjgAdOlC88Lz/2cxjuFlvQgTesW0soiMeOW5GSHwTglQD6gopc7DmZt6LayWWj29vJGsbxggquB3POASAT1IBIBOATVWbQTNNdSfa2QznO5QQ6jIPBz7YHHAJFFFLmd7kSSluQf8I9LMJDJNFFl2+VIvlcbgRuG7kDHA7Z71bbRV+x3MSSJ5s20ea8ZcqAqgDrk8rnr1NFFPnYuRFabw35szy/agGcsXHlfK2X3cjPIHT8B6Yp58Ox+WF81GbqS8e7eQ6su7JycbSOv8Rooo55ByRNpRtQLxwMcDArCXQHmuJ5biSNUeYt5Yjz5i+YrgOc/NwuBxwGPWiikpNbFBL4aSSa5kMqMJXV0V4twXaysARnDAbcDgYBxU1zoSzw3SrJCr3EiszNAG+QKo2YJ6ZXPXjNFFPnkA1dALSr9ouI5ocRb0MPMhRSuWJY5zn07CmxeH23ItxcxzxARB0aHmQopXLEsc5z6dhRRRzyAj0TSbrT9SnknAkUxLGJiBk7QoHOSTnBJzjb0GRzXQUUUpNt3YHC+HfD2qWPxM8Ravc2uywvEAgl8xTvPy9gcjoeoruqKKcpOTuxt3OS+JOi6hr/AINnsNMt/PumljYJvVcgNk8sQK6HSIJLXRbC3mXbLFbxo65BwwUAjiiijmfLyhfSxSn0PzI4IUlJigheNC55ORhQQB0HBz1yKki0G3S4S5eWWSYEMxbbhiMY4xxjaOmKKKOZk8qNKWKOaJopUWSNxhlYZBHoRVOHR7K1Rvs1vDHN85SXylLIWJPHHQbjx6cUUUrsZl2/hZoVY/bE87cjROtuAIiGDHaM8buQfqalttAltYokNzFLHHI8jRCDbvDJtKr82F7n6miiq55DuFnodwv2G5nnU3URZpGkTezZAGM5wCAqjIGeOvJqSDQ5IpLF2uISbV3I22+MqxztALEL2568duaKKXOwubVU59Ls7l2eaHczHJO8jnAXsfQCiipJaT3C302G2u/PjGMR+Wg54BJY5JPPJq5RRQCSWwUUUUDP/9k=",
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_token=Tokenizer(strings)\n",
        "#wordcloud\n",
        "long_string = ','.join(clean_token)\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('model', 1840),\n",
              " ('question', 981),\n",
              " ('reasoning', 686),\n",
              " ('performance', 573),\n",
              " ('qa', 473),\n",
              " ('approach', 432),\n",
              " ('data', 431),\n",
              " ('dataset', 415),\n",
              " ('knowledge', 413),\n",
              " ('answer', 407),\n",
              " ('result', 402),\n",
              " ('show', 392),\n",
              " ('datasets', 391),\n",
              " ('system', 383),\n",
              " ('retrieval', 381),\n",
              " ('training', 344),\n",
              " ('proposed', 302),\n",
              " ('different', 301),\n",
              " ('benchmark', 285),\n",
              " ('new', 284),\n",
              " ('experiment', 283),\n",
              " ('learning', 280),\n",
              " ('using', 277),\n",
              " ('propose', 274),\n",
              " ('information', 254),\n",
              " ('multihop', 248),\n",
              " ('two', 237),\n",
              " ('document', 232),\n",
              " ('answering', 231),\n",
              " ('better', 222),\n",
              " ('llm', 221),\n",
              " ('large', 214),\n",
              " ('study', 213),\n",
              " ('context', 213),\n",
              " ('framework', 211),\n",
              " ('human', 204),\n",
              " ('generation', 197),\n",
              " ('baseline', 195),\n",
              " ('based', 195),\n",
              " ('text', 194),\n",
              " ('graph', 192),\n",
              " ('stateoftheart', 185),\n",
              " ('set', 185),\n",
              " ('query', 184),\n",
              " ('improve', 182),\n",
              " ('existing', 181),\n",
              " ('domain', 180),\n",
              " ('may', 179),\n",
              " ('present', 177),\n",
              " ('analysis', 175),\n",
              " ('problem', 175),\n",
              " ('novel', 174),\n",
              " ('first', 169),\n",
              " ('complex', 165),\n",
              " ('multiple', 165),\n",
              " ('ability', 164),\n",
              " ('used', 162),\n",
              " ('representation', 158),\n",
              " ('pretraining', 157),\n",
              " ('setting', 155),\n",
              " ('one', 154),\n",
              " ('type', 152),\n",
              " ('pretrained', 138),\n",
              " ('effectiveness', 135),\n",
              " ('could', 134),\n",
              " ('challenge', 133),\n",
              " ('process', 133),\n",
              " ('c', 133),\n",
              " ('prediction', 132),\n",
              " ('improvement', 130),\n",
              " ('well', 130),\n",
              " ('demonstrate', 130),\n",
              " ('evidence', 129),\n",
              " ('effective', 126),\n",
              " ('user', 126),\n",
              " ('sentence', 125),\n",
              " ('several', 124),\n",
              " ('explanation', 118),\n",
              " ('n', 118),\n",
              " ('input', 117),\n",
              " ('introduce', 116),\n",
              " ('three', 116),\n",
              " ('reading', 116),\n",
              " ('simple', 115),\n",
              " ('comprehension', 115),\n",
              " ('capability', 114),\n",
              " ('passage', 114),\n",
              " ('provide', 113),\n",
              " ('potential', 112),\n",
              " ('prompt', 111),\n",
              " ('presented', 111),\n",
              " ('mrc', 111),\n",
              " ('without', 111),\n",
              " ('find', 110),\n",
              " ('including', 106),\n",
              " ('generated', 106),\n",
              " ('limitation', 106),\n",
              " ('score', 106),\n",
              " ('make', 106),\n",
              " ('best', 106),\n",
              " ('inference', 106),\n",
              " ('understanding', 104),\n",
              " ('fact', 104),\n",
              " ('entity', 103),\n",
              " ('neural', 103),\n",
              " ('metric', 103),\n",
              " ('retriever', 103),\n",
              " ('number', 103),\n",
              " ('explore', 102),\n",
              " ('hope', 102),\n",
              " ('generate', 102),\n",
              " ('long', 102),\n",
              " ('experimental', 100),\n",
              " ('across', 100),\n",
              " ('would', 100),\n",
              " ('strong', 100),\n",
              " ('time', 100),\n",
              " ('current', 99),\n",
              " ('accuracy', 99),\n",
              " ('module', 99),\n",
              " ('still', 98),\n",
              " ('even', 98),\n",
              " ('step', 97),\n",
              " ('finetuning', 97),\n",
              " ('various', 96),\n",
              " ('need', 95),\n",
              " ('semantic', 95),\n",
              " ('dense', 95),\n",
              " ('logical', 95),\n",
              " ('parameter', 94),\n",
              " ('challenging', 94),\n",
              " ('network', 94),\n",
              " ('diverse', 93),\n",
              " ('outperforms', 93),\n",
              " ('lm', 92),\n",
              " ('technique', 92),\n",
              " ('evaluate', 92),\n",
              " ('given', 92),\n",
              " ('direction', 91),\n",
              " ('source', 91),\n",
              " ('architecture', 91),\n",
              " ('quality', 91),\n",
              " ('size', 90),\n",
              " ('significantly', 90),\n",
              " ('search', 90),\n",
              " ('trained', 90),\n",
              " ('way', 90),\n",
              " ('fewshot', 90),\n",
              " ('like', 89),\n",
              " ('general', 89),\n",
              " ('bias', 89),\n",
              " ('help', 88),\n",
              " ('test', 88),\n",
              " ('agent', 87),\n",
              " ('bert', 87),\n",
              " ('finding', 86),\n",
              " ('achieves', 86),\n",
              " ('relevant', 85),\n",
              " ('strategy', 85),\n",
              " ('design', 84),\n",
              " ('compared', 84),\n",
              " ('target', 83),\n",
              " ('corpus', 82),\n",
              " ('relation', 81),\n",
              " ('achieve', 81),\n",
              " ('similar', 80),\n",
              " ('perform', 80),\n",
              " ('learn', 80),\n",
              " ('single', 79),\n",
              " ('generalization', 79),\n",
              " ('showed', 79),\n",
              " ('introduced', 79),\n",
              " ('modeling', 78),\n",
              " ('require', 78),\n",
              " ('structure', 78),\n",
              " ('zeroshot', 77),\n",
              " ('annotation', 75),\n",
              " ('extensive', 75),\n",
              " ('mechanism', 75),\n",
              " ('selection', 74),\n",
              " ('open', 74),\n",
              " ('eg', 73),\n",
              " ('attention', 73),\n",
              " ('via', 73),\n",
              " ('application', 71),\n",
              " ('additional', 71),\n",
              " ('hotpotqa', 71),\n",
              " ('improves', 71),\n",
              " ('case', 70),\n",
              " ('improving', 70),\n",
              " ('focus', 70),\n",
              " ('decomposition', 70),\n",
              " ('previous', 70),\n",
              " ('tool', 69),\n",
              " ('gap', 69),\n",
              " ('pair', 69),\n",
              " ('interaction', 69),\n",
              " ('limited', 68),\n",
              " ('le', 68),\n",
              " ('output', 68),\n",
              " ('opendomain', 68),\n",
              " ('scenario', 67),\n",
              " ('found', 67),\n",
              " ('important', 67),\n",
              " ('might', 67),\n",
              " ('development', 67),\n",
              " ('provides', 67),\n",
              " ('sequence', 67),\n",
              " ('chain', 66),\n",
              " ('specific', 66),\n",
              " ('believe', 66),\n",
              " ('prompting', 65),\n",
              " ('point', 65),\n",
              " ('algorithm', 65),\n",
              " ('machine', 65),\n",
              " ('transformer', 65),\n",
              " ('form', 64),\n",
              " ('benefit', 64),\n",
              " ('many', 63),\n",
              " ('sample', 63),\n",
              " ('token', 63),\n",
              " ('key', 63),\n",
              " ('largescale', 62),\n",
              " ('high', 62),\n",
              " ('transfer', 62),\n",
              " ('instruction', 61),\n",
              " ('shown', 61),\n",
              " ('standard', 61),\n",
              " ('adversarial', 61),\n",
              " ('possible', 61),\n",
              " ('significant', 60),\n",
              " ('due', 60),\n",
              " ('skill', 60),\n",
              " ('available', 59),\n",
              " ('robustness', 59),\n",
              " ('generative', 58),\n",
              " ('level', 58),\n",
              " ('investigate', 58),\n",
              " ('distribution', 58),\n",
              " ('designed', 57),\n",
              " ('error', 57),\n",
              " ('solve', 57),\n",
              " ('efficiency', 56),\n",
              " ('much', 56),\n",
              " ('span', 56),\n",
              " ('efficient', 56),\n",
              " ('external', 55),\n",
              " ('endtoend', 55),\n",
              " ('term', 55),\n",
              " ('requires', 55),\n",
              " ('collection', 55),\n",
              " ('support', 55),\n",
              " ('comprehensive', 54),\n",
              " ('resource', 54),\n",
              " ('lead', 54),\n",
              " ('overall', 54),\n",
              " ('paradigm', 54),\n",
              " ('address', 53),\n",
              " ('effect', 53),\n",
              " ('larger', 52),\n",
              " ('multitask', 52),\n",
              " ('path', 52),\n",
              " ('solution', 52),\n",
              " ('u', 52),\n",
              " ('component', 51),\n",
              " ('include', 51),\n",
              " ('especially', 51),\n",
              " ('develop', 51),\n",
              " ('train', 51),\n",
              " ('embeddings', 51),\n",
              " ('conduct', 51),\n",
              " ('whether', 51),\n",
              " ('memory', 50),\n",
              " ('able', 50),\n",
              " ('applied', 50),\n",
              " ('label', 50),\n",
              " ('robust', 50),\n",
              " ('synthetic', 50),\n",
              " ('scale', 50),\n",
              " ('ir', 50),\n",
              " ('environment', 49),\n",
              " ('supporting', 49),\n",
              " ('generating', 49),\n",
              " ('range', 49),\n",
              " ('since', 49),\n",
              " ('candidate', 49),\n",
              " ('node', 49),\n",
              " ('effectively', 49),\n",
              " ('contrastive', 49),\n",
              " ('correct', 49),\n",
              " ('feature', 49),\n",
              " ('related', 48),\n",
              " ('demonstrated', 48),\n",
              " ('pipeline', 48),\n",
              " ('ie', 48),\n",
              " ('limit', 48),\n",
              " ('aim', 47),\n",
              " ('value', 47),\n",
              " ('word', 47),\n",
              " ('multilingual', 47),\n",
              " ('retrieved', 47),\n",
              " ('extractive', 47),\n",
              " ('function', 47),\n",
              " ('required', 47),\n",
              " ('paragraph', 47),\n",
              " ('meson', 47),\n",
              " ('automatic', 46),\n",
              " ('construct', 46),\n",
              " ('extraction', 46),\n",
              " ('supervision', 46),\n",
              " ('improved', 46),\n",
              " ('unsupervised', 46),\n",
              " ('small', 46),\n",
              " ('downstream', 46),\n",
              " ('complexity', 46),\n",
              " ('force', 46),\n",
              " ('specifically', 45),\n",
              " ('thus', 45),\n",
              " ('towards', 45),\n",
              " ('although', 45),\n",
              " ('insight', 45),\n",
              " ('section', 45),\n",
              " ('addition', 45),\n",
              " ('augmentation', 45),\n",
              " ('hallucination', 45),\n",
              " ('iterative', 44),\n",
              " ('promising', 44),\n",
              " ('good', 44),\n",
              " ('despite', 44),\n",
              " ('realworld', 44),\n",
              " ('wikipedia', 44),\n",
              " ('higher', 44),\n",
              " ('us', 44),\n",
              " ('real', 44),\n",
              " ('highlight', 44),\n",
              " ('cost', 44),\n",
              " ('allows', 44),\n",
              " ('plan', 43),\n",
              " ('state', 43),\n",
              " ('build', 43),\n",
              " ('competitive', 43),\n",
              " ('identify', 43),\n",
              " ('adaptation', 43),\n",
              " ('objective', 43),\n",
              " ('reader', 43),\n",
              " ('understand', 43),\n",
              " ('see', 43),\n",
              " ('particular', 42),\n",
              " ('four', 42),\n",
              " ('evaluated', 42),\n",
              " ('structured', 42),\n",
              " ('comparison', 42),\n",
              " ('sparse', 42),\n",
              " ('moreover', 42),\n",
              " ('finetuned', 42),\n",
              " ('instance', 42),\n",
              " ('gain', 42),\n",
              " ('beyond', 41),\n",
              " ('multimodal', 41),\n",
              " ('yet', 41),\n",
              " ('community', 41),\n",
              " ('classification', 41),\n",
              " ('impact', 41),\n",
              " ('relevance', 41),\n",
              " ('interpretability', 41),\n",
              " ('vector', 41),\n",
              " ('following', 41),\n",
              " ('main', 41),\n",
              " ('space', 41),\n",
              " ('evaluating', 41),\n",
              " ('code', 41),\n",
              " ('response', 41),\n",
              " ('encoder', 41),\n",
              " ('increase', 41),\n",
              " ('leverage', 40),\n",
              " ('take', 40),\n",
              " ('leveraging', 40),\n",
              " ('variety', 40),\n",
              " ('web', 40),\n",
              " ('dialogue', 40),\n",
              " ('consider', 40),\n",
              " ('explicit', 40),\n",
              " ('tuning', 40),\n",
              " ('relationship', 40),\n",
              " ('concept', 39),\n",
              " ('property', 39),\n",
              " ('english', 39),\n",
              " ('full', 39),\n",
              " ('effort', 39),\n",
              " ('among', 39),\n",
              " ('building', 39),\n",
              " ('suggest', 39),\n",
              " ('recent', 39),\n",
              " ('behavior', 39),\n",
              " ('length', 39),\n",
              " ('within', 39),\n",
              " ('content', 38),\n",
              " ('furthermore', 38),\n",
              " ('capture', 38),\n",
              " ('aspect', 38),\n",
              " ('original', 38),\n",
              " ('rather', 38),\n",
              " ('questionanswering', 38),\n",
              " ('event', 38),\n",
              " ('achieved', 38),\n",
              " ('produce', 38),\n",
              " ('extend', 38),\n",
              " ('order', 37),\n",
              " ('annotated', 37),\n",
              " ('annotator', 37),\n",
              " ('apply', 37),\n",
              " ('embedding', 37),\n",
              " ('supervised', 37),\n",
              " ('contains', 37),\n",
              " ('loss', 37),\n",
              " ('summarization', 37),\n",
              " ('directly', 37),\n",
              " ('k', 37),\n",
              " ('matching', 37),\n",
              " ('multistep', 36),\n",
              " ('useful', 36),\n",
              " ('prior', 36),\n",
              " ('provided', 36),\n",
              " ('news', 36),\n",
              " ('performs', 36),\n",
              " ('hybrid', 36),\n",
              " ('squad', 36),\n",
              " ('combine', 36),\n",
              " ('qg', 36),\n",
              " ('made', 35),\n",
              " ('requiring', 35),\n",
              " ('expert', 35),\n",
              " ('ai', 35),\n",
              " ('second', 35),\n",
              " ('lexical', 35),\n",
              " ('discussed', 35),\n",
              " ('textual', 35),\n",
              " ('condition', 35),\n",
              " ('computational', 35),\n",
              " ('progress', 35),\n",
              " ('faithfulness', 35),\n",
              " ('plms', 35),\n",
              " ('ranking', 35),\n",
              " ('field', 35),\n",
              " ('cot', 35),\n",
              " ('exploration', 34),\n",
              " ('practical', 34),\n",
              " ('article', 34),\n",
              " ('wide', 34),\n",
              " ('diversity', 34),\n",
              " ('empirical', 34),\n",
              " ('unified', 34),\n",
              " ('reduce', 34),\n",
              " ('claim', 34),\n",
              " ('base', 34),\n",
              " ('measure', 34),\n",
              " ('v', 34),\n",
              " ('needed', 33),\n",
              " ('generator', 33),\n",
              " ('explored', 33),\n",
              " ('another', 33),\n",
              " ('enable', 33),\n",
              " ('access', 33),\n",
              " ('instead', 33),\n",
              " ('final', 33),\n",
              " ('often', 33),\n",
              " ('collected', 33),\n",
              " ('class', 33),\n",
              " ('likely', 33),\n",
              " ('factor', 33),\n",
              " ('corresponding', 33),\n",
              " ('alignment', 33),\n",
              " ('program', 33),\n",
              " ('signal', 33),\n",
              " ('p', 33),\n",
              " ('nuclear', 33),\n",
              " ('consistently', 32),\n",
              " ('focused', 32),\n",
              " ('detection', 32),\n",
              " ('developing', 32),\n",
              " ('tackle', 32),\n",
              " ('difficult', 32),\n",
              " ('automatically', 32),\n",
              " ('contribution', 32),\n",
              " ('probe', 32),\n",
              " ('extract', 32),\n",
              " ('attack', 32),\n",
              " ('implicit', 32),\n",
              " ('tree', 32),\n",
              " ('advantage', 31),\n",
              " ('negative', 31),\n",
              " ('average', 31),\n",
              " ('light', 31),\n",
              " ('create', 31),\n",
              " ('linguistic', 31),\n",
              " ('rc', 31),\n",
              " ('deep', 31),\n",
              " ('rate', 31),\n",
              " ('rule', 31),\n",
              " ('variable', 31),\n",
              " ('combination', 31),\n",
              " ('interpretable', 31),\n",
              " ('role', 31),\n",
              " ('get', 31),\n",
              " ('image', 30),\n",
              " ('world', 30),\n",
              " ('control', 30),\n",
              " ('f1', 30),\n",
              " ('offer', 30),\n",
              " ('unanswerable', 30),\n",
              " ('outperform', 30),\n",
              " ('facilitate', 30),\n",
              " ('showing', 30),\n",
              " ('select', 30),\n",
              " ('layer', 30),\n",
              " ('additionally', 30),\n",
              " ('yield', 30),\n",
              " ('proof', 30),\n",
              " ('review', 30),\n",
              " ('exploring', 29),\n",
              " ('topic', 29),\n",
              " ('r', 29),\n",
              " ('stage', 29),\n",
              " ('enhance', 29),\n",
              " ('therefore', 29),\n",
              " ('b', 29),\n",
              " ('importance', 29),\n",
              " ('outofdomain', 29),\n",
              " ('nli', 29),\n",
              " ('idea', 29),\n",
              " ('modality', 29),\n",
              " ('influence', 29),\n",
              " ('generalize', 29),\n",
              " ('analyze', 29),\n",
              " ('interesting', 29),\n",
              " ('λ', 29),\n",
              " ('boost', 28),\n",
              " ('explainable', 28),\n",
              " ('compositional', 28),\n",
              " ('intermediate', 28),\n",
              " ('f', 28),\n",
              " ('give', 28),\n",
              " ('indicate', 28),\n",
              " ('feedback', 28),\n",
              " ('critical', 28),\n",
              " ('nlu', 28),\n",
              " ('goal', 28),\n",
              " ('leading', 28),\n",
              " ('choice', 28),\n",
              " ('detailed', 28),\n",
              " ('along', 28),\n",
              " ('joint', 28),\n",
              " ('kind', 28),\n",
              " ('observed', 28),\n",
              " ('encourage', 28),\n",
              " ('performing', 28),\n",
              " ('report', 28),\n",
              " ('extension', 27),\n",
              " ('allow', 27),\n",
              " ('end', 27),\n",
              " ('respectively', 27),\n",
              " ('researcher', 27),\n",
              " ('incorporate', 27),\n",
              " ('serve', 27),\n",
              " ('come', 27),\n",
              " ('contextual', 27),\n",
              " ('helpful', 27),\n",
              " ('difficulty', 27),\n",
              " ('created', 27),\n",
              " ('background', 27),\n",
              " ('making', 27),\n",
              " ('accurate', 27),\n",
              " ('pattern', 27),\n",
              " ('reason', 27),\n",
              " ('incontext', 27),\n",
              " ('learned', 27),\n",
              " ('far', 27),\n",
              " ('engine', 27),\n",
              " ('chatgpt', 27),\n",
              " ('suggests', 27),\n",
              " ('note', 27),\n",
              " ('practice', 27),\n",
              " ('providing', 26),\n",
              " ('verification', 26),\n",
              " ('fully', 26),\n",
              " ('gpt3', 26),\n",
              " ('match', 26),\n",
              " ('consistent', 26),\n",
              " ('characteristic', 26),\n",
              " ('hypothesis', 26),\n",
              " ('public', 26),\n",
              " ('contain', 26),\n",
              " ('shared', 26),\n",
              " ('taskspecific', 26),\n",
              " ('area', 26),\n",
              " ('hierarchical', 26),\n",
              " ('shortcut', 26),\n",
              " ('advance', 26),\n",
              " ('change', 26),\n",
              " ('popular', 26),\n",
              " ('highly', 26),\n",
              " ('chinese', 26),\n",
              " ('top', 26),\n",
              " ('gold', 26),\n",
              " ('nucleon', 26),\n",
              " ('indomain', 25),\n",
              " ('logic', 25),\n",
              " ('obtained', 25),\n",
              " ('item', 25),\n",
              " ('korean', 25),\n",
              " ('includes', 25),\n",
              " ('adapt', 25),\n",
              " ('dynamic', 25),\n",
              " ('learns', 25),\n",
              " ('temporal', 25),\n",
              " ('ensure', 25),\n",
              " ('reranking', 25),\n",
              " ('recall', 25),\n",
              " ('according', 25),\n",
              " ('conducted', 25),\n",
              " ('solving', 25),\n",
              " ('introduces', 25),\n",
              " ('modular', 25),\n",
              " ('symbolic', 25),\n",
              " ('called', 25),\n",
              " ('numerical', 25),\n",
              " ('retrieving', 25),\n",
              " ('rationale', 25),\n",
              " ('leave', 24),\n",
              " ('demonstrates', 24),\n",
              " ('resulting', 24),\n",
              " ('ass', 24),\n",
              " ('recommendation', 24),\n",
              " ('discus', 24),\n",
              " ('capable', 24),\n",
              " ('certain', 24),\n",
              " ('literature', 24),\n",
              " ('category', 24),\n",
              " ('style', 24),\n",
              " ('similarity', 24),\n",
              " ('commonsense', 24),\n",
              " ('low', 24),\n",
              " ('increasing', 24),\n",
              " ('nature', 24),\n",
              " ('perturbation', 24),\n",
              " ('right', 24),\n",
              " ('difference', 24),\n",
              " ('systematic', 24),\n",
              " ('enables', 24),\n",
              " ('retrieve', 24),\n",
              " ('superior', 24),\n",
              " ('part', 24),\n",
              " ('explicitly', 24),\n",
              " ('smaller', 24),\n",
              " ('unit', 24),\n",
              " ('knowledgeintensive', 24),\n",
              " ('g', 24),\n",
              " ('∼', 24),\n",
              " ('baryon', 24),\n",
              " ('hard', 23),\n",
              " ('upon', 23),\n",
              " ('developed', 23),\n",
              " ('indeed', 23),\n",
              " ('testing', 23),\n",
              " ('alternative', 23),\n",
              " ('kb', 23),\n",
              " ('named', 23),\n",
              " ('efficiently', 23),\n",
              " ('bridge', 23),\n",
              " ('exhibit', 23),\n",
              " ('correlation', 23),\n",
              " ('correctness', 23),\n",
              " ('compare', 23),\n",
              " ('short', 23),\n",
              " ('beir', 23),\n",
              " ('obtain', 23),\n",
              " ('random', 23),\n",
              " ('reveal', 23),\n",
              " ('necessary', 23),\n",
              " ('sampling', 23),\n",
              " ('methodology', 23),\n",
              " ('selected', 23),\n",
              " ('substantially', 23),\n",
              " ('j', 23),\n",
              " ('optimal', 22),\n",
              " ('built', 22),\n",
              " ('investigated', 22),\n",
              " ('lack', 22),\n",
              " ('underlying', 22),\n",
              " ('remains', 22),\n",
              " ('initial', 22),\n",
              " ('substantial', 22),\n",
              " ('factual', 22),\n",
              " ('cover', 22),\n",
              " ('verify', 22),\n",
              " ('suitable', 22),\n",
              " ('scheme', 22),\n",
              " ('teacher', 22),\n",
              " ('room', 22),\n",
              " ('five', 22),\n",
              " ('observe', 22),\n",
              " ('enabling', 22),\n",
              " ('longer', 22),\n",
              " ('reliable', 22),\n",
              " ('lower', 22),\n",
              " ('empirically', 22),\n",
              " ('predicted', 22),\n",
              " ('traditional', 22),\n",
              " ('capacity', 22),\n",
              " ('applying', 22),\n",
              " ('causal', 22),\n",
              " ('introducing', 22),\n",
              " ('position', 22),\n",
              " ('encoding', 22),\n",
              " ('addressing', 22),\n",
              " ('power', 22),\n",
              " ('region', 22),\n",
              " ('view', 21),\n",
              " ('usage', 21),\n",
              " ('collect', 21),\n",
              " ('studied', 21),\n",
              " ('explainability', 21),\n",
              " ('around', 21),\n",
              " ('perspective', 21),\n",
              " ('potentially', 21),\n",
              " ('expression', 21),\n",
              " ('premise', 21),\n",
              " ('etc', 21),\n",
              " ('forward', 21),\n",
              " ('exploit', 21),\n",
              " ('kg', 21),\n",
              " ('reward', 21),\n",
              " ('consists', 21),\n",
              " ('procedure', 21),\n",
              " ('tradeoff', 21),\n",
              " ('shed', 21),\n",
              " ('studying', 21),\n",
              " ('adding', 21),\n",
              " ('easily', 21),\n",
              " ('amount', 21),\n",
              " ('discourse', 21),\n",
              " ('reducing', 21),\n",
              " ('conditional', 21),\n",
              " ('represent', 21),\n",
              " ('common', 21),\n",
              " ('creating', 21),\n",
              " ('singlehop', 21),\n",
              " ('movie', 21),\n",
              " ('structural', 21),\n",
              " ('positive', 20),\n",
              " ('applicable', 20),\n",
              " ('become', 20),\n",
              " ('identified', 20),\n",
              " ('relatively', 20),\n",
              " ('detail', 20),\n",
              " ('expected', 20),\n",
              " ('advanced', 20),\n",
              " ('hop', 20),\n",
              " ('mention', 20),\n",
              " ('achieving', 20),\n",
              " ('purpose', 20),\n",
              " ('investigation', 20),\n",
              " ('demonstrating', 20),\n",
              " ('currently', 20),\n",
              " ('decoding', 20),\n",
              " ('semantics', 20),\n",
              " ('allowing', 20),\n",
              " ('subset', 20),\n",
              " ('handle', 20),\n",
              " ('highquality', 20),\n",
              " ('rich', 20),\n",
              " ('contrast', 20),\n",
              " ('theory', 20),\n",
              " ('described', 20),\n",
              " ('definition', 20),\n",
              " ('→', 20),\n",
              " ('video', 19),\n",
              " ('distillation', 19),\n",
              " ('degree', 19),\n",
              " ('heterogeneous', 19),\n",
              " ('summary', 19),\n",
              " ('observation', 19),\n",
              " ('expect', 19),\n",
              " ('formal', 19),\n",
              " ('consisting', 19),\n",
              " ('scaling', 19),\n",
              " ('construction', 19),\n",
              " ('dynamically', 19),\n",
              " ('qualitative', 19),\n",
              " ('consistency', 19),\n",
              " ('correctly', 19),\n",
              " ('manner', 19),\n",
              " ('carefully', 19),\n",
              " ('predict', 19),\n",
              " ('list', 19),\n",
              " ('greatly', 19),\n",
              " ('drop', 19),\n",
              " ('dual', 19),\n",
              " ('confidence', 19),\n",
              " ('variant', 19),\n",
              " ('generates', 19),\n",
              " ('optimization', 19),\n",
              " ('powerful', 19),\n",
              " ('implement', 19),\n",
              " ('phrase', 19),\n",
              " ('shift', 19),\n",
              " ('de', 19),\n",
              " ('constraint', 19),\n",
              " ('mitigate', 19),\n",
              " ('realistic', 19),\n",
              " ('known', 19),\n",
              " ('student', 19),\n",
              " ('year', 19),\n",
              " ('growth', 19),\n",
              " ('version', 19),\n",
              " ('cognitive', 19),\n",
              " ('foundation', 19),\n",
              " ('specialized', 18),\n",
              " ('description', 18),\n",
              " ('together', 18),\n",
              " ('close', 18),\n",
              " ('beneficial', 18),\n",
              " ('per', 18),\n",
              " ('always', 18),\n",
              " ('identifying', 18),\n",
              " ('either', 18),\n",
              " ('contribute', 18),\n",
              " ('constructed', 18),\n",
              " ('format', 18),\n",
              " ('jointly', 18),\n",
              " ('encode', 18),\n",
              " ('subquestions', 18),\n",
              " ('extending', 18),\n",
              " ('past', 18),\n",
              " ('incorporating', 18),\n",
              " ('unseen', 18),\n",
              " ('weight', 18),\n",
              " ('assumption', 18),\n",
              " ('look', 18),\n",
              " ('discrete', 18),\n",
              " ('backbone', 18),\n",
              " ('involve', 18),\n",
              " ('release', 18),\n",
              " ('hence', 18),\n",
              " ('ablation', 18),\n",
              " ('faithful', 18),\n",
              " ('labeled', 18),\n",
              " ('enough', 18),\n",
              " ('representative', 18),\n",
              " ('mr', 18),\n",
              " ('incorrect', 18),\n",
              " ('epoch', 18),\n",
              " ('analyzed', 18),\n",
              " ('qcd', 18),\n",
              " ('considered', 17),\n",
              " ('seen', 17),\n",
              " ('protein', 17),\n",
              " ('crucial', 17),\n",
              " ('performed', 17),\n",
              " ('conversation', 17),\n",
              " ('combining', 17),\n",
              " ('openqa', 17),\n",
              " ('continual', 17),\n",
              " ('answerable', 17),\n",
              " ('indicates', 17),\n",
              " ('gene', 17),\n",
              " ('easy', 17),\n",
              " ('calibration', 17),\n",
              " ('examine', 17),\n",
              " ('client', 17),\n",
              " ('finegrained', 17),\n",
              " ('widely', 17),\n",
              " ('besides', 17),\n",
              " ('name', 17),\n",
              " ('situation', 17),\n",
              " ('biomedical', 17),\n",
              " ('utilizing', 17),\n",
              " ('affect', 17),\n",
              " ('adapter', 17),\n",
              " ('extracted', 17),\n",
              " ('loop', 17),\n",
              " ('reported', 17),\n",
              " ('deductive', 17),\n",
              " ('true', 17),\n",
              " ('proposes', 17),\n",
              " ('sophisticated', 17),\n",
              " ('must', 17),\n",
              " ('roberta', 17),\n",
              " ('parallel', 17),\n",
              " ('dr', 17),\n",
              " ('reduces', 17),\n",
              " ('comparable', 17),\n",
              " ('relative', 17),\n",
              " ('template', 17),\n",
              " ('medical', 17),\n",
              " ('selfsupervised', 17),\n",
              " ('option', 17),\n",
              " ('hyperparameters', 17),\n",
              " ('individual', 16),\n",
              " ('extent', 16),\n",
              " ('combined', 16),\n",
              " ('regularization', 16),\n",
              " ('thought', 16),\n",
              " ('automated', 16),\n",
              " ('unique', 16),\n",
              " ('covering', 16),\n",
              " ('broader', 16),\n",
              " ('already', 16),\n",
              " ('established', 16),\n",
              " ('broad', 16),\n",
              " ('valuable', 16),\n",
              " ('connection', 16),\n",
              " ('augmented', 16),\n",
              " ('guide', 16),\n",
              " ('art', 16),\n",
              " ('integrate', 16),\n",
              " ('line', 16),\n",
              " ('run', 16),\n",
              " ('mteb', 16),\n",
              " ('utilize', 16),\n",
              " ('transformerbased', 16),\n",
              " ('previously', 16),\n",
              " ('energy', 16),\n",
              " ('cell', 16),\n",
              " ('conversational', 16),\n",
              " ('containing', 16),\n",
              " ('attempt', 16),\n",
              " ('series', 16),\n",
              " ('stronger', 16),\n",
              " ('classifier', 16),\n",
              " ('turn', 16),\n",
              " ('formulation', 16),\n",
              " ('trend', 16),\n",
              " ('depends', 16),\n",
              " ('reranker', 16),\n",
              " ('bring', 16),\n",
              " ('graphbased', 16),\n",
              " ('em', 16),\n",
              " ('heuristic', 16),\n",
              " ('setup', 16),\n",
              " ('implementation', 16),\n",
              " ('discussion', 16),\n",
              " ('firm', 16),\n",
              " ('hidden', 16),\n",
              " ('operation', 16),\n",
              " ('explanatory', 16),\n",
              " ('quantum', 16),\n",
              " ('liquid', 16),\n",
              " ('visual', 15),\n",
              " ('informationseeking', 15),\n",
              " ('ii', 15),\n",
              " ('add', 15),\n",
              " ('attribute', 15),\n",
              " ('distance', 15),\n",
              " ('statement', 15),\n",
              " ('efficacy', 15),\n",
              " ('exact', 15),\n",
              " ('least', 15),\n",
              " ('selfcorrection', 15),\n",
              " ('humanannotated', 15),\n",
              " ('six', 15),\n",
              " ('decision', 15),\n",
              " ('computation', 15),\n",
              " ('generally', 15),\n",
              " ('avenue', 15),\n",
              " ('constructing', 15),\n",
              " ('×', 15),\n",
              " ('appear', 15),\n",
              " ('successfully', 15),\n",
              " ('predicting', 15),\n",
              " ('phase', 15),\n",
              " ('expansion', 15),\n",
              " ('informative', 15),\n",
              " ('auxiliary', 15),\n",
              " ('major', 15),\n",
              " ('cause', 15),\n",
              " ('submission', 15),\n",
              " ('update', 15),\n",
              " ('indicated', 15),\n",
              " ('selecting', 15),\n",
              " ('entailment', 15),\n",
              " ('massive', 15),\n",
              " ('adaptive', 15),\n",
              " ('alleviate', 15),\n",
              " ('though', 15),\n",
              " ('play', 15),\n",
              " ('balance', 15),\n",
              " ('manually', 15),\n",
              " ('successful', 15),\n",
              " ('bootstrap', 15),\n",
              " ('publicly', 15),\n",
              " ('operator', 15),\n",
              " ('reference', 15),\n",
              " ('crosslingual', 15),\n",
              " ('financial', 15),\n",
              " ('increased', 15),\n",
              " ('attractive', 15),\n",
              " ('exchange', 15),\n",
              " ('σ600', 15),\n",
              " ('scalar', 15),\n",
              " ...]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#count\n",
        "freq_dist = nltk.FreqDist(clean_token)\n",
        "freq_dist.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paper-title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "strings=''\n",
        "for i in range(len(Insight_dataset)):\n",
        "    a=Insight_dataset['paper-title'][i]\n",
        "    strings=strings+' '+a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_token=Tokenizer(strings)\n",
        "#wordcloud\n",
        "long_string = ','.join(clean_token)\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('question', 367),\n",
              " ('answering', 260),\n",
              " ('model', 195),\n",
              " ('reasoning', 164),\n",
              " ('retrieval', 120),\n",
              " ('multihop', 108),\n",
              " ('comprehension', 101),\n",
              " ('reading', 99),\n",
              " ('learning', 73),\n",
              " ('generation', 71),\n",
              " ('knowledge', 70),\n",
              " ('dataset', 65),\n",
              " ('large', 58),\n",
              " ('graph', 55),\n",
              " ('via', 54),\n",
              " ('machine', 52),\n",
              " ('qa', 48),\n",
              " ('data', 47),\n",
              " ('text', 47),\n",
              " ('information', 42),\n",
              " ('benchmark', 42),\n",
              " ('understanding', 39),\n",
              " ('dense', 39),\n",
              " ('network', 38),\n",
              " ('domain', 37),\n",
              " ('complex', 36),\n",
              " ('using', 36),\n",
              " ('pretraining', 33),\n",
              " ('opendomain', 32),\n",
              " ('neural', 31),\n",
              " ('transformer', 31),\n",
              " ('answer', 31),\n",
              " ('system', 30),\n",
              " ('document', 30),\n",
              " ('approach', 29),\n",
              " ('efficient', 29),\n",
              " ('pretrained', 27),\n",
              " ('improving', 26),\n",
              " ('fewshot', 26),\n",
              " ('representation', 25),\n",
              " ('towards', 24),\n",
              " ('long', 24),\n",
              " ('zeroshot', 24),\n",
              " ('adaptation', 23),\n",
              " ('evaluating', 23),\n",
              " ('explanation', 23),\n",
              " ('based', 23),\n",
              " ('generative', 23),\n",
              " ('query', 23),\n",
              " ('multitask', 21),\n",
              " ('semantic', 21),\n",
              " ('unsupervised', 20),\n",
              " ('contrastive', 20),\n",
              " ('context', 20),\n",
              " ('agent', 20),\n",
              " ('logical', 20),\n",
              " ('explainable', 20),\n",
              " ('dynamic', 19),\n",
              " ('study', 19),\n",
              " ('modeling', 19),\n",
              " ('inference', 19),\n",
              " ('generalization', 18),\n",
              " ('decomposition', 18),\n",
              " ('prediction', 18),\n",
              " ('sentence', 18),\n",
              " ('exploring', 17),\n",
              " ('generating', 17),\n",
              " ('adversarial', 17),\n",
              " ('analysis', 17),\n",
              " ('prompting', 17),\n",
              " ('augmentation', 17),\n",
              " ('prompt', 17),\n",
              " ('passage', 16),\n",
              " ('interpretable', 16),\n",
              " ('framework', 16),\n",
              " ('entity', 16),\n",
              " ('unified', 16),\n",
              " ('open', 15),\n",
              " ('training', 15),\n",
              " ('chain', 15),\n",
              " ('search', 15),\n",
              " ('textual', 14),\n",
              " ('simple', 14),\n",
              " ('fact', 14),\n",
              " ('iterative', 13),\n",
              " ('new', 13),\n",
              " ('memory', 13),\n",
              " ('evidence', 13),\n",
              " ('robustness', 13),\n",
              " ('multilingual', 13),\n",
              " ('automatic', 12),\n",
              " ('retriever', 12),\n",
              " ('extraction', 12),\n",
              " ('verification', 12),\n",
              " ('datasets', 12),\n",
              " ('robust', 12),\n",
              " ('instruction', 12),\n",
              " ('sparse', 11),\n",
              " ('deep', 11),\n",
              " ('hybrid', 11),\n",
              " ('dialogue', 11),\n",
              " ('challenge', 11),\n",
              " ('transfer', 11),\n",
              " ('largescale', 11),\n",
              " ('hierarchical', 11),\n",
              " ('ai', 11),\n",
              " ('llm', 11),\n",
              " ('distillation', 10),\n",
              " ('korean', 10),\n",
              " ('improve', 10),\n",
              " ('multiple', 10),\n",
              " ('structured', 10),\n",
              " ('feedback', 10),\n",
              " ('relation', 10),\n",
              " ('tuning', 10),\n",
              " ('soft', 10),\n",
              " ('enhancing', 10),\n",
              " ('sequence', 10),\n",
              " ('compositional', 9),\n",
              " ('klue', 9),\n",
              " ('seungwon', 9),\n",
              " ('base', 9),\n",
              " ('embeddings', 9),\n",
              " ('hallucination', 9),\n",
              " ('selfsupervised', 9),\n",
              " ('multimodal', 9),\n",
              " ('summarization', 9),\n",
              " ('ability', 9),\n",
              " ('multidocument', 9),\n",
              " ('improves', 9),\n",
              " ('better', 9),\n",
              " ('adaptive', 9),\n",
              " ('effective', 8),\n",
              " ('skill', 8),\n",
              " ('interaction', 8),\n",
              " ('masked', 8),\n",
              " ('event', 8),\n",
              " ('span', 8),\n",
              " ('selection', 8),\n",
              " ('comprehensive', 8),\n",
              " ('numerical', 8),\n",
              " ('claim', 8),\n",
              " ('chinese', 8),\n",
              " ('reasoner', 8),\n",
              " ('human', 8),\n",
              " ('questionanswering', 8),\n",
              " ('benchmarking', 8),\n",
              " ('conversational', 8),\n",
              " ('knowledgeintensive', 8),\n",
              " ('alignment', 8),\n",
              " ('performance', 8),\n",
              " ('finegrained', 8),\n",
              " ('thought', 8),\n",
              " ('extractive', 8),\n",
              " ('know', 7),\n",
              " ('detection', 7),\n",
              " ('across', 7),\n",
              " ('attention', 7),\n",
              " ('retrievalaugmented', 7),\n",
              " ('conditional', 7),\n",
              " ('tool', 7),\n",
              " ('ask', 7),\n",
              " ('visual', 7),\n",
              " ('planning', 7),\n",
              " ('problem', 7),\n",
              " ('quality', 7),\n",
              " ('review', 7),\n",
              " ('implicit', 7),\n",
              " ('multistep', 7),\n",
              " ('expert', 7),\n",
              " ('logic', 7),\n",
              " ('ranking', 7),\n",
              " ('image', 6),\n",
              " ('need', 6),\n",
              " ('effect', 6),\n",
              " ('science', 6),\n",
              " ('identification', 6),\n",
              " ('relevance', 6),\n",
              " ('case', 6),\n",
              " ('teaching', 6),\n",
              " ('supervised', 6),\n",
              " ('informationseeking', 6),\n",
              " ('tabular', 6),\n",
              " ('novel', 6),\n",
              " ('unanswerable', 6),\n",
              " ('faithfulness', 6),\n",
              " ('reader', 6),\n",
              " ('bias', 6),\n",
              " ('capability', 6),\n",
              " ('diverse', 6),\n",
              " ('input', 6),\n",
              " ('modular', 6),\n",
              " ('behavior', 6),\n",
              " ('time', 6),\n",
              " ('help', 6),\n",
              " ('e', 6),\n",
              " ('annotation', 6),\n",
              " ('matching', 6),\n",
              " ('named', 6),\n",
              " ('process', 6),\n",
              " ('synthetic', 6),\n",
              " ('programming', 6),\n",
              " ('set', 6),\n",
              " ('single', 6),\n",
              " ('bert', 6),\n",
              " ('learn', 6),\n",
              " ('paragraph', 6),\n",
              " ('rationale', 6),\n",
              " ('causal', 6),\n",
              " ('structure', 6),\n",
              " ('outofdistribution', 6),\n",
              " ('composition', 6),\n",
              " ('token', 6),\n",
              " ('pair', 6),\n",
              " ('retrievalbased', 6),\n",
              " ('hard', 5),\n",
              " ('user', 5),\n",
              " ('finetuning', 5),\n",
              " ('management', 5),\n",
              " ('heterogeneous', 5),\n",
              " ('crosstask', 5),\n",
              " ('knowledgebased', 5),\n",
              " ('covid19', 5),\n",
              " ('biomedical', 5),\n",
              " ('random', 5),\n",
              " ('building', 5),\n",
              " ('mixture', 5),\n",
              " ('global', 5),\n",
              " ('structural', 5),\n",
              " ('r', 5),\n",
              " ('reinforcement', 5),\n",
              " ('investigating', 5),\n",
              " ('solving', 5),\n",
              " ('massive', 5),\n",
              " ('dont', 5),\n",
              " ('key', 5),\n",
              " ('test', 5),\n",
              " ('classification', 5),\n",
              " ('v', 5),\n",
              " ('proof', 5),\n",
              " ('reranking', 5),\n",
              " ('protein', 5),\n",
              " ('multiplechoice', 5),\n",
              " ('content', 5),\n",
              " ('phrase', 5),\n",
              " ('augmented', 5),\n",
              " ('attack', 5),\n",
              " ('resource', 5),\n",
              " ('lexical', 5),\n",
              " ('encoding', 5),\n",
              " ('web', 5),\n",
              " ('crosslingual', 5),\n",
              " ('state', 5),\n",
              " ('contextual', 5),\n",
              " ('big', 5),\n",
              " ('perspective', 5),\n",
              " ('database', 5),\n",
              " ('autonomous', 5),\n",
              " ('gap', 5),\n",
              " ('bridging', 5),\n",
              " ('commonsense', 5),\n",
              " ('demonstration', 5),\n",
              " ('predicting', 5),\n",
              " ('code', 5),\n",
              " ('mitigating', 5),\n",
              " ('controllable', 5),\n",
              " ('embedding', 5),\n",
              " ('ensemble', 5),\n",
              " ('deduction', 5),\n",
              " ('design', 4),\n",
              " ('yet', 4),\n",
              " ('contextualized', 4),\n",
              " ('frustratingly', 4),\n",
              " ('temporal', 4),\n",
              " ('general', 4),\n",
              " ('one', 4),\n",
              " ('continual', 4),\n",
              " ('pipeline', 4),\n",
              " ('automatically', 4),\n",
              " ('conversation', 4),\n",
              " ('unstructured', 4),\n",
              " ('identifying', 4),\n",
              " ('counterfactual', 4),\n",
              " ('news', 4),\n",
              " ('combining', 4),\n",
              " ('le', 4),\n",
              " ('published', 4),\n",
              " ('iclr', 4),\n",
              " ('path', 4),\n",
              " ('crossdocument', 4),\n",
              " ('attribution', 4),\n",
              " ('strategy', 4),\n",
              " ('parameterefficient', 4),\n",
              " ('retrieving', 4),\n",
              " ('reference', 4),\n",
              " ('entailment', 4),\n",
              " ('interactive', 4),\n",
              " ('intermediate', 4),\n",
              " ('break', 4),\n",
              " ('french', 4),\n",
              " ('probing', 4),\n",
              " ('linguistic', 4),\n",
              " ('chainofthought', 4),\n",
              " ('rethinking', 4),\n",
              " ('modelgenerated', 4),\n",
              " ('external', 4),\n",
              " ('much', 4),\n",
              " ('train', 4),\n",
              " ('advance', 4),\n",
              " ('reason', 4),\n",
              " ('asking', 4),\n",
              " ('software', 4),\n",
              " ('metric', 4),\n",
              " ('analyzing', 4),\n",
              " ('toolkit', 4),\n",
              " ('solution', 4),\n",
              " ('routing', 4),\n",
              " ('innovation', 4),\n",
              " ('growth', 4),\n",
              " ('detecting', 4),\n",
              " ('multivector', 4),\n",
              " ('explain', 4),\n",
              " ('right', 4),\n",
              " ('guided', 4),\n",
              " ('deductive', 4),\n",
              " ('joint', 4),\n",
              " ('differentiable', 4),\n",
              " ('choice', 4),\n",
              " ('promptbased', 4),\n",
              " ('spoken', 4),\n",
              " ('development', 4),\n",
              " ('multilevel', 4),\n",
              " ('scale', 4),\n",
              " ('module', 4),\n",
              " ('dual', 4),\n",
              " ('discrete', 4),\n",
              " ('tracking', 4),\n",
              " ('questionanswer', 4),\n",
              " ('following', 4),\n",
              " ('rewriting', 4),\n",
              " ('environment', 4),\n",
              " ('parameter', 4),\n",
              " ('understand', 4),\n",
              " ('firstorder', 4),\n",
              " ('response', 4),\n",
              " ('generalized', 4),\n",
              " ('decoupling', 4),\n",
              " ('reflexion', 4),\n",
              " ('shared', 4),\n",
              " ('article', 4),\n",
              " ('word', 4),\n",
              " ('casebased', 4),\n",
              " ('label', 4),\n",
              " ('objective', 4),\n",
              " ('auxiliary', 4),\n",
              " ('interpretation', 4),\n",
              " ('space', 4),\n",
              " ('type', 4),\n",
              " ('relationentitycentric', 4),\n",
              " ('selfrefinement', 3),\n",
              " ('sampling', 3),\n",
              " ('healthcare', 3),\n",
              " ('weakly', 3),\n",
              " ('tasklevel', 3),\n",
              " ('molecular', 3),\n",
              " ('access', 3),\n",
              " ('gpt3', 3),\n",
              " ('corpus', 3),\n",
              " ('source', 3),\n",
              " ('autoencoder', 3),\n",
              " ('limit', 3),\n",
              " ('selfattention', 3),\n",
              " ('step', 3),\n",
              " ('calibrating', 3),\n",
              " ('measuring', 3),\n",
              " ('math', 3),\n",
              " ('de', 3),\n",
              " ('easy', 3),\n",
              " ('position', 3),\n",
              " ('highlevel', 3),\n",
              " ('llmbased', 3),\n",
              " ('stepbystep', 3),\n",
              " ('chatgpts', 3),\n",
              " ('calibration', 3),\n",
              " ('connecting', 3),\n",
              " ('realistic', 3),\n",
              " ('expression', 3),\n",
              " ('property', 3),\n",
              " ('tree', 3),\n",
              " ('ukpsquare', 3),\n",
              " ('autoregressive', 3),\n",
              " ('chatgpt', 3),\n",
              " ('benefit', 3),\n",
              " ('local', 3),\n",
              " ('explaining', 3),\n",
              " ('revisiting', 3),\n",
              " ('parallel', 3),\n",
              " ('window', 3),\n",
              " ('medical', 3),\n",
              " ('narrative', 3),\n",
              " ('consistency', 3),\n",
              " ('worth', 3),\n",
              " ('recent', 3),\n",
              " ('point', 3),\n",
              " ('exploiting', 3),\n",
              " ('testing', 3),\n",
              " ('systematic', 3),\n",
              " ('perturbation', 3),\n",
              " ('distant', 3),\n",
              " ('knowledgeaugmented', 3),\n",
              " ('technology', 3),\n",
              " ('technological', 3),\n",
              " ('firm', 3),\n",
              " ('rerankers', 3),\n",
              " ('retrospective', 3),\n",
              " ('decomposing', 3),\n",
              " ('first', 3),\n",
              " ('translation', 3),\n",
              " ('multi', 3),\n",
              " ('empowering', 3),\n",
              " ('fusion', 3),\n",
              " ('situation', 3),\n",
              " ('diagnostic', 3),\n",
              " ('momentum', 3),\n",
              " ('invariant', 3),\n",
              " ('cognitive', 3),\n",
              " ('architecture', 3),\n",
              " ('accurate', 3),\n",
              " ('world', 3),\n",
              " ('program', 3),\n",
              " ('augmenting', 3),\n",
              " ('landscape', 3),\n",
              " ('shortcut', 3),\n",
              " ('vision', 3),\n",
              " ('scientific', 3),\n",
              " ('enhanced', 3),\n",
              " ('aware', 3),\n",
              " ('imitate', 3),\n",
              " ('supervision', 3),\n",
              " ('select', 3),\n",
              " ('scalable', 3),\n",
              " ('regeneration', 3),\n",
              " ('role', 3),\n",
              " ('beyond', 3),\n",
              " ('convolutional', 3),\n",
              " ('longform', 3),\n",
              " ('bilingual', 3),\n",
              " ('encoder', 3),\n",
              " ('discourseaware', 3),\n",
              " ('transferability', 3),\n",
              " ('frozen', 3),\n",
              " ('beijing', 3),\n",
              " ('laboratory', 3),\n",
              " ('efficiently', 3),\n",
              " ('longer', 3),\n",
              " ('n', 3),\n",
              " ('form', 3),\n",
              " ('preprint', 3),\n",
              " ('generate', 3),\n",
              " ('faithful', 3),\n",
              " ('semiparametric', 3),\n",
              " ('selftraining', 3),\n",
              " ('meet', 3),\n",
              " ('procedural', 3),\n",
              " ('fast', 3),\n",
              " ('collaborative', 3),\n",
              " ('folio', 3),\n",
              " ('decompose', 3),\n",
              " ('focused', 3),\n",
              " ('computational', 3),\n",
              " ('dna', 3),\n",
              " ('learner', 3),\n",
              " ('taskaware', 3),\n",
              " ('meta', 3),\n",
              " ('truthful', 3),\n",
              " ('incontext', 3),\n",
              " ('longformer', 3),\n",
              " ('observation', 3),\n",
              " ('labeling', 3),\n",
              " ('examining', 3),\n",
              " ('textgraphs', 3),\n",
              " ('improved', 3),\n",
              " ('interpretability', 3),\n",
              " ('rule', 3),\n",
              " ('name', 3),\n",
              " ('automated', 3),\n",
              " ('assessment', 3),\n",
              " ('specialization', 3),\n",
              " ('lowresource', 3),\n",
              " ('expansion', 3),\n",
              " ('formal', 3),\n",
              " ('analogical', 3),\n",
              " ('leakage', 3),\n",
              " ('association', 3),\n",
              " ('public', 3),\n",
              " ('private', 3),\n",
              " ('generalizable', 3),\n",
              " ('forest', 3),\n",
              " ('incomplete', 3),\n",
              " ('f1', 3),\n",
              " ('enough', 3),\n",
              " ('usercentered', 3),\n",
              " ('idea2img', 2),\n",
              " ('gpt4vision', 2),\n",
              " ('negative', 2),\n",
              " ('making', 2),\n",
              " ('ir', 2),\n",
              " ('else', 2),\n",
              " ('background', 2),\n",
              " ('reliance', 2),\n",
              " ('perform', 2),\n",
              " ('carcinoma', 2),\n",
              " ('selfcorrect', 2),\n",
              " ('intelligent', 2),\n",
              " ('ernie', 2),\n",
              " ('tiny', 2),\n",
              " ('taskagnostic', 2),\n",
              " ('reward', 2),\n",
              " ('well', 2),\n",
              " ('broad', 2),\n",
              " ('embedder', 2),\n",
              " ('instructionfinetuned', 2),\n",
              " ('collapse', 2),\n",
              " ('region', 2),\n",
              " ('grounded', 2),\n",
              " ('modelbased', 2),\n",
              " ('artificial', 2),\n",
              " ('intelligence', 2),\n",
              " ('creating', 2),\n",
              " ('eliciting', 2),\n",
              " ('mixtureofexperts', 2),\n",
              " ('receptor', 2),\n",
              " ('correctness', 2),\n",
              " ('instructionfollowing', 2),\n",
              " ('reliable', 2),\n",
              " ('retrofitting', 2),\n",
              " ('crossdomain', 2),\n",
              " ('exploration', 2),\n",
              " ('v2', 2),\n",
              " ('reducing', 2),\n",
              " ('disconnected', 2),\n",
              " ('capacity', 2),\n",
              " ('testtime', 2),\n",
              " ('modelling', 2),\n",
              " ('trust', 2),\n",
              " ('decompositional', 2),\n",
              " ('probe', 2),\n",
              " ('robustly', 2),\n",
              " ('optimized', 2),\n",
              " ('finqa', 2),\n",
              " ('team', 2),\n",
              " ('direction', 2),\n",
              " ('improvement', 2),\n",
              " ('technique', 2),\n",
              " ('domainagnostic', 2),\n",
              " ('knowing', 2),\n",
              " ('counterfactuals', 2),\n",
              " ('selector', 2),\n",
              " ('c', 2),\n",
              " ('retrievalgeneration', 2),\n",
              " ('synergy', 2),\n",
              " ('querybased', 2),\n",
              " ('collection', 2),\n",
              " ('randomized', 2),\n",
              " ('doubt', 2),\n",
              " ('answerable', 2),\n",
              " ('creative', 2),\n",
              " ('common', 2),\n",
              " ('pretrain', 2),\n",
              " ('subspace', 2),\n",
              " ('synthesis', 2),\n",
              " ('fquad20', 2),\n",
              " ('overfit', 2),\n",
              " ('underfit', 2),\n",
              " ('fall', 2),\n",
              " ('short', 2),\n",
              " ('viquae', 2),\n",
              " ('thorough', 2),\n",
              " ('examination', 2),\n",
              " ('power', 2),\n",
              " ('selecting', 2),\n",
              " ('timesensitive', 2),\n",
              " ('edge', 2),\n",
              " ('deterioration', 2),\n",
              " ('tacr', 2),\n",
              " ('tablealignmentbased', 2),\n",
              " ('cellselection', 2),\n",
              " ('answerability', 2),\n",
              " ('federated', 2),\n",
              " ('accuracy', 2),\n",
              " ('late', 2),\n",
              " ('deeplearning', 2),\n",
              " ('involved', 2),\n",
              " ('p', 2),\n",
              " ('diversity', 2),\n",
              " ('cairecovid', 2),\n",
              " ('physic', 2),\n",
              " ('recursive', 2),\n",
              " ('probabilistic', 2),\n",
              " ('allinone', 2),\n",
              " ('literature', 2),\n",
              " ('discovering', 2),\n",
              " ('listening', 2),\n",
              " ('reproducible', 2),\n",
              " ('optimal', 2),\n",
              " ('service', 2),\n",
              " ('compression', 2),\n",
              " ('selective', 2),\n",
              " ('unit', 2),\n",
              " ('enables', 2),\n",
              " ('without', 2),\n",
              " ('adapter', 2),\n",
              " ('variable', 2),\n",
              " ('plugandplay', 2),\n",
              " ('memoryefficient', 2),\n",
              " ('discourse', 2),\n",
              " ('detrimental', 2),\n",
              " ('outofdomain', 2),\n",
              " ('explainability', 2),\n",
              " ('mastering', 2),\n",
              " ('abcds', 2),\n",
              " ('answerbased', 2),\n",
              " ('selfevaluation', 2),\n",
              " ('make', 2),\n",
              " ('quran', 2),\n",
              " ('robustlr', 2),\n",
              " ('logicnmr', 2),\n",
              " ('nonmonotonic', 2),\n",
              " ('coreference', 2),\n",
              " ('coffee', 2),\n",
              " ('consumed', 2),\n",
              " ('emnlp', 2),\n",
              " ('fermi', 2),\n",
              " ('singlehop', 2),\n",
              " ('vectorquantized', 2),\n",
              " ('inputcontextualized', 2),\n",
              " ('searchinthechain', 2),\n",
              " ('credible', 2),\n",
              " ('traceable', 2),\n",
              " ('virtual', 2),\n",
              " ('video', 2),\n",
              " ('infused', 2),\n",
              " ('tableandtext', 2),\n",
              " ('incorporating', 2),\n",
              " ('recognition', 2),\n",
              " ('written', 2),\n",
              " ('way', 2),\n",
              " ('avoiding', 2),\n",
              " ('parsing', 2),\n",
              " ('demystifying', 2),\n",
              " ('unifiedio', 2),\n",
              " ('aspectbased', 2),\n",
              " ('recommenders', 2),\n",
              " ('acm', 2),\n",
              " ('format', 2),\n",
              " ('policy', 2),\n",
              " ('socratic', 2),\n",
              " ('may', 2),\n",
              " ('self', 2),\n",
              " ('profiling', 2),\n",
              " ('posttraining', 2),\n",
              " ('prerequisite', 2),\n",
              " ('standard', 2),\n",
              " ('paradigm', 2),\n",
              " ('link', 2),\n",
              " ('advanced', 2),\n",
              " ('length', 2),\n",
              " ('documentlevel', 2),\n",
              " ('firststage', 2),\n",
              " ('engineering', 2),\n",
              " ('template', 2),\n",
              " ('plan', 2),\n",
              " ('informative', 2),\n",
              " ('cloze', 2),\n",
              " ('minimal', 2),\n",
              " ('legal', 2),\n",
              " ('valid', 2),\n",
              " ('toward', 2),\n",
              " ('monitoring', 2),\n",
              " ('getting', 2),\n",
              " ('bird', 2),\n",
              " ('etc', 2),\n",
              " ('complexity', 2),\n",
              " ('accelerating', 2),\n",
              " ('realtime', 2),\n",
              " ('hotpotqa', 2),\n",
              " ('pic', 2),\n",
              " ('phraseincontext', 2),\n",
              " ('sourcefree', 2),\n",
              " ('factchecking', 2),\n",
              " ('strong', 2),\n",
              " ('adapting', 2),\n",
              " ('value', 2),\n",
              " ('asqa', 2),\n",
              " ('factoid', 2),\n",
              " ('requiring', 2),\n",
              " ('go', 2),\n",
              " ('flow', 2),\n",
              " ('net', 2),\n",
              " ('multiperspective', 2),\n",
              " ('nonparametric', 2),\n",
              " ('decoding', 2),\n",
              " ('critic', 2),\n",
              " ('inferring', 2),\n",
              " ('hypothetical', 2),\n",
              " ('comparable', 2),\n",
              " ('extrapolation', 2),\n",
              " ('abstractive', 2),\n",
              " ('cc', 2),\n",
              " ('linguistics', 2),\n",
              " ('checking', 2),\n",
              " ('whats', 2),\n",
              " ('bridge', 2),\n",
              " ('stochastic', 2),\n",
              " ('integrated', 2),\n",
              " ('suite', 2),\n",
              " ('analysing', 2),\n",
              " ('unpredictable', 2),\n",
              " ('autoencoders', 2),\n",
              " ('mr', 2),\n",
              " ('construction', 2),\n",
              " ('combating', 2),\n",
              " ('substitute', 2),\n",
              " ('queryascontext', 2),\n",
              " ('fusionindecoder', 2),\n",
              " ('commonsenseqa', 2),\n",
              " ('leveraging', 2),\n",
              " ('revealing', 2),\n",
              " ('spatial', 2),\n",
              " ('trustworthy', 2),\n",
              " ('principle', 2),\n",
              " ('practice', 2),\n",
              " ('instance', 2),\n",
              " ('rewoo', 2),\n",
              " ('longcontext', 2),\n",
              " ('verbal', 2),\n",
              " ('recommendation', 2),\n",
              " ('arise', 2),\n",
              " ('working', 2),\n",
              " ('business', 2),\n",
              " ('dragon', 2),\n",
              " ('empirical', 2),\n",
              " ('generalisation', 2),\n",
              " ('apollo', 2),\n",
              " ('openended', 2),\n",
              " ('grammar', 2),\n",
              " ('concept', 2),\n",
              " ('intervention', 2),\n",
              " ('sequential', 2),\n",
              " ('social', 2),\n",
              " ('medium', 2),\n",
              " ('optimization', 2),\n",
              " ('beam', 2),\n",
              " ('endtoend', 2),\n",
              " ('equation', 2),\n",
              " ('scholarly', 2),\n",
              " ('longdocument', 2),\n",
              " ('g', 2),\n",
              " ('error', 2),\n",
              " ('fix', 2),\n",
              " ('humanlike', 2),\n",
              " ('shift', 2),\n",
              " ('factdriven', 2),\n",
              " ('filling', 2),\n",
              " ('abductive', 2),\n",
              " ('taxonomy', 2),\n",
              " ('unreliability', 2),\n",
              " ('stable', 2),\n",
              " ('underlying', 2),\n",
              " ('take', 2),\n",
              " ('yesno', 2),\n",
              " ('beir', 2),\n",
              " ('stitch', 2),\n",
              " ('save', 2),\n",
              " ('nine', 2),\n",
              " ('validating', 2),\n",
              " ('lowconfidence', 2),\n",
              " ('platform', 2),\n",
              " ('discriminative', 2),\n",
              " ('aggregation', 2),\n",
              " ('crowdsourcing', 2),\n",
              " ('explicit', 2),\n",
              " ('rating', 2),\n",
              " ('nl', 2),\n",
              " ('rather', 2),\n",
              " ('difficulty', 2),\n",
              " ('start', 2),\n",
              " ('quest', 2),\n",
              " ('entityseeking', 2),\n",
              " ('operation', 2),\n",
              " ('correction', 2),\n",
              " ('generator', 2),\n",
              " ('statement', 2),\n",
              " ('selfreflection', 2),\n",
              " ('financial', 2),\n",
              " ('generated', 2),\n",
              " ('contextaware', 2),\n",
              " ('connection', 2),\n",
              " ('qualitative', 2),\n",
              " ('assessing', 2),\n",
              " ('rna', 2),\n",
              " ('contradiction', 2),\n",
              " ('retrieve', 2),\n",
              " ('hop', 2),\n",
              " ('wikipedia', 2),\n",
              " ('diagnosing', 2),\n",
              " ('exam', 2),\n",
              " ('distributional', 2),\n",
              " ('reranker', 2),\n",
              " ('multihiertt', 2),\n",
              " ('scaling', 2),\n",
              " ('important', 2),\n",
              " ('supporting', 2),\n",
              " ('boosting', 2),\n",
              " ('metalearning', 2),\n",
              " ('hidden', 2),\n",
              " ('graphbased', 2),\n",
              " ('targeting', 2),\n",
              " ('according', 2),\n",
              " ('quoting', 2),\n",
              " ('tbx22', 2),\n",
              " ('halueval', 2),\n",
              " ('outcomebased', 2),\n",
              " ('linear', 2),\n",
              " ('mirnas', 2),\n",
              " ('chessboard', 2),\n",
              " ('mteb', 2),\n",
              " ('longtext', 2),\n",
              " ('shorttext', 2),\n",
              " ('circuit', 2),\n",
              " ('unseen', 2),\n",
              " ('smaller', 2),\n",
              " ('talk2data', 2),\n",
              " ('dataoriented', 2),\n",
              " ('albert', 2),\n",
              " ('estimation', 2),\n",
              " ('knowledgpt', 2),\n",
              " ('storage', 2),\n",
              " ('continuous', 2),\n",
              " ('spanextraction', 2),\n",
              " ('distributionally', 2),\n",
              " ('citadel', 2),\n",
              " ('semistructured', 2),\n",
              " ('spot', 2),\n",
              " ('cell', 2),\n",
              " ('knowledgeenhanced', 2),\n",
              " ('encoders', 2),\n",
              " ('proteome', 1),\n",
              " ('nap2', 1),\n",
              " ('biomarker', 1),\n",
              " ('hepatitis', 1),\n",
              " ('brelated', 1),\n",
              " ('hepatocellular', 1),\n",
              " ('proteomic', 1),\n",
              " ('generalizing', 1),\n",
              " ('nanosensors', 1),\n",
              " ('knowledgeaware', 1),\n",
              " ('encyclopedic', 1),\n",
              " ('deontological', 1),\n",
              " ('ethic', 1),\n",
              " ('questionspecific', 1),\n",
              " ('nli', 1),\n",
              " ('kilt', 1),\n",
              " ('intensive', 1),\n",
              " ('openqa', 1),\n",
              " ('relying', 1),\n",
              " ('nonstructured', 1),\n",
              " ('multistage', 1),\n",
              " ('decompositionguided', 1),\n",
              " ('ii', 1),\n",
              " ('λ4686', 1),\n",
              " ('η', 1),\n",
              " ('carina', 1),\n",
              " ('windwind', 1),\n",
              " ('collision', 1),\n",
              " ('periastron', 1),\n",
              " ('uncertaintyaware', 1),\n",
              " ('fixed', 1),\n",
              " ('hybridialogue', 1),\n",
              " ('indepth', 1),\n",
              " ('snake', 1),\n",
              " ('venom', 1),\n",
              " ('rhodocytin', 1),\n",
              " ('platelet', 1),\n",
              " ('clec2', 1),\n",
              " ('range', 1),\n",
              " ('arena', 1),\n",
              " ('clozing', 1),\n",
              " ('comprehending', 1),\n",
              " ('crush4sql', 1),\n",
              " ('collective', 1),\n",
              " ('schema', 1),\n",
              " ('text2sql', 1),\n",
              " ('poisoning', 1),\n",
              " ('injecting', 1),\n",
              " ('logiqa', 1),\n",
              " ('multireqa', 1),\n",
              " ('concurrent', 1),\n",
              " ('documentcentered', 1),\n",
              " ('assistance', 1),\n",
              " ('recommender', 1),\n",
              " ('dialog', 1),\n",
              " ('aligning', 1),\n",
              " ('multigrained', 1),\n",
              " ('retromae', 1),\n",
              " ('duplex', 1),\n",
              " ('retrievaloriented', 1),\n",
              " ('forecastqa', 1),\n",
              " ('forecasting', 1),\n",
              " ('readtwice', 1),\n",
              " ('covidread', 1),\n",
              " ('causeeffect', 1),\n",
              " ('bamboo', 1),\n",
              " ('blockwise', 1),\n",
              " ('manymodalqa', 1),\n",
              " ('modality', 1),\n",
              " ('disambiguation', 1),\n",
              " ('constructing', 1),\n",
              " ('mumuqa', 1),\n",
              " ('multimedia', 1),\n",
              " ('crossmedia', 1),\n",
              " ('grounding', 1),\n",
              " ('coherence', 1),\n",
              " ('dire', 1),\n",
              " ('condition', 1),\n",
              " ('beirpl', 1),\n",
              " ('zero', 1),\n",
              " ('shot', 1),\n",
              " ('polish', 1),\n",
              " ('papelo', 1),\n",
              " ('feverous', 1),\n",
              " ('pursuit', 1),\n",
              " ('intégration', 1),\n",
              " ('du', 1),\n",
              " ('raisonnement', 1),\n",
              " ('numérique', 1),\n",
              " ('dans', 1),\n",
              " ('modèles', 1),\n",
              " ('langue', 1),\n",
              " ('état', 1),\n",
              " ('lart', 1),\n",
              " ('recherche', 1),\n",
              " ('videogrounded', 1),\n",
              " ('pubmedqa', 1),\n",
              " ('padding', 1),\n",
              " ('formally', 1),\n",
              " ('specifying', 1),\n",
              " ('changing', 1),\n",
              " ('overnight', 1),\n",
              " ('ing', 1),\n",
              " ('webqa', 1),\n",
              " ('graphfree', 1),\n",
              " ('selecttoguide', 1),\n",
              " ('attempt', 1),\n",
              " ('attentional', 1),\n",
              " ('capturing', 1),\n",
              " ('compressive', 1),\n",
              " ('answered', 1),\n",
              " ('filtering', 1),\n",
              " ('l', 1),\n",
              " ('literary', 1),\n",
              " ('hyperlink', 1),\n",
              " ('fully', 1),\n",
              " ('explored', 1),\n",
              " ('dualchannel', 1),\n",
              " ('hierarchy', 1),\n",
              " ('symbolic', 1),\n",
              " ('semanticsaware', 1),\n",
              " ('inferential', 1),\n",
              " ('hyperdecoders', 1),\n",
              " ('instancespecific', 1),\n",
              " ('decoder', 1),\n",
              " ('referring', 1),\n",
              " ('involve', 1),\n",
              " ('gradable', 1),\n",
              " ('rlet', 1),\n",
              " ('twostage', 1),\n",
              " ('beat', 1),\n",
              " ('hybridqa', 1),\n",
              " ('relevanceaware', 1),\n",
              " ('efficacy', 1),\n",
              " ('result', 1),\n",
              " ('mvp', 1),\n",
              " ('esta', 1),\n",
              " ('obra', 1),\n",
              " ('está', 1),\n",
              " ('bajo', 1),\n",
              " ('licencia', 1),\n",
              " ('internacional', 1),\n",
              " ('reconocimientonocomercialcompartirigual', 1),\n",
              " ('revista', 1),\n",
              " ('innova', 1),\n",
              " ('educación', 1),\n",
              " ('decomposed', 1),\n",
              " ('muppet', 1),\n",
              " ('prefinetuning', 1),\n",
              " ('lottery', 1),\n",
              " ('ticket', 1),\n",
              " ('fewshotqa', 1),\n",
              " ('texttotext', 1),\n",
              " ...]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#count\n",
        "freq_dist = nltk.FreqDist(clean_token)\n",
        "freq_dist.most_common()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gnn_citations",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
